
2019-09-14 01:25:43,843 - training_jobs - DEBUG - ('tasks/gnn_assertion_problem', '.yaml')
2019-09-14 01:25:46,843 - training_jobs - DEBUG - training trains/task_4.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 50,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-14 01:25:46,858 - training_jobs - DEBUG - training with: 
2019-09-14 01:25:46,858 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max/
2019-09-14 01:25:46,858 - training_jobs - DEBUG - GGNN1
2019-09-14 01:25:46,858 - training_jobs - DEBUG - 
2019-09-14 01:25:46,858 - training_jobs - DEBUG - ggnn training
2019-09-14 01:25:54,469 - training_jobs - DEBUG -  saving results to results/20190914_012554_ggnn_.json
2019-09-14 01:25:54,469 - training_jobs - DEBUG -  calling modelSelection
{'best_models': {'accuracy': {'accuracy': 0.0,
                              'batch_size': 32,
                              'cv_score': 0.0053345869543231185,
                              'cv_val_accuracy': 0.6666666666666666,
                              'cv_val_loss': 0.09920807182788849,
                              'cv_val_macroF1': 0.0053345869543231185,
                              'cv_val_microF1': 0.066966913828706,
                              'epochs': 200,
                              'final_model': GGNN1(
  (ggnn): GatedGraphConv(60, num_layers=2)
  (fc1): Linear(in_features=60, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                              'kwargs': {'aggr_type': 'mean',
                                         'd1': 60,
                                         'd2': 50,
                                         'num_classes': 24,
                                         'num_layers': 2},
                              'learning_rate': 0.01,
                              'macroF1': 0.006748704489763125,
                              'microF1': 0.08414023372287145,
                              'model': <class 'TFM_graph_classification_models.GGNN1'>,
                              'model_instance': GGNN1(
  (ggnn): GatedGraphConv(60, num_layers=2)
  (fc1): Linear(in_features=60, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                              'score': 'f1_macro',
                              'time': 2764.828370809555,
                              'train_loss_history': [362103776.0,
                                                     1984.16552734375,
                                                     4208493824.0,
                                                     660907968.0,
                                                     0.08377974480390549,
                                                     1243555968.0,
                                                     1017.9180297851562,
                                                     76571.46875,
                                                     308.40338134765625,
                                                     23220.69921875,
                                                     156603248.0,
                                                     138.022216796875,
                                                     0.08351406455039978,
                                                     1823248640.0,
                                                     155343744.0,
                                                     0.0847235918045044,
                                                     0.08452852815389633,
                                                     0.08426041901111603,
                                                     0.0840001031756401,
                                                     0.0840369313955307,
                                                     0.0836704894900322,
                                                     0.08393912762403488,
                                                     0.08424623310565948,
                                                     0.08425412327051163,
                                                     0.08409769088029861,
                                                     0.08399365842342377,
                                                     0.08431313186883926,
                                                     0.08411368727684021,
                                                     19.82664680480957,
                                                     0.08441364020109177,
                                                     0.08446607738733292,
                                                     0.08450149744749069,
                                                     0.08464515209197998,
                                                     0.08455231040716171,
                                                     0.08460909128189087,
                                                     0.08457177132368088,
                                                     0.0844896212220192,
                                                     0.08443144708871841,
                                                     7615.97705078125,
                                                     5.158772945404053,
                                                     0.08454208821058273,
                                                     0.08437103033065796,
                                                     0.08442801982164383,
                                                     0.08480384200811386,
                                                     0.08451271802186966,
                                                     0.0846419557929039,
                                                     0.08445870131254196,
                                                     0.08451291173696518,
                                                     7.508358001708984,
                                                     7220202.5,
                                                     208.47743225097656,
                                                     0.08452831208705902,
                                                     0.08444917947053909,
                                                     0.08455681055784225,
                                                     0.08457385748624802,
                                                     0.08448221534490585,
                                                     0.08441825956106186,
                                                     0.08439182490110397,
                                                     0.08427322655916214,
                                                     0.08453415334224701,
                                                     0.08448854833841324,
                                                     0.08462214469909668,
                                                     0.08452510088682175,
                                                     0.08454762399196625,
                                                     0.08453730493783951,
                                                     0.08453856408596039,
                                                     0.08457324653863907,
                                                     0.08439980447292328,
                                                     0.08457770943641663,
                                                     0.08448946475982666,
                                                     0.08455544710159302,
                                                     0.08448326587677002,
                                                     0.0853704884648323,
                                                     0.0845303013920784,
                                                     0.08449558913707733,
                                                     0.08448614180088043,
                                                     0.08432622253894806,
                                                     0.08439280092716217,
                                                     0.08428298681974411,
                                                     0.08441869169473648,
                                                     2.7614922523498535,
                                                     0.0844237357378006,
                                                     0.0845165103673935,
                                                     3.4590933322906494,
                                                     0.08449532091617584,
                                                     0.0845257043838501,
                                                     0.08447345346212387,
                                                     0.08450763672590256,
                                                     0.08446920663118362,
                                                     0.08451981097459793,
                                                     385.1795959472656,
                                                     0.08447851985692978,
                                                     0.08457182347774506,
                                                     0.08445488661527634,
                                                     0.7349956631660461,
                                                     0.0845390260219574,
                                                     437.2469787597656,
                                                     0.08416557312011719,
                                                     0.08421652764081955,
                                                     0.08440131694078445,
                                                     0.08433077484369278,
                                                     0.08428847789764404,
                                                     0.08411938697099686,
                                                     0.08432593941688538,
                                                     0.08455510437488556,
                                                     0.08454146236181259,
                                                     0.08451537787914276,
                                                     0.08447898924350739,
                                                     0.08454417437314987,
                                                     0.08451075106859207,
                                                     0.08446026593446732,
                                                     0.08439700305461884,
                                                     0.08444203436374664,
                                                     324.5517578125,
                                                     0.084525465965271,
                                                     0.08450353890657425,
                                                     0.08454328775405884,
                                                     0.08446735888719559,
                                                     0.0844758078455925,
                                                     0.08446711301803589,
                                                     0.08434131741523743,
                                                     0.0844181552529335,
                                                     0.08407874405384064,
                                                     0.08442263305187225,
                                                     0.08430887013673782,
                                                     0.08445868641138077,
                                                     0.08432488143444061,
                                                     0.08438342809677124,
                                                     0.08434324711561203,
                                                     0.08433690667152405,
                                                     0.08442634344100952,
                                                     0.08430664241313934,
                                                     0.08431776612997055,
                                                     3179.368408203125,
                                                     0.08431804180145264,
                                                     0.0844300165772438,
                                                     327.7389831542969,
                                                     0.08442189544439316,
                                                     0.08446188271045685,
                                                     0.08456380665302277,
                                                     0.08448801934719086,
                                                     0.08452309668064117,
                                                     0.08443786203861237,
                                                     0.08450938016176224,
                                                     0.08452746272087097,
                                                     0.0845022052526474,
                                                     0.08435150235891342,
                                                     0.0843622088432312,
                                                     0.08451417833566666,
                                                     0.08449152857065201,
                                                     2792218.0,
                                                     0.08448152989149094,
                                                     0.08447089046239853,
                                                     0.08448109775781631,
                                                     0.08455649018287659,
                                                     11.825798988342285,
                                                     0.0844212993979454,
                                                     0.08447621017694473,
                                                     0.08445154875516891,
                                                     0.08450216054916382,
                                                     0.08448535948991776,
                                                     0.08452288061380386,
                                                     2.2303872108459473,
                                                     0.08445139229297638,
                                                     22.94561195373535,
                                                     0.08438808470964432,
                                                     0.08447092026472092,
                                                     0.0844135656952858,
                                                     0.08435672521591187,
                                                     4.1318182945251465,
                                                     0.0844360813498497,
                                                     0.08448848873376846,
                                                     0.08441183716058731,
                                                     0.08452562987804413,
                                                     0.08434139937162399,
                                                     15509.2685546875,
                                                     0.08440298587083817,
                                                     0.0843038558959961,
                                                     0.08409386873245239,
                                                     0.08434053510427475,
                                                     0.08435279130935669,
                                                     0.0843585804104805,
                                                     1622.2698974609375,
                                                     6.130885601043701,
                                                     513.2769165039062,
                                                     0.08451929688453674,
                                                     0.0844656303524971,
                                                     0.08448315411806107,
                                                     0.08443838357925415,
                                                     657.5328979492188,
                                                     0.08391217887401581,
                                                     0.08355443924665451,
                                                     0.08408589661121368,
                                                     0.08395949006080627,
                                                     0.08368541300296783,
                                                     0.09440038353204727,
                                                     0.08442547917366028,
                                                     0.08448342233896255,
                                                     0.08452529460191727,
                                                     0.08449804782867432,
                                                     0.0958980917930603,
                                                     0.0944596529006958,
                                                     0.09449319541454315,
                                                     0.0945969820022583,
                                                     0.09455905109643936,
                                                     0.09455728530883789,
                                                     0.0946040078997612,
                                                     0.5912741422653198,
                                                     0.0945107489824295,
                                                     0.09456121921539307,
                                                     0.09452136605978012,
                                                     0.09451890736818314,
                                                     0.09491711109876633,
                                                     0.0936649888753891,
                                                     0.17639370262622833,
                                                     227.92086791992188,
                                                     0.0940268412232399,
                                                     0.21170301735401154,
                                                     0.09379062056541443,
                                                     0.09365381300449371,
                                                     0.09371867030858994,
                                                     0.09373131394386292,
                                                     0.09375094622373581,
                                                     0.09409426152706146,
                                                     0.09444309771060944,
                                                     2.2909059524536133,
                                                     408.70733642578125,
                                                     0.09448052197694778,
                                                     0.09452381730079651,
                                                     0.09452628344297409,
                                                     0.09451580047607422,
                                                     0.09457345306873322,
                                                     0.09455623477697372,
                                                     0.09454062581062317,
                                                     0.09454816579818726,
                                                     0.09447815269231796,
                                                     0.09455271065235138,
                                                     0.0945919007062912,
                                                     0.09438098222017288,
                                                     0.09447444975376129,
                                                     0.09452714771032333,
                                                     0.09452454000711441,
                                                     0.09455770254135132,
                                                     0.09453456103801727,
                                                     0.09453591704368591,
                                                     0.0945412740111351,
                                                     5.31784200668335,
                                                     0.09444911777973175,
                                                     1.2043771743774414,
                                                     0.09460223466157913,
                                                     0.09458258748054504,
                                                     0.09454575926065445,
                                                     0.09464789927005768,
                                                     0.09462427347898483,
                                                     0.2161496877670288,
                                                     0.09452006220817566,
                                                     0.0945173129439354,
                                                     0.09459420293569565,
                                                     45.2691764831543,
                                                     0.0946098044514656,
                                                     0.09459919482469559,
                                                     0.09453676640987396,
                                                     0.09455496072769165,
                                                     0.09455988556146622,
                                                     0.09453597664833069,
                                                     1460.126708984375,
                                                     0.09449618309736252,
                                                     0.09447479248046875,
                                                     0.0946631208062172,
                                                     0.09447914361953735,
                                                     0.09451417624950409,
                                                     0.09457054734230042,
                                                     578.6171264648438,
                                                     0.09508994966745377,
                                                     0.09456158429384232,
                                                     0.09453702718019485,
                                                     92.70061492919922,
                                                     575.2883911132812,
                                                     0.09449885785579681,
                                                     0.09459429234266281,
                                                     0.09453769028186798,
                                                     0.09457365423440933,
                                                     0.2661488950252533,
                                                     0.5785210728645325,
                                                     1.0697529315948486,
                                                     9533.2490234375,
                                                     36278084.0,
                                                     0.09451647102832794,
                                                     0.0945221334695816,
                                                     0.0945245772600174,
                                                     0.49862831830978394,
                                                     0.1389312595129013,
                                                     0.10536698997020721,
                                                     0.6453412175178528,
                                                     0.09668745845556259,
                                                     7082514.5,
                                                     0.09513116627931595,
                                                     0.09418614208698273,
                                                     0.09460048377513885,
                                                     0.09451752156019211,
                                                     0.09452079981565475,
                                                     0.11058186739683151,
                                                     0.09448027610778809,
                                                     95075.0,
                                                     0.09450844675302505,
                                                     0.40205472707748413,
                                                     0.09456601738929749,
                                                     21503.685546875,
                                                     25433.630859375,
                                                     0.09459114074707031,
                                                     1.827946662902832,
                                                     0.09454144537448883,
                                                     0.09408380091190338,
                                                     0.09439128637313843,
                                                     0.09441068768501282,
                                                     75790.1328125,
                                                     2222.14990234375,
                                                     0.09460681676864624,
                                                     0.09455598890781403,
                                                     0.09462475776672363,
                                                     0.09453312307596207,
                                                     0.0945381447672844,
                                                     0.09712633490562439,
                                                     15.168710708618164,
                                                     0.09447479993104935,
                                                     0.09447994828224182,
                                                     0.09455700218677521,
                                                     1.1578611135482788,
                                                     0.09453558921813965,
                                                     0.0945739671587944,
                                                     0.09440802037715912,
                                                     7.872211933135986,
                                                     0.09455535560846329,
                                                     0.09453728795051575,
                                                     0.09456964582204819,
                                                     0.09449167549610138,
                                                     403448.90625,
                                                     0.11504217982292175,
                                                     0.09461946040391922,
                                                     0.09448207169771194,
                                                     1.0811370611190796,
                                                     0.09459654241800308,
                                                     0.09441133588552475,
                                                     0.0943896472454071,
                                                     5106.0791015625,
                                                     0.09468978643417358,
                                                     0.09459315240383148,
                                                     0.09458445757627487,
                                                     12.510099411010742,
                                                     0.09466719627380371,
                                                     0.09455475211143494,
                                                     20.623863220214844,
                                                     0.09454236924648285,
                                                     8.770496368408203,
                                                     12.305490493774414,
                                                     0.09461303800344467,
                                                     0.09462690353393555,
                                                     0.09455135464668274,
                                                     0.09457971900701523,
                                                     0.09463999420404434,
                                                     0.09459368139505386,
                                                     39.026275634765625,
                                                     0.09459409862756729,
                                                     0.14653317630290985,
                                                     0.09447965025901794,
                                                     0.09448003023862839,
                                                     0.09420546144247055,
                                                     0.10086872428655624,
                                                     0.09402845054864883,
                                                     0.09396755695343018,
                                                     0.09385461360216141,
                                                     0.7617712020874023,
                                                     3.599776268005371,
                                                     0.0940251424908638,
                                                     0.09385236352682114,
                                                     1.5394641160964966,
                                                     555.6378784179688,
                                                     21.09204864501953,
                                                     0.09456101059913635,
                                                     0.09457747638225555,
                                                     0.20754975080490112,
                                                     0.09456492215394974,
                                                     0.09465780109167099,
                                                     0.09462746232748032,
                                                     0.09462113678455353,
                                                     4181469.0,
                                                     0.11877608299255371,
                                                     0.09787335991859436,
                                                     0.09376204758882523,
                                                     0.09390174597501755,
                                                     609.8927001953125,
                                                     0.0945817232131958,
                                                     56.260704040527344,
                                                     0.09460467100143433,
                                                     0.16732440888881683,
                                                     0.09459473192691803,
                                                     0.09458056092262268,
                                                     0.09451653063297272,
                                                     0.09456532448530197,
                                                     0.09456890821456909,
                                                     0.09472556412220001,
                                                     0.09467514604330063,
                                                     0.09467890858650208,
                                                     0.09461469203233719,
                                                     0.09461411088705063,
                                                     0.09406077861785889,
                                                     0.09436465054750443,
                                                     6.517511367797852,
                                                     0.09469257295131683,
                                                     22.83505630493164,
                                                     21.33751106262207,
                                                     0.09463278949260712,
                                                     1789.422119140625,
                                                     435.6246337890625,
                                                     0.24851779639720917,
                                                     0.09462091326713562,
                                                     614.681640625,
                                                     0.09470755606889725,
                                                     178.90574645996094,
                                                     0.09464573115110397,
                                                     0.09471963346004486,
                                                     0.09465103596448898,
                                                     0.6563721895217896,
                                                     704190.0,
                                                     0.09396739304065704,
                                                     0.09447432309389114,
                                                     0.09416063874959946,
                                                     0.0944053903222084,
                                                     65.62581634521484,
                                                     0.09450186043977737,
                                                     1024.021240234375,
                                                     0.09452372044324875,
                                                     0.09453118592500687,
                                                     0.09463518112897873,
                                                     0.09458576887845993,
                                                     0.10458575189113617,
                                                     47422.390625,
                                                     0.09453989565372467,
                                                     0.19003771245479584,
                                                     0.09361455589532852,
                                                     0.09875020384788513,
                                                     0.10593827068805695,
                                                     0.09450165182352066,
                                                     2.2607531547546387,
                                                     146.7675018310547,
                                                     0.09460194408893585,
                                                     0.09467209130525589,
                                                     0.09463077038526535,
                                                     0.09450284391641617,
                                                     0.09454219788312912,
                                                     0.09467772394418716,
                                                     0.09461057186126709,
                                                     301.4787902832031,
                                                     0.0944533422589302,
                                                     0.09459809958934784,
                                                     0.09464976191520691,
                                                     0.09458554536104202,
                                                     0.09468982368707657,
                                                     0.09466410428285599,
                                                     0.5748986601829529,
                                                     0.09391465783119202,
                                                     0.09361892193555832,
                                                     13.32568073272705,
                                                     0.09446100890636444,
                                                     77.79401397705078,
                                                     2.8922955989837646,
                                                     0.1703469306230545,
                                                     7399.919921875,
                                                     4709.5478515625,
                                                     0.09465955197811127,
                                                     0.09466127306222916,
                                                     0.09468183666467667,
                                                     0.09468203783035278,
                                                     19.022647857666016,
                                                     0.09639277309179306,
                                                     0.09449823945760727,
                                                     0.09443557262420654,
                                                     0.09912924468517303,
                                                     0.09453266113996506,
                                                     0.09452731162309647,
                                                     0.09462825953960419,
                                                     0.09465084969997406,
                                                     0.09445875138044357,
                                                     9395.8203125,
                                                     4131.25390625,
                                                     0.09448602050542831,
                                                     0.09457829594612122,
                                                     0.09452693164348602,
                                                     0.41866934299468994,
                                                     0.09443027526140213,
                                                     1.5612469911575317,
                                                     0.09447776526212692,
                                                     0.09464254975318909,
                                                     0.0947151929140091,
                                                     0.09461060166358948,
                                                     1.6179649829864502,
                                                     151.97926330566406,
                                                     0.09670396894216537,
                                                     0.09371296316385269,
                                                     0.22721385955810547,
                                                     0.09369426220655441,
                                                     0.09369219094514847,
                                                     0.0936376303434372,
                                                     519.0321655273438,
                                                     2733.38671875,
                                                     0.09362778812646866,
                                                     0.1038171648979187,
                                                     0.36694493889808655,
                                                     0.09366429597139359,
                                                     0.09367946535348892,
                                                     0.093511663377285,
                                                     363061.71875,
                                                     0.09354864805936813,
                                                     0.09355273097753525,
                                                     0.0940803587436676,
                                                     0.09443733096122742,
                                                     0.09446991235017776,
                                                     0.09449158608913422,
                                                     0.09442201256752014,
                                                     0.09459392726421356,
                                                     0.09461551159620285,
                                                     3.2891151905059814,
                                                     0.09446682780981064,
                                                     214763.484375,
                                                     0.0945248231291771,
                                                     0.0945204347372055,
                                                     1.0939825773239136,
                                                     0.09466331452131271,
                                                     0.09470713883638382,
                                                     0.09462140500545502,
                                                     0.0944676548242569,
                                                     0.09462480992078781,
                                                     0.0944826528429985,
                                                     62.18036651611328,
                                                     1.1048225164413452,
                                                     7.396787166595459,
                                                     0.09464084357023239,
                                                     0.09469672292470932,
                                                     0.09460265189409256,
                                                     37.437835693359375,
                                                     0.09448858350515366,
                                                     13.001826286315918,
                                                     45.448062896728516,
                                                     0.0946521982550621,
                                                     0.09455090761184692,
                                                     0.2181532233953476,
                                                     248.3521728515625,
                                                     0.094516322016716,
                                                     0.09450066834688187,
                                                     0.09460214525461197,
                                                     1419.5380859375,
                                                     0.09455197304487228,
                                                     89.7099380493164,
                                                     0.09380219131708145,
                                                     0.09361878782510757,
                                                     0.09440010786056519,
                                                     33.71955490112305,
                                                     0.0946076437830925,
                                                     0.09467274695634842,
                                                     0.09467364847660065,
                                                     6.568045139312744,
                                                     0.0946686714887619,
                                                     0.11860518157482147,
                                                     144.5919952392578,
                                                     0.0945608839392662,
                                                     0.0977327898144722,
                                                     0.09378885477781296,
                                                     0.4567955732345581,
                                                     0.28246936202049255,
                                                     0.0937114804983139,
                                                     3.4494714736938477,
                                                     0.09347078949213028,
                                                     1.9993853569030762,
                                                     694.1502685546875,
                                                     0.09461234509944916,
                                                     0.09463471919298172,
                                                     0.09461282938718796,
                                                     0.09465289115905762,
                                                     0.1086336076259613,
                                                     0.09450510889291763,
                                                     0.09465640783309937,
                                                     0.0946459248661995,
                                                     44.427127838134766,
                                                     0.0945444405078888,
                                                     0.09469397366046906,
                                                     0.09456057101488113,
                                                     0.09453918039798737,
                                                     0.09460191428661346,
                                                     16.45922088623047,
                                                     0.09442133456468582,
                                                     766.4246826171875,
                                                     0.0944996252655983,
                                                     57.03985595703125,
                                                     0.09441376477479935,
                                                     0.09458398073911667,
                                                     4.6840386390686035,
                                                     0.10124408453702927,
                                                     0.09464626759290695,
                                                     0.0945911556482315,
                                                     34.607200622558594],
                              'val_accuracy_history': [1.736842105263158,
                                                       2.210526315789474,
                                                       0.05263157894736842,
                                                       1.0,
                                                       0.8947368421052632,
                                                       0.9473684210526315,
                                                       0.2631578947368421,
                                                       0.6842105263157895,
                                                       1.5789473684210527,
                                                       0.7368421052631579,
                                                       1.0,
                                                       1.368421052631579,
                                                       0.8947368421052632,
                                                       0.0,
                                                       0.15789473684210525,
                                                       0.3684210526315789,
                                                       1.5789473684210527,
                                                       1.368421052631579,
                                                       1.4736842105263157,
                                                       0.2631578947368421,
                                                       0.3684210526315789,
                                                       1.631578947368421,
                                                       0.47368421052631576,
                                                       1.368421052631579,
                                                       0.5789473684210527,
                                                       0.42105263157894735,
                                                       1.0,
                                                       0.5789473684210527,
                                                       0.6842105263157895,
                                                       1.0,
                                                       0.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       2.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       1.105263157894737,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       2.0,
                                                       0.3684210526315789,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       4.0,
                                                       0.0,
                                                       0.0,
                                                       3.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       3.0,
                                                       1.0,
                                                       0.0,
                                                       2.0,
                                                       0.0,
                                                       4.0,
                                                       0.0,
                                                       1.0,
                                                       1.0526315789473684,
                                                       0.3157894736842105,
                                                       0.0,
                                                       2.0,
                                                       1.0,
                                                       1.4736842105263157,
                                                       0.42105263157894735,
                                                       1.0,
                                                       2.0,
                                                       1.0,
                                                       0.0,
                                                       2.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       2.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       1.4736842105263157,
                                                       1.0,
                                                       0.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       2.0,
                                                       1.0,
                                                       0.0,
                                                       3.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       2.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       2.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       3.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       2.0,
                                                       2.0,
                                                       2.0,
                                                       1.0,
                                                       1.0,
                                                       2.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       2.0,
                                                       3.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.15789473684210525,
                                                       0.0,
                                                       2.0,
                                                       2.0,
                                                       0.0,
                                                       2.0,
                                                       0.0,
                                                       3.0,
                                                       1.0,
                                                       1.0,
                                                       1.894736842105263,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       2.0,
                                                       4.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       2.0,
                                                       1.0,
                                                       0.8947368421052632,
                                                       0.21052631578947367,
                                                       1.5263157894736843,
                                                       0.21052631578947367,
                                                       0.5789473684210527,
                                                       0.2631578947368421,
                                                       0.8947368421052632,
                                                       1.0526315789473684,
                                                       1.0,
                                                       2.0,
                                                       2.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       0.6666666666666666,
                                                       3.0,
                                                       0.0,
                                                       0.2222222222222222,
                                                       1.0,
                                                       0.5555555555555556,
                                                       0.2222222222222222,
                                                       0.2222222222222222,
                                                       1.7777777777777777,
                                                       1.8888888888888888,
                                                       1.0,
                                                       2.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       2.0,
                                                       2.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       1.5555555555555556,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.8888888888888888,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       2.0,
                                                       0.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       0.2222222222222222,
                                                       1.7777777777777777,
                                                       0.4444444444444444,
                                                       0.0,
                                                       0.4444444444444444,
                                                       0.0,
                                                       2.0,
                                                       0.0,
                                                       0.2222222222222222,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       2.0,
                                                       1.0,
                                                       1.0,
                                                       3.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       0.7777777777777778,
                                                       0.8888888888888888,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       2.0,
                                                       3.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       5.0,
                                                       1.0,
                                                       2.0,
                                                       1.0,
                                                       2.0,
                                                       4.0,
                                                       4.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       2.0,
                                                       2.0,
                                                       1.0,
                                                       3.0,
                                                       1.0,
                                                       2.263157894736842,
                                                       3.0,
                                                       2.0,
                                                       1.6842105263157894,
                                                       0.0,
                                                       1.0,
                                                       3.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       2.0,
                                                       1.0,
                                                       5.0,
                                                       0.0,
                                                       2.789473684210526,
                                                       0.0,
                                                       1.894736842105263,
                                                       1.5789473684210527,
                                                       1.0,
                                                       2.0,
                                                       1.0,
                                                       2.0,
                                                       4.0,
                                                       2.0,
                                                       1.0,
                                                       3.0,
                                                       2.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       2.0,
                                                       5.0,
                                                       4.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       1.894736842105263,
                                                       1.0526315789473684,
                                                       1.4736842105263157,
                                                       0.0,
                                                       0.0,
                                                       3.0,
                                                       1.894736842105263,
                                                       3.0,
                                                       2.0,
                                                       1.0,
                                                       2.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       0.631578947368421,
                                                       1.0,
                                                       2.0,
                                                       3.0,
                                                       2.0,
                                                       2.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       3.0,
                                                       1.0,
                                                       0.0,
                                                       5.0,
                                                       2.0,
                                                       0.0,
                                                       3.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       2.0,
                                                       2.0,
                                                       0.0,
                                                       1.6842105263157894,
                                                       2.0526315789473686,
                                                       1.6842105263157894,
                                                       2.4210526315789473,
                                                       1.894736842105263,
                                                       2.526315789473684,
                                                       1.7894736842105263,
                                                       1.631578947368421,
                                                       1.5789473684210527,
                                                       1.631578947368421,
                                                       0.7894736842105263,
                                                       1.0,
                                                       1.7894736842105263,
                                                       0.8947368421052632,
                                                       1.0526315789473684,
                                                       2.6315789473684212,
                                                       1.1578947368421053,
                                                       3.0,
                                                       1.0,
                                                       3.0,
                                                       2.0,
                                                       2.0,
                                                       2.0,
                                                       3.0,
                                                       0.0,
                                                       2.0,
                                                       2.0,
                                                       2.0,
                                                       2.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       1.0,
                                                       2.0,
                                                       2.0,
                                                       4.0,
                                                       1.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       2.0,
                                                       1.0,
                                                       0.0,
                                                       3.0,
                                                       4.0,
                                                       1.0,
                                                       2.0,
                                                       3.0,
                                                       1.1578947368421053,
                                                       0.0,
                                                       1.0,
                                                       2.0,
                                                       2.0,
                                                       1.0,
                                                       3.0,
                                                       2.1052631578947367,
                                                       3.9473684210526314,
                                                       0.3157894736842105,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       3.0,
                                                       3.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       2.0,
                                                       0.9473684210526315,
                                                       1.8421052631578947,
                                                       1.631578947368421,
                                                       2.1052631578947367,
                                                       1.4736842105263157,
                                                       1.1578947368421053,
                                                       2.8421052631578947,
                                                       3.0,
                                                       0.21052631578947367,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       3.0,
                                                       0.0,
                                                       0.0,
                                                       3.0,
                                                       0.0,
                                                       2.0,
                                                       1.0,
                                                       0.0,
                                                       3.0,
                                                       1.0,
                                                       0.0,
                                                       2.0,
                                                       1.0,
                                                       0.0,
                                                       3.0,
                                                       1.0,
                                                       1.0,
                                                       2.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       3.0,
                                                       0.0],
                              'val_loss': 0.08975100517272949,
                              'val_loss_history': [0.12889361381530762,
                                                   0.12675809860229492,
                                                   929197312.0,
                                                   0.12727877497673035,
                                                   0.12555724382400513,
                                                   1098212864.0,
                                                   0.13582992553710938,
                                                   0.1321955770254135,
                                                   0.1356307566165924,
                                                   1054040704.0,
                                                   0.13556765019893646,
                                                   0.14156989753246307,
                                                   0.13816937804222107,
                                                   268273632.0,
                                                   0.13720349967479706,
                                                   0.13542649149894714,
                                                   0.1288645714521408,
                                                   0.1226494163274765,
                                                   0.12051358073949814,
                                                   0.12420963495969772,
                                                   0.12739074230194092,
                                                   0.13060890138149261,
                                                   0.1263168305158615,
                                                   0.12670856714248657,
                                                   0.11823848634958267,
                                                   0.12219959497451782,
                                                   0.11411118507385254,
                                                   0.12208209931850433,
                                                   0.12046002596616745,
                                                   0.11428070068359375,
                                                   0.11432527750730515,
                                                   0.11463426798582077,
                                                   0.1144825890660286,
                                                   0.11444626748561859,
                                                   0.11485874652862549,
                                                   0.11457077413797379,
                                                   0.1145758330821991,
                                                   0.11442591995000839,
                                                   0.1148984432220459,
                                                   0.1149170994758606,
                                                   0.11488619446754456,
                                                   0.11486360430717468,
                                                   0.114899180829525,
                                                   0.1149524450302124,
                                                   0.11508650332689285,
                                                   0.1152985617518425,
                                                   0.11511247605085373,
                                                   223967184.0,
                                                   0.1151353120803833,
                                                   0.11522237956523895,
                                                   0.11538894474506378,
                                                   0.11536173522472382,
                                                   0.11543510854244232,
                                                   0.11553690582513809,
                                                   0.11534439027309418,
                                                   0.1152355745434761,
                                                   0.11538137495517731,
                                                   0.11679191887378693,
                                                   0.11562331020832062,
                                                   0.11570201814174652,
                                                   0.11553406715393066,
                                                   0.11560743302106857,
                                                   0.11558980494737625,
                                                   0.11557500809431076,
                                                   0.11566941440105438,
                                                   0.11569506675004959,
                                                   0.1155826523900032,
                                                   0.11563906818628311,
                                                   0.11583434790372849,
                                                   0.11583562195301056,
                                                   0.11584048718214035,
                                                   0.11579956114292145,
                                                   0.1158096119761467,
                                                   0.11577166616916656,
                                                   0.11578544974327087,
                                                   0.11576415598392487,
                                                   0.11577217280864716,
                                                   0.1157451793551445,
                                                   0.1157572790980339,
                                                   0.1158934161067009,
                                                   0.11580001562833786,
                                                   0.11593358963727951,
                                                   0.11615775525569916,
                                                   0.11587931215763092,
                                                   0.1159670352935791,
                                                   0.11585234105587006,
                                                   0.11583954095840454,
                                                   0.11609325557947159,
                                                   0.11583058536052704,
                                                   0.11599991470575333,
                                                   0.11598140746355057,
                                                   0.11592470854520798,
                                                   0.11593374609947205,
                                                   0.11602354794740677,
                                                   0.11578560620546341,
                                                   0.11599435657262802,
                                                   0.11563146859407425,
                                                   0.11705952137708664,
                                                   0.11598608642816544,
                                                   0.11604960262775421,
                                                   0.11593892425298691,
                                                   0.1176956295967102,
                                                   0.12361403554677963,
                                                   0.11607111245393753,
                                                   0.11594823747873306,
                                                   0.11593722552061081,
                                                   0.11586553603410721,
                                                   0.11618652194738388,
                                                   0.11610959470272064,
                                                   0.11593214422464371,
                                                   0.11584636569023132,
                                                   0.1159195527434349,
                                                   0.11607397347688675,
                                                   0.1159905418753624,
                                                   0.1159055307507515,
                                                   0.11624743789434433,
                                                   0.11588098853826523,
                                                   0.11602722853422165,
                                                   0.11587972939014435,
                                                   0.11616445332765579,
                                                   0.11607798933982849,
                                                   0.11569825559854507,
                                                   0.11504572629928589,
                                                   0.11600799113512039,
                                                   0.11600035429000854,
                                                   0.11612346768379211,
                                                   0.11608055979013443,
                                                   0.11566175520420074,
                                                   0.1160079687833786,
                                                   0.11582132428884506,
                                                   0.11605935543775558,
                                                   0.11587560176849365,
                                                   0.11589808017015457,
                                                   0.11591998487710953,
                                                   0.11596931517124176,
                                                   0.11618545651435852,
                                                   0.11614586412906647,
                                                   0.116184763610363,
                                                   0.11593402177095413,
                                                   0.11613350361585617,
                                                   0.11606571078300476,
                                                   0.11616233736276627,
                                                   0.11608292907476425,
                                                   0.11600427329540253,
                                                   0.11627335101366043,
                                                   0.11593248695135117,
                                                   0.11602398753166199,
                                                   0.11632940173149109,
                                                   0.11617862433195114,
                                                   0.11615864932537079,
                                                   0.11609946191310883,
                                                   0.11620981246232986,
                                                   0.11623332649469376,
                                                   0.11621405184268951,
                                                   0.11617275327444077,
                                                   0.11618194729089737,
                                                   0.11618026345968246,
                                                   0.11604853719472885,
                                                   0.11610307544469833,
                                                   0.11616066843271255,
                                                   0.11609618365764618,
                                                   0.11606502532958984,
                                                   0.11607180535793304,
                                                   0.11610879749059677,
                                                   0.11615218967199326,
                                                   0.11595457047224045,
                                                   0.1159059926867485,
                                                   0.11616579443216324,
                                                   0.11522948741912842,
                                                   0.11619925498962402,
                                                   0.11620134115219116,
                                                   0.11613330245018005,
                                                   0.11608000099658966,
                                                   0.11618129909038544,
                                                   0.11634077876806259,
                                                   0.11618518084287643,
                                                   0.11617344617843628,
                                                   0.11647146195173264,
                                                   0.11618098616600037,
                                                   0.11633583903312683,
                                                   0.11608415842056274,
                                                   0.11629339307546616,
                                                   0.11636802554130554,
                                                   0.11612612754106522,
                                                   0.11618626117706299,
                                                   0.11609125882387161,
                                                   0.11604174226522446,
                                                   0.11616165190935135,
                                                   0.11629405617713928,
                                                   0.12194935232400894,
                                                   0.11632195115089417,
                                                   0.1196022555232048,
                                                   0.11585008352994919,
                                                   0.12247772514820099,
                                                   0.12818971276283264,
                                                   0.12019037455320358,
                                                   0.12467972934246063,
                                                   0.11633002012968063,
                                                   0.1162014976143837,
                                                   0.11621222645044327,
                                                   0.09137231856584549,
                                                   0.09150426834821701,
                                                   0.09157953411340714,
                                                   0.09157821536064148,
                                                   0.09164956212043762,
                                                   0.09170088171958923,
                                                   0.09174489974975586,
                                                   0.09180380403995514,
                                                   0.09149286895990372,
                                                   0.09157092124223709,
                                                   0.09140341728925705,
                                                   0.09140137583017349,
                                                   0.09056924283504486,
                                                   0.09073775261640549,
                                                   0.09158705919981003,
                                                   0.0913533940911293,
                                                   2.4094927310943604,
                                                   0.09106162190437317,
                                                   0.0903748944401741,
                                                   0.09066660702228546,
                                                   0.09095553308725357,
                                                   0.09037058055400848,
                                                   0.09065017104148865,
                                                   0.09140996634960175,
                                                   0.0915386751294136,
                                                   0.09155571460723877,
                                                   0.09167596697807312,
                                                   0.09157241880893707,
                                                   0.09159300476312637,
                                                   0.09173470735549927,
                                                   0.09168659150600433,
                                                   0.09159538149833679,
                                                   0.09157337993383408,
                                                   0.09162420779466629,
                                                   0.0916445180773735,
                                                   0.09159063547849655,
                                                   0.09151946753263474,
                                                   0.09143755584955215,
                                                   0.09167628735303879,
                                                   0.09162566065788269,
                                                   0.09144790470600128,
                                                   0.09167981147766113,
                                                   0.09183721989393234,
                                                   0.09176892042160034,
                                                   0.09148874878883362,
                                                   0.09179455041885376,
                                                   0.09149231016635895,
                                                   0.09171196818351746,
                                                   0.09166552871465683,
                                                   0.09163936972618103,
                                                   0.09166969358921051,
                                                   0.09118418395519257,
                                                   0.09167936444282532,
                                                   0.09164978563785553,
                                                   0.0916447639465332,
                                                   0.091525137424469,
                                                   0.0916038230061531,
                                                   0.09174405038356781,
                                                   0.09161648899316788,
                                                   0.09172605723142624,
                                                   0.09169495105743408,
                                                   0.09160944074392319,
                                                   0.09177764505147934,
                                                   0.09175115078687668,
                                                   0.09168726205825806,
                                                   0.09168113023042679,
                                                   0.0916929617524147,
                                                   0.0916459709405899,
                                                   0.09187834709882736,
                                                   0.09167644381523132,
                                                   0.0916120707988739,
                                                   0.09165194630622864,
                                                   0.09167271107435226,
                                                   0.09164658933877945,
                                                   0.09172767400741577,
                                                   0.09166054427623749,
                                                   0.09191282838582993,
                                                   0.0919322669506073,
                                                   0.09164082258939743,
                                                   0.09164590388536453,
                                                   0.09164344519376755,
                                                   0.09161072224378586,
                                                   0.09165661782026291,
                                                   0.09178908169269562,
                                                   0.09179839491844177,
                                                   0.09184478968381882,
                                                   0.09157416224479675,
                                                   0.09172283858060837,
                                                   0.09165951609611511,
                                                   0.09154213964939117,
                                                   0.09159321337938309,
                                                   0.09175139665603638,
                                                   0.0916604995727539,
                                                   0.09152602404356003,
                                                   0.09157373756170273,
                                                   0.09160105139017105,
                                                   0.09054151177406311,
                                                   0.09138046205043793,
                                                   0.0918545126914978,
                                                   0.09164437651634216,
                                                   0.09164205938577652,
                                                   0.09161880612373352,
                                                   0.09176848083734512,
                                                   0.0915747582912445,
                                                   0.09165892750024796,
                                                   0.09155618399381638,
                                                   0.09165655821561813,
                                                   0.09176354855298996,
                                                   0.0917825847864151,
                                                   0.09167511016130447,
                                                   0.09172207117080688,
                                                   0.09178581088781357,
                                                   0.09048595279455185,
                                                   0.09156113862991333,
                                                   0.09158486872911453,
                                                   0.09151524305343628,
                                                   0.09164683520793915,
                                                   0.09167765825986862,
                                                   0.09159915894269943,
                                                   0.09176617115736008,
                                                   0.09172510355710983,
                                                   0.09160268306732178,
                                                   0.09161321818828583,
                                                   0.0915767028927803,
                                                   0.0914856418967247,
                                                   0.0916421189904213,
                                                   0.09157934784889221,
                                                   0.09156793355941772,
                                                   0.09154218435287476,
                                                   0.09154070168733597,
                                                   134901216.0,
                                                   0.09168075770139694,
                                                   0.09144774079322815,
                                                   0.09149137884378433,
                                                   0.09155774861574173,
                                                   0.09148041158914566,
                                                   0.09169764071702957,
                                                   0.0916762501001358,
                                                   0.09158093482255936,
                                                   0.09154773503541946,
                                                   0.0916462242603302,
                                                   0.0915762335062027,
                                                   0.09153687953948975,
                                                   0.09146840870380402,
                                                   0.09179212152957916,
                                                   0.0917789414525032,
                                                   0.09176136553287506,
                                                   0.09159605950117111,
                                                   0.09162770956754684,
                                                   0.09160606563091278,
                                                   0.09176835417747498,
                                                   0.09157654643058777,
                                                   0.0916082113981247,
                                                   0.09162461757659912,
                                                   0.09182053804397583,
                                                   0.09177893400192261,
                                                   0.09165219217538834,
                                                   0.09159030020236969,
                                                   0.09154009819030762,
                                                   0.09164740145206451,
                                                   0.09176388382911682,
                                                   0.0915454551577568,
                                                   0.09177467226982117,
                                                   0.09191010892391205,
                                                   0.09156614542007446,
                                                   0.09143801033496857,
                                                   0.09114047884941101,
                                                   0.09068604558706284,
                                                   0.09070765227079391,
                                                   0.09103754162788391,
                                                   0.09052861481904984,
                                                   0.09132906794548035,
                                                   0.09160716831684113,
                                                   0.09083017706871033,
                                                   0.09090922772884369,
                                                   0.09165212512016296,
                                                   0.09168437123298645,
                                                   0.09145098924636841,
                                                   0.09153489023447037,
                                                   8632.75390625,
                                                   0.09173749387264252,
                                                   0.09159638732671738,
                                                   0.09155716747045517,
                                                   0.09143556654453278,
                                                   0.09150324761867523,
                                                   0.09115756303071976,
                                                   0.09095091372728348,
                                                   0.09065255522727966,
                                                   0.09077665954828262,
                                                   0.09125294536352158,
                                                   0.09146241843700409,
                                                   0.0917263850569725,
                                                   0.09158481657505035,
                                                   0.09154310077428818,
                                                   0.09155087172985077,
                                                   0.09157835692167282,
                                                   0.09175891429185867,
                                                   0.09174496680498123,
                                                   0.0915360376238823,
                                                   0.0916609838604927,
                                                   0.09006042033433914,
                                                   0.09004908800125122,
                                                   0.08979449421167374,
                                                   0.0896245688199997,
                                                   0.08980648964643478,
                                                   0.0895053967833519,
                                                   0.08965669572353363,
                                                   0.09002640843391418,
                                                   0.08996382355690002,
                                                   0.08966709673404694,
                                                   0.0897647812962532,
                                                   0.08978300541639328,
                                                   0.08981018513441086,
                                                   0.0896415039896965,
                                                   0.0901375487446785,
                                                   0.089830681681633,
                                                   0.09017007797956467,
                                                   0.0898798406124115,
                                                   0.09025020152330399,
                                                   0.0899721086025238,
                                                   0.08988645672798157,
                                                   0.09003996104001999,
                                                   0.08998358994722366,
                                                   0.0893966481089592,
                                                   0.08915132284164429,
                                                   0.08969417959451675,
                                                   0.08966122567653656,
                                                   0.08971626311540604,
                                                   0.08974547684192657,
                                                   0.08988585323095322,
                                                   0.09006890654563904,
                                                   0.09015759825706482,
                                                   0.08985879272222519,
                                                   0.09011870622634888,
                                                   0.089875727891922,
                                                   0.08984020352363586,
                                                   0.08999879658222198,
                                                   0.08929476141929626,
                                                   0.08917342871427536,
                                                   0.0890253409743309,
                                                   0.0890451967716217,
                                                   0.08985283225774765,
                                                   0.08977603167295456,
                                                   0.08965098857879639,
                                                   0.09000735729932785,
                                                   0.0896926000714302,
                                                   0.0897701233625412,
                                                   0.08995763957500458,
                                                   0.0898318812251091,
                                                   0.08998756855726242,
                                                   0.09007910639047623,
                                                   0.09007957577705383,
                                                   0.08997736871242523,
                                                   0.08961615711450577,
                                                   0.09001019597053528,
                                                   0.08989803493022919,
                                                   0.09013975411653519,
                                                   0.0902264267206192,
                                                   0.0899982675909996,
                                                   0.08958622068166733,
                                                   0.08898205310106277,
                                                   0.08928899466991425,
                                                   0.08966180682182312,
                                                   0.0896264836192131,
                                                   0.0899060070514679,
                                                   165786832.0,
                                                   0.08958649635314941,
                                                   0.08978855609893799,
                                                   0.0899762436747551,
                                                   0.09000223875045776,
                                                   0.08978422731161118,
                                                   0.09043993800878525,
                                                   0.09028058499097824,
                                                   0.1005210429430008,
                                                   0.08993940055370331,
                                                   0.08960286527872086,
                                                   0.08972615748643875,
                                                   0.08975361287593842,
                                                   0.08965158462524414,
                                                   0.08964148908853531,
                                                   0.08972785621881485,
                                                   0.0897819921374321,
                                                   0.08990558981895447,
                                                   0.08987738937139511,
                                                   0.09000679105520248,
                                                   0.08997351676225662,
                                                   0.08983514457941055,
                                                   0.09013941884040833,
                                                   0.09014367312192917,
                                                   0.09003666788339615,
                                                   0.09019093215465546,
                                                   0.08984865993261337,
                                                   0.08995110541582108,
                                                   0.08969511836767197,
                                                   0.08997870236635208,
                                                   0.08957649022340775,
                                                   0.08970191329717636,
                                                   0.08933154493570328,
                                                   0.08878691494464874,
                                                   0.08927902579307556,
                                                   0.08946020156145096,
                                                   0.08946799486875534,
                                                   0.08914709836244583,
                                                   0.08887948840856552,
                                                   0.08919765800237656,
                                                   0.0895165354013443,
                                                   0.08895201981067657,
                                                   0.08957294374704361,
                                                   0.08974824845790863,
                                                   0.08876839280128479,
                                                   0.08959838002920151,
                                                   0.08914162218570709,
                                                   0.08941791206598282,
                                                   0.08913740515708923,
                                                   0.08939765393733978,
                                                   0.09013853222131729,
                                                   0.08985241502523422,
                                                   0.0897478461265564,
                                                   0.08989280462265015,
                                                   0.08953097462654114,
                                                   0.08996637165546417,
                                                   0.08999262005090714,
                                                   0.09000848233699799,
                                                   0.08972601592540741,
                                                   0.08974424004554749,
                                                   0.08952955156564713,
                                                   0.08970942348241806,
                                                   0.0896824523806572,
                                                   0.0899357870221138,
                                                   0.0900910422205925,
                                                   0.08987075090408325,
                                                   0.09012211114168167,
                                                   0.0899343490600586,
                                                   0.08979938924312592,
                                                   0.08996124565601349,
                                                   0.09016357362270355,
                                                   0.09030314534902573,
                                                   0.09023184329271317,
                                                   0.09009277075529099,
                                                   0.08956193923950195,
                                                   0.09018462896347046,
                                                   0.08991622179746628,
                                                   0.08998739719390869,
                                                   0.09010801464319229,
                                                   0.08995979279279709,
                                                   0.08951814472675323,
                                                   0.09035605937242508,
                                                   0.08999628573656082,
                                                   0.08997727185487747,
                                                   0.09033078700304031,
                                                   0.08996672928333282,
                                                   0.09028850495815277,
                                                   0.0895623043179512,
                                                   0.08894254267215729,
                                                   0.09029853343963623,
                                                   0.08997199684381485,
                                                   0.08996271342039108,
                                                   0.09009357541799545,
                                                   0.09012578427791595,
                                                   0.08983942866325378,
                                                   0.08987858891487122,
                                                   0.08974911272525787,
                                                   0.08983664214611053,
                                                   0.08988182246685028,
                                                   0.0900752916932106,
                                                   0.08942669630050659,
                                                   0.08933917433023453,
                                                   0.08779210597276688,
                                                   0.08927730470895767,
                                                   0.08909048140048981,
                                                   0.0898263081908226,
                                                   0.08931832760572433,
                                                   0.08928386121988297,
                                                   0.08991234749555588,
                                                   0.08994374424219131,
                                                   0.09000713378190994,
                                                   0.08975329250097275,
                                                   0.09016995877027512,
                                                   0.08982622623443604,
                                                   0.09043461084365845,
                                                   0.09003646671772003,
                                                   0.0898413360118866,
                                                   0.08992080390453339,
                                                   0.08976344019174576,
                                                   0.09005337953567505,
                                                   0.09002504497766495,
                                                   0.08987294137477875,
                                                   0.08990441262722015,
                                                   0.08986706286668777,
                                                   0.09007862210273743,
                                                   0.08986128121614456,
                                                   0.08975748717784882,
                                                   0.0894976556301117,
                                                   0.08977871388196945,
                                                   0.08997070044279099,
                                                   0.0896330401301384,
                                                   0.09019531309604645,
                                                   0.09008292108774185,
                                                   0.09006659686565399,
                                                   0.08975100517272949],
                              'weight_decay': 0.0005},
                 'loss': {'accuracy': 0.0,
                          'batch_size': 32,
                          'cv_score': 0.0053345869543231185,
                          'cv_val_accuracy': 0.6666666666666666,
                          'cv_val_loss': 0.09920807182788849,
                          'cv_val_macroF1': 0.0053345869543231185,
                          'cv_val_microF1': 0.066966913828706,
                          'epochs': 200,
                          'final_model': GGNN1(
  (ggnn): GatedGraphConv(60, num_layers=2)
  (fc1): Linear(in_features=60, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                          'kwargs': {'aggr_type': 'mean',
                                     'd1': 60,
                                     'd2': 50,
                                     'num_classes': 24,
                                     'num_layers': 2},
                          'learning_rate': 0.01,
                          'macroF1': 0.006748704489763125,
                          'microF1': 0.08414023372287145,
                          'model': <class 'TFM_graph_classification_models.GGNN1'>,
                          'model_instance': GGNN1(
  (ggnn): GatedGraphConv(60, num_layers=2)
  (fc1): Linear(in_features=60, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                          'score': 'f1_macro',
                          'time': 2764.828370809555,
                          'train_loss_history': [362103776.0,
                                                 1984.16552734375,
                                                 4208493824.0,
                                                 660907968.0,
                                                 0.08377974480390549,
                                                 1243555968.0,
                                                 1017.9180297851562,
                                                 76571.46875,
                                                 308.40338134765625,
                                                 23220.69921875,
                                                 156603248.0,
                                                 138.022216796875,
                                                 0.08351406455039978,
                                                 1823248640.0,
                                                 155343744.0,
                                                 0.0847235918045044,
                                                 0.08452852815389633,
                                                 0.08426041901111603,
                                                 0.0840001031756401,
                                                 0.0840369313955307,
                                                 0.0836704894900322,
                                                 0.08393912762403488,
                                                 0.08424623310565948,
                                                 0.08425412327051163,
                                                 0.08409769088029861,
                                                 0.08399365842342377,
                                                 0.08431313186883926,
                                                 0.08411368727684021,
                                                 19.82664680480957,
                                                 0.08441364020109177,
                                                 0.08446607738733292,
                                                 0.08450149744749069,
                                                 0.08464515209197998,
                                                 0.08455231040716171,
                                                 0.08460909128189087,
                                                 0.08457177132368088,
                                                 0.0844896212220192,
                                                 0.08443144708871841,
                                                 7615.97705078125,
                                                 5.158772945404053,
                                                 0.08454208821058273,
                                                 0.08437103033065796,
                                                 0.08442801982164383,
                                                 0.08480384200811386,
                                                 0.08451271802186966,
                                                 0.0846419557929039,
                                                 0.08445870131254196,
                                                 0.08451291173696518,
                                                 7.508358001708984,
                                                 7220202.5,
                                                 208.47743225097656,
                                                 0.08452831208705902,
                                                 0.08444917947053909,
                                                 0.08455681055784225,
                                                 0.08457385748624802,
                                                 0.08448221534490585,
                                                 0.08441825956106186,
                                                 0.08439182490110397,
                                                 0.08427322655916214,
                                                 0.08453415334224701,
                                                 0.08448854833841324,
                                                 0.08462214469909668,
                                                 0.08452510088682175,
                                                 0.08454762399196625,
                                                 0.08453730493783951,
                                                 0.08453856408596039,
                                                 0.08457324653863907,
                                                 0.08439980447292328,
                                                 0.08457770943641663,
                                                 0.08448946475982666,
                                                 0.08455544710159302,
                                                 0.08448326587677002,
                                                 0.0853704884648323,
                                                 0.0845303013920784,
                                                 0.08449558913707733,
                                                 0.08448614180088043,
                                                 0.08432622253894806,
                                                 0.08439280092716217,
                                                 0.08428298681974411,
                                                 0.08441869169473648,
                                                 2.7614922523498535,
                                                 0.0844237357378006,
                                                 0.0845165103673935,
                                                 3.4590933322906494,
                                                 0.08449532091617584,
                                                 0.0845257043838501,
                                                 0.08447345346212387,
                                                 0.08450763672590256,
                                                 0.08446920663118362,
                                                 0.08451981097459793,
                                                 385.1795959472656,
                                                 0.08447851985692978,
                                                 0.08457182347774506,
                                                 0.08445488661527634,
                                                 0.7349956631660461,
                                                 0.0845390260219574,
                                                 437.2469787597656,
                                                 0.08416557312011719,
                                                 0.08421652764081955,
                                                 0.08440131694078445,
                                                 0.08433077484369278,
                                                 0.08428847789764404,
                                                 0.08411938697099686,
                                                 0.08432593941688538,
                                                 0.08455510437488556,
                                                 0.08454146236181259,
                                                 0.08451537787914276,
                                                 0.08447898924350739,
                                                 0.08454417437314987,
                                                 0.08451075106859207,
                                                 0.08446026593446732,
                                                 0.08439700305461884,
                                                 0.08444203436374664,
                                                 324.5517578125,
                                                 0.084525465965271,
                                                 0.08450353890657425,
                                                 0.08454328775405884,
                                                 0.08446735888719559,
                                                 0.0844758078455925,
                                                 0.08446711301803589,
                                                 0.08434131741523743,
                                                 0.0844181552529335,
                                                 0.08407874405384064,
                                                 0.08442263305187225,
                                                 0.08430887013673782,
                                                 0.08445868641138077,
                                                 0.08432488143444061,
                                                 0.08438342809677124,
                                                 0.08434324711561203,
                                                 0.08433690667152405,
                                                 0.08442634344100952,
                                                 0.08430664241313934,
                                                 0.08431776612997055,
                                                 3179.368408203125,
                                                 0.08431804180145264,
                                                 0.0844300165772438,
                                                 327.7389831542969,
                                                 0.08442189544439316,
                                                 0.08446188271045685,
                                                 0.08456380665302277,
                                                 0.08448801934719086,
                                                 0.08452309668064117,
                                                 0.08443786203861237,
                                                 0.08450938016176224,
                                                 0.08452746272087097,
                                                 0.0845022052526474,
                                                 0.08435150235891342,
                                                 0.0843622088432312,
                                                 0.08451417833566666,
                                                 0.08449152857065201,
                                                 2792218.0,
                                                 0.08448152989149094,
                                                 0.08447089046239853,
                                                 0.08448109775781631,
                                                 0.08455649018287659,
                                                 11.825798988342285,
                                                 0.0844212993979454,
                                                 0.08447621017694473,
                                                 0.08445154875516891,
                                                 0.08450216054916382,
                                                 0.08448535948991776,
                                                 0.08452288061380386,
                                                 2.2303872108459473,
                                                 0.08445139229297638,
                                                 22.94561195373535,
                                                 0.08438808470964432,
                                                 0.08447092026472092,
                                                 0.0844135656952858,
                                                 0.08435672521591187,
                                                 4.1318182945251465,
                                                 0.0844360813498497,
                                                 0.08448848873376846,
                                                 0.08441183716058731,
                                                 0.08452562987804413,
                                                 0.08434139937162399,
                                                 15509.2685546875,
                                                 0.08440298587083817,
                                                 0.0843038558959961,
                                                 0.08409386873245239,
                                                 0.08434053510427475,
                                                 0.08435279130935669,
                                                 0.0843585804104805,
                                                 1622.2698974609375,
                                                 6.130885601043701,
                                                 513.2769165039062,
                                                 0.08451929688453674,
                                                 0.0844656303524971,
                                                 0.08448315411806107,
                                                 0.08443838357925415,
                                                 657.5328979492188,
                                                 0.08391217887401581,
                                                 0.08355443924665451,
                                                 0.08408589661121368,
                                                 0.08395949006080627,
                                                 0.08368541300296783,
                                                 0.09440038353204727,
                                                 0.08442547917366028,
                                                 0.08448342233896255,
                                                 0.08452529460191727,
                                                 0.08449804782867432,
                                                 0.0958980917930603,
                                                 0.0944596529006958,
                                                 0.09449319541454315,
                                                 0.0945969820022583,
                                                 0.09455905109643936,
                                                 0.09455728530883789,
                                                 0.0946040078997612,
                                                 0.5912741422653198,
                                                 0.0945107489824295,
                                                 0.09456121921539307,
                                                 0.09452136605978012,
                                                 0.09451890736818314,
                                                 0.09491711109876633,
                                                 0.0936649888753891,
                                                 0.17639370262622833,
                                                 227.92086791992188,
                                                 0.0940268412232399,
                                                 0.21170301735401154,
                                                 0.09379062056541443,
                                                 0.09365381300449371,
                                                 0.09371867030858994,
                                                 0.09373131394386292,
                                                 0.09375094622373581,
                                                 0.09409426152706146,
                                                 0.09444309771060944,
                                                 2.2909059524536133,
                                                 408.70733642578125,
                                                 0.09448052197694778,
                                                 0.09452381730079651,
                                                 0.09452628344297409,
                                                 0.09451580047607422,
                                                 0.09457345306873322,
                                                 0.09455623477697372,
                                                 0.09454062581062317,
                                                 0.09454816579818726,
                                                 0.09447815269231796,
                                                 0.09455271065235138,
                                                 0.0945919007062912,
                                                 0.09438098222017288,
                                                 0.09447444975376129,
                                                 0.09452714771032333,
                                                 0.09452454000711441,
                                                 0.09455770254135132,
                                                 0.09453456103801727,
                                                 0.09453591704368591,
                                                 0.0945412740111351,
                                                 5.31784200668335,
                                                 0.09444911777973175,
                                                 1.2043771743774414,
                                                 0.09460223466157913,
                                                 0.09458258748054504,
                                                 0.09454575926065445,
                                                 0.09464789927005768,
                                                 0.09462427347898483,
                                                 0.2161496877670288,
                                                 0.09452006220817566,
                                                 0.0945173129439354,
                                                 0.09459420293569565,
                                                 45.2691764831543,
                                                 0.0946098044514656,
                                                 0.09459919482469559,
                                                 0.09453676640987396,
                                                 0.09455496072769165,
                                                 0.09455988556146622,
                                                 0.09453597664833069,
                                                 1460.126708984375,
                                                 0.09449618309736252,
                                                 0.09447479248046875,
                                                 0.0946631208062172,
                                                 0.09447914361953735,
                                                 0.09451417624950409,
                                                 0.09457054734230042,
                                                 578.6171264648438,
                                                 0.09508994966745377,
                                                 0.09456158429384232,
                                                 0.09453702718019485,
                                                 92.70061492919922,
                                                 575.2883911132812,
                                                 0.09449885785579681,
                                                 0.09459429234266281,
                                                 0.09453769028186798,
                                                 0.09457365423440933,
                                                 0.2661488950252533,
                                                 0.5785210728645325,
                                                 1.0697529315948486,
                                                 9533.2490234375,
                                                 36278084.0,
                                                 0.09451647102832794,
                                                 0.0945221334695816,
                                                 0.0945245772600174,
                                                 0.49862831830978394,
                                                 0.1389312595129013,
                                                 0.10536698997020721,
                                                 0.6453412175178528,
                                                 0.09668745845556259,
                                                 7082514.5,
                                                 0.09513116627931595,
                                                 0.09418614208698273,
                                                 0.09460048377513885,
                                                 0.09451752156019211,
                                                 0.09452079981565475,
                                                 0.11058186739683151,
                                                 0.09448027610778809,
                                                 95075.0,
                                                 0.09450844675302505,
                                                 0.40205472707748413,
                                                 0.09456601738929749,
                                                 21503.685546875,
                                                 25433.630859375,
                                                 0.09459114074707031,
                                                 1.827946662902832,
                                                 0.09454144537448883,
                                                 0.09408380091190338,
                                                 0.09439128637313843,
                                                 0.09441068768501282,
                                                 75790.1328125,
                                                 2222.14990234375,
                                                 0.09460681676864624,
                                                 0.09455598890781403,
                                                 0.09462475776672363,
                                                 0.09453312307596207,
                                                 0.0945381447672844,
                                                 0.09712633490562439,
                                                 15.168710708618164,
                                                 0.09447479993104935,
                                                 0.09447994828224182,
                                                 0.09455700218677521,
                                                 1.1578611135482788,
                                                 0.09453558921813965,
                                                 0.0945739671587944,
                                                 0.09440802037715912,
                                                 7.872211933135986,
                                                 0.09455535560846329,
                                                 0.09453728795051575,
                                                 0.09456964582204819,
                                                 0.09449167549610138,
                                                 403448.90625,
                                                 0.11504217982292175,
                                                 0.09461946040391922,
                                                 0.09448207169771194,
                                                 1.0811370611190796,
                                                 0.09459654241800308,
                                                 0.09441133588552475,
                                                 0.0943896472454071,
                                                 5106.0791015625,
                                                 0.09468978643417358,
                                                 0.09459315240383148,
                                                 0.09458445757627487,
                                                 12.510099411010742,
                                                 0.09466719627380371,
                                                 0.09455475211143494,
                                                 20.623863220214844,
                                                 0.09454236924648285,
                                                 8.770496368408203,
                                                 12.305490493774414,
                                                 0.09461303800344467,
                                                 0.09462690353393555,
                                                 0.09455135464668274,
                                                 0.09457971900701523,
                                                 0.09463999420404434,
                                                 0.09459368139505386,
                                                 39.026275634765625,
                                                 0.09459409862756729,
                                                 0.14653317630290985,
                                                 0.09447965025901794,
                                                 0.09448003023862839,
                                                 0.09420546144247055,
                                                 0.10086872428655624,
                                                 0.09402845054864883,
                                                 0.09396755695343018,
                                                 0.09385461360216141,
                                                 0.7617712020874023,
                                                 3.599776268005371,
                                                 0.0940251424908638,
                                                 0.09385236352682114,
                                                 1.5394641160964966,
                                                 555.6378784179688,
                                                 21.09204864501953,
                                                 0.09456101059913635,
                                                 0.09457747638225555,
                                                 0.20754975080490112,
                                                 0.09456492215394974,
                                                 0.09465780109167099,
                                                 0.09462746232748032,
                                                 0.09462113678455353,
                                                 4181469.0,
                                                 0.11877608299255371,
                                                 0.09787335991859436,
                                                 0.09376204758882523,
                                                 0.09390174597501755,
                                                 609.8927001953125,
                                                 0.0945817232131958,
                                                 56.260704040527344,
                                                 0.09460467100143433,
                                                 0.16732440888881683,
                                                 0.09459473192691803,
                                                 0.09458056092262268,
                                                 0.09451653063297272,
                                                 0.09456532448530197,
                                                 0.09456890821456909,
                                                 0.09472556412220001,
                                                 0.09467514604330063,
                                                 0.09467890858650208,
                                                 0.09461469203233719,
                                                 0.09461411088705063,
                                                 0.09406077861785889,
                                                 0.09436465054750443,
                                                 6.517511367797852,
                                                 0.09469257295131683,
                                                 22.83505630493164,
                                                 21.33751106262207,
                                                 0.09463278949260712,
                                                 1789.422119140625,
                                                 435.6246337890625,
                                                 0.24851779639720917,
                                                 0.09462091326713562,
                                                 614.681640625,
                                                 0.09470755606889725,
                                                 178.90574645996094,
                                                 0.09464573115110397,
                                                 0.09471963346004486,
                                                 0.09465103596448898,
                                                 0.6563721895217896,
                                                 704190.0,
                                                 0.09396739304065704,
                                                 0.09447432309389114,
                                                 0.09416063874959946,
                                                 0.0944053903222084,
                                                 65.62581634521484,
                                                 0.09450186043977737,
                                                 1024.021240234375,
                                                 0.09452372044324875,
                                                 0.09453118592500687,
                                                 0.09463518112897873,
                                                 0.09458576887845993,
                                                 0.10458575189113617,
                                                 47422.390625,
                                                 0.09453989565372467,
                                                 0.19003771245479584,
                                                 0.09361455589532852,
                                                 0.09875020384788513,
                                                 0.10593827068805695,
                                                 0.09450165182352066,
                                                 2.2607531547546387,
                                                 146.7675018310547,
                                                 0.09460194408893585,
                                                 0.09467209130525589,
                                                 0.09463077038526535,
                                                 0.09450284391641617,
                                                 0.09454219788312912,
                                                 0.09467772394418716,
                                                 0.09461057186126709,
                                                 301.4787902832031,
                                                 0.0944533422589302,
                                                 0.09459809958934784,
                                                 0.09464976191520691,
                                                 0.09458554536104202,
                                                 0.09468982368707657,
                                                 0.09466410428285599,
                                                 0.5748986601829529,
                                                 0.09391465783119202,
                                                 0.09361892193555832,
                                                 13.32568073272705,
                                                 0.09446100890636444,
                                                 77.79401397705078,
                                                 2.8922955989837646,
                                                 0.1703469306230545,
                                                 7399.919921875,
                                                 4709.5478515625,
                                                 0.09465955197811127,
                                                 0.09466127306222916,
                                                 0.09468183666467667,
                                                 0.09468203783035278,
                                                 19.022647857666016,
                                                 0.09639277309179306,
                                                 0.09449823945760727,
                                                 0.09443557262420654,
                                                 0.09912924468517303,
                                                 0.09453266113996506,
                                                 0.09452731162309647,
                                                 0.09462825953960419,
                                                 0.09465084969997406,
                                                 0.09445875138044357,
                                                 9395.8203125,
                                                 4131.25390625,
                                                 0.09448602050542831,
                                                 0.09457829594612122,
                                                 0.09452693164348602,
                                                 0.41866934299468994,
                                                 0.09443027526140213,
                                                 1.5612469911575317,
                                                 0.09447776526212692,
                                                 0.09464254975318909,
                                                 0.0947151929140091,
                                                 0.09461060166358948,
                                                 1.6179649829864502,
                                                 151.97926330566406,
                                                 0.09670396894216537,
                                                 0.09371296316385269,
                                                 0.22721385955810547,
                                                 0.09369426220655441,
                                                 0.09369219094514847,
                                                 0.0936376303434372,
                                                 519.0321655273438,
                                                 2733.38671875,
                                                 0.09362778812646866,
                                                 0.1038171648979187,
                                                 0.36694493889808655,
                                                 0.09366429597139359,
                                                 0.09367946535348892,
                                                 0.093511663377285,
                                                 363061.71875,
                                                 0.09354864805936813,
                                                 0.09355273097753525,
                                                 0.0940803587436676,
                                                 0.09443733096122742,
                                                 0.09446991235017776,
                                                 0.09449158608913422,
                                                 0.09442201256752014,
                                                 0.09459392726421356,
                                                 0.09461551159620285,
                                                 3.2891151905059814,
                                                 0.09446682780981064,
                                                 214763.484375,
                                                 0.0945248231291771,
                                                 0.0945204347372055,
                                                 1.0939825773239136,
                                                 0.09466331452131271,
                                                 0.09470713883638382,
                                                 0.09462140500545502,
                                                 0.0944676548242569,
                                                 0.09462480992078781,
                                                 0.0944826528429985,
                                                 62.18036651611328,
                                                 1.1048225164413452,
                                                 7.396787166595459,
                                                 0.09464084357023239,
                                                 0.09469672292470932,
                                                 0.09460265189409256,
                                                 37.437835693359375,
                                                 0.09448858350515366,
                                                 13.001826286315918,
                                                 45.448062896728516,
                                                 0.0946521982550621,
                                                 0.09455090761184692,
                                                 0.2181532233953476,
                                                 248.3521728515625,
                                                 0.094516322016716,
                                                 0.09450066834688187,
                                                 0.09460214525461197,
                                                 1419.5380859375,
                                                 0.09455197304487228,
                                                 89.7099380493164,
                                                 0.09380219131708145,
                                                 0.09361878782510757,
                                                 0.09440010786056519,
                                                 33.71955490112305,
                                                 0.0946076437830925,
                                                 0.09467274695634842,
                                                 0.09467364847660065,
                                                 6.568045139312744,
                                                 0.0946686714887619,
                                                 0.11860518157482147,
                                                 144.5919952392578,
                                                 0.0945608839392662,
                                                 0.0977327898144722,
                                                 0.09378885477781296,
                                                 0.4567955732345581,
                                                 0.28246936202049255,
                                                 0.0937114804983139,
                                                 3.4494714736938477,
                                                 0.09347078949213028,
                                                 1.9993853569030762,
                                                 694.1502685546875,
                                                 0.09461234509944916,
                                                 0.09463471919298172,
                                                 0.09461282938718796,
                                                 0.09465289115905762,
                                                 0.1086336076259613,
                                                 0.09450510889291763,
                                                 0.09465640783309937,
                                                 0.0946459248661995,
                                                 44.427127838134766,
                                                 0.0945444405078888,
                                                 0.09469397366046906,
                                                 0.09456057101488113,
                                                 0.09453918039798737,
                                                 0.09460191428661346,
                                                 16.45922088623047,
                                                 0.09442133456468582,
                                                 766.4246826171875,
                                                 0.0944996252655983,
                                                 57.03985595703125,
                                                 0.09441376477479935,
                                                 0.09458398073911667,
                                                 4.6840386390686035,
                                                 0.10124408453702927,
                                                 0.09464626759290695,
                                                 0.0945911556482315,
                                                 34.607200622558594],
                          'val_accuracy_history': [1.736842105263158,
                                                   2.210526315789474,
                                                   0.05263157894736842,
                                                   1.0,
                                                   0.8947368421052632,
                                                   0.9473684210526315,
                                                   0.2631578947368421,
                                                   0.6842105263157895,
                                                   1.5789473684210527,
                                                   0.7368421052631579,
                                                   1.0,
                                                   1.368421052631579,
                                                   0.8947368421052632,
                                                   0.0,
                                                   0.15789473684210525,
                                                   0.3684210526315789,
                                                   1.5789473684210527,
                                                   1.368421052631579,
                                                   1.4736842105263157,
                                                   0.2631578947368421,
                                                   0.3684210526315789,
                                                   1.631578947368421,
                                                   0.47368421052631576,
                                                   1.368421052631579,
                                                   0.5789473684210527,
                                                   0.42105263157894735,
                                                   1.0,
                                                   0.5789473684210527,
                                                   0.6842105263157895,
                                                   1.0,
                                                   0.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   2.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   1.105263157894737,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   2.0,
                                                   0.3684210526315789,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   4.0,
                                                   0.0,
                                                   0.0,
                                                   3.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   3.0,
                                                   1.0,
                                                   0.0,
                                                   2.0,
                                                   0.0,
                                                   4.0,
                                                   0.0,
                                                   1.0,
                                                   1.0526315789473684,
                                                   0.3157894736842105,
                                                   0.0,
                                                   2.0,
                                                   1.0,
                                                   1.4736842105263157,
                                                   0.42105263157894735,
                                                   1.0,
                                                   2.0,
                                                   1.0,
                                                   0.0,
                                                   2.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   2.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   1.4736842105263157,
                                                   1.0,
                                                   0.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   2.0,
                                                   1.0,
                                                   0.0,
                                                   3.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   2.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   2.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   3.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   2.0,
                                                   2.0,
                                                   2.0,
                                                   1.0,
                                                   1.0,
                                                   2.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   2.0,
                                                   3.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.15789473684210525,
                                                   0.0,
                                                   2.0,
                                                   2.0,
                                                   0.0,
                                                   2.0,
                                                   0.0,
                                                   3.0,
                                                   1.0,
                                                   1.0,
                                                   1.894736842105263,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   2.0,
                                                   4.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   2.0,
                                                   1.0,
                                                   0.8947368421052632,
                                                   0.21052631578947367,
                                                   1.5263157894736843,
                                                   0.21052631578947367,
                                                   0.5789473684210527,
                                                   0.2631578947368421,
                                                   0.8947368421052632,
                                                   1.0526315789473684,
                                                   1.0,
                                                   2.0,
                                                   2.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   0.6666666666666666,
                                                   3.0,
                                                   0.0,
                                                   0.2222222222222222,
                                                   1.0,
                                                   0.5555555555555556,
                                                   0.2222222222222222,
                                                   0.2222222222222222,
                                                   1.7777777777777777,
                                                   1.8888888888888888,
                                                   1.0,
                                                   2.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   2.0,
                                                   2.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   1.5555555555555556,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.8888888888888888,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   2.0,
                                                   0.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   0.2222222222222222,
                                                   1.7777777777777777,
                                                   0.4444444444444444,
                                                   0.0,
                                                   0.4444444444444444,
                                                   0.0,
                                                   2.0,
                                                   0.0,
                                                   0.2222222222222222,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   2.0,
                                                   1.0,
                                                   1.0,
                                                   3.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   0.7777777777777778,
                                                   0.8888888888888888,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   2.0,
                                                   3.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   5.0,
                                                   1.0,
                                                   2.0,
                                                   1.0,
                                                   2.0,
                                                   4.0,
                                                   4.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   2.0,
                                                   2.0,
                                                   1.0,
                                                   3.0,
                                                   1.0,
                                                   2.263157894736842,
                                                   3.0,
                                                   2.0,
                                                   1.6842105263157894,
                                                   0.0,
                                                   1.0,
                                                   3.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   2.0,
                                                   1.0,
                                                   5.0,
                                                   0.0,
                                                   2.789473684210526,
                                                   0.0,
                                                   1.894736842105263,
                                                   1.5789473684210527,
                                                   1.0,
                                                   2.0,
                                                   1.0,
                                                   2.0,
                                                   4.0,
                                                   2.0,
                                                   1.0,
                                                   3.0,
                                                   2.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   2.0,
                                                   5.0,
                                                   4.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   1.894736842105263,
                                                   1.0526315789473684,
                                                   1.4736842105263157,
                                                   0.0,
                                                   0.0,
                                                   3.0,
                                                   1.894736842105263,
                                                   3.0,
                                                   2.0,
                                                   1.0,
                                                   2.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   0.631578947368421,
                                                   1.0,
                                                   2.0,
                                                   3.0,
                                                   2.0,
                                                   2.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   3.0,
                                                   1.0,
                                                   0.0,
                                                   5.0,
                                                   2.0,
                                                   0.0,
                                                   3.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   2.0,
                                                   2.0,
                                                   0.0,
                                                   1.6842105263157894,
                                                   2.0526315789473686,
                                                   1.6842105263157894,
                                                   2.4210526315789473,
                                                   1.894736842105263,
                                                   2.526315789473684,
                                                   1.7894736842105263,
                                                   1.631578947368421,
                                                   1.5789473684210527,
                                                   1.631578947368421,
                                                   0.7894736842105263,
                                                   1.0,
                                                   1.7894736842105263,
                                                   0.8947368421052632,
                                                   1.0526315789473684,
                                                   2.6315789473684212,
                                                   1.1578947368421053,
                                                   3.0,
                                                   1.0,
                                                   3.0,
                                                   2.0,
                                                   2.0,
                                                   2.0,
                                                   3.0,
                                                   0.0,
                                                   2.0,
                                                   2.0,
                                                   2.0,
                                                   2.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   1.0,
                                                   2.0,
                                                   2.0,
                                                   4.0,
                                                   1.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   2.0,
                                                   1.0,
                                                   0.0,
                                                   3.0,
                                                   4.0,
                                                   1.0,
                                                   2.0,
                                                   3.0,
                                                   1.1578947368421053,
                                                   0.0,
                                                   1.0,
                                                   2.0,
                                                   2.0,
                                                   1.0,
                                                   3.0,
                                                   2.1052631578947367,
                                                   3.9473684210526314,
                                                   0.3157894736842105,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   3.0,
                                                   3.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   2.0,
                                                   0.9473684210526315,
                                                   1.8421052631578947,
                                                   1.631578947368421,
                                                   2.1052631578947367,
                                                   1.4736842105263157,
                                                   1.1578947368421053,
                                                   2.8421052631578947,
                                                   3.0,
                                                   0.21052631578947367,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   3.0,
                                                   0.0,
                                                   0.0,
                                                   3.0,
                                                   0.0,
                                                   2.0,
                                                   1.0,
                                                   0.0,
                                                   3.0,
                                                   1.0,
                                                   0.0,
                                                   2.0,
                                                   1.0,
                                                   0.0,
                                                   3.0,
                                                   1.0,
                                                   1.0,
                                                   2.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   3.0,
                                                   0.0],
                          'val_loss': 0.08975100517272949,
                          'val_loss_history': [0.12889361381530762,
                                               0.12675809860229492,
                                               929197312.0,
                                               0.12727877497673035,
                                               0.12555724382400513,
                                               1098212864.0,
                                               0.13582992553710938,
                                               0.1321955770254135,
                                               0.1356307566165924,
                                               1054040704.0,
                                               0.13556765019893646,
                                               0.14156989753246307,
                                               0.13816937804222107,
                                               268273632.0,
                                               0.13720349967479706,
                                               0.13542649149894714,
                                               0.1288645714521408,
                                               0.1226494163274765,
                                               0.12051358073949814,
                                               0.12420963495969772,
                                               0.12739074230194092,
                                               0.13060890138149261,
                                               0.1263168305158615,
                                               0.12670856714248657,
                                               0.11823848634958267,
                                               0.12219959497451782,
                                               0.11411118507385254,
                                               0.12208209931850433,
                                               0.12046002596616745,
                                               0.11428070068359375,
                                               0.11432527750730515,
                                               0.11463426798582077,
                                               0.1144825890660286,
                                               0.11444626748561859,
                                               0.11485874652862549,
                                               0.11457077413797379,
                                               0.1145758330821991,
                                               0.11442591995000839,
                                               0.1148984432220459,
                                               0.1149170994758606,
                                               0.11488619446754456,
                                               0.11486360430717468,
                                               0.114899180829525,
                                               0.1149524450302124,
                                               0.11508650332689285,
                                               0.1152985617518425,
                                               0.11511247605085373,
                                               223967184.0,
                                               0.1151353120803833,
                                               0.11522237956523895,
                                               0.11538894474506378,
                                               0.11536173522472382,
                                               0.11543510854244232,
                                               0.11553690582513809,
                                               0.11534439027309418,
                                               0.1152355745434761,
                                               0.11538137495517731,
                                               0.11679191887378693,
                                               0.11562331020832062,
                                               0.11570201814174652,
                                               0.11553406715393066,
                                               0.11560743302106857,
                                               0.11558980494737625,
                                               0.11557500809431076,
                                               0.11566941440105438,
                                               0.11569506675004959,
                                               0.1155826523900032,
                                               0.11563906818628311,
                                               0.11583434790372849,
                                               0.11583562195301056,
                                               0.11584048718214035,
                                               0.11579956114292145,
                                               0.1158096119761467,
                                               0.11577166616916656,
                                               0.11578544974327087,
                                               0.11576415598392487,
                                               0.11577217280864716,
                                               0.1157451793551445,
                                               0.1157572790980339,
                                               0.1158934161067009,
                                               0.11580001562833786,
                                               0.11593358963727951,
                                               0.11615775525569916,
                                               0.11587931215763092,
                                               0.1159670352935791,
                                               0.11585234105587006,
                                               0.11583954095840454,
                                               0.11609325557947159,
                                               0.11583058536052704,
                                               0.11599991470575333,
                                               0.11598140746355057,
                                               0.11592470854520798,
                                               0.11593374609947205,
                                               0.11602354794740677,
                                               0.11578560620546341,
                                               0.11599435657262802,
                                               0.11563146859407425,
                                               0.11705952137708664,
                                               0.11598608642816544,
                                               0.11604960262775421,
                                               0.11593892425298691,
                                               0.1176956295967102,
                                               0.12361403554677963,
                                               0.11607111245393753,
                                               0.11594823747873306,
                                               0.11593722552061081,
                                               0.11586553603410721,
                                               0.11618652194738388,
                                               0.11610959470272064,
                                               0.11593214422464371,
                                               0.11584636569023132,
                                               0.1159195527434349,
                                               0.11607397347688675,
                                               0.1159905418753624,
                                               0.1159055307507515,
                                               0.11624743789434433,
                                               0.11588098853826523,
                                               0.11602722853422165,
                                               0.11587972939014435,
                                               0.11616445332765579,
                                               0.11607798933982849,
                                               0.11569825559854507,
                                               0.11504572629928589,
                                               0.11600799113512039,
                                               0.11600035429000854,
                                               0.11612346768379211,
                                               0.11608055979013443,
                                               0.11566175520420074,
                                               0.1160079687833786,
                                               0.11582132428884506,
                                               0.11605935543775558,
                                               0.11587560176849365,
                                               0.11589808017015457,
                                               0.11591998487710953,
                                               0.11596931517124176,
                                               0.11618545651435852,
                                               0.11614586412906647,
                                               0.116184763610363,
                                               0.11593402177095413,
                                               0.11613350361585617,
                                               0.11606571078300476,
                                               0.11616233736276627,
                                               0.11608292907476425,
                                               0.11600427329540253,
                                               0.11627335101366043,
                                               0.11593248695135117,
                                               0.11602398753166199,
                                               0.11632940173149109,
                                               0.11617862433195114,
                                               0.11615864932537079,
                                               0.11609946191310883,
                                               0.11620981246232986,
                                               0.11623332649469376,
                                               0.11621405184268951,
                                               0.11617275327444077,
                                               0.11618194729089737,
                                               0.11618026345968246,
                                               0.11604853719472885,
                                               0.11610307544469833,
                                               0.11616066843271255,
                                               0.11609618365764618,
                                               0.11606502532958984,
                                               0.11607180535793304,
                                               0.11610879749059677,
                                               0.11615218967199326,
                                               0.11595457047224045,
                                               0.1159059926867485,
                                               0.11616579443216324,
                                               0.11522948741912842,
                                               0.11619925498962402,
                                               0.11620134115219116,
                                               0.11613330245018005,
                                               0.11608000099658966,
                                               0.11618129909038544,
                                               0.11634077876806259,
                                               0.11618518084287643,
                                               0.11617344617843628,
                                               0.11647146195173264,
                                               0.11618098616600037,
                                               0.11633583903312683,
                                               0.11608415842056274,
                                               0.11629339307546616,
                                               0.11636802554130554,
                                               0.11612612754106522,
                                               0.11618626117706299,
                                               0.11609125882387161,
                                               0.11604174226522446,
                                               0.11616165190935135,
                                               0.11629405617713928,
                                               0.12194935232400894,
                                               0.11632195115089417,
                                               0.1196022555232048,
                                               0.11585008352994919,
                                               0.12247772514820099,
                                               0.12818971276283264,
                                               0.12019037455320358,
                                               0.12467972934246063,
                                               0.11633002012968063,
                                               0.1162014976143837,
                                               0.11621222645044327,
                                               0.09137231856584549,
                                               0.09150426834821701,
                                               0.09157953411340714,
                                               0.09157821536064148,
                                               0.09164956212043762,
                                               0.09170088171958923,
                                               0.09174489974975586,
                                               0.09180380403995514,
                                               0.09149286895990372,
                                               0.09157092124223709,
                                               0.09140341728925705,
                                               0.09140137583017349,
                                               0.09056924283504486,
                                               0.09073775261640549,
                                               0.09158705919981003,
                                               0.0913533940911293,
                                               2.4094927310943604,
                                               0.09106162190437317,
                                               0.0903748944401741,
                                               0.09066660702228546,
                                               0.09095553308725357,
                                               0.09037058055400848,
                                               0.09065017104148865,
                                               0.09140996634960175,
                                               0.0915386751294136,
                                               0.09155571460723877,
                                               0.09167596697807312,
                                               0.09157241880893707,
                                               0.09159300476312637,
                                               0.09173470735549927,
                                               0.09168659150600433,
                                               0.09159538149833679,
                                               0.09157337993383408,
                                               0.09162420779466629,
                                               0.0916445180773735,
                                               0.09159063547849655,
                                               0.09151946753263474,
                                               0.09143755584955215,
                                               0.09167628735303879,
                                               0.09162566065788269,
                                               0.09144790470600128,
                                               0.09167981147766113,
                                               0.09183721989393234,
                                               0.09176892042160034,
                                               0.09148874878883362,
                                               0.09179455041885376,
                                               0.09149231016635895,
                                               0.09171196818351746,
                                               0.09166552871465683,
                                               0.09163936972618103,
                                               0.09166969358921051,
                                               0.09118418395519257,
                                               0.09167936444282532,
                                               0.09164978563785553,
                                               0.0916447639465332,
                                               0.091525137424469,
                                               0.0916038230061531,
                                               0.09174405038356781,
                                               0.09161648899316788,
                                               0.09172605723142624,
                                               0.09169495105743408,
                                               0.09160944074392319,
                                               0.09177764505147934,
                                               0.09175115078687668,
                                               0.09168726205825806,
                                               0.09168113023042679,
                                               0.0916929617524147,
                                               0.0916459709405899,
                                               0.09187834709882736,
                                               0.09167644381523132,
                                               0.0916120707988739,
                                               0.09165194630622864,
                                               0.09167271107435226,
                                               0.09164658933877945,
                                               0.09172767400741577,
                                               0.09166054427623749,
                                               0.09191282838582993,
                                               0.0919322669506073,
                                               0.09164082258939743,
                                               0.09164590388536453,
                                               0.09164344519376755,
                                               0.09161072224378586,
                                               0.09165661782026291,
                                               0.09178908169269562,
                                               0.09179839491844177,
                                               0.09184478968381882,
                                               0.09157416224479675,
                                               0.09172283858060837,
                                               0.09165951609611511,
                                               0.09154213964939117,
                                               0.09159321337938309,
                                               0.09175139665603638,
                                               0.0916604995727539,
                                               0.09152602404356003,
                                               0.09157373756170273,
                                               0.09160105139017105,
                                               0.09054151177406311,
                                               0.09138046205043793,
                                               0.0918545126914978,
                                               0.09164437651634216,
                                               0.09164205938577652,
                                               0.09161880612373352,
                                               0.09176848083734512,
                                               0.0915747582912445,
                                               0.09165892750024796,
                                               0.09155618399381638,
                                               0.09165655821561813,
                                               0.09176354855298996,
                                               0.0917825847864151,
                                               0.09167511016130447,
                                               0.09172207117080688,
                                               0.09178581088781357,
                                               0.09048595279455185,
                                               0.09156113862991333,
                                               0.09158486872911453,
                                               0.09151524305343628,
                                               0.09164683520793915,
                                               0.09167765825986862,
                                               0.09159915894269943,
                                               0.09176617115736008,
                                               0.09172510355710983,
                                               0.09160268306732178,
                                               0.09161321818828583,
                                               0.0915767028927803,
                                               0.0914856418967247,
                                               0.0916421189904213,
                                               0.09157934784889221,
                                               0.09156793355941772,
                                               0.09154218435287476,
                                               0.09154070168733597,
                                               134901216.0,
                                               0.09168075770139694,
                                               0.09144774079322815,
                                               0.09149137884378433,
                                               0.09155774861574173,
                                               0.09148041158914566,
                                               0.09169764071702957,
                                               0.0916762501001358,
                                               0.09158093482255936,
                                               0.09154773503541946,
                                               0.0916462242603302,
                                               0.0915762335062027,
                                               0.09153687953948975,
                                               0.09146840870380402,
                                               0.09179212152957916,
                                               0.0917789414525032,
                                               0.09176136553287506,
                                               0.09159605950117111,
                                               0.09162770956754684,
                                               0.09160606563091278,
                                               0.09176835417747498,
                                               0.09157654643058777,
                                               0.0916082113981247,
                                               0.09162461757659912,
                                               0.09182053804397583,
                                               0.09177893400192261,
                                               0.09165219217538834,
                                               0.09159030020236969,
                                               0.09154009819030762,
                                               0.09164740145206451,
                                               0.09176388382911682,
                                               0.0915454551577568,
                                               0.09177467226982117,
                                               0.09191010892391205,
                                               0.09156614542007446,
                                               0.09143801033496857,
                                               0.09114047884941101,
                                               0.09068604558706284,
                                               0.09070765227079391,
                                               0.09103754162788391,
                                               0.09052861481904984,
                                               0.09132906794548035,
                                               0.09160716831684113,
                                               0.09083017706871033,
                                               0.09090922772884369,
                                               0.09165212512016296,
                                               0.09168437123298645,
                                               0.09145098924636841,
                                               0.09153489023447037,
                                               8632.75390625,
                                               0.09173749387264252,
                                               0.09159638732671738,
                                               0.09155716747045517,
                                               0.09143556654453278,
                                               0.09150324761867523,
                                               0.09115756303071976,
                                               0.09095091372728348,
                                               0.09065255522727966,
                                               0.09077665954828262,
                                               0.09125294536352158,
                                               0.09146241843700409,
                                               0.0917263850569725,
                                               0.09158481657505035,
                                               0.09154310077428818,
                                               0.09155087172985077,
                                               0.09157835692167282,
                                               0.09175891429185867,
                                               0.09174496680498123,
                                               0.0915360376238823,
                                               0.0916609838604927,
                                               0.09006042033433914,
                                               0.09004908800125122,
                                               0.08979449421167374,
                                               0.0896245688199997,
                                               0.08980648964643478,
                                               0.0895053967833519,
                                               0.08965669572353363,
                                               0.09002640843391418,
                                               0.08996382355690002,
                                               0.08966709673404694,
                                               0.0897647812962532,
                                               0.08978300541639328,
                                               0.08981018513441086,
                                               0.0896415039896965,
                                               0.0901375487446785,
                                               0.089830681681633,
                                               0.09017007797956467,
                                               0.0898798406124115,
                                               0.09025020152330399,
                                               0.0899721086025238,
                                               0.08988645672798157,
                                               0.09003996104001999,
                                               0.08998358994722366,
                                               0.0893966481089592,
                                               0.08915132284164429,
                                               0.08969417959451675,
                                               0.08966122567653656,
                                               0.08971626311540604,
                                               0.08974547684192657,
                                               0.08988585323095322,
                                               0.09006890654563904,
                                               0.09015759825706482,
                                               0.08985879272222519,
                                               0.09011870622634888,
                                               0.089875727891922,
                                               0.08984020352363586,
                                               0.08999879658222198,
                                               0.08929476141929626,
                                               0.08917342871427536,
                                               0.0890253409743309,
                                               0.0890451967716217,
                                               0.08985283225774765,
                                               0.08977603167295456,
                                               0.08965098857879639,
                                               0.09000735729932785,
                                               0.0896926000714302,
                                               0.0897701233625412,
                                               0.08995763957500458,
                                               0.0898318812251091,
                                               0.08998756855726242,
                                               0.09007910639047623,
                                               0.09007957577705383,
                                               0.08997736871242523,
                                               0.08961615711450577,
                                               0.09001019597053528,
                                               0.08989803493022919,
                                               0.09013975411653519,
                                               0.0902264267206192,
                                               0.0899982675909996,
                                               0.08958622068166733,
                                               0.08898205310106277,
                                               0.08928899466991425,
                                               0.08966180682182312,
                                               0.0896264836192131,
                                               0.0899060070514679,
                                               165786832.0,
                                               0.08958649635314941,
                                               0.08978855609893799,
                                               0.0899762436747551,
                                               0.09000223875045776,
                                               0.08978422731161118,
                                               0.09043993800878525,
                                               0.09028058499097824,
                                               0.1005210429430008,
                                               0.08993940055370331,
                                               0.08960286527872086,
                                               0.08972615748643875,
                                               0.08975361287593842,
                                               0.08965158462524414,
                                               0.08964148908853531,
                                               0.08972785621881485,
                                               0.0897819921374321,
                                               0.08990558981895447,
                                               0.08987738937139511,
                                               0.09000679105520248,
                                               0.08997351676225662,
                                               0.08983514457941055,
                                               0.09013941884040833,
                                               0.09014367312192917,
                                               0.09003666788339615,
                                               0.09019093215465546,
                                               0.08984865993261337,
                                               0.08995110541582108,
                                               0.08969511836767197,
                                               0.08997870236635208,
                                               0.08957649022340775,
                                               0.08970191329717636,
                                               0.08933154493570328,
                                               0.08878691494464874,
                                               0.08927902579307556,
                                               0.08946020156145096,
                                               0.08946799486875534,
                                               0.08914709836244583,
                                               0.08887948840856552,
                                               0.08919765800237656,
                                               0.0895165354013443,
                                               0.08895201981067657,
                                               0.08957294374704361,
                                               0.08974824845790863,
                                               0.08876839280128479,
                                               0.08959838002920151,
                                               0.08914162218570709,
                                               0.08941791206598282,
                                               0.08913740515708923,
                                               0.08939765393733978,
                                               0.09013853222131729,
                                               0.08985241502523422,
                                               0.0897478461265564,
                                               0.08989280462265015,
                                               0.08953097462654114,
                                               0.08996637165546417,
                                               0.08999262005090714,
                                               0.09000848233699799,
                                               0.08972601592540741,
                                               0.08974424004554749,
                                               0.08952955156564713,
                                               0.08970942348241806,
                                               0.0896824523806572,
                                               0.0899357870221138,
                                               0.0900910422205925,
                                               0.08987075090408325,
                                               0.09012211114168167,
                                               0.0899343490600586,
                                               0.08979938924312592,
                                               0.08996124565601349,
                                               0.09016357362270355,
                                               0.09030314534902573,
                                               0.09023184329271317,
                                               0.09009277075529099,
                                               0.08956193923950195,
                                               0.09018462896347046,
                                               0.08991622179746628,
                                               0.08998739719390869,
                                               0.09010801464319229,
                                               0.08995979279279709,
                                               0.08951814472675323,
                                               0.09035605937242508,
                                               0.08999628573656082,
                                               0.08997727185487747,
                                               0.09033078700304031,
                                               0.08996672928333282,
                                               0.09028850495815277,
                                               0.0895623043179512,
                                               0.08894254267215729,
                                               0.09029853343963623,
                                               0.08997199684381485,
                                               0.08996271342039108,
                                               0.09009357541799545,
                                               0.09012578427791595,
                                               0.08983942866325378,
                                               0.08987858891487122,
                                               0.08974911272525787,
                                               0.08983664214611053,
                                               0.08988182246685028,
                                               0.0900752916932106,
                                               0.08942669630050659,
                                               0.08933917433023453,
                                               0.08779210597276688,
                                               0.08927730470895767,
                                               0.08909048140048981,
                                               0.0898263081908226,
                                               0.08931832760572433,
                                               0.08928386121988297,
                                               0.08991234749555588,
                                               0.08994374424219131,
                                               0.09000713378190994,
                                               0.08975329250097275,
                                               0.09016995877027512,
                                               0.08982622623443604,
                                               0.09043461084365845,
                                               0.09003646671772003,
                                               0.0898413360118866,
                                               0.08992080390453339,
                                               0.08976344019174576,
                                               0.09005337953567505,
                                               0.09002504497766495,
                                               0.08987294137477875,
                                               0.08990441262722015,
                                               0.08986706286668777,
                                               0.09007862210273743,
                                               0.08986128121614456,
                                               0.08975748717784882,
                                               0.0894976556301117,
                                               0.08977871388196945,
                                               0.08997070044279099,
                                               0.0896330401301384,
                                               0.09019531309604645,
                                               0.09008292108774185,
                                               0.09006659686565399,
                                               0.08975100517272949],
                          'weight_decay': 0.0005},
                 'macroF1': {'accuracy': 0.0,
                             'batch_size': 32,
                             'cv_score': 0.0053345869543231185,
                             'cv_val_accuracy': 0.6666666666666666,
                             'cv_val_loss': 0.09920807182788849,
                             'cv_val_macroF1': 0.0053345869543231185,
                             'cv_val_microF1': 0.066966913828706,
                             'epochs': 200,
                             'final_model': GGNN1(
  (ggnn): GatedGraphConv(60, num_layers=2)
  (fc1): Linear(in_features=60, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                             'kwargs': {'aggr_type': 'mean',
                                        'd1': 60,
                                        'd2': 50,
                                        'num_classes': 24,
                                        'num_layers': 2},
                             'learning_rate': 0.01,
                             'macroF1': 0.006748704489763125,
                             'microF1': 0.08414023372287145,
                             'model': <class 'TFM_graph_classification_models.GGNN1'>,
                             'model_instance': GGNN1(
  (ggnn): GatedGraphConv(60, num_layers=2)
  (fc1): Linear(in_features=60, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                             'score': 'f1_macro',
                             'time': 2764.828370809555,
                             'train_loss_history': [362103776.0,
                                                    1984.16552734375,
                                                    4208493824.0,
                                                    660907968.0,
                                                    0.08377974480390549,
                                                    1243555968.0,
                                                    1017.9180297851562,
                                                    76571.46875,
                                                    308.40338134765625,
                                                    23220.69921875,
                                                    156603248.0,
                                                    138.022216796875,
                                                    0.08351406455039978,
                                                    1823248640.0,
                                                    155343744.0,
                                                    0.0847235918045044,
                                                    0.08452852815389633,
                                                    0.08426041901111603,
                                                    0.0840001031756401,
                                                    0.0840369313955307,
                                                    0.0836704894900322,
                                                    0.08393912762403488,
                                                    0.08424623310565948,
                                                    0.08425412327051163,
                                                    0.08409769088029861,
                                                    0.08399365842342377,
                                                    0.08431313186883926,
                                                    0.08411368727684021,
                                                    19.82664680480957,
                                                    0.08441364020109177,
                                                    0.08446607738733292,
                                                    0.08450149744749069,
                                                    0.08464515209197998,
                                                    0.08455231040716171,
                                                    0.08460909128189087,
                                                    0.08457177132368088,
                                                    0.0844896212220192,
                                                    0.08443144708871841,
                                                    7615.97705078125,
                                                    5.158772945404053,
                                                    0.08454208821058273,
                                                    0.08437103033065796,
                                                    0.08442801982164383,
                                                    0.08480384200811386,
                                                    0.08451271802186966,
                                                    0.0846419557929039,
                                                    0.08445870131254196,
                                                    0.08451291173696518,
                                                    7.508358001708984,
                                                    7220202.5,
                                                    208.47743225097656,
                                                    0.08452831208705902,
                                                    0.08444917947053909,
                                                    0.08455681055784225,
                                                    0.08457385748624802,
                                                    0.08448221534490585,
                                                    0.08441825956106186,
                                                    0.08439182490110397,
                                                    0.08427322655916214,
                                                    0.08453415334224701,
                                                    0.08448854833841324,
                                                    0.08462214469909668,
                                                    0.08452510088682175,
                                                    0.08454762399196625,
                                                    0.08453730493783951,
                                                    0.08453856408596039,
                                                    0.08457324653863907,
                                                    0.08439980447292328,
                                                    0.08457770943641663,
                                                    0.08448946475982666,
                                                    0.08455544710159302,
                                                    0.08448326587677002,
                                                    0.0853704884648323,
                                                    0.0845303013920784,
                                                    0.08449558913707733,
                                                    0.08448614180088043,
                                                    0.08432622253894806,
                                                    0.08439280092716217,
                                                    0.08428298681974411,
                                                    0.08441869169473648,
                                                    2.7614922523498535,
                                                    0.0844237357378006,
                                                    0.0845165103673935,
                                                    3.4590933322906494,
                                                    0.08449532091617584,
                                                    0.0845257043838501,
                                                    0.08447345346212387,
                                                    0.08450763672590256,
                                                    0.08446920663118362,
                                                    0.08451981097459793,
                                                    385.1795959472656,
                                                    0.08447851985692978,
                                                    0.08457182347774506,
                                                    0.08445488661527634,
                                                    0.7349956631660461,
                                                    0.0845390260219574,
                                                    437.2469787597656,
                                                    0.08416557312011719,
                                                    0.08421652764081955,
                                                    0.08440131694078445,
                                                    0.08433077484369278,
                                                    0.08428847789764404,
                                                    0.08411938697099686,
                                                    0.08432593941688538,
                                                    0.08455510437488556,
                                                    0.08454146236181259,
                                                    0.08451537787914276,
                                                    0.08447898924350739,
                                                    0.08454417437314987,
                                                    0.08451075106859207,
                                                    0.08446026593446732,
                                                    0.08439700305461884,
                                                    0.08444203436374664,
                                                    324.5517578125,
                                                    0.084525465965271,
                                                    0.08450353890657425,
                                                    0.08454328775405884,
                                                    0.08446735888719559,
                                                    0.0844758078455925,
                                                    0.08446711301803589,
                                                    0.08434131741523743,
                                                    0.0844181552529335,
                                                    0.08407874405384064,
                                                    0.08442263305187225,
                                                    0.08430887013673782,
                                                    0.08445868641138077,
                                                    0.08432488143444061,
                                                    0.08438342809677124,
                                                    0.08434324711561203,
                                                    0.08433690667152405,
                                                    0.08442634344100952,
                                                    0.08430664241313934,
                                                    0.08431776612997055,
                                                    3179.368408203125,
                                                    0.08431804180145264,
                                                    0.0844300165772438,
                                                    327.7389831542969,
                                                    0.08442189544439316,
                                                    0.08446188271045685,
                                                    0.08456380665302277,
                                                    0.08448801934719086,
                                                    0.08452309668064117,
                                                    0.08443786203861237,
                                                    0.08450938016176224,
                                                    0.08452746272087097,
                                                    0.0845022052526474,
                                                    0.08435150235891342,
                                                    0.0843622088432312,
                                                    0.08451417833566666,
                                                    0.08449152857065201,
                                                    2792218.0,
                                                    0.08448152989149094,
                                                    0.08447089046239853,
                                                    0.08448109775781631,
                                                    0.08455649018287659,
                                                    11.825798988342285,
                                                    0.0844212993979454,
                                                    0.08447621017694473,
                                                    0.08445154875516891,
                                                    0.08450216054916382,
                                                    0.08448535948991776,
                                                    0.08452288061380386,
                                                    2.2303872108459473,
                                                    0.08445139229297638,
                                                    22.94561195373535,
                                                    0.08438808470964432,
                                                    0.08447092026472092,
                                                    0.0844135656952858,
                                                    0.08435672521591187,
                                                    4.1318182945251465,
                                                    0.0844360813498497,
                                                    0.08448848873376846,
                                                    0.08441183716058731,
                                                    0.08452562987804413,
                                                    0.08434139937162399,
                                                    15509.2685546875,
                                                    0.08440298587083817,
                                                    0.0843038558959961,
                                                    0.08409386873245239,
                                                    0.08434053510427475,
                                                    0.08435279130935669,
                                                    0.0843585804104805,
                                                    1622.2698974609375,
                                                    6.130885601043701,
                                                    513.2769165039062,
                                                    0.08451929688453674,
                                                    0.0844656303524971,
                                                    0.08448315411806107,
                                                    0.08443838357925415,
                                                    657.5328979492188,
                                                    0.08391217887401581,
                                                    0.08355443924665451,
                                                    0.08408589661121368,
                                                    0.08395949006080627,
                                                    0.08368541300296783,
                                                    0.09440038353204727,
                                                    0.08442547917366028,
                                                    0.08448342233896255,
                                                    0.08452529460191727,
                                                    0.08449804782867432,
                                                    0.0958980917930603,
                                                    0.0944596529006958,
                                                    0.09449319541454315,
                                                    0.0945969820022583,
                                                    0.09455905109643936,
                                                    0.09455728530883789,
                                                    0.0946040078997612,
                                                    0.5912741422653198,
                                                    0.0945107489824295,
                                                    0.09456121921539307,
                                                    0.09452136605978012,
                                                    0.09451890736818314,
                                                    0.09491711109876633,
                                                    0.0936649888753891,
                                                    0.17639370262622833,
                                                    227.92086791992188,
                                                    0.0940268412232399,
                                                    0.21170301735401154,
                                                    0.09379062056541443,
                                                    0.09365381300449371,
                                                    0.09371867030858994,
                                                    0.09373131394386292,
                                                    0.09375094622373581,
                                                    0.09409426152706146,
                                                    0.09444309771060944,
                                                    2.2909059524536133,
                                                    408.70733642578125,
                                                    0.09448052197694778,
                                                    0.09452381730079651,
                                                    0.09452628344297409,
                                                    0.09451580047607422,
                                                    0.09457345306873322,
                                                    0.09455623477697372,
                                                    0.09454062581062317,
                                                    0.09454816579818726,
                                                    0.09447815269231796,
                                                    0.09455271065235138,
                                                    0.0945919007062912,
                                                    0.09438098222017288,
                                                    0.09447444975376129,
                                                    0.09452714771032333,
                                                    0.09452454000711441,
                                                    0.09455770254135132,
                                                    0.09453456103801727,
                                                    0.09453591704368591,
                                                    0.0945412740111351,
                                                    5.31784200668335,
                                                    0.09444911777973175,
                                                    1.2043771743774414,
                                                    0.09460223466157913,
                                                    0.09458258748054504,
                                                    0.09454575926065445,
                                                    0.09464789927005768,
                                                    0.09462427347898483,
                                                    0.2161496877670288,
                                                    0.09452006220817566,
                                                    0.0945173129439354,
                                                    0.09459420293569565,
                                                    45.2691764831543,
                                                    0.0946098044514656,
                                                    0.09459919482469559,
                                                    0.09453676640987396,
                                                    0.09455496072769165,
                                                    0.09455988556146622,
                                                    0.09453597664833069,
                                                    1460.126708984375,
                                                    0.09449618309736252,
                                                    0.09447479248046875,
                                                    0.0946631208062172,
                                                    0.09447914361953735,
                                                    0.09451417624950409,
                                                    0.09457054734230042,
                                                    578.6171264648438,
                                                    0.09508994966745377,
                                                    0.09456158429384232,
                                                    0.09453702718019485,
                                                    92.70061492919922,
                                                    575.2883911132812,
                                                    0.09449885785579681,
                                                    0.09459429234266281,
                                                    0.09453769028186798,
                                                    0.09457365423440933,
                                                    0.2661488950252533,
                                                    0.5785210728645325,
                                                    1.0697529315948486,
                                                    9533.2490234375,
                                                    36278084.0,
                                                    0.09451647102832794,
                                                    0.0945221334695816,
                                                    0.0945245772600174,
                                                    0.49862831830978394,
                                                    0.1389312595129013,
                                                    0.10536698997020721,
                                                    0.6453412175178528,
                                                    0.09668745845556259,
                                                    7082514.5,
                                                    0.09513116627931595,
                                                    0.09418614208698273,
                                                    0.09460048377513885,
                                                    0.09451752156019211,
                                                    0.09452079981565475,
                                                    0.11058186739683151,
                                                    0.09448027610778809,
                                                    95075.0,
                                                    0.09450844675302505,
                                                    0.40205472707748413,
                                                    0.09456601738929749,
                                                    21503.685546875,
                                                    25433.630859375,
                                                    0.09459114074707031,
                                                    1.827946662902832,
                                                    0.09454144537448883,
                                                    0.09408380091190338,
                                                    0.09439128637313843,
                                                    0.09441068768501282,
                                                    75790.1328125,
                                                    2222.14990234375,
                                                    0.09460681676864624,
                                                    0.09455598890781403,
                                                    0.09462475776672363,
                                                    0.09453312307596207,
                                                    0.0945381447672844,
                                                    0.09712633490562439,
                                                    15.168710708618164,
                                                    0.09447479993104935,
                                                    0.09447994828224182,
                                                    0.09455700218677521,
                                                    1.1578611135482788,
                                                    0.09453558921813965,
                                                    0.0945739671587944,
                                                    0.09440802037715912,
                                                    7.872211933135986,
                                                    0.09455535560846329,
                                                    0.09453728795051575,
                                                    0.09456964582204819,
                                                    0.09449167549610138,
                                                    403448.90625,
                                                    0.11504217982292175,
                                                    0.09461946040391922,
                                                    0.09448207169771194,
                                                    1.0811370611190796,
                                                    0.09459654241800308,
                                                    0.09441133588552475,
                                                    0.0943896472454071,
                                                    5106.0791015625,
                                                    0.09468978643417358,
                                                    0.09459315240383148,
                                                    0.09458445757627487,
                                                    12.510099411010742,
                                                    0.09466719627380371,
                                                    0.09455475211143494,
                                                    20.623863220214844,
                                                    0.09454236924648285,
                                                    8.770496368408203,
                                                    12.305490493774414,
                                                    0.09461303800344467,
                                                    0.09462690353393555,
                                                    0.09455135464668274,
                                                    0.09457971900701523,
                                                    0.09463999420404434,
                                                    0.09459368139505386,
                                                    39.026275634765625,
                                                    0.09459409862756729,
                                                    0.14653317630290985,
                                                    0.09447965025901794,
                                                    0.09448003023862839,
                                                    0.09420546144247055,
                                                    0.10086872428655624,
                                                    0.09402845054864883,
                                                    0.09396755695343018,
                                                    0.09385461360216141,
                                                    0.7617712020874023,
                                                    3.599776268005371,
                                                    0.0940251424908638,
                                                    0.09385236352682114,
                                                    1.5394641160964966,
                                                    555.6378784179688,
                                                    21.09204864501953,
                                                    0.09456101059913635,
                                                    0.09457747638225555,
                                                    0.20754975080490112,
                                                    0.09456492215394974,
                                                    0.09465780109167099,
                                                    0.09462746232748032,
                                                    0.09462113678455353,
                                                    4181469.0,
                                                    0.11877608299255371,
                                                    0.09787335991859436,
                                                    0.09376204758882523,
                                                    0.09390174597501755,
                                                    609.8927001953125,
                                                    0.0945817232131958,
                                                    56.260704040527344,
                                                    0.09460467100143433,
                                                    0.16732440888881683,
                                                    0.09459473192691803,
                                                    0.09458056092262268,
                                                    0.09451653063297272,
                                                    0.09456532448530197,
                                                    0.09456890821456909,
                                                    0.09472556412220001,
                                                    0.09467514604330063,
                                                    0.09467890858650208,
                                                    0.09461469203233719,
                                                    0.09461411088705063,
                                                    0.09406077861785889,
                                                    0.09436465054750443,
                                                    6.517511367797852,
                                                    0.09469257295131683,
                                                    22.83505630493164,
                                                    21.33751106262207,
                                                    0.09463278949260712,
                                                    1789.422119140625,
                                                    435.6246337890625,
                                                    0.24851779639720917,
                                                    0.09462091326713562,
                                                    614.681640625,
                                                    0.09470755606889725,
                                                    178.90574645996094,
                                                    0.09464573115110397,
                                                    0.09471963346004486,
                                                    0.09465103596448898,
                                                    0.6563721895217896,
                                                    704190.0,
                                                    0.09396739304065704,
                                                    0.09447432309389114,
                                                    0.09416063874959946,
                                                    0.0944053903222084,
                                                    65.62581634521484,
                                                    0.09450186043977737,
                                                    1024.021240234375,
                                                    0.09452372044324875,
                                                    0.09453118592500687,
                                                    0.09463518112897873,
                                                    0.09458576887845993,
                                                    0.10458575189113617,
                                                    47422.390625,
                                                    0.09453989565372467,
                                                    0.19003771245479584,
                                                    0.09361455589532852,
                                                    0.09875020384788513,
                                                    0.10593827068805695,
                                                    0.09450165182352066,
                                                    2.2607531547546387,
                                                    146.7675018310547,
                                                    0.09460194408893585,
                                                    0.09467209130525589,
                                                    0.09463077038526535,
                                                    0.09450284391641617,
                                                    0.09454219788312912,
                                                    0.09467772394418716,
                                                    0.09461057186126709,
                                                    301.4787902832031,
                                                    0.0944533422589302,
                                                    0.09459809958934784,
                                                    0.09464976191520691,
                                                    0.09458554536104202,
                                                    0.09468982368707657,
                                                    0.09466410428285599,
                                                    0.5748986601829529,
                                                    0.09391465783119202,
                                                    0.09361892193555832,
                                                    13.32568073272705,
                                                    0.09446100890636444,
                                                    77.79401397705078,
                                                    2.8922955989837646,
                                                    0.1703469306230545,
                                                    7399.919921875,
                                                    4709.5478515625,
                                                    0.09465955197811127,
                                                    0.09466127306222916,
                                                    0.09468183666467667,
                                                    0.09468203783035278,
                                                    19.022647857666016,
                                                    0.09639277309179306,
                                                    0.09449823945760727,
                                                    0.09443557262420654,
                                                    0.09912924468517303,
                                                    0.09453266113996506,
                                                    0.09452731162309647,
                                                    0.09462825953960419,
                                                    0.09465084969997406,
                                                    0.09445875138044357,
                                                    9395.8203125,
                                                    4131.25390625,
                                                    0.09448602050542831,
                                                    0.09457829594612122,
                                                    0.09452693164348602,
                                                    0.41866934299468994,
                                                    0.09443027526140213,
                                                    1.5612469911575317,
                                                    0.09447776526212692,
                                                    0.09464254975318909,
                                                    0.0947151929140091,
                                                    0.09461060166358948,
                                                    1.6179649829864502,
                                                    151.97926330566406,
                                                    0.09670396894216537,
                                                    0.09371296316385269,
                                                    0.22721385955810547,
                                                    0.09369426220655441,
                                                    0.09369219094514847,
                                                    0.0936376303434372,
                                                    519.0321655273438,
                                                    2733.38671875,
                                                    0.09362778812646866,
                                                    0.1038171648979187,
                                                    0.36694493889808655,
                                                    0.09366429597139359,
                                                    0.09367946535348892,
                                                    0.093511663377285,
                                                    363061.71875,
                                                    0.09354864805936813,
                                                    0.09355273097753525,
                                                    0.0940803587436676,
                                                    0.09443733096122742,
                                                    0.09446991235017776,
                                                    0.09449158608913422,
                                                    0.09442201256752014,
                                                    0.09459392726421356,
                                                    0.09461551159620285,
                                                    3.2891151905059814,
                                                    0.09446682780981064,
                                                    214763.484375,
                                                    0.0945248231291771,
                                                    0.0945204347372055,
                                                    1.0939825773239136,
                                                    0.09466331452131271,
                                                    0.09470713883638382,
                                                    0.09462140500545502,
                                                    0.0944676548242569,
                                                    0.09462480992078781,
                                                    0.0944826528429985,
                                                    62.18036651611328,
                                                    1.1048225164413452,
                                                    7.396787166595459,
                                                    0.09464084357023239,
                                                    0.09469672292470932,
                                                    0.09460265189409256,
                                                    37.437835693359375,
                                                    0.09448858350515366,
                                                    13.001826286315918,
                                                    45.448062896728516,
                                                    0.0946521982550621,
                                                    0.09455090761184692,
                                                    0.2181532233953476,
                                                    248.3521728515625,
                                                    0.094516322016716,
                                                    0.09450066834688187,
                                                    0.09460214525461197,
                                                    1419.5380859375,
                                                    0.09455197304487228,
                                                    89.7099380493164,
                                                    0.09380219131708145,
                                                    0.09361878782510757,
                                                    0.09440010786056519,
                                                    33.71955490112305,
                                                    0.0946076437830925,
                                                    0.09467274695634842,
                                                    0.09467364847660065,
                                                    6.568045139312744,
                                                    0.0946686714887619,
                                                    0.11860518157482147,
                                                    144.5919952392578,
                                                    0.0945608839392662,
                                                    0.0977327898144722,
                                                    0.09378885477781296,
                                                    0.4567955732345581,
                                                    0.28246936202049255,
                                                    0.0937114804983139,
                                                    3.4494714736938477,
                                                    0.09347078949213028,
                                                    1.9993853569030762,
                                                    694.1502685546875,
                                                    0.09461234509944916,
                                                    0.09463471919298172,
                                                    0.09461282938718796,
                                                    0.09465289115905762,
                                                    0.1086336076259613,
                                                    0.09450510889291763,
                                                    0.09465640783309937,
                                                    0.0946459248661995,
                                                    44.427127838134766,
                                                    0.0945444405078888,
                                                    0.09469397366046906,
                                                    0.09456057101488113,
                                                    0.09453918039798737,
                                                    0.09460191428661346,
                                                    16.45922088623047,
                                                    0.09442133456468582,
                                                    766.4246826171875,
                                                    0.0944996252655983,
                                                    57.03985595703125,
                                                    0.09441376477479935,
                                                    0.09458398073911667,
                                                    4.6840386390686035,
                                                    0.10124408453702927,
                                                    0.09464626759290695,
                                                    0.0945911556482315,
                                                    34.607200622558594],
                             'val_accuracy_history': [1.736842105263158,
                                                      2.210526315789474,
                                                      0.05263157894736842,
                                                      1.0,
                                                      0.8947368421052632,
                                                      0.9473684210526315,
                                                      0.2631578947368421,
                                                      0.6842105263157895,
                                                      1.5789473684210527,
                                                      0.7368421052631579,
                                                      1.0,
                                                      1.368421052631579,
                                                      0.8947368421052632,
                                                      0.0,
                                                      0.15789473684210525,
                                                      0.3684210526315789,
                                                      1.5789473684210527,
                                                      1.368421052631579,
                                                      1.4736842105263157,
                                                      0.2631578947368421,
                                                      0.3684210526315789,
                                                      1.631578947368421,
                                                      0.47368421052631576,
                                                      1.368421052631579,
                                                      0.5789473684210527,
                                                      0.42105263157894735,
                                                      1.0,
                                                      0.5789473684210527,
                                                      0.6842105263157895,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.105263157894737,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      0.3684210526315789,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      4.0,
                                                      0.0,
                                                      0.0,
                                                      3.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      3.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      4.0,
                                                      0.0,
                                                      1.0,
                                                      1.0526315789473684,
                                                      0.3157894736842105,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      1.4736842105263157,
                                                      0.42105263157894735,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.4736842105263157,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      3.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      3.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      3.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.15789473684210525,
                                                      0.0,
                                                      2.0,
                                                      2.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      3.0,
                                                      1.0,
                                                      1.0,
                                                      1.894736842105263,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      4.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      0.8947368421052632,
                                                      0.21052631578947367,
                                                      1.5263157894736843,
                                                      0.21052631578947367,
                                                      0.5789473684210527,
                                                      0.2631578947368421,
                                                      0.8947368421052632,
                                                      1.0526315789473684,
                                                      1.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.6666666666666666,
                                                      3.0,
                                                      0.0,
                                                      0.2222222222222222,
                                                      1.0,
                                                      0.5555555555555556,
                                                      0.2222222222222222,
                                                      0.2222222222222222,
                                                      1.7777777777777777,
                                                      1.8888888888888888,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.5555555555555556,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.8888888888888888,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.2222222222222222,
                                                      1.7777777777777777,
                                                      0.4444444444444444,
                                                      0.0,
                                                      0.4444444444444444,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.2222222222222222,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      3.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      0.7777777777777778,
                                                      0.8888888888888888,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      3.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      5.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      4.0,
                                                      4.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      3.0,
                                                      1.0,
                                                      2.263157894736842,
                                                      3.0,
                                                      2.0,
                                                      1.6842105263157894,
                                                      0.0,
                                                      1.0,
                                                      3.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      5.0,
                                                      0.0,
                                                      2.789473684210526,
                                                      0.0,
                                                      1.894736842105263,
                                                      1.5789473684210527,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      4.0,
                                                      2.0,
                                                      1.0,
                                                      3.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      5.0,
                                                      4.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.894736842105263,
                                                      1.0526315789473684,
                                                      1.4736842105263157,
                                                      0.0,
                                                      0.0,
                                                      3.0,
                                                      1.894736842105263,
                                                      3.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.631578947368421,
                                                      1.0,
                                                      2.0,
                                                      3.0,
                                                      2.0,
                                                      2.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      3.0,
                                                      1.0,
                                                      0.0,
                                                      5.0,
                                                      2.0,
                                                      0.0,
                                                      3.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      2.0,
                                                      2.0,
                                                      0.0,
                                                      1.6842105263157894,
                                                      2.0526315789473686,
                                                      1.6842105263157894,
                                                      2.4210526315789473,
                                                      1.894736842105263,
                                                      2.526315789473684,
                                                      1.7894736842105263,
                                                      1.631578947368421,
                                                      1.5789473684210527,
                                                      1.631578947368421,
                                                      0.7894736842105263,
                                                      1.0,
                                                      1.7894736842105263,
                                                      0.8947368421052632,
                                                      1.0526315789473684,
                                                      2.6315789473684212,
                                                      1.1578947368421053,
                                                      3.0,
                                                      1.0,
                                                      3.0,
                                                      2.0,
                                                      2.0,
                                                      2.0,
                                                      3.0,
                                                      0.0,
                                                      2.0,
                                                      2.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      2.0,
                                                      4.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      3.0,
                                                      4.0,
                                                      1.0,
                                                      2.0,
                                                      3.0,
                                                      1.1578947368421053,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      3.0,
                                                      2.1052631578947367,
                                                      3.9473684210526314,
                                                      0.3157894736842105,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      3.0,
                                                      3.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      0.9473684210526315,
                                                      1.8421052631578947,
                                                      1.631578947368421,
                                                      2.1052631578947367,
                                                      1.4736842105263157,
                                                      1.1578947368421053,
                                                      2.8421052631578947,
                                                      3.0,
                                                      0.21052631578947367,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      3.0,
                                                      0.0,
                                                      0.0,
                                                      3.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      3.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      3.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      3.0,
                                                      0.0],
                             'val_loss': 0.08975100517272949,
                             'val_loss_history': [0.12889361381530762,
                                                  0.12675809860229492,
                                                  929197312.0,
                                                  0.12727877497673035,
                                                  0.12555724382400513,
                                                  1098212864.0,
                                                  0.13582992553710938,
                                                  0.1321955770254135,
                                                  0.1356307566165924,
                                                  1054040704.0,
                                                  0.13556765019893646,
                                                  0.14156989753246307,
                                                  0.13816937804222107,
                                                  268273632.0,
                                                  0.13720349967479706,
                                                  0.13542649149894714,
                                                  0.1288645714521408,
                                                  0.1226494163274765,
                                                  0.12051358073949814,
                                                  0.12420963495969772,
                                                  0.12739074230194092,
                                                  0.13060890138149261,
                                                  0.1263168305158615,
                                                  0.12670856714248657,
                                                  0.11823848634958267,
                                                  0.12219959497451782,
                                                  0.11411118507385254,
                                                  0.12208209931850433,
                                                  0.12046002596616745,
                                                  0.11428070068359375,
                                                  0.11432527750730515,
                                                  0.11463426798582077,
                                                  0.1144825890660286,
                                                  0.11444626748561859,
                                                  0.11485874652862549,
                                                  0.11457077413797379,
                                                  0.1145758330821991,
                                                  0.11442591995000839,
                                                  0.1148984432220459,
                                                  0.1149170994758606,
                                                  0.11488619446754456,
                                                  0.11486360430717468,
                                                  0.114899180829525,
                                                  0.1149524450302124,
                                                  0.11508650332689285,
                                                  0.1152985617518425,
                                                  0.11511247605085373,
                                                  223967184.0,
                                                  0.1151353120803833,
                                                  0.11522237956523895,
                                                  0.11538894474506378,
                                                  0.11536173522472382,
                                                  0.11543510854244232,
                                                  0.11553690582513809,
                                                  0.11534439027309418,
                                                  0.1152355745434761,
                                                  0.11538137495517731,
                                                  0.11679191887378693,
                                                  0.11562331020832062,
                                                  0.11570201814174652,
                                                  0.11553406715393066,
                                                  0.11560743302106857,
                                                  0.11558980494737625,
                                                  0.11557500809431076,
                                                  0.11566941440105438,
                                                  0.11569506675004959,
                                                  0.1155826523900032,
                                                  0.11563906818628311,
                                                  0.11583434790372849,
                                                  0.11583562195301056,
                                                  0.11584048718214035,
                                                  0.11579956114292145,
                                                  0.1158096119761467,
                                                  0.11577166616916656,
                                                  0.11578544974327087,
                                                  0.11576415598392487,
                                                  0.11577217280864716,
                                                  0.1157451793551445,
                                                  0.1157572790980339,
                                                  0.1158934161067009,
                                                  0.11580001562833786,
                                                  0.11593358963727951,
                                                  0.11615775525569916,
                                                  0.11587931215763092,
                                                  0.1159670352935791,
                                                  0.11585234105587006,
                                                  0.11583954095840454,
                                                  0.11609325557947159,
                                                  0.11583058536052704,
                                                  0.11599991470575333,
                                                  0.11598140746355057,
                                                  0.11592470854520798,
                                                  0.11593374609947205,
                                                  0.11602354794740677,
                                                  0.11578560620546341,
                                                  0.11599435657262802,
                                                  0.11563146859407425,
                                                  0.11705952137708664,
                                                  0.11598608642816544,
                                                  0.11604960262775421,
                                                  0.11593892425298691,
                                                  0.1176956295967102,
                                                  0.12361403554677963,
                                                  0.11607111245393753,
                                                  0.11594823747873306,
                                                  0.11593722552061081,
                                                  0.11586553603410721,
                                                  0.11618652194738388,
                                                  0.11610959470272064,
                                                  0.11593214422464371,
                                                  0.11584636569023132,
                                                  0.1159195527434349,
                                                  0.11607397347688675,
                                                  0.1159905418753624,
                                                  0.1159055307507515,
                                                  0.11624743789434433,
                                                  0.11588098853826523,
                                                  0.11602722853422165,
                                                  0.11587972939014435,
                                                  0.11616445332765579,
                                                  0.11607798933982849,
                                                  0.11569825559854507,
                                                  0.11504572629928589,
                                                  0.11600799113512039,
                                                  0.11600035429000854,
                                                  0.11612346768379211,
                                                  0.11608055979013443,
                                                  0.11566175520420074,
                                                  0.1160079687833786,
                                                  0.11582132428884506,
                                                  0.11605935543775558,
                                                  0.11587560176849365,
                                                  0.11589808017015457,
                                                  0.11591998487710953,
                                                  0.11596931517124176,
                                                  0.11618545651435852,
                                                  0.11614586412906647,
                                                  0.116184763610363,
                                                  0.11593402177095413,
                                                  0.11613350361585617,
                                                  0.11606571078300476,
                                                  0.11616233736276627,
                                                  0.11608292907476425,
                                                  0.11600427329540253,
                                                  0.11627335101366043,
                                                  0.11593248695135117,
                                                  0.11602398753166199,
                                                  0.11632940173149109,
                                                  0.11617862433195114,
                                                  0.11615864932537079,
                                                  0.11609946191310883,
                                                  0.11620981246232986,
                                                  0.11623332649469376,
                                                  0.11621405184268951,
                                                  0.11617275327444077,
                                                  0.11618194729089737,
                                                  0.11618026345968246,
                                                  0.11604853719472885,
                                                  0.11610307544469833,
                                                  0.11616066843271255,
                                                  0.11609618365764618,
                                                  0.11606502532958984,
                                                  0.11607180535793304,
                                                  0.11610879749059677,
                                                  0.11615218967199326,
                                                  0.11595457047224045,
                                                  0.1159059926867485,
                                                  0.11616579443216324,
                                                  0.11522948741912842,
                                                  0.11619925498962402,
                                                  0.11620134115219116,
                                                  0.11613330245018005,
                                                  0.11608000099658966,
                                                  0.11618129909038544,
                                                  0.11634077876806259,
                                                  0.11618518084287643,
                                                  0.11617344617843628,
                                                  0.11647146195173264,
                                                  0.11618098616600037,
                                                  0.11633583903312683,
                                                  0.11608415842056274,
                                                  0.11629339307546616,
                                                  0.11636802554130554,
                                                  0.11612612754106522,
                                                  0.11618626117706299,
                                                  0.11609125882387161,
                                                  0.11604174226522446,
                                                  0.11616165190935135,
                                                  0.11629405617713928,
                                                  0.12194935232400894,
                                                  0.11632195115089417,
                                                  0.1196022555232048,
                                                  0.11585008352994919,
                                                  0.12247772514820099,
                                                  0.12818971276283264,
                                                  0.12019037455320358,
                                                  0.12467972934246063,
                                                  0.11633002012968063,
                                                  0.1162014976143837,
                                                  0.11621222645044327,
                                                  0.09137231856584549,
                                                  0.09150426834821701,
                                                  0.09157953411340714,
                                                  0.09157821536064148,
                                                  0.09164956212043762,
                                                  0.09170088171958923,
                                                  0.09174489974975586,
                                                  0.09180380403995514,
                                                  0.09149286895990372,
                                                  0.09157092124223709,
                                                  0.09140341728925705,
                                                  0.09140137583017349,
                                                  0.09056924283504486,
                                                  0.09073775261640549,
                                                  0.09158705919981003,
                                                  0.0913533940911293,
                                                  2.4094927310943604,
                                                  0.09106162190437317,
                                                  0.0903748944401741,
                                                  0.09066660702228546,
                                                  0.09095553308725357,
                                                  0.09037058055400848,
                                                  0.09065017104148865,
                                                  0.09140996634960175,
                                                  0.0915386751294136,
                                                  0.09155571460723877,
                                                  0.09167596697807312,
                                                  0.09157241880893707,
                                                  0.09159300476312637,
                                                  0.09173470735549927,
                                                  0.09168659150600433,
                                                  0.09159538149833679,
                                                  0.09157337993383408,
                                                  0.09162420779466629,
                                                  0.0916445180773735,
                                                  0.09159063547849655,
                                                  0.09151946753263474,
                                                  0.09143755584955215,
                                                  0.09167628735303879,
                                                  0.09162566065788269,
                                                  0.09144790470600128,
                                                  0.09167981147766113,
                                                  0.09183721989393234,
                                                  0.09176892042160034,
                                                  0.09148874878883362,
                                                  0.09179455041885376,
                                                  0.09149231016635895,
                                                  0.09171196818351746,
                                                  0.09166552871465683,
                                                  0.09163936972618103,
                                                  0.09166969358921051,
                                                  0.09118418395519257,
                                                  0.09167936444282532,
                                                  0.09164978563785553,
                                                  0.0916447639465332,
                                                  0.091525137424469,
                                                  0.0916038230061531,
                                                  0.09174405038356781,
                                                  0.09161648899316788,
                                                  0.09172605723142624,
                                                  0.09169495105743408,
                                                  0.09160944074392319,
                                                  0.09177764505147934,
                                                  0.09175115078687668,
                                                  0.09168726205825806,
                                                  0.09168113023042679,
                                                  0.0916929617524147,
                                                  0.0916459709405899,
                                                  0.09187834709882736,
                                                  0.09167644381523132,
                                                  0.0916120707988739,
                                                  0.09165194630622864,
                                                  0.09167271107435226,
                                                  0.09164658933877945,
                                                  0.09172767400741577,
                                                  0.09166054427623749,
                                                  0.09191282838582993,
                                                  0.0919322669506073,
                                                  0.09164082258939743,
                                                  0.09164590388536453,
                                                  0.09164344519376755,
                                                  0.09161072224378586,
                                                  0.09165661782026291,
                                                  0.09178908169269562,
                                                  0.09179839491844177,
                                                  0.09184478968381882,
                                                  0.09157416224479675,
                                                  0.09172283858060837,
                                                  0.09165951609611511,
                                                  0.09154213964939117,
                                                  0.09159321337938309,
                                                  0.09175139665603638,
                                                  0.0916604995727539,
                                                  0.09152602404356003,
                                                  0.09157373756170273,
                                                  0.09160105139017105,
                                                  0.09054151177406311,
                                                  0.09138046205043793,
                                                  0.0918545126914978,
                                                  0.09164437651634216,
                                                  0.09164205938577652,
                                                  0.09161880612373352,
                                                  0.09176848083734512,
                                                  0.0915747582912445,
                                                  0.09165892750024796,
                                                  0.09155618399381638,
                                                  0.09165655821561813,
                                                  0.09176354855298996,
                                                  0.0917825847864151,
                                                  0.09167511016130447,
                                                  0.09172207117080688,
                                                  0.09178581088781357,
                                                  0.09048595279455185,
                                                  0.09156113862991333,
                                                  0.09158486872911453,
                                                  0.09151524305343628,
                                                  0.09164683520793915,
                                                  0.09167765825986862,
                                                  0.09159915894269943,
                                                  0.09176617115736008,
                                                  0.09172510355710983,
                                                  0.09160268306732178,
                                                  0.09161321818828583,
                                                  0.0915767028927803,
                                                  0.0914856418967247,
                                                  0.0916421189904213,
                                                  0.09157934784889221,
                                                  0.09156793355941772,
                                                  0.09154218435287476,
                                                  0.09154070168733597,
                                                  134901216.0,
                                                  0.09168075770139694,
                                                  0.09144774079322815,
                                                  0.09149137884378433,
                                                  0.09155774861574173,
                                                  0.09148041158914566,
                                                  0.09169764071702957,
                                                  0.0916762501001358,
                                                  0.09158093482255936,
                                                  0.09154773503541946,
                                                  0.0916462242603302,
                                                  0.0915762335062027,
                                                  0.09153687953948975,
                                                  0.09146840870380402,
                                                  0.09179212152957916,
                                                  0.0917789414525032,
                                                  0.09176136553287506,
                                                  0.09159605950117111,
                                                  0.09162770956754684,
                                                  0.09160606563091278,
                                                  0.09176835417747498,
                                                  0.09157654643058777,
                                                  0.0916082113981247,
                                                  0.09162461757659912,
                                                  0.09182053804397583,
                                                  0.09177893400192261,
                                                  0.09165219217538834,
                                                  0.09159030020236969,
                                                  0.09154009819030762,
                                                  0.09164740145206451,
                                                  0.09176388382911682,
                                                  0.0915454551577568,
                                                  0.09177467226982117,
                                                  0.09191010892391205,
                                                  0.09156614542007446,
                                                  0.09143801033496857,
                                                  0.09114047884941101,
                                                  0.09068604558706284,
                                                  0.09070765227079391,
                                                  0.09103754162788391,
                                                  0.09052861481904984,
                                                  0.09132906794548035,
                                                  0.09160716831684113,
                                                  0.09083017706871033,
                                                  0.09090922772884369,
                                                  0.09165212512016296,
                                                  0.09168437123298645,
                                                  0.09145098924636841,
                                                  0.09153489023447037,
                                                  8632.75390625,
                                                  0.09173749387264252,
                                                  0.09159638732671738,
                                                  0.09155716747045517,
                                                  0.09143556654453278,
                                                  0.09150324761867523,
                                                  0.09115756303071976,
                                                  0.09095091372728348,
                                                  0.09065255522727966,
                                                  0.09077665954828262,
                                                  0.09125294536352158,
                                                  0.09146241843700409,
                                                  0.0917263850569725,
                                                  0.09158481657505035,
                                                  0.09154310077428818,
                                                  0.09155087172985077,
                                                  0.09157835692167282,
                                                  0.09175891429185867,
                                                  0.09174496680498123,
                                                  0.0915360376238823,
                                                  0.0916609838604927,
                                                  0.09006042033433914,
                                                  0.09004908800125122,
                                                  0.08979449421167374,
                                                  0.0896245688199997,
                                                  0.08980648964643478,
                                                  0.0895053967833519,
                                                  0.08965669572353363,
                                                  0.09002640843391418,
                                                  0.08996382355690002,
                                                  0.08966709673404694,
                                                  0.0897647812962532,
                                                  0.08978300541639328,
                                                  0.08981018513441086,
                                                  0.0896415039896965,
                                                  0.0901375487446785,
                                                  0.089830681681633,
                                                  0.09017007797956467,
                                                  0.0898798406124115,
                                                  0.09025020152330399,
                                                  0.0899721086025238,
                                                  0.08988645672798157,
                                                  0.09003996104001999,
                                                  0.08998358994722366,
                                                  0.0893966481089592,
                                                  0.08915132284164429,
                                                  0.08969417959451675,
                                                  0.08966122567653656,
                                                  0.08971626311540604,
                                                  0.08974547684192657,
                                                  0.08988585323095322,
                                                  0.09006890654563904,
                                                  0.09015759825706482,
                                                  0.08985879272222519,
                                                  0.09011870622634888,
                                                  0.089875727891922,
                                                  0.08984020352363586,
                                                  0.08999879658222198,
                                                  0.08929476141929626,
                                                  0.08917342871427536,
                                                  0.0890253409743309,
                                                  0.0890451967716217,
                                                  0.08985283225774765,
                                                  0.08977603167295456,
                                                  0.08965098857879639,
                                                  0.09000735729932785,
                                                  0.0896926000714302,
                                                  0.0897701233625412,
                                                  0.08995763957500458,
                                                  0.0898318812251091,
                                                  0.08998756855726242,
                                                  0.09007910639047623,
                                                  0.09007957577705383,
                                                  0.08997736871242523,
                                                  0.08961615711450577,
                                                  0.09001019597053528,
                                                  0.08989803493022919,
                                                  0.09013975411653519,
                                                  0.0902264267206192,
                                                  0.0899982675909996,
                                                  0.08958622068166733,
                                                  0.08898205310106277,
                                                  0.08928899466991425,
                                                  0.08966180682182312,
                                                  0.0896264836192131,
                                                  0.0899060070514679,
                                                  165786832.0,
                                                  0.08958649635314941,
                                                  0.08978855609893799,
                                                  0.0899762436747551,
                                                  0.09000223875045776,
                                                  0.08978422731161118,
                                                  0.09043993800878525,
                                                  0.09028058499097824,
                                                  0.1005210429430008,
                                                  0.08993940055370331,
                                                  0.08960286527872086,
                                                  0.08972615748643875,
                                                  0.08975361287593842,
                                                  0.08965158462524414,
                                                  0.08964148908853531,
                                                  0.08972785621881485,
                                                  0.0897819921374321,
                                                  0.08990558981895447,
                                                  0.08987738937139511,
                                                  0.09000679105520248,
                                                  0.08997351676225662,
                                                  0.08983514457941055,
                                                  0.09013941884040833,
                                                  0.09014367312192917,
                                                  0.09003666788339615,
                                                  0.09019093215465546,
                                                  0.08984865993261337,
                                                  0.08995110541582108,
                                                  0.08969511836767197,
                                                  0.08997870236635208,
                                                  0.08957649022340775,
                                                  0.08970191329717636,
                                                  0.08933154493570328,
                                                  0.08878691494464874,
                                                  0.08927902579307556,
                                                  0.08946020156145096,
                                                  0.08946799486875534,
                                                  0.08914709836244583,
                                                  0.08887948840856552,
                                                  0.08919765800237656,
                                                  0.0895165354013443,
                                                  0.08895201981067657,
                                                  0.08957294374704361,
                                                  0.08974824845790863,
                                                  0.08876839280128479,
                                                  0.08959838002920151,
                                                  0.08914162218570709,
                                                  0.08941791206598282,
                                                  0.08913740515708923,
                                                  0.08939765393733978,
                                                  0.09013853222131729,
                                                  0.08985241502523422,
                                                  0.0897478461265564,
                                                  0.08989280462265015,
                                                  0.08953097462654114,
                                                  0.08996637165546417,
                                                  0.08999262005090714,
                                                  0.09000848233699799,
                                                  0.08972601592540741,
                                                  0.08974424004554749,
                                                  0.08952955156564713,
                                                  0.08970942348241806,
                                                  0.0896824523806572,
                                                  0.0899357870221138,
                                                  0.0900910422205925,
                                                  0.08987075090408325,
                                                  0.09012211114168167,
                                                  0.0899343490600586,
                                                  0.08979938924312592,
                                                  0.08996124565601349,
                                                  0.09016357362270355,
                                                  0.09030314534902573,
                                                  0.09023184329271317,
                                                  0.09009277075529099,
                                                  0.08956193923950195,
                                                  0.09018462896347046,
                                                  0.08991622179746628,
                                                  0.08998739719390869,
                                                  0.09010801464319229,
                                                  0.08995979279279709,
                                                  0.08951814472675323,
                                                  0.09035605937242508,
                                                  0.08999628573656082,
                                                  0.08997727185487747,
                                                  0.09033078700304031,
                                                  0.08996672928333282,
                                                  0.09028850495815277,
                                                  0.0895623043179512,
                                                  0.08894254267215729,
                                                  0.09029853343963623,
                                                  0.08997199684381485,
                                                  0.08996271342039108,
                                                  0.09009357541799545,
                                                  0.09012578427791595,
                                                  0.08983942866325378,
                                                  0.08987858891487122,
                                                  0.08974911272525787,
                                                  0.08983664214611053,
                                                  0.08988182246685028,
                                                  0.0900752916932106,
                                                  0.08942669630050659,
                                                  0.08933917433023453,
                                                  0.08779210597276688,
                                                  0.08927730470895767,
                                                  0.08909048140048981,
                                                  0.0898263081908226,
                                                  0.08931832760572433,
                                                  0.08928386121988297,
                                                  0.08991234749555588,
                                                  0.08994374424219131,
                                                  0.09000713378190994,
                                                  0.08975329250097275,
                                                  0.09016995877027512,
                                                  0.08982622623443604,
                                                  0.09043461084365845,
                                                  0.09003646671772003,
                                                  0.0898413360118866,
                                                  0.08992080390453339,
                                                  0.08976344019174576,
                                                  0.09005337953567505,
                                                  0.09002504497766495,
                                                  0.08987294137477875,
                                                  0.08990441262722015,
                                                  0.08986706286668777,
                                                  0.09007862210273743,
                                                  0.08986128121614456,
                                                  0.08975748717784882,
                                                  0.0894976556301117,
                                                  0.08977871388196945,
                                                  0.08997070044279099,
                                                  0.0896330401301384,
                                                  0.09019531309604645,
                                                  0.09008292108774185,
                                                  0.09006659686565399,
                                                  0.08975100517272949],
                             'weight_decay': 0.0005},
                 'microF1': {'accuracy': 0.0,
                             'batch_size': 32,
                             'cv_score': 0.0053345869543231185,
                             'cv_val_accuracy': 0.6666666666666666,
                             'cv_val_loss': 0.09920807182788849,
                             'cv_val_macroF1': 0.0053345869543231185,
                             'cv_val_microF1': 0.066966913828706,
                             'epochs': 200,
                             'final_model': GGNN1(
  (ggnn): GatedGraphConv(60, num_layers=2)
  (fc1): Linear(in_features=60, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                             'kwargs': {'aggr_type': 'mean',
                                        'd1': 60,
                                        'd2': 50,
                                        'num_classes': 24,
                                        'num_layers': 2},
                             'learning_rate': 0.01,
                             'macroF1': 0.006748704489763125,
                             'microF1': 0.08414023372287145,
                             'model': <class 'TFM_graph_classification_models.GGNN1'>,
                             'model_instance': GGNN1(
  (ggnn): GatedGraphConv(60, num_layers=2)
  (fc1): Linear(in_features=60, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                             'score': 'f1_macro',
                             'time': 2764.828370809555,
                             'train_loss_history': [362103776.0,
                                                    1984.16552734375,
                                                    4208493824.0,
                                                    660907968.0,
                                                    0.08377974480390549,
                                                    1243555968.0,
                                                    1017.9180297851562,
                                                    76571.46875,
                                                    308.40338134765625,
                                                    23220.69921875,
                                                    156603248.0,
                                                    138.022216796875,
                                                    0.08351406455039978,
                                                    1823248640.0,
                                                    155343744.0,
                                                    0.0847235918045044,
                                                    0.08452852815389633,
                                                    0.08426041901111603,
                                                    0.0840001031756401,
                                                    0.0840369313955307,
                                                    0.0836704894900322,
                                                    0.08393912762403488,
                                                    0.08424623310565948,
                                                    0.08425412327051163,
                                                    0.08409769088029861,
                                                    0.08399365842342377,
                                                    0.08431313186883926,
                                                    0.08411368727684021,
                                                    19.82664680480957,
                                                    0.08441364020109177,
                                                    0.08446607738733292,
                                                    0.08450149744749069,
                                                    0.08464515209197998,
                                                    0.08455231040716171,
                                                    0.08460909128189087,
                                                    0.08457177132368088,
                                                    0.0844896212220192,
                                                    0.08443144708871841,
                                                    7615.97705078125,
                                                    5.158772945404053,
                                                    0.08454208821058273,
                                                    0.08437103033065796,
                                                    0.08442801982164383,
                                                    0.08480384200811386,
                                                    0.08451271802186966,
                                                    0.0846419557929039,
                                                    0.08445870131254196,
                                                    0.08451291173696518,
                                                    7.508358001708984,
                                                    7220202.5,
                                                    208.47743225097656,
                                                    0.08452831208705902,
                                                    0.08444917947053909,
                                                    0.08455681055784225,
                                                    0.08457385748624802,
                                                    0.08448221534490585,
                                                    0.08441825956106186,
                                                    0.08439182490110397,
                                                    0.08427322655916214,
                                                    0.08453415334224701,
                                                    0.08448854833841324,
                                                    0.08462214469909668,
                                                    0.08452510088682175,
                                                    0.08454762399196625,
                                                    0.08453730493783951,
                                                    0.08453856408596039,
                                                    0.08457324653863907,
                                                    0.08439980447292328,
                                                    0.08457770943641663,
                                                    0.08448946475982666,
                                                    0.08455544710159302,
                                                    0.08448326587677002,
                                                    0.0853704884648323,
                                                    0.0845303013920784,
                                                    0.08449558913707733,
                                                    0.08448614180088043,
                                                    0.08432622253894806,
                                                    0.08439280092716217,
                                                    0.08428298681974411,
                                                    0.08441869169473648,
                                                    2.7614922523498535,
                                                    0.0844237357378006,
                                                    0.0845165103673935,
                                                    3.4590933322906494,
                                                    0.08449532091617584,
                                                    0.0845257043838501,
                                                    0.08447345346212387,
                                                    0.08450763672590256,
                                                    0.08446920663118362,
                                                    0.08451981097459793,
                                                    385.1795959472656,
                                                    0.08447851985692978,
                                                    0.08457182347774506,
                                                    0.08445488661527634,
                                                    0.7349956631660461,
                                                    0.0845390260219574,
                                                    437.2469787597656,
                                                    0.08416557312011719,
                                                    0.08421652764081955,
                                                    0.08440131694078445,
                                                    0.08433077484369278,
                                                    0.08428847789764404,
                                                    0.08411938697099686,
                                                    0.08432593941688538,
                                                    0.08455510437488556,
                                                    0.08454146236181259,
                                                    0.08451537787914276,
                                                    0.08447898924350739,
                                                    0.08454417437314987,
                                                    0.08451075106859207,
                                                    0.08446026593446732,
                                                    0.08439700305461884,
                                                    0.08444203436374664,
                                                    324.5517578125,
                                                    0.084525465965271,
                                                    0.08450353890657425,
                                                    0.08454328775405884,
                                                    0.08446735888719559,
                                                    0.0844758078455925,
                                                    0.08446711301803589,
                                                    0.08434131741523743,
                                                    0.0844181552529335,
                                                    0.08407874405384064,
                                                    0.08442263305187225,
                                                    0.08430887013673782,
                                                    0.08445868641138077,
                                                    0.08432488143444061,
                                                    0.08438342809677124,
                                                    0.08434324711561203,
                                                    0.08433690667152405,
                                                    0.08442634344100952,
                                                    0.08430664241313934,
                                                    0.08431776612997055,
                                                    3179.368408203125,
                                                    0.08431804180145264,
                                                    0.0844300165772438,
                                                    327.7389831542969,
                                                    0.08442189544439316,
                                                    0.08446188271045685,
                                                    0.08456380665302277,
                                                    0.08448801934719086,
                                                    0.08452309668064117,
                                                    0.08443786203861237,
                                                    0.08450938016176224,
                                                    0.08452746272087097,
                                                    0.0845022052526474,
                                                    0.08435150235891342,
                                                    0.0843622088432312,
                                                    0.08451417833566666,
                                                    0.08449152857065201,
                                                    2792218.0,
                                                    0.08448152989149094,
                                                    0.08447089046239853,
                                                    0.08448109775781631,
                                                    0.08455649018287659,
                                                    11.825798988342285,
                                                    0.0844212993979454,
                                                    0.08447621017694473,
                                                    0.08445154875516891,
                                                    0.08450216054916382,
                                                    0.08448535948991776,
                                                    0.08452288061380386,
                                                    2.2303872108459473,
                                                    0.08445139229297638,
                                                    22.94561195373535,
                                                    0.08438808470964432,
                                                    0.08447092026472092,
                                                    0.0844135656952858,
                                                    0.08435672521591187,
                                                    4.1318182945251465,
                                                    0.0844360813498497,
                                                    0.08448848873376846,
                                                    0.08441183716058731,
                                                    0.08452562987804413,
                                                    0.08434139937162399,
                                                    15509.2685546875,
                                                    0.08440298587083817,
                                                    0.0843038558959961,
                                                    0.08409386873245239,
                                                    0.08434053510427475,
                                                    0.08435279130935669,
                                                    0.0843585804104805,
                                                    1622.2698974609375,
                                                    6.130885601043701,
                                                    513.2769165039062,
                                                    0.08451929688453674,
                                                    0.0844656303524971,
                                                    0.08448315411806107,
                                                    0.08443838357925415,
                                                    657.5328979492188,
                                                    0.08391217887401581,
                                                    0.08355443924665451,
                                                    0.08408589661121368,
                                                    0.08395949006080627,
                                                    0.08368541300296783,
                                                    0.09440038353204727,
                                                    0.08442547917366028,
                                                    0.08448342233896255,
                                                    0.08452529460191727,
                                                    0.08449804782867432,
                                                    0.0958980917930603,
                                                    0.0944596529006958,
                                                    0.09449319541454315,
                                                    0.0945969820022583,
                                                    0.09455905109643936,
                                                    0.09455728530883789,
                                                    0.0946040078997612,
                                                    0.5912741422653198,
                                                    0.0945107489824295,
                                                    0.09456121921539307,
                                                    0.09452136605978012,
                                                    0.09451890736818314,
                                                    0.09491711109876633,
                                                    0.0936649888753891,
                                                    0.17639370262622833,
                                                    227.92086791992188,
                                                    0.0940268412232399,
                                                    0.21170301735401154,
                                                    0.09379062056541443,
                                                    0.09365381300449371,
                                                    0.09371867030858994,
                                                    0.09373131394386292,
                                                    0.09375094622373581,
                                                    0.09409426152706146,
                                                    0.09444309771060944,
                                                    2.2909059524536133,
                                                    408.70733642578125,
                                                    0.09448052197694778,
                                                    0.09452381730079651,
                                                    0.09452628344297409,
                                                    0.09451580047607422,
                                                    0.09457345306873322,
                                                    0.09455623477697372,
                                                    0.09454062581062317,
                                                    0.09454816579818726,
                                                    0.09447815269231796,
                                                    0.09455271065235138,
                                                    0.0945919007062912,
                                                    0.09438098222017288,
                                                    0.09447444975376129,
                                                    0.09452714771032333,
                                                    0.09452454000711441,
                                                    0.09455770254135132,
                                                    0.09453456103801727,
                                                    0.09453591704368591,
                                                    0.0945412740111351,
                                                    5.31784200668335,
                                                    0.09444911777973175,
                                                    1.2043771743774414,
                                                    0.09460223466157913,
                                                    0.09458258748054504,
                                                    0.09454575926065445,
                                                    0.09464789927005768,
                                                    0.09462427347898483,
                                                    0.2161496877670288,
                                                    0.09452006220817566,
                                                    0.0945173129439354,
                                                    0.09459420293569565,
                                                    45.2691764831543,
                                                    0.0946098044514656,
                                                    0.09459919482469559,
                                                    0.09453676640987396,
                                                    0.09455496072769165,
                                                    0.09455988556146622,
                                                    0.09453597664833069,
                                                    1460.126708984375,
                                                    0.09449618309736252,
                                                    0.09447479248046875,
                                                    0.0946631208062172,
                                                    0.09447914361953735,
                                                    0.09451417624950409,
                                                    0.09457054734230042,
                                                    578.6171264648438,
                                                    0.09508994966745377,
                                                    0.09456158429384232,
                                                    0.09453702718019485,
                                                    92.70061492919922,
                                                    575.2883911132812,
                                                    0.09449885785579681,
                                                    0.09459429234266281,
                                                    0.09453769028186798,
                                                    0.09457365423440933,
                                                    0.2661488950252533,
                                                    0.5785210728645325,
                                                    1.0697529315948486,
                                                    9533.2490234375,
                                                    36278084.0,
                                                    0.09451647102832794,
                                                    0.0945221334695816,
                                                    0.0945245772600174,
                                                    0.49862831830978394,
                                                    0.1389312595129013,
                                                    0.10536698997020721,
                                                    0.6453412175178528,
                                                    0.09668745845556259,
                                                    7082514.5,
                                                    0.09513116627931595,
                                                    0.09418614208698273,
                                                    0.09460048377513885,
                                                    0.09451752156019211,
                                                    0.09452079981565475,
                                                    0.11058186739683151,
                                                    0.09448027610778809,
                                                    95075.0,
                                                    0.09450844675302505,
                                                    0.40205472707748413,
                                                    0.09456601738929749,
                                                    21503.685546875,
                                                    25433.630859375,
                                                    0.09459114074707031,
                                                    1.827946662902832,
                                                    0.09454144537448883,
                                                    0.09408380091190338,
                                                    0.09439128637313843,
                                                    0.09441068768501282,
                                                    75790.1328125,
                                                    2222.14990234375,
                                                    0.09460681676864624,
                                                    0.09455598890781403,
                                                    0.09462475776672363,
                                                    0.09453312307596207,
                                                    0.0945381447672844,
                                                    0.09712633490562439,
                                                    15.168710708618164,
                                                    0.09447479993104935,
                                                    0.09447994828224182,
                                                    0.09455700218677521,
                                                    1.1578611135482788,
                                                    0.09453558921813965,
                                                    0.0945739671587944,
                                                    0.09440802037715912,
                                                    7.872211933135986,
                                                    0.09455535560846329,
                                                    0.09453728795051575,
                                                    0.09456964582204819,
                                                    0.09449167549610138,
                                                    403448.90625,
                                                    0.11504217982292175,
                                                    0.09461946040391922,
                                                    0.09448207169771194,
                                                    1.0811370611190796,
                                                    0.09459654241800308,
                                                    0.09441133588552475,
                                                    0.0943896472454071,
                                                    5106.0791015625,
                                                    0.09468978643417358,
                                                    0.09459315240383148,
                                                    0.09458445757627487,
                                                    12.510099411010742,
                                                    0.09466719627380371,
                                                    0.09455475211143494,
                                                    20.623863220214844,
                                                    0.09454236924648285,
                                                    8.770496368408203,
                                                    12.305490493774414,
                                                    0.09461303800344467,
                                                    0.09462690353393555,
                                                    0.09455135464668274,
                                                    0.09457971900701523,
                                                    0.09463999420404434,
                                                    0.09459368139505386,
                                                    39.026275634765625,
                                                    0.09459409862756729,
                                                    0.14653317630290985,
                                                    0.09447965025901794,
                                                    0.09448003023862839,
                                                    0.09420546144247055,
                                                    0.10086872428655624,
                                                    0.09402845054864883,
                                                    0.09396755695343018,
                                                    0.09385461360216141,
                                                    0.7617712020874023,
                                                    3.599776268005371,
                                                    0.0940251424908638,
                                                    0.09385236352682114,
                                                    1.5394641160964966,
                                                    555.6378784179688,
                                                    21.09204864501953,
                                                    0.09456101059913635,
                                                    0.09457747638225555,
                                                    0.20754975080490112,
                                                    0.09456492215394974,
                                                    0.09465780109167099,
                                                    0.09462746232748032,
                                                    0.09462113678455353,
                                                    4181469.0,
                                                    0.11877608299255371,
                                                    0.09787335991859436,
                                                    0.09376204758882523,
                                                    0.09390174597501755,
                                                    609.8927001953125,
                                                    0.0945817232131958,
                                                    56.260704040527344,
                                                    0.09460467100143433,
                                                    0.16732440888881683,
                                                    0.09459473192691803,
                                                    0.09458056092262268,
                                                    0.09451653063297272,
                                                    0.09456532448530197,
                                                    0.09456890821456909,
                                                    0.09472556412220001,
                                                    0.09467514604330063,
                                                    0.09467890858650208,
                                                    0.09461469203233719,
                                                    0.09461411088705063,
                                                    0.09406077861785889,
                                                    0.09436465054750443,
                                                    6.517511367797852,
                                                    0.09469257295131683,
                                                    22.83505630493164,
                                                    21.33751106262207,
                                                    0.09463278949260712,
                                                    1789.422119140625,
                                                    435.6246337890625,
                                                    0.24851779639720917,
                                                    0.09462091326713562,
                                                    614.681640625,
                                                    0.09470755606889725,
                                                    178.90574645996094,
                                                    0.09464573115110397,
                                                    0.09471963346004486,
                                                    0.09465103596448898,
                                                    0.6563721895217896,
                                                    704190.0,
                                                    0.09396739304065704,
                                                    0.09447432309389114,
                                                    0.09416063874959946,
                                                    0.0944053903222084,
                                                    65.62581634521484,
                                                    0.09450186043977737,
                                                    1024.021240234375,
                                                    0.09452372044324875,
                                                    0.09453118592500687,
                                                    0.09463518112897873,
                                                    0.09458576887845993,
                                                    0.10458575189113617,
                                                    47422.390625,
                                                    0.09453989565372467,
                                                    0.19003771245479584,
                                                    0.09361455589532852,
                                                    0.09875020384788513,
                                                    0.10593827068805695,
                                                    0.09450165182352066,
                                                    2.2607531547546387,
                                                    146.7675018310547,
                                                    0.09460194408893585,
                                                    0.09467209130525589,
                                                    0.09463077038526535,
                                                    0.09450284391641617,
                                                    0.09454219788312912,
                                                    0.09467772394418716,
                                                    0.09461057186126709,
                                                    301.4787902832031,
                                                    0.0944533422589302,
                                                    0.09459809958934784,
                                                    0.09464976191520691,
                                                    0.09458554536104202,
                                                    0.09468982368707657,
                                                    0.09466410428285599,
                                                    0.5748986601829529,
                                                    0.09391465783119202,
                                                    0.09361892193555832,
                                                    13.32568073272705,
                                                    0.09446100890636444,
                                                    77.79401397705078,
                                                    2.8922955989837646,
                                                    0.1703469306230545,
                                                    7399.919921875,
                                                    4709.5478515625,
                                                    0.09465955197811127,
                                                    0.09466127306222916,
                                                    0.09468183666467667,
                                                    0.09468203783035278,
                                                    19.022647857666016,
                                                    0.09639277309179306,
                                                    0.09449823945760727,
                                                    0.09443557262420654,
                                                    0.09912924468517303,
                                                    0.09453266113996506,
                                                    0.09452731162309647,
                                                    0.09462825953960419,
                                                    0.09465084969997406,
                                                    0.09445875138044357,
                                                    9395.8203125,
                                                    4131.25390625,
                                                    0.09448602050542831,
                                                    0.09457829594612122,
                                                    0.09452693164348602,
                                                    0.41866934299468994,
                                                    0.09443027526140213,
                                                    1.5612469911575317,
                                                    0.09447776526212692,
                                                    0.09464254975318909,
                                                    0.0947151929140091,
                                                    0.09461060166358948,
                                                    1.6179649829864502,
                                                    151.97926330566406,
                                                    0.09670396894216537,
                                                    0.09371296316385269,
                                                    0.22721385955810547,
                                                    0.09369426220655441,
                                                    0.09369219094514847,
                                                    0.0936376303434372,
                                                    519.0321655273438,
                                                    2733.38671875,
                                                    0.09362778812646866,
                                                    0.1038171648979187,
                                                    0.36694493889808655,
                                                    0.09366429597139359,
                                                    0.09367946535348892,
                                                    0.093511663377285,
                                                    363061.71875,
                                                    0.09354864805936813,
                                                    0.09355273097753525,
                                                    0.0940803587436676,
                                                    0.09443733096122742,
                                                    0.09446991235017776,
                                                    0.09449158608913422,
                                                    0.09442201256752014,
                                                    0.09459392726421356,
                                                    0.09461551159620285,
                                                    3.2891151905059814,
                                                    0.09446682780981064,
                                                    214763.484375,
                                                    0.0945248231291771,
                                                    0.0945204347372055,
                                                    1.0939825773239136,
                                                    0.09466331452131271,
                                                    0.09470713883638382,
                                                    0.09462140500545502,
                                                    0.0944676548242569,
                                                    0.09462480992078781,
                                                    0.0944826528429985,
                                                    62.18036651611328,
                                                    1.1048225164413452,
                                                    7.396787166595459,
                                                    0.09464084357023239,
                                                    0.09469672292470932,
                                                    0.09460265189409256,
                                                    37.437835693359375,
                                                    0.09448858350515366,
                                                    13.001826286315918,
                                                    45.448062896728516,
                                                    0.0946521982550621,
                                                    0.09455090761184692,
                                                    0.2181532233953476,
                                                    248.3521728515625,
                                                    0.094516322016716,
                                                    0.09450066834688187,
                                                    0.09460214525461197,
                                                    1419.5380859375,
                                                    0.09455197304487228,
                                                    89.7099380493164,
                                                    0.09380219131708145,
                                                    0.09361878782510757,
                                                    0.09440010786056519,
                                                    33.71955490112305,
                                                    0.0946076437830925,
                                                    0.09467274695634842,
                                                    0.09467364847660065,
                                                    6.568045139312744,
                                                    0.0946686714887619,
                                                    0.11860518157482147,
                                                    144.5919952392578,
                                                    0.0945608839392662,
                                                    0.0977327898144722,
                                                    0.09378885477781296,
                                                    0.4567955732345581,
                                                    0.28246936202049255,
                                                    0.0937114804983139,
                                                    3.4494714736938477,
                                                    0.09347078949213028,
                                                    1.9993853569030762,
                                                    694.1502685546875,
                                                    0.09461234509944916,
                                                    0.09463471919298172,
                                                    0.09461282938718796,
                                                    0.09465289115905762,
                                                    0.1086336076259613,
                                                    0.09450510889291763,
                                                    0.09465640783309937,
                                                    0.0946459248661995,
                                                    44.427127838134766,
                                                    0.0945444405078888,
                                                    0.09469397366046906,
                                                    0.09456057101488113,
                                                    0.09453918039798737,
                                                    0.09460191428661346,
                                                    16.45922088623047,
                                                    0.09442133456468582,
                                                    766.4246826171875,
                                                    0.0944996252655983,
                                                    57.03985595703125,
                                                    0.09441376477479935,
                                                    0.09458398073911667,
                                                    4.6840386390686035,
                                                    0.10124408453702927,
                                                    0.09464626759290695,
                                                    0.0945911556482315,
                                                    34.607200622558594],
                             'val_accuracy_history': [1.736842105263158,
                                                      2.210526315789474,
                                                      0.05263157894736842,
                                                      1.0,
                                                      0.8947368421052632,
                                                      0.9473684210526315,
                                                      0.2631578947368421,
                                                      0.6842105263157895,
                                                      1.5789473684210527,
                                                      0.7368421052631579,
                                                      1.0,
                                                      1.368421052631579,
                                                      0.8947368421052632,
                                                      0.0,
                                                      0.15789473684210525,
                                                      0.3684210526315789,
                                                      1.5789473684210527,
                                                      1.368421052631579,
                                                      1.4736842105263157,
                                                      0.2631578947368421,
                                                      0.3684210526315789,
                                                      1.631578947368421,
                                                      0.47368421052631576,
                                                      1.368421052631579,
                                                      0.5789473684210527,
                                                      0.42105263157894735,
                                                      1.0,
                                                      0.5789473684210527,
                                                      0.6842105263157895,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.105263157894737,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      0.3684210526315789,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      4.0,
                                                      0.0,
                                                      0.0,
                                                      3.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      3.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      4.0,
                                                      0.0,
                                                      1.0,
                                                      1.0526315789473684,
                                                      0.3157894736842105,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      1.4736842105263157,
                                                      0.42105263157894735,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.4736842105263157,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      3.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      3.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      3.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.15789473684210525,
                                                      0.0,
                                                      2.0,
                                                      2.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      3.0,
                                                      1.0,
                                                      1.0,
                                                      1.894736842105263,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      4.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      0.8947368421052632,
                                                      0.21052631578947367,
                                                      1.5263157894736843,
                                                      0.21052631578947367,
                                                      0.5789473684210527,
                                                      0.2631578947368421,
                                                      0.8947368421052632,
                                                      1.0526315789473684,
                                                      1.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.6666666666666666,
                                                      3.0,
                                                      0.0,
                                                      0.2222222222222222,
                                                      1.0,
                                                      0.5555555555555556,
                                                      0.2222222222222222,
                                                      0.2222222222222222,
                                                      1.7777777777777777,
                                                      1.8888888888888888,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.5555555555555556,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.8888888888888888,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.2222222222222222,
                                                      1.7777777777777777,
                                                      0.4444444444444444,
                                                      0.0,
                                                      0.4444444444444444,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.2222222222222222,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      3.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      0.7777777777777778,
                                                      0.8888888888888888,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      3.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      5.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      4.0,
                                                      4.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      3.0,
                                                      1.0,
                                                      2.263157894736842,
                                                      3.0,
                                                      2.0,
                                                      1.6842105263157894,
                                                      0.0,
                                                      1.0,
                                                      3.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      5.0,
                                                      0.0,
                                                      2.789473684210526,
                                                      0.0,
                                                      1.894736842105263,
                                                      1.5789473684210527,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      4.0,
                                                      2.0,
                                                      1.0,
                                                      3.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      5.0,
                                                      4.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.894736842105263,
                                                      1.0526315789473684,
                                                      1.4736842105263157,
                                                      0.0,
                                                      0.0,
                                                      3.0,
                                                      1.894736842105263,
                                                      3.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.631578947368421,
                                                      1.0,
                                                      2.0,
                                                      3.0,
                                                      2.0,
                                                      2.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      3.0,
                                                      1.0,
                                                      0.0,
                                                      5.0,
                                                      2.0,
                                                      0.0,
                                                      3.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      2.0,
                                                      2.0,
                                                      0.0,
                                                      1.6842105263157894,
                                                      2.0526315789473686,
                                                      1.6842105263157894,
                                                      2.4210526315789473,
                                                      1.894736842105263,
                                                      2.526315789473684,
                                                      1.7894736842105263,
                                                      1.631578947368421,
                                                      1.5789473684210527,
                                                      1.631578947368421,
                                                      0.7894736842105263,
                                                      1.0,
                                                      1.7894736842105263,
                                                      0.8947368421052632,
                                                      1.0526315789473684,
                                                      2.6315789473684212,
                                                      1.1578947368421053,
                                                      3.0,
                                                      1.0,
                                                      3.0,
                                                      2.0,
                                                      2.0,
                                                      2.0,
                                                      3.0,
                                                      0.0,
                                                      2.0,
                                                      2.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      2.0,
                                                      4.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      3.0,
                                                      4.0,
                                                      1.0,
                                                      2.0,
                                                      3.0,
                                                      1.1578947368421053,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      3.0,
                                                      2.1052631578947367,
                                                      3.9473684210526314,
                                                      0.3157894736842105,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      3.0,
                                                      3.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      0.9473684210526315,
                                                      1.8421052631578947,
                                                      1.631578947368421,
                                                      2.1052631578947367,
                                                      1.4736842105263157,
                                                      1.1578947368421053,
                                                      2.8421052631578947,
                                                      3.0,
                                                      0.21052631578947367,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      3.0,
                                                      0.0,
                                                      0.0,
                                                      3.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      3.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      3.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      3.0,
                                                      0.0],
                             'val_loss': 0.08975100517272949,
                             'val_loss_history': [0.12889361381530762,
                                                  0.12675809860229492,
                                                  929197312.0,
                                                  0.12727877497673035,
                                                  0.12555724382400513,
                                                  1098212864.0,
                                                  0.13582992553710938,
                                                  0.1321955770254135,
                                                  0.1356307566165924,
                                                  1054040704.0,
                                                  0.13556765019893646,
                                                  0.14156989753246307,
                                                  0.13816937804222107,
                                                  268273632.0,
                                                  0.13720349967479706,
                                                  0.13542649149894714,
                                                  0.1288645714521408,
                                                  0.1226494163274765,
                                                  0.12051358073949814,
                                                  0.12420963495969772,
                                                  0.12739074230194092,
                                                  0.13060890138149261,
                                                  0.1263168305158615,
                                                  0.12670856714248657,
                                                  0.11823848634958267,
                                                  0.12219959497451782,
                                                  0.11411118507385254,
                                                  0.12208209931850433,
                                                  0.12046002596616745,
                                                  0.11428070068359375,
                                                  0.11432527750730515,
                                                  0.11463426798582077,
                                                  0.1144825890660286,
                                                  0.11444626748561859,
                                                  0.11485874652862549,
                                                  0.11457077413797379,
                                                  0.1145758330821991,
                                                  0.11442591995000839,
                                                  0.1148984432220459,
                                                  0.1149170994758606,
                                                  0.11488619446754456,
                                                  0.11486360430717468,
                                                  0.114899180829525,
                                                  0.1149524450302124,
                                                  0.11508650332689285,
                                                  0.1152985617518425,
                                                  0.11511247605085373,
                                                  223967184.0,
                                                  0.1151353120803833,
                                                  0.11522237956523895,
                                                  0.11538894474506378,
                                                  0.11536173522472382,
                                                  0.11543510854244232,
                                                  0.11553690582513809,
                                                  0.11534439027309418,
                                                  0.1152355745434761,
                                                  0.11538137495517731,
                                                  0.11679191887378693,
                                                  0.11562331020832062,
                                                  0.11570201814174652,
                                                  0.11553406715393066,
                                                  0.11560743302106857,
                                                  0.11558980494737625,
                                                  0.11557500809431076,
                                                  0.11566941440105438,
                                                  0.11569506675004959,
                                                  0.1155826523900032,
                                                  0.11563906818628311,
                                                  0.11583434790372849,
                                                  0.11583562195301056,
                                                  0.11584048718214035,
                                                  0.11579956114292145,
                                                  0.1158096119761467,
                                                  0.11577166616916656,
                                                  0.11578544974327087,
                                                  0.11576415598392487,
                                                  0.11577217280864716,
                                                  0.1157451793551445,
                                                  0.1157572790980339,
                                                  0.1158934161067009,
                                                  0.11580001562833786,
                                                  0.11593358963727951,
                                                  0.11615775525569916,
                                                  0.11587931215763092,
                                                  0.1159670352935791,
                                                  0.11585234105587006,
                                                  0.11583954095840454,
                                                  0.11609325557947159,
                                                  0.11583058536052704,
                                                  0.11599991470575333,
                                                  0.11598140746355057,
                                                  0.11592470854520798,
                                                  0.11593374609947205,
                                                  0.11602354794740677,
                                                  0.11578560620546341,
                                                  0.11599435657262802,
                                                  0.11563146859407425,
                                                  0.11705952137708664,
                                                  0.11598608642816544,
                                                  0.11604960262775421,
                                                  0.11593892425298691,
                                                  0.1176956295967102,
                                                  0.12361403554677963,
                                                  0.11607111245393753,
                                                  0.11594823747873306,
                                                  0.11593722552061081,
                                                  0.11586553603410721,
                                                  0.11618652194738388,
                                                  0.11610959470272064,
                                                  0.11593214422464371,
                                                  0.11584636569023132,
                                                  0.1159195527434349,
                                                  0.11607397347688675,
                                                  0.1159905418753624,
                                                  0.1159055307507515,
                                                  0.11624743789434433,
                                                  0.11588098853826523,
                                                  0.11602722853422165,
                                                  0.11587972939014435,
                                                  0.11616445332765579,
                                                  0.11607798933982849,
                                                  0.11569825559854507,
                                                  0.11504572629928589,
                                                  0.11600799113512039,
                                                  0.11600035429000854,
                                                  0.11612346768379211,
                                                  0.11608055979013443,
                                                  0.11566175520420074,
                                                  0.1160079687833786,
                                                  0.11582132428884506,
                                                  0.11605935543775558,
                                                  0.11587560176849365,
                                                  0.11589808017015457,
                                                  0.11591998487710953,
                                                  0.11596931517124176,
                                                  0.11618545651435852,
                                                  0.11614586412906647,
                                                  0.116184763610363,
                                                  0.11593402177095413,
                                                  0.11613350361585617,
                                                  0.11606571078300476,
                                                  0.11616233736276627,
                                                  0.11608292907476425,
                                                  0.11600427329540253,
                                                  0.11627335101366043,
                                                  0.11593248695135117,
                                                  0.11602398753166199,
                                                  0.11632940173149109,
                                                  0.11617862433195114,
                                                  0.11615864932537079,
                                                  0.11609946191310883,
                                                  0.11620981246232986,
                                                  0.11623332649469376,
                                                  0.11621405184268951,
                                                  0.11617275327444077,
                                                  0.11618194729089737,
                                                  0.11618026345968246,
                                                  0.11604853719472885,
                                                  0.11610307544469833,
                                                  0.11616066843271255,
                                                  0.11609618365764618,
                                                  0.11606502532958984,
                                                  0.11607180535793304,
                                                  0.11610879749059677,
                                                  0.11615218967199326,
                                                  0.11595457047224045,
                                                  0.1159059926867485,
                                                  0.11616579443216324,
                                                  0.11522948741912842,
                                                  0.11619925498962402,
                                                  0.11620134115219116,
                                                  0.11613330245018005,
                                                  0.11608000099658966,
                                                  0.11618129909038544,
                                                  0.11634077876806259,
                                                  0.11618518084287643,
                                                  0.11617344617843628,
                                                  0.11647146195173264,
                                                  0.11618098616600037,
                                                  0.11633583903312683,
                                                  0.11608415842056274,
                                                  0.11629339307546616,
                                                  0.11636802554130554,
                                                  0.11612612754106522,
                                                  0.11618626117706299,
                                                  0.11609125882387161,
                                                  0.11604174226522446,
                                                  0.11616165190935135,
                                                  0.11629405617713928,
                                                  0.12194935232400894,
                                                  0.11632195115089417,
                                                  0.1196022555232048,
                                                  0.11585008352994919,
                                                  0.12247772514820099,
                                                  0.12818971276283264,
                                                  0.12019037455320358,
                                                  0.12467972934246063,
                                                  0.11633002012968063,
                                                  0.1162014976143837,
                                                  0.11621222645044327,
                                                  0.09137231856584549,
                                                  0.09150426834821701,
                                                  0.09157953411340714,
                                                  0.09157821536064148,
                                                  0.09164956212043762,
                                                  0.09170088171958923,
                                                  0.09174489974975586,
                                                  0.09180380403995514,
                                                  0.09149286895990372,
                                                  0.09157092124223709,
                                                  0.09140341728925705,
                                                  0.09140137583017349,
                                                  0.09056924283504486,
                                                  0.09073775261640549,
                                                  0.09158705919981003,
                                                  0.0913533940911293,
                                                  2.4094927310943604,
                                                  0.09106162190437317,
                                                  0.0903748944401741,
                                                  0.09066660702228546,
                                                  0.09095553308725357,
                                                  0.09037058055400848,
                                                  0.09065017104148865,
                                                  0.09140996634960175,
                                                  0.0915386751294136,
                                                  0.09155571460723877,
                                                  0.09167596697807312,
                                                  0.09157241880893707,
                                                  0.09159300476312637,
                                                  0.09173470735549927,
                                                  0.09168659150600433,
                                                  0.09159538149833679,
                                                  0.09157337993383408,
                                                  0.09162420779466629,
                                                  0.0916445180773735,
                                                  0.09159063547849655,
                                                  0.09151946753263474,
                                                  0.09143755584955215,
                                                  0.09167628735303879,
                                                  0.09162566065788269,
                                                  0.09144790470600128,
                                                  0.09167981147766113,
                                                  0.09183721989393234,
                                                  0.09176892042160034,
                                                  0.09148874878883362,
                                                  0.09179455041885376,
                                                  0.09149231016635895,
                                                  0.09171196818351746,
                                                  0.09166552871465683,
                                                  0.09163936972618103,
                                                  0.09166969358921051,
                                                  0.09118418395519257,
                                                  0.09167936444282532,
                                                  0.09164978563785553,
                                                  0.0916447639465332,
                                                  0.091525137424469,
                                                  0.0916038230061531,
                                                  0.09174405038356781,
                                                  0.09161648899316788,
                                                  0.09172605723142624,
                                                  0.09169495105743408,
                                                  0.09160944074392319,
                                                  0.09177764505147934,
                                                  0.09175115078687668,
                                                  0.09168726205825806,
                                                  0.09168113023042679,
                                                  0.0916929617524147,
                                                  0.0916459709405899,
                                                  0.09187834709882736,
                                                  0.09167644381523132,
                                                  0.0916120707988739,
                                                  0.09165194630622864,
                                                  0.09167271107435226,
                                                  0.09164658933877945,
                                                  0.09172767400741577,
                                                  0.09166054427623749,
                                                  0.09191282838582993,
                                                  0.0919322669506073,
                                                  0.09164082258939743,
                                                  0.09164590388536453,
                                                  0.09164344519376755,
                                                  0.09161072224378586,
                                                  0.09165661782026291,
                                                  0.09178908169269562,
                                                  0.09179839491844177,
                                                  0.09184478968381882,
                                                  0.09157416224479675,
                                                  0.09172283858060837,
                                                  0.09165951609611511,
                                                  0.09154213964939117,
                                                  0.09159321337938309,
                                                  0.09175139665603638,
                                                  0.0916604995727539,
                                                  0.09152602404356003,
                                                  0.09157373756170273,
                                                  0.09160105139017105,
                                                  0.09054151177406311,
                                                  0.09138046205043793,
                                                  0.0918545126914978,
                                                  0.09164437651634216,
                                                  0.09164205938577652,
                                                  0.09161880612373352,
                                                  0.09176848083734512,
                                                  0.0915747582912445,
                                                  0.09165892750024796,
                                                  0.09155618399381638,
                                                  0.09165655821561813,
                                                  0.09176354855298996,
                                                  0.0917825847864151,
                                                  0.09167511016130447,
                                                  0.09172207117080688,
                                                  0.09178581088781357,
                                                  0.09048595279455185,
                                                  0.09156113862991333,
                                                  0.09158486872911453,
                                                  0.09151524305343628,
                                                  0.09164683520793915,
                                                  0.09167765825986862,
                                                  0.09159915894269943,
                                                  0.09176617115736008,
                                                  0.09172510355710983,
                                                  0.09160268306732178,
                                                  0.09161321818828583,
                                                  0.0915767028927803,
                                                  0.0914856418967247,
                                                  0.0916421189904213,
                                                  0.09157934784889221,
                                                  0.09156793355941772,
                                                  0.09154218435287476,
                                                  0.09154070168733597,
                                                  134901216.0,
                                                  0.09168075770139694,
                                                  0.09144774079322815,
                                                  0.09149137884378433,
                                                  0.09155774861574173,
                                                  0.09148041158914566,
                                                  0.09169764071702957,
                                                  0.0916762501001358,
                                                  0.09158093482255936,
                                                  0.09154773503541946,
                                                  0.0916462242603302,
                                                  0.0915762335062027,
                                                  0.09153687953948975,
                                                  0.09146840870380402,
                                                  0.09179212152957916,
                                                  0.0917789414525032,
                                                  0.09176136553287506,
                                                  0.09159605950117111,
                                                  0.09162770956754684,
                                                  0.09160606563091278,
                                                  0.09176835417747498,
                                                  0.09157654643058777,
                                                  0.0916082113981247,
                                                  0.09162461757659912,
                                                  0.09182053804397583,
                                                  0.09177893400192261,
                                                  0.09165219217538834,
                                                  0.09159030020236969,
                                                  0.09154009819030762,
                                                  0.09164740145206451,
                                                  0.09176388382911682,
                                                  0.0915454551577568,
                                                  0.09177467226982117,
                                                  0.09191010892391205,
                                                  0.09156614542007446,
                                                  0.09143801033496857,
                                                  0.09114047884941101,
                                                  0.09068604558706284,
                                                  0.09070765227079391,
                                                  0.09103754162788391,
                                                  0.09052861481904984,
                                                  0.09132906794548035,
                                                  0.09160716831684113,
                                                  0.09083017706871033,
                                                  0.09090922772884369,
                                                  0.09165212512016296,
                                                  0.09168437123298645,
                                                  0.09145098924636841,
                                                  0.09153489023447037,
                                                  8632.75390625,
                                                  0.09173749387264252,
                                                  0.09159638732671738,
                                                  0.09155716747045517,
                                                  0.09143556654453278,
                                                  0.09150324761867523,
                                                  0.09115756303071976,
                                                  0.09095091372728348,
                                                  0.09065255522727966,
                                                  0.09077665954828262,
                                                  0.09125294536352158,
                                                  0.09146241843700409,
                                                  0.0917263850569725,
                                                  0.09158481657505035,
                                                  0.09154310077428818,
                                                  0.09155087172985077,
                                                  0.09157835692167282,
                                                  0.09175891429185867,
                                                  0.09174496680498123,
                                                  0.0915360376238823,
                                                  0.0916609838604927,
                                                  0.09006042033433914,
                                                  0.09004908800125122,
                                                  0.08979449421167374,
                                                  0.0896245688199997,
                                                  0.08980648964643478,
                                                  0.0895053967833519,
                                                  0.08965669572353363,
                                                  0.09002640843391418,
                                                  0.08996382355690002,
                                                  0.08966709673404694,
                                                  0.0897647812962532,
                                                  0.08978300541639328,
                                                  0.08981018513441086,
                                                  0.0896415039896965,
                                                  0.0901375487446785,
                                                  0.089830681681633,
                                                  0.09017007797956467,
                                                  0.0898798406124115,
                                                  0.09025020152330399,
                                                  0.0899721086025238,
                                                  0.08988645672798157,
                                                  0.09003996104001999,
                                                  0.08998358994722366,
                                                  0.0893966481089592,
                                                  0.08915132284164429,
                                                  0.08969417959451675,
                                                  0.08966122567653656,
                                                  0.08971626311540604,
                                                  0.08974547684192657,
                                                  0.08988585323095322,
                                                  0.09006890654563904,
                                                  0.09015759825706482,
                                                  0.08985879272222519,
                                                  0.09011870622634888,
                                                  0.089875727891922,
                                                  0.08984020352363586,
                                                  0.08999879658222198,
                                                  0.08929476141929626,
                                                  0.08917342871427536,
                                                  0.0890253409743309,
                                                  0.0890451967716217,
                                                  0.08985283225774765,
                                                  0.08977603167295456,
                                                  0.08965098857879639,
                                                  0.09000735729932785,
                                                  0.0896926000714302,
                                                  0.0897701233625412,
                                                  0.08995763957500458,
                                                  0.0898318812251091,
                                                  0.08998756855726242,
                                                  0.09007910639047623,
                                                  0.09007957577705383,
                                                  0.08997736871242523,
                                                  0.08961615711450577,
                                                  0.09001019597053528,
                                                  0.08989803493022919,
                                                  0.09013975411653519,
                                                  0.0902264267206192,
                                                  0.0899982675909996,
                                                  0.08958622068166733,
                                                  0.08898205310106277,
                                                  0.08928899466991425,
                                                  0.08966180682182312,
                                                  0.0896264836192131,
                                                  0.0899060070514679,
                                                  165786832.0,
                                                  0.08958649635314941,
                                                  0.08978855609893799,
                                                  0.0899762436747551,
                                                  0.09000223875045776,
                                                  0.08978422731161118,
                                                  0.09043993800878525,
                                                  0.09028058499097824,
                                                  0.1005210429430008,
                                                  0.08993940055370331,
                                                  0.08960286527872086,
                                                  0.08972615748643875,
                                                  0.08975361287593842,
                                                  0.08965158462524414,
                                                  0.08964148908853531,
                                                  0.08972785621881485,
                                                  0.0897819921374321,
                                                  0.08990558981895447,
                                                  0.08987738937139511,
                                                  0.09000679105520248,
                                                  0.08997351676225662,
                                                  0.08983514457941055,
                                                  0.09013941884040833,
                                                  0.09014367312192917,
                                                  0.09003666788339615,
                                                  0.09019093215465546,
                                                  0.08984865993261337,
                                                  0.08995110541582108,
                                                  0.08969511836767197,
                                                  0.08997870236635208,
                                                  0.08957649022340775,
                                                  0.08970191329717636,
                                                  0.08933154493570328,
                                                  0.08878691494464874,
                                                  0.08927902579307556,
                                                  0.08946020156145096,
                                                  0.08946799486875534,
                                                  0.08914709836244583,
                                                  0.08887948840856552,
                                                  0.08919765800237656,
                                                  0.0895165354013443,
                                                  0.08895201981067657,
                                                  0.08957294374704361,
                                                  0.08974824845790863,
                                                  0.08876839280128479,
                                                  0.08959838002920151,
                                                  0.08914162218570709,
                                                  0.08941791206598282,
                                                  0.08913740515708923,
                                                  0.08939765393733978,
                                                  0.09013853222131729,
                                                  0.08985241502523422,
                                                  0.0897478461265564,
                                                  0.08989280462265015,
                                                  0.08953097462654114,
                                                  0.08996637165546417,
                                                  0.08999262005090714,
                                                  0.09000848233699799,
                                                  0.08972601592540741,
                                                  0.08974424004554749,
                                                  0.08952955156564713,
                                                  0.08970942348241806,
                                                  0.0896824523806572,
                                                  0.0899357870221138,
                                                  0.0900910422205925,
                                                  0.08987075090408325,
                                                  0.09012211114168167,
                                                  0.0899343490600586,
                                                  0.08979938924312592,
                                                  0.08996124565601349,
                                                  0.09016357362270355,
                                                  0.09030314534902573,
                                                  0.09023184329271317,
                                                  0.09009277075529099,
                                                  0.08956193923950195,
                                                  0.09018462896347046,
                                                  0.08991622179746628,
                                                  0.08998739719390869,
                                                  0.09010801464319229,
                                                  0.08995979279279709,
                                                  0.08951814472675323,
                                                  0.09035605937242508,
                                                  0.08999628573656082,
                                                  0.08997727185487747,
                                                  0.09033078700304031,
                                                  0.08996672928333282,
                                                  0.09028850495815277,
                                                  0.0895623043179512,
                                                  0.08894254267215729,
                                                  0.09029853343963623,
                                                  0.08997199684381485,
                                                  0.08996271342039108,
                                                  0.09009357541799545,
                                                  0.09012578427791595,
                                                  0.08983942866325378,
                                                  0.08987858891487122,
                                                  0.08974911272525787,
                                                  0.08983664214611053,
                                                  0.08988182246685028,
                                                  0.0900752916932106,
                                                  0.08942669630050659,
                                                  0.08933917433023453,
                                                  0.08779210597276688,
                                                  0.08927730470895767,
                                                  0.08909048140048981,
                                                  0.0898263081908226,
                                                  0.08931832760572433,
                                                  0.08928386121988297,
                                                  0.08991234749555588,
                                                  0.08994374424219131,
                                                  0.09000713378190994,
                                                  0.08975329250097275,
                                                  0.09016995877027512,
                                                  0.08982622623443604,
                                                  0.09043461084365845,
                                                  0.09003646671772003,
                                                  0.0898413360118866,
                                                  0.08992080390453339,
                                                  0.08976344019174576,
                                                  0.09005337953567505,
                                                  0.09002504497766495,
                                                  0.08987294137477875,
                                                  0.08990441262722015,
                                                  0.08986706286668777,
                                                  0.09007862210273743,
                                                  0.08986128121614456,
                                                  0.08975748717784882,
                                                  0.0894976556301117,
                                                  0.08977871388196945,
                                                  0.08997070044279099,
                                                  0.0896330401301384,
                                                  0.09019531309604645,
                                                  0.09008292108774185,
                                                  0.09006659686565399,
                                                  0.08975100517272949],
                             'weight_decay': 0.0005}},
 'models': [{'accuracy': 0.0,
             'batch_size': 32,
             'cv_score': 0.0053345869543231185,
             'cv_val_accuracy': 0.6666666666666666,
             'cv_val_loss': 0.09920807182788849,
             'cv_val_macroF1': 0.0053345869543231185,
             'cv_val_microF1': 0.066966913828706,
             'epochs': 200,
             'final_model': GGNN1(
  (ggnn): GatedGraphConv(60, num_layers=2)
  (fc1): Linear(in_features=60, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
             'kwargs': {'aggr_type': 'mean',
                        'd1': 60,
                        'd2': 50,
                        'num_classes': 24,
                        'num_layers': 2},
             'learning_rate': 0.01,
             'macroF1': 0.006748704489763125,
             'microF1': 0.08414023372287145,
             'model': <class 'TFM_graph_classification_models.GGNN1'>,
             'model_instance': GGNN1(
  (ggnn): GatedGraphConv(60, num_layers=2)
  (fc1): Linear(in_features=60, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
             'score': 'f1_macro',
             'time': 2764.828370809555,
             'train_loss_history': [362103776.0,
                                    1984.16552734375,
                                    4208493824.0,
                                    660907968.0,
                                    0.08377974480390549,
                                    1243555968.0,
                                    1017.9180297851562,
                                    76571.46875,
                                    308.40338134765625,
                                    23220.69921875,
                                    156603248.0,
                                    138.022216796875,
                                    0.08351406455039978,
                                    1823248640.0,
                                    155343744.0,
                                    0.0847235918045044,
                                    0.08452852815389633,
                                    0.08426041901111603,
                                    0.0840001031756401,
                                    0.0840369313955307,
                                    0.0836704894900322,
                                    0.08393912762403488,
                                    0.08424623310565948,
                                    0.08425412327051163,
                                    0.08409769088029861,
                                    0.08399365842342377,
                                    0.08431313186883926,
                                    0.08411368727684021,
                                    19.82664680480957,
                                    0.08441364020109177,
                                    0.08446607738733292,
                                    0.08450149744749069,
                                    0.08464515209197998,
                                    0.08455231040716171,
                                    0.08460909128189087,
                                    0.08457177132368088,
                                    0.0844896212220192,
                                    0.08443144708871841,
                                    7615.97705078125,
                                    5.158772945404053,
                                    0.08454208821058273,
                                    0.08437103033065796,
                                    0.08442801982164383,
                                    0.08480384200811386,
                                    0.08451271802186966,
                                    0.0846419557929039,
                                    0.08445870131254196,
                                    0.08451291173696518,
                                    7.508358001708984,
                                    7220202.5,
                                    208.47743225097656,
                                    0.08452831208705902,
                                    0.08444917947053909,
                                    0.08455681055784225,
                                    0.08457385748624802,
                                    0.08448221534490585,
                                    0.08441825956106186,
                                    0.08439182490110397,
                                    0.08427322655916214,
                                    0.08453415334224701,
                                    0.08448854833841324,
                                    0.08462214469909668,
                                    0.08452510088682175,
                                    0.08454762399196625,
                                    0.08453730493783951,
                                    0.08453856408596039,
                                    0.08457324653863907,
                                    0.08439980447292328,
                                    0.08457770943641663,
                                    0.08448946475982666,
                                    0.08455544710159302,
                                    0.08448326587677002,
                                    0.0853704884648323,
                                    0.0845303013920784,
                                    0.08449558913707733,
                                    0.08448614180088043,
                                    0.08432622253894806,
                                    0.08439280092716217,
                                    0.08428298681974411,
                                    0.08441869169473648,
                                    2.7614922523498535,
                                    0.0844237357378006,
                                    0.0845165103673935,
                                    3.4590933322906494,
                                    0.08449532091617584,
                                    0.0845257043838501,
                                    0.08447345346212387,
                                    0.08450763672590256,
                                    0.08446920663118362,
                                    0.08451981097459793,
                                    385.1795959472656,
                                    0.08447851985692978,
                                    0.08457182347774506,
                                    0.08445488661527634,
                                    0.7349956631660461,
                                    0.0845390260219574,
                                    437.2469787597656,
                                    0.08416557312011719,
                                    0.08421652764081955,
                                    0.08440131694078445,
                                    0.08433077484369278,
                                    0.08428847789764404,
                                    0.08411938697099686,
                                    0.08432593941688538,
                                    0.08455510437488556,
                                    0.08454146236181259,
                                    0.08451537787914276,
                                    0.08447898924350739,
                                    0.08454417437314987,
                                    0.08451075106859207,
                                    0.08446026593446732,
                                    0.08439700305461884,
                                    0.08444203436374664,
                                    324.5517578125,
                                    0.084525465965271,
                                    0.08450353890657425,
                                    0.08454328775405884,
                                    0.08446735888719559,
                                    0.0844758078455925,
                                    0.08446711301803589,
                                    0.08434131741523743,
                                    0.0844181552529335,
                                    0.08407874405384064,
                                    0.08442263305187225,
                                    0.08430887013673782,
                                    0.08445868641138077,
                                    0.08432488143444061,
                                    0.08438342809677124,
                                    0.08434324711561203,
                                    0.08433690667152405,
                                    0.08442634344100952,
                                    0.08430664241313934,
                                    0.08431776612997055,
                                    3179.368408203125,
                                    0.08431804180145264,
                                    0.0844300165772438,
                                    327.7389831542969,
                                    0.08442189544439316,
                                    0.08446188271045685,
                                    0.08456380665302277,
                                    0.08448801934719086,
                                    0.08452309668064117,
                                    0.08443786203861237,
                                    0.08450938016176224,
                                    0.08452746272087097,
                                    0.0845022052526474,
                                    0.08435150235891342,
                                    0.0843622088432312,
                                    0.08451417833566666,
                                    0.08449152857065201,
                                    2792218.0,
                                    0.08448152989149094,
                                    0.08447089046239853,
                                    0.08448109775781631,
                                    0.08455649018287659,
                                    11.825798988342285,
                                    0.0844212993979454,
                                    0.08447621017694473,
                                    0.08445154875516891,
                                    0.08450216054916382,
                                    0.08448535948991776,
                                    0.08452288061380386,
                                    2.2303872108459473,
                                    0.08445139229297638,
                                    22.94561195373535,
                                    0.08438808470964432,
                                    0.08447092026472092,
                                    0.0844135656952858,
                                    0.08435672521591187,
                                    4.1318182945251465,
                                    0.0844360813498497,
                                    0.08448848873376846,
                                    0.08441183716058731,
                                    0.08452562987804413,
                                    0.08434139937162399,
                                    15509.2685546875,
                                    0.08440298587083817,
                                    0.0843038558959961,
                                    0.08409386873245239,
                                    0.08434053510427475,
                                    0.08435279130935669,
                                    0.0843585804104805,
                                    1622.2698974609375,
                                    6.130885601043701,
                                    513.2769165039062,
                                    0.08451929688453674,
                                    0.0844656303524971,
                                    0.08448315411806107,
                                    0.08443838357925415,
                                    657.5328979492188,
                                    0.08391217887401581,
                                    0.08355443924665451,
                                    0.08408589661121368,
                                    0.08395949006080627,
                                    0.08368541300296783,
                                    0.09440038353204727,
                                    0.08442547917366028,
                                    0.08448342233896255,
                                    0.08452529460191727,
                                    0.08449804782867432,
                                    0.0958980917930603,
                                    0.0944596529006958,
                                    0.09449319541454315,
                                    0.0945969820022583,
                                    0.09455905109643936,
                                    0.09455728530883789,
                                    0.0946040078997612,
                                    0.5912741422653198,
                                    0.0945107489824295,
                                    0.09456121921539307,
                                    0.09452136605978012,
                                    0.09451890736818314,
                                    0.09491711109876633,
                                    0.0936649888753891,
                                    0.17639370262622833,
                                    227.92086791992188,
                                    0.0940268412232399,
                                    0.21170301735401154,
                                    0.09379062056541443,
                                    0.09365381300449371,
                                    0.09371867030858994,
                                    0.09373131394386292,
                                    0.09375094622373581,
                                    0.09409426152706146,
                                    0.09444309771060944,
                                    2.2909059524536133,
                                    408.70733642578125,
                                    0.09448052197694778,
                                    0.09452381730079651,
                                    0.09452628344297409,
                                    0.09451580047607422,
                                    0.09457345306873322,
                                    0.09455623477697372,
                                    0.09454062581062317,
                                    0.09454816579818726,
                                    0.09447815269231796,
                                    0.09455271065235138,
                                    0.0945919007062912,
                                    0.09438098222017288,
                                    0.09447444975376129,
                                    0.09452714771032333,
                                    0.09452454000711441,
                                    0.09455770254135132,
                                    0.09453456103801727,
                                    0.09453591704368591,
                                    0.0945412740111351,
                                    5.31784200668335,
                                    0.09444911777973175,
                                    1.2043771743774414,
                                    0.09460223466157913,
                                    0.09458258748054504,
                                    0.09454575926065445,
                                    0.09464789927005768,
                                    0.09462427347898483,
                                    0.2161496877670288,
                                    0.09452006220817566,
                                    0.0945173129439354,
                                    0.09459420293569565,
                                    45.2691764831543,
                                    0.0946098044514656,
                                    0.09459919482469559,
                                    0.09453676640987396,
                                    0.09455496072769165,
                                    0.09455988556146622,
                                    0.09453597664833069,
                                    1460.126708984375,
                                    0.09449618309736252,
                                    0.09447479248046875,
                                    0.0946631208062172,
                                    0.09447914361953735,
                                    0.09451417624950409,
                                    0.09457054734230042,
                                    578.6171264648438,
                                    0.09508994966745377,
                                    0.09456158429384232,
                                    0.09453702718019485,
                                    92.70061492919922,
                                    575.2883911132812,
                                    0.09449885785579681,
                                    0.09459429234266281,
                                    0.09453769028186798,
                                    0.09457365423440933,
                                    0.2661488950252533,
                                    0.5785210728645325,
                                    1.0697529315948486,
                                    9533.2490234375,
                                    36278084.0,
                                    0.09451647102832794,
                                    0.0945221334695816,
                                    0.0945245772600174,
                                    0.49862831830978394,
                                    0.1389312595129013,
                                    0.10536698997020721,
                                    0.6453412175178528,
                                    0.09668745845556259,
                                    7082514.5,
                                    0.09513116627931595,
                                    0.09418614208698273,
                                    0.09460048377513885,
                                    0.09451752156019211,
                                    0.09452079981565475,
                                    0.11058186739683151,
                                    0.09448027610778809,
                                    95075.0,
                                    0.09450844675302505,
                                    0.40205472707748413,
                                    0.09456601738929749,
                                    21503.685546875,
                                    25433.630859375,
                                    0.09459114074707031,
                                    1.827946662902832,
                                    0.09454144537448883,
                                    0.09408380091190338,
                                    0.09439128637313843,
                                    0.09441068768501282,
                                    75790.1328125,
                                    2222.14990234375,
                                    0.09460681676864624,
                                    0.09455598890781403,
                                    0.09462475776672363,
                                    0.09453312307596207,
                                    0.0945381447672844,
                                    0.09712633490562439,
                                    15.168710708618164,
                                    0.09447479993104935,
                                    0.09447994828224182,
                                    0.09455700218677521,
                                    1.1578611135482788,
                                    0.09453558921813965,
                                    0.0945739671587944,
                                    0.09440802037715912,
                                    7.872211933135986,
                                    0.09455535560846329,
                                    0.09453728795051575,
                                    0.09456964582204819,
                                    0.09449167549610138,
                                    403448.90625,
                                    0.11504217982292175,
                                    0.09461946040391922,
                                    0.09448207169771194,
                                    1.0811370611190796,
                                    0.09459654241800308,
                                    0.09441133588552475,
                                    0.0943896472454071,
                                    5106.0791015625,
                                    0.09468978643417358,
                                    0.09459315240383148,
                                    0.09458445757627487,
                                    12.510099411010742,
                                    0.09466719627380371,
                                    0.09455475211143494,
                                    20.623863220214844,
                                    0.09454236924648285,
                                    8.770496368408203,
                                    12.305490493774414,
                                    0.09461303800344467,
                                    0.09462690353393555,
                                    0.09455135464668274,
                                    0.09457971900701523,
                                    0.09463999420404434,
                                    0.09459368139505386,
                                    39.026275634765625,
                                    0.09459409862756729,
                                    0.14653317630290985,
                                    0.09447965025901794,
                                    0.09448003023862839,
                                    0.09420546144247055,
                                    0.10086872428655624,
                                    0.09402845054864883,
                                    0.09396755695343018,
                                    0.09385461360216141,
                                    0.7617712020874023,
                                    3.599776268005371,
                                    0.0940251424908638,
                                    0.09385236352682114,
                                    1.5394641160964966,
                                    555.6378784179688,
                                    21.09204864501953,
                                    0.09456101059913635,
                                    0.09457747638225555,
                                    0.20754975080490112,
                                    0.09456492215394974,
                                    0.09465780109167099,
                                    0.09462746232748032,
                                    0.09462113678455353,
                                    4181469.0,
                                    0.11877608299255371,
                                    0.09787335991859436,
                                    0.09376204758882523,
                                    0.09390174597501755,
                                    609.8927001953125,
                                    0.0945817232131958,
                                    56.260704040527344,
                                    0.09460467100143433,
                                    0.16732440888881683,
                                    0.09459473192691803,
                                    0.09458056092262268,
                                    0.09451653063297272,
                                    0.09456532448530197,
                                    0.09456890821456909,
                                    0.09472556412220001,
                                    0.09467514604330063,
                                    0.09467890858650208,
                                    0.09461469203233719,
                                    0.09461411088705063,
                                    0.09406077861785889,
                                    0.09436465054750443,
                                    6.517511367797852,
                                    0.09469257295131683,
                                    22.83505630493164,
                                    21.33751106262207,
                                    0.09463278949260712,
                                    1789.422119140625,
                                    435.6246337890625,
                                    0.24851779639720917,
                                    0.09462091326713562,
                                    614.681640625,
                                    0.09470755606889725,
                                    178.90574645996094,
                                    0.09464573115110397,
                                    0.09471963346004486,
                                    0.09465103596448898,
                                    0.6563721895217896,
                                    704190.0,
                                    0.09396739304065704,
                                    0.09447432309389114,
                                    0.09416063874959946,
                                    0.0944053903222084,
                                    65.62581634521484,
                                    0.09450186043977737,
                                    1024.021240234375,
                                    0.09452372044324875,
                                    0.09453118592500687,
                                    0.09463518112897873,
                                    0.09458576887845993,
                                    0.10458575189113617,
                                    47422.390625,
                                    0.09453989565372467,
                                    0.19003771245479584,
                                    0.09361455589532852,
                                    0.09875020384788513,
                                    0.10593827068805695,
                                    0.09450165182352066,
                                    2.2607531547546387,
                                    146.7675018310547,
                                    0.09460194408893585,
                                    0.09467209130525589,
                                    0.09463077038526535,
                                    0.09450284391641617,
                                    0.09454219788312912,
                                    0.09467772394418716,
                                    0.09461057186126709,
                                    301.4787902832031,
                                    0.0944533422589302,
                                    0.09459809958934784,
                                    0.09464976191520691,
                                    0.09458554536104202,
                                    0.09468982368707657,
                                    0.09466410428285599,
                                    0.5748986601829529,
                                    0.09391465783119202,
                                    0.09361892193555832,
                                    13.32568073272705,
                                    0.09446100890636444,
                                    77.79401397705078,
                                    2.8922955989837646,
                                    0.1703469306230545,
                                    7399.919921875,
                                    4709.5478515625,
                                    0.09465955197811127,
                                    0.09466127306222916,
                                    0.09468183666467667,
                                    0.09468203783035278,
                                    19.022647857666016,
                                    0.09639277309179306,
                                    0.09449823945760727,
                                    0.09443557262420654,
                                    0.09912924468517303,
                                    0.09453266113996506,
                                    0.09452731162309647,
                                    0.09462825953960419,
                                    0.09465084969997406,
                                    0.09445875138044357,
                                    9395.8203125,
                                    4131.25390625,
                                    0.09448602050542831,
                                    0.09457829594612122,
                                    0.09452693164348602,
                                    0.41866934299468994,
                                    0.09443027526140213,
                                    1.5612469911575317,
                                    0.09447776526212692,
                                    0.09464254975318909,
                                    0.0947151929140091,
                                    0.09461060166358948,
                                    1.6179649829864502,
                                    151.97926330566406,
                                    0.09670396894216537,
                                    0.09371296316385269,
                                    0.22721385955810547,
                                    0.09369426220655441,
                                    0.09369219094514847,
                                    0.0936376303434372,
                                    519.0321655273438,
                                    2733.38671875,
                                    0.09362778812646866,
                                    0.1038171648979187,
                                    0.36694493889808655,
                                    0.09366429597139359,
                                    0.09367946535348892,
                                    0.093511663377285,
                                    363061.71875,
                                    0.09354864805936813,
                                    0.09355273097753525,
                                    0.0940803587436676,
                                    0.09443733096122742,
                                    0.09446991235017776,
                                    0.09449158608913422,
                                    0.09442201256752014,
                                    0.09459392726421356,
                                    0.09461551159620285,
                                    3.2891151905059814,
                                    0.09446682780981064,
                                    214763.484375,
                                    0.0945248231291771,
                                    0.0945204347372055,
                                    1.0939825773239136,
                                    0.09466331452131271,
                                    0.09470713883638382,
                                    0.09462140500545502,
                                    0.0944676548242569,
                                    0.09462480992078781,
                                    0.0944826528429985,
                                    62.18036651611328,
                                    1.1048225164413452,
                                    7.396787166595459,
                                    0.09464084357023239,
                                    0.09469672292470932,
                                    0.09460265189409256,
                                    37.437835693359375,
                                    0.09448858350515366,
                                    13.001826286315918,
                                    45.448062896728516,
                                    0.0946521982550621,
                                    0.09455090761184692,
                                    0.2181532233953476,
                                    248.3521728515625,
                                    0.094516322016716,
                                    0.09450066834688187,
                                    0.09460214525461197,
                                    1419.5380859375,
                                    0.09455197304487228,
                                    89.7099380493164,
                                    0.09380219131708145,
                                    0.09361878782510757,
                                    0.09440010786056519,
                                    33.71955490112305,
                                    0.0946076437830925,
                                    0.09467274695634842,
                                    0.09467364847660065,
                                    6.568045139312744,
                                    0.0946686714887619,
                                    0.11860518157482147,
                                    144.5919952392578,
                                    0.0945608839392662,
                                    0.0977327898144722,
                                    0.09378885477781296,
                                    0.4567955732345581,
                                    0.28246936202049255,
                                    0.0937114804983139,
                                    3.4494714736938477,
                                    0.09347078949213028,
                                    1.9993853569030762,
                                    694.1502685546875,
                                    0.09461234509944916,
                                    0.09463471919298172,
                                    0.09461282938718796,
                                    0.09465289115905762,
                                    0.1086336076259613,
                                    0.09450510889291763,
                                    0.09465640783309937,
                                    0.0946459248661995,
                                    44.427127838134766,
                                    0.0945444405078888,
                                    0.09469397366046906,
                                    0.09456057101488113,
                                    0.09453918039798737,
                                    0.09460191428661346,
                                    16.45922088623047,
                                    0.09442133456468582,
                                    766.4246826171875,
                                    0.0944996252655983,
                                    57.03985595703125,
                                    0.09441376477479935,
                                    0.09458398073911667,
                                    4.6840386390686035,
                                    0.10124408453702927,
                                    0.09464626759290695,
                                    0.0945911556482315,
                                    34.607200622558594],
             'val_accuracy_history': [1.736842105263158,
                                      2.210526315789474,
                                      0.05263157894736842,
                                      1.0,
                                      0.8947368421052632,
                                      0.9473684210526315,
                                      0.2631578947368421,
                                      0.6842105263157895,
                                      1.5789473684210527,
                                      0.7368421052631579,
                                      1.0,
                                      1.368421052631579,
                                      0.8947368421052632,
                                      0.0,
                                      0.15789473684210525,
                                      0.3684210526315789,
                                      1.5789473684210527,
                                      1.368421052631579,
                                      1.4736842105263157,
                                      0.2631578947368421,
                                      0.3684210526315789,
                                      1.631578947368421,
                                      0.47368421052631576,
                                      1.368421052631579,
                                      0.5789473684210527,
                                      0.42105263157894735,
                                      1.0,
                                      0.5789473684210527,
                                      0.6842105263157895,
                                      1.0,
                                      0.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      2.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      1.105263157894737,
                                      1.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      2.0,
                                      0.3684210526315789,
                                      0.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      4.0,
                                      0.0,
                                      0.0,
                                      3.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      3.0,
                                      1.0,
                                      0.0,
                                      2.0,
                                      0.0,
                                      4.0,
                                      0.0,
                                      1.0,
                                      1.0526315789473684,
                                      0.3157894736842105,
                                      0.0,
                                      2.0,
                                      1.0,
                                      1.4736842105263157,
                                      0.42105263157894735,
                                      1.0,
                                      2.0,
                                      1.0,
                                      0.0,
                                      2.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      2.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      1.4736842105263157,
                                      1.0,
                                      0.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      2.0,
                                      1.0,
                                      0.0,
                                      3.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      2.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      2.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      3.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      2.0,
                                      2.0,
                                      2.0,
                                      1.0,
                                      1.0,
                                      2.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      2.0,
                                      3.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.15789473684210525,
                                      0.0,
                                      2.0,
                                      2.0,
                                      0.0,
                                      2.0,
                                      0.0,
                                      3.0,
                                      1.0,
                                      1.0,
                                      1.894736842105263,
                                      1.0,
                                      0.0,
                                      1.0,
                                      2.0,
                                      4.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      2.0,
                                      1.0,
                                      0.8947368421052632,
                                      0.21052631578947367,
                                      1.5263157894736843,
                                      0.21052631578947367,
                                      0.5789473684210527,
                                      0.2631578947368421,
                                      0.8947368421052632,
                                      1.0526315789473684,
                                      1.0,
                                      2.0,
                                      2.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      0.6666666666666666,
                                      3.0,
                                      0.0,
                                      0.2222222222222222,
                                      1.0,
                                      0.5555555555555556,
                                      0.2222222222222222,
                                      0.2222222222222222,
                                      1.7777777777777777,
                                      1.8888888888888888,
                                      1.0,
                                      2.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      2.0,
                                      2.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      1.5555555555555556,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.8888888888888888,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      2.0,
                                      0.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      0.2222222222222222,
                                      1.7777777777777777,
                                      0.4444444444444444,
                                      0.0,
                                      0.4444444444444444,
                                      0.0,
                                      2.0,
                                      0.0,
                                      0.2222222222222222,
                                      0.0,
                                      1.0,
                                      0.0,
                                      2.0,
                                      1.0,
                                      1.0,
                                      3.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      0.7777777777777778,
                                      0.8888888888888888,
                                      1.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      2.0,
                                      3.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      5.0,
                                      1.0,
                                      2.0,
                                      1.0,
                                      2.0,
                                      4.0,
                                      4.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      2.0,
                                      2.0,
                                      1.0,
                                      3.0,
                                      1.0,
                                      2.263157894736842,
                                      3.0,
                                      2.0,
                                      1.6842105263157894,
                                      0.0,
                                      1.0,
                                      3.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      2.0,
                                      1.0,
                                      5.0,
                                      0.0,
                                      2.789473684210526,
                                      0.0,
                                      1.894736842105263,
                                      1.5789473684210527,
                                      1.0,
                                      2.0,
                                      1.0,
                                      2.0,
                                      4.0,
                                      2.0,
                                      1.0,
                                      3.0,
                                      2.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      2.0,
                                      5.0,
                                      4.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      1.894736842105263,
                                      1.0526315789473684,
                                      1.4736842105263157,
                                      0.0,
                                      0.0,
                                      3.0,
                                      1.894736842105263,
                                      3.0,
                                      2.0,
                                      1.0,
                                      2.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      0.631578947368421,
                                      1.0,
                                      2.0,
                                      3.0,
                                      2.0,
                                      2.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      3.0,
                                      1.0,
                                      0.0,
                                      5.0,
                                      2.0,
                                      0.0,
                                      3.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      2.0,
                                      2.0,
                                      0.0,
                                      1.6842105263157894,
                                      2.0526315789473686,
                                      1.6842105263157894,
                                      2.4210526315789473,
                                      1.894736842105263,
                                      2.526315789473684,
                                      1.7894736842105263,
                                      1.631578947368421,
                                      1.5789473684210527,
                                      1.631578947368421,
                                      0.7894736842105263,
                                      1.0,
                                      1.7894736842105263,
                                      0.8947368421052632,
                                      1.0526315789473684,
                                      2.6315789473684212,
                                      1.1578947368421053,
                                      3.0,
                                      1.0,
                                      3.0,
                                      2.0,
                                      2.0,
                                      2.0,
                                      3.0,
                                      0.0,
                                      2.0,
                                      2.0,
                                      2.0,
                                      2.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      1.0,
                                      2.0,
                                      2.0,
                                      4.0,
                                      1.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      2.0,
                                      1.0,
                                      0.0,
                                      3.0,
                                      4.0,
                                      1.0,
                                      2.0,
                                      3.0,
                                      1.1578947368421053,
                                      0.0,
                                      1.0,
                                      2.0,
                                      2.0,
                                      1.0,
                                      3.0,
                                      2.1052631578947367,
                                      3.9473684210526314,
                                      0.3157894736842105,
                                      1.0,
                                      1.0,
                                      1.0,
                                      3.0,
                                      3.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      2.0,
                                      0.9473684210526315,
                                      1.8421052631578947,
                                      1.631578947368421,
                                      2.1052631578947367,
                                      1.4736842105263157,
                                      1.1578947368421053,
                                      2.8421052631578947,
                                      3.0,
                                      0.21052631578947367,
                                      1.0,
                                      1.0,
                                      1.0,
                                      3.0,
                                      0.0,
                                      0.0,
                                      3.0,
                                      0.0,
                                      2.0,
                                      1.0,
                                      0.0,
                                      3.0,
                                      1.0,
                                      0.0,
                                      2.0,
                                      1.0,
                                      0.0,
                                      3.0,
                                      1.0,
                                      1.0,
                                      2.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      3.0,
                                      0.0],
             'val_loss': 0.08975100517272949,
             'val_loss_history': [0.12889361381530762,
                                  0.12675809860229492,
                                  929197312.0,
                                  0.12727877497673035,
                                  0.12555724382400513,
                                  1098212864.0,
                                  0.13582992553710938,
                                  0.1321955770254135,
                                  0.1356307566165924,
                                  1054040704.0,
                                  0.13556765019893646,
                                  0.14156989753246307,
                                  0.13816937804222107,
                                  268273632.0,
                                  0.13720349967479706,
                                  0.13542649149894714,
                                  0.1288645714521408,
                                  0.1226494163274765,
                                  0.12051358073949814,
                                  0.12420963495969772,
                                  0.12739074230194092,
                                  0.13060890138149261,
                                  0.1263168305158615,
                                  0.12670856714248657,
                                  0.11823848634958267,
                                  0.12219959497451782,
                                  0.11411118507385254,
                                  0.12208209931850433,
                                  0.12046002596616745,
                                  0.11428070068359375,
                                  0.11432527750730515,
                                  0.11463426798582077,
                                  0.1144825890660286,
                                  0.11444626748561859,
                                  0.11485874652862549,
                                  0.11457077413797379,
                                  0.1145758330821991,
                                  0.11442591995000839,
                                  0.1148984432220459,
                                  0.1149170994758606,
                                  0.11488619446754456,
                                  0.11486360430717468,
                                  0.114899180829525,
                                  0.1149524450302124,
                                  0.11508650332689285,
                                  0.1152985617518425,
                                  0.11511247605085373,
                                  223967184.0,
                                  0.1151353120803833,
                                  0.11522237956523895,
                                  0.11538894474506378,
                                  0.11536173522472382,
                                  0.11543510854244232,
                                  0.11553690582513809,
                                  0.11534439027309418,
                                  0.1152355745434761,
                                  0.11538137495517731,
                                  0.11679191887378693,
                                  0.11562331020832062,
                                  0.11570201814174652,
                                  0.11553406715393066,
                                  0.11560743302106857,
                                  0.11558980494737625,
                                  0.11557500809431076,
                                  0.11566941440105438,
                                  0.11569506675004959,
                                  0.1155826523900032,
                                  0.11563906818628311,
                                  0.11583434790372849,
                                  0.11583562195301056,
                                  0.11584048718214035,
                                  0.11579956114292145,
                                  0.1158096119761467,
                                  0.11577166616916656,
                                  0.11578544974327087,
                                  0.11576415598392487,
                                  0.11577217280864716,
                                  0.1157451793551445,
                                  0.1157572790980339,
                                  0.1158934161067009,
                                  0.11580001562833786,
                                  0.11593358963727951,
                                  0.11615775525569916,
                                  0.11587931215763092,
                                  0.1159670352935791,
                                  0.11585234105587006,
                                  0.11583954095840454,
                                  0.11609325557947159,
                                  0.11583058536052704,
                                  0.11599991470575333,
                                  0.11598140746355057,
                                  0.11592470854520798,
                                  0.11593374609947205,
                                  0.11602354794740677,
                                  0.11578560620546341,
                                  0.11599435657262802,
                                  0.11563146859407425,
                                  0.11705952137708664,
                                  0.11598608642816544,
                                  0.11604960262775421,
                                  0.11593892425298691,
                                  0.1176956295967102,
                                  0.12361403554677963,
                                  0.11607111245393753,
                                  0.11594823747873306,
                                  0.11593722552061081,
                                  0.11586553603410721,
                                  0.11618652194738388,
                                  0.11610959470272064,
                                  0.11593214422464371,
                                  0.11584636569023132,
                                  0.1159195527434349,
                                  0.11607397347688675,
                                  0.1159905418753624,
                                  0.1159055307507515,
                                  0.11624743789434433,
                                  0.11588098853826523,
                                  0.11602722853422165,
                                  0.11587972939014435,
                                  0.11616445332765579,
                                  0.11607798933982849,
                                  0.11569825559854507,
                                  0.11504572629928589,
                                  0.11600799113512039,
                                  0.11600035429000854,
                                  0.11612346768379211,
                                  0.11608055979013443,
                                  0.11566175520420074,
                                  0.1160079687833786,
                                  0.11582132428884506,
                                  0.11605935543775558,
                                  0.11587560176849365,
                                  0.11589808017015457,
                                  0.11591998487710953,
                                  0.11596931517124176,
                                  0.11618545651435852,
                                  0.11614586412906647,
                                  0.116184763610363,
                                  0.11593402177095413,
                                  0.11613350361585617,
                                  0.11606571078300476,
                                  0.11616233736276627,
                                  0.11608292907476425,
                                  0.11600427329540253,
                                  0.11627335101366043,
                                  0.11593248695135117,
                                  0.11602398753166199,
                                  0.11632940173149109,
                                  0.11617862433195114,
                                  0.11615864932537079,
                                  0.11609946191310883,
                                  0.11620981246232986,
                                  0.11623332649469376,
                                  0.11621405184268951,
                                  0.11617275327444077,
                                  0.11618194729089737,
                                  0.11618026345968246,
                                  0.11604853719472885,
                                  0.11610307544469833,
                                  0.11616066843271255,
                                  0.11609618365764618,
                                  0.11606502532958984,
                                  0.11607180535793304,
                                  0.11610879749059677,
                                  0.11615218967199326,
                                  0.11595457047224045,
                                  0.1159059926867485,
                                  0.11616579443216324,
                                  0.11522948741912842,
                                  0.11619925498962402,
                                  0.11620134115219116,
                                  0.11613330245018005,
                                  0.11608000099658966,
                                  0.11618129909038544,
                                  0.11634077876806259,
                                  0.11618518084287643,
                                  0.11617344617843628,
                                  0.11647146195173264,
                                  0.11618098616600037,
                                  0.11633583903312683,
                                  0.11608415842056274,
                                  0.11629339307546616,
                                  0.11636802554130554,
                                  0.11612612754106522,
                                  0.11618626117706299,
                                  0.11609125882387161,
                                  0.11604174226522446,
                                  0.11616165190935135,
                                  0.11629405617713928,
                                  0.12194935232400894,
                                  0.11632195115089417,
                                  0.1196022555232048,
                                  0.11585008352994919,
                                  0.12247772514820099,
                                  0.12818971276283264,
                                  0.12019037455320358,
                                  0.12467972934246063,
                                  0.11633002012968063,
                                  0.1162014976143837,
                                  0.11621222645044327,
                                  0.09137231856584549,
                                  0.09150426834821701,
                                  0.09157953411340714,
                                  0.09157821536064148,
                                  0.09164956212043762,
                                  0.09170088171958923,
                                  0.09174489974975586,
                                  0.09180380403995514,
                                  0.09149286895990372,
                                  0.09157092124223709,
                                  0.09140341728925705,
                                  0.09140137583017349,
                                  0.09056924283504486,
                                  0.09073775261640549,
                                  0.09158705919981003,
                                  0.0913533940911293,
                                  2.4094927310943604,
                                  0.09106162190437317,
                                  0.0903748944401741,
                                  0.09066660702228546,
                                  0.09095553308725357,
                                  0.09037058055400848,
                                  0.09065017104148865,
                                  0.09140996634960175,
                                  0.0915386751294136,
                                  0.09155571460723877,
                                  0.09167596697807312,
                                  0.09157241880893707,
                                  0.09159300476312637,
                                  0.09173470735549927,
                                  0.09168659150600433,
                                  0.09159538149833679,
                                  0.09157337993383408,
                                  0.09162420779466629,
                                  0.0916445180773735,
                                  0.09159063547849655,
                                  0.09151946753263474,
                                  0.09143755584955215,
                                  0.09167628735303879,
                                  0.09162566065788269,
                                  0.09144790470600128,
                                  0.09167981147766113,
                                  0.09183721989393234,
                                  0.09176892042160034,
                                  0.09148874878883362,
                                  0.09179455041885376,
                                  0.09149231016635895,
                                  0.09171196818351746,
                                  0.09166552871465683,
                                  0.09163936972618103,
                                  0.09166969358921051,
                                  0.09118418395519257,
                                  0.09167936444282532,
                                  0.09164978563785553,
                                  0.0916447639465332,
                                  0.091525137424469,
                                  0.0916038230061531,
                                  0.09174405038356781,
                                  0.09161648899316788,
                                  0.09172605723142624,
                                  0.09169495105743408,
                                  0.09160944074392319,
                                  0.09177764505147934,
                                  0.09175115078687668,
                                  0.09168726205825806,
                                  0.09168113023042679,
                                  0.0916929617524147,
                                  0.0916459709405899,
                                  0.09187834709882736,
                                  0.09167644381523132,
                                  0.0916120707988739,
                                  0.09165194630622864,
                                  0.09167271107435226,
                                  0.09164658933877945,
                                  0.09172767400741577,
                                  0.09166054427623749,
                                  0.09191282838582993,
                                  0.0919322669506073,
                                  0.09164082258939743,
                                  0.09164590388536453,
                                  0.09164344519376755,
                                  0.09161072224378586,
                                  0.09165661782026291,
                                  0.09178908169269562,
                                  0.09179839491844177,
                                  0.09184478968381882,
                                  0.09157416224479675,
                                  0.09172283858060837,
                                  0.09165951609611511,
                                  0.09154213964939117,
                                  0.09159321337938309,
                                  0.09175139665603638,
                                  0.0916604995727539,
                                  0.09152602404356003,
                                  0.09157373756170273,
                                  0.09160105139017105,
                                  0.09054151177406311,
                                  0.09138046205043793,
                                  0.0918545126914978,
                                  0.09164437651634216,
                                  0.09164205938577652,
                                  0.09161880612373352,
                                  0.09176848083734512,
                                  0.0915747582912445,
                                  0.09165892750024796,
                                  0.09155618399381638,
                                  0.09165655821561813,
                                  0.09176354855298996,
                                  0.0917825847864151,
                                  0.09167511016130447,
                                  0.09172207117080688,
                                  0.09178581088781357,
                                  0.09048595279455185,
                                  0.09156113862991333,
                                  0.09158486872911453,
                                  0.09151524305343628,
                                  0.09164683520793915,
                                  0.09167765825986862,
                                  0.09159915894269943,
                                  0.09176617115736008,
                                  0.09172510355710983,
                                  0.09160268306732178,
                                  0.09161321818828583,
                                  0.0915767028927803,
                                  0.0914856418967247,
                                  0.0916421189904213,
                                  0.09157934784889221,
                                  0.09156793355941772,
                                  0.09154218435287476,
                                  0.09154070168733597,
                                  134901216.0,
                                  0.09168075770139694,
                                  0.09144774079322815,
                                  0.09149137884378433,
                                  0.09155774861574173,
                                  0.09148041158914566,
                                  0.09169764071702957,
                                  0.0916762501001358,
                                  0.09158093482255936,
                                  0.09154773503541946,
                                  0.0916462242603302,
                                  0.0915762335062027,
                                  0.09153687953948975,
                                  0.09146840870380402,
                                  0.09179212152957916,
                                  0.0917789414525032,
                                  0.09176136553287506,
                                  0.09159605950117111,
                                  0.09162770956754684,
                                  0.09160606563091278,
                                  0.09176835417747498,
                                  0.09157654643058777,
                                  0.0916082113981247,
                                  0.09162461757659912,
                                  0.09182053804397583,
                                  0.09177893400192261,
                                  0.09165219217538834,
                                  0.09159030020236969,
                                  0.09154009819030762,
                                  0.09164740145206451,
                                  0.09176388382911682,
                                  0.0915454551577568,
                                  0.09177467226982117,
                                  0.09191010892391205,
                                  0.09156614542007446,
                                  0.09143801033496857,
                                  0.09114047884941101,
                                  0.09068604558706284,
                                  0.09070765227079391,
                                  0.09103754162788391,
                                  0.09052861481904984,
                                  0.09132906794548035,
                                  0.09160716831684113,
                                  0.09083017706871033,
                                  0.09090922772884369,
                                  0.09165212512016296,
                                  0.09168437123298645,
                                  0.09145098924636841,
                                  0.09153489023447037,
                                  8632.75390625,
                                  0.09173749387264252,
                                  0.09159638732671738,
                                  0.09155716747045517,
                                  0.09143556654453278,
                                  0.09150324761867523,
                                  0.09115756303071976,
                                  0.09095091372728348,
                                  0.09065255522727966,
                                  0.09077665954828262,
                                  0.09125294536352158,
                                  0.09146241843700409,
                                  0.0917263850569725,
                                  0.09158481657505035,
                                  0.09154310077428818,
                                  0.09155087172985077,
                                  0.09157835692167282,
                                  0.09175891429185867,
                                  0.09174496680498123,
                                  0.0915360376238823,
                                  0.0916609838604927,
                                  0.09006042033433914,
                                  0.09004908800125122,
                                  0.08979449421167374,
                                  0.0896245688199997,
                                  0.08980648964643478,
                                  0.0895053967833519,
                                  0.08965669572353363,
                                  0.09002640843391418,
                                  0.08996382355690002,
                                  0.08966709673404694,
                                  0.0897647812962532,
                                  0.08978300541639328,
                                  0.08981018513441086,
                                  0.0896415039896965,
                                  0.0901375487446785,
                                  0.089830681681633,
                                  0.09017007797956467,
                                  0.0898798406124115,
                                  0.09025020152330399,
                                  0.0899721086025238,
                                  0.08988645672798157,
                                  0.09003996104001999,
                                  0.08998358994722366,
                                  0.0893966481089592,
                                  0.08915132284164429,
                                  0.08969417959451675,
                                  0.08966122567653656,
                                  0.08971626311540604,
                                  0.08974547684192657,
                                  0.08988585323095322,
                                  0.09006890654563904,
                                  0.09015759825706482,
                                  0.08985879272222519,
                                  0.09011870622634888,
                                  0.089875727891922,
                                  0.08984020352363586,
                                  0.08999879658222198,
                                  0.08929476141929626,
                                  0.08917342871427536,
                                  0.0890253409743309,
                                  0.0890451967716217,
                                  0.08985283225774765,
                                  0.08977603167295456,
                                  0.08965098857879639,
                                  0.09000735729932785,
                                  0.0896926000714302,
                                  0.0897701233625412,
                                  0.08995763957500458,
                                  0.0898318812251091,
                                  0.08998756855726242,
                                  0.09007910639047623,
                                  0.09007957577705383,
                                  0.08997736871242523,
                                  0.08961615711450577,
                                  0.09001019597053528,
                                  0.08989803493022919,
                                  0.09013975411653519,
                                  0.0902264267206192,
                                  0.0899982675909996,
                                  0.08958622068166733,
                                  0.08898205310106277,
                                  0.08928899466991425,
                                  0.08966180682182312,
                                  0.0896264836192131,
                                  0.0899060070514679,
                                  165786832.0,
                                  0.08958649635314941,
                                  0.08978855609893799,
                                  0.0899762436747551,
                                  0.09000223875045776,
                                  0.08978422731161118,
                                  0.09043993800878525,
                                  0.09028058499097824,
                                  0.1005210429430008,
                                  0.08993940055370331,
                                  0.08960286527872086,
                                  0.08972615748643875,
                                  0.08975361287593842,
                                  0.08965158462524414,
                                  0.08964148908853531,
                                  0.08972785621881485,
                                  0.0897819921374321,
                                  0.08990558981895447,
                                  0.08987738937139511,
                                  0.09000679105520248,
                                  0.08997351676225662,
                                  0.08983514457941055,
                                  0.09013941884040833,
                                  0.09014367312192917,
                                  0.09003666788339615,
                                  0.09019093215465546,
                                  0.08984865993261337,
                                  0.08995110541582108,
                                  0.08969511836767197,
                                  0.08997870236635208,
                                  0.08957649022340775,
                                  0.08970191329717636,
                                  0.08933154493570328,
                                  0.08878691494464874,
                                  0.08927902579307556,
                                  0.08946020156145096,
                                  0.08946799486875534,
                                  0.08914709836244583,
                                  0.08887948840856552,
                                  0.08919765800237656,
                                  0.0895165354013443,
                                  0.08895201981067657,
                                  0.08957294374704361,
                                  0.08974824845790863,
                                  0.08876839280128479,
                                  0.08959838002920151,
                                  0.08914162218570709,
                                  0.08941791206598282,
                                  0.08913740515708923,
                                  0.08939765393733978,
                                  0.09013853222131729,
                                  0.08985241502523422,
                                  0.0897478461265564,
                                  0.08989280462265015,
                                  0.08953097462654114,
                                  0.08996637165546417,
                                  0.08999262005090714,
                                  0.09000848233699799,
                                  0.08972601592540741,
                                  0.08974424004554749,
                                  0.08952955156564713,
                                  0.08970942348241806,
                                  0.0896824523806572,
                                  0.0899357870221138,
                                  0.0900910422205925,
                                  0.08987075090408325,
                                  0.09012211114168167,
                                  0.0899343490600586,
                                  0.08979938924312592,
                                  0.08996124565601349,
                                  0.09016357362270355,
                                  0.09030314534902573,
                                  0.09023184329271317,
                                  0.09009277075529099,
                                  0.08956193923950195,
                                  0.09018462896347046,
                                  0.08991622179746628,
                                  0.08998739719390869,
                                  0.09010801464319229,
                                  0.08995979279279709,
                                  0.08951814472675323,
                                  0.09035605937242508,
                                  0.08999628573656082,
                                  0.08997727185487747,
                                  0.09033078700304031,
                                  0.08996672928333282,
                                  0.09028850495815277,
                                  0.0895623043179512,
                                  0.08894254267215729,
                                  0.09029853343963623,
                                  0.08997199684381485,
                                  0.08996271342039108,
                                  0.09009357541799545,
                                  0.09012578427791595,
                                  0.08983942866325378,
                                  0.08987858891487122,
                                  0.08974911272525787,
                                  0.08983664214611053,
                                  0.08988182246685028,
                                  0.0900752916932106,
                                  0.08942669630050659,
                                  0.08933917433023453,
                                  0.08779210597276688,
                                  0.08927730470895767,
                                  0.08909048140048981,
                                  0.0898263081908226,
                                  0.08931832760572433,
                                  0.08928386121988297,
                                  0.08991234749555588,
                                  0.08994374424219131,
                                  0.09000713378190994,
                                  0.08975329250097275,
                                  0.09016995877027512,
                                  0.08982622623443604,
                                  0.09043461084365845,
                                  0.09003646671772003,
                                  0.0898413360118866,
                                  0.08992080390453339,
                                  0.08976344019174576,
                                  0.09005337953567505,
                                  0.09002504497766495,
                                  0.08987294137477875,
                                  0.08990441262722015,
                                  0.08986706286668777,
                                  0.09007862210273743,
                                  0.08986128121614456,
                                  0.08975748717784882,
                                  0.0894976556301117,
                                  0.08977871388196945,
                                  0.08997070044279099,
                                  0.0896330401301384,
                                  0.09019531309604645,
                                  0.09008292108774185,
                                  0.09006659686565399,
                                  0.08975100517272949],
             'weight_decay': 0.0005}],
 'tests': {},
 'training_time': 8509.204961776733}
{'autoincrement': 4,
 'best_models': {},
 'best_models_list': [{'accuracy': 0.0,
                       'batch_size': 32,
                       'cv_score': 0.0053345869543231185,
                       'cv_val_accuracy': 0.6666666666666666,
                       'cv_val_loss': 0.09920807182788849,
                       'cv_val_macroF1': 0.0053345869543231185,
                       'cv_val_microF1': 0.066966913828706,
                       'epochs': 200,
                       'final_model': GGNN1(
  (ggnn): GatedGraphConv(60, num_layers=2)
  (fc1): Linear(in_features=60, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                       'kwargs': {'aggr_type': 'mean',
                                  'd1': 60,
                                  'd2': 50,
                                  'num_classes': 24,
                                  'num_layers': 2},
                       'learning_rate': 0.01,
                       'macroF1': 0.006748704489763125,
                       'microF1': 0.08414023372287145,
                       'model': <class 'TFM_graph_classification_models.GGNN1'>,
                       'model_instance': GGNN1(
  (ggnn): GatedGraphConv(60, num_layers=2)
  (fc1): Linear(in_features=60, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                       'name': '3_GGNN1',
                       'score': 'macroF1',
                       'time': 2764.828370809555,
                       'train_loss_history': [362103776.0,
                                              1984.16552734375,
                                              4208493824.0,
                                              660907968.0,
                                              0.08377974480390549,
                                              1243555968.0,
                                              1017.9180297851562,
                                              76571.46875,
                                              308.40338134765625,
                                              23220.69921875,
                                              156603248.0,
                                              138.022216796875,
                                              0.08351406455039978,
                                              1823248640.0,
                                              155343744.0,
                                              0.0847235918045044,
                                              0.08452852815389633,
                                              0.08426041901111603,
                                              0.0840001031756401,
                                              0.0840369313955307,
                                              0.0836704894900322,
                                              0.08393912762403488,
                                              0.08424623310565948,
                                              0.08425412327051163,
                                              0.08409769088029861,
                                              0.08399365842342377,
                                              0.08431313186883926,
                                              0.08411368727684021,
                                              19.82664680480957,
                                              0.08441364020109177,
                                              0.08446607738733292,
                                              0.08450149744749069,
                                              0.08464515209197998,
                                              0.08455231040716171,
                                              0.08460909128189087,
                                              0.08457177132368088,
                                              0.0844896212220192,
                                              0.08443144708871841,
                                              7615.97705078125,
                                              5.158772945404053,
                                              0.08454208821058273,
                                              0.08437103033065796,
                                              0.08442801982164383,
                                              0.08480384200811386,
                                              0.08451271802186966,
                                              0.0846419557929039,
                                              0.08445870131254196,
                                              0.08451291173696518,
                                              7.508358001708984,
                                              7220202.5,
                                              208.47743225097656,
                                              0.08452831208705902,
                                              0.08444917947053909,
                                              0.08455681055784225,
                                              0.08457385748624802,
                                              0.08448221534490585,
                                              0.08441825956106186,
                                              0.08439182490110397,
                                              0.08427322655916214,
                                              0.08453415334224701,
                                              0.08448854833841324,
                                              0.08462214469909668,
                                              0.08452510088682175,
                                              0.08454762399196625,
                                              0.08453730493783951,
                                              0.08453856408596039,
                                              0.08457324653863907,
                                              0.08439980447292328,
                                              0.08457770943641663,
                                              0.08448946475982666,
                                              0.08455544710159302,
                                              0.08448326587677002,
                                              0.0853704884648323,
                                              0.0845303013920784,
                                              0.08449558913707733,
                                              0.08448614180088043,
                                              0.08432622253894806,
                                              0.08439280092716217,
                                              0.08428298681974411,
                                              0.08441869169473648,
                                              2.7614922523498535,
                                              0.0844237357378006,
                                              0.0845165103673935,
                                              3.4590933322906494,
                                              0.08449532091617584,
                                              0.0845257043838501,
                                              0.08447345346212387,
                                              0.08450763672590256,
                                              0.08446920663118362,
                                              0.08451981097459793,
                                              385.1795959472656,
                                              0.08447851985692978,
                                              0.08457182347774506,
                                              0.08445488661527634,
                                              0.7349956631660461,
                                              0.0845390260219574,
                                              437.2469787597656,
                                              0.08416557312011719,
                                              0.08421652764081955,
                                              0.08440131694078445,
                                              0.08433077484369278,
                                              0.08428847789764404,
                                              0.08411938697099686,
                                              0.08432593941688538,
                                              0.08455510437488556,
                                              0.08454146236181259,
                                              0.08451537787914276,
                                              0.08447898924350739,
                                              0.08454417437314987,
                                              0.08451075106859207,
                                              0.08446026593446732,
                                              0.08439700305461884,
                                              0.08444203436374664,
                                              324.5517578125,
                                              0.084525465965271,
                                              0.08450353890657425,
                                              0.08454328775405884,
                                              0.08446735888719559,
                                              0.0844758078455925,
                                              0.08446711301803589,
                                              0.08434131741523743,
                                              0.0844181552529335,
                                              0.08407874405384064,
                                              0.08442263305187225,
                                              0.08430887013673782,
                                              0.08445868641138077,
                                              0.08432488143444061,
                                              0.08438342809677124,
                                              0.08434324711561203,
                                              0.08433690667152405,
                                              0.08442634344100952,
                                              0.08430664241313934,
                                              0.08431776612997055,
                                              3179.368408203125,
                                              0.08431804180145264,
                                              0.0844300165772438,
                                              327.7389831542969,
                                              0.08442189544439316,
                                              0.08446188271045685,
                                              0.08456380665302277,
                                              0.08448801934719086,
                                              0.08452309668064117,
                                              0.08443786203861237,
                                              0.08450938016176224,
                                              0.08452746272087097,
                                              0.0845022052526474,
                                              0.08435150235891342,
                                              0.0843622088432312,
                                              0.08451417833566666,
                                              0.08449152857065201,
                                              2792218.0,
                                              0.08448152989149094,
                                              0.08447089046239853,
                                              0.08448109775781631,
                                              0.08455649018287659,
                                              11.825798988342285,
                                              0.0844212993979454,
                                              0.08447621017694473,
                                              0.08445154875516891,
                                              0.08450216054916382,
                                              0.08448535948991776,
                                              0.08452288061380386,
                                              2.2303872108459473,
                                              0.08445139229297638,
                                              22.94561195373535,
                                              0.08438808470964432,
                                              0.08447092026472092,
                                              0.0844135656952858,
                                              0.08435672521591187,
                                              4.1318182945251465,
                                              0.0844360813498497,
                                              0.08448848873376846,
                                              0.08441183716058731,
                                              0.08452562987804413,
                                              0.08434139937162399,
                                              15509.2685546875,
                                              0.08440298587083817,
                                              0.0843038558959961,
                                              0.08409386873245239,
                                              0.08434053510427475,
                                              0.08435279130935669,
                                              0.0843585804104805,
                                              1622.2698974609375,
                                              6.130885601043701,
                                              513.2769165039062,
                                              0.08451929688453674,
                                              0.0844656303524971,
                                              0.08448315411806107,
                                              0.08443838357925415,
                                              657.5328979492188,
                                              0.08391217887401581,
                                              0.08355443924665451,
                                              0.08408589661121368,
                                              0.08395949006080627,
                                              0.08368541300296783,
                                              0.09440038353204727,
                                              0.08442547917366028,
                                              0.08448342233896255,
                                              0.08452529460191727,
                                              0.08449804782867432,
                                              0.0958980917930603,
                                              0.0944596529006958,
                                              0.09449319541454315,
                                              0.0945969820022583,
                                              0.09455905109643936,
                                              0.09455728530883789,
                                              0.0946040078997612,
                                              0.5912741422653198,
                                              0.0945107489824295,
                                              0.09456121921539307,
                                              0.09452136605978012,
                                              0.09451890736818314,
                                              0.09491711109876633,
                                              0.0936649888753891,
                                              0.17639370262622833,
                                              227.92086791992188,
                                              0.0940268412232399,
                                              0.21170301735401154,
                                              0.09379062056541443,
                                              0.09365381300449371,
                                              0.09371867030858994,
                                              0.09373131394386292,
                                              0.09375094622373581,
                                              0.09409426152706146,
                                              0.09444309771060944,
                                              2.2909059524536133,
                                              408.70733642578125,
                                              0.09448052197694778,
                                              0.09452381730079651,
                                              0.09452628344297409,
                                              0.09451580047607422,
                                              0.09457345306873322,
                                              0.09455623477697372,
                                              0.09454062581062317,
                                              0.09454816579818726,
                                              0.09447815269231796,
                                              0.09455271065235138,
                                              0.0945919007062912,
                                              0.09438098222017288,
                                              0.09447444975376129,
                                              0.09452714771032333,
                                              0.09452454000711441,
                                              0.09455770254135132,
                                              0.09453456103801727,
                                              0.09453591704368591,
                                              0.0945412740111351,
                                              5.31784200668335,
                                              0.09444911777973175,
                                              1.2043771743774414,
                                              0.09460223466157913,
                                              0.09458258748054504,
                                              0.09454575926065445,
                                              0.09464789927005768,
                                              0.09462427347898483,
                                              0.2161496877670288,
                                              0.09452006220817566,
                                              0.0945173129439354,
                                              0.09459420293569565,
                                              45.2691764831543,
                                              0.0946098044514656,
                                              0.09459919482469559,
                                              0.09453676640987396,
                                              0.09455496072769165,
                                              0.09455988556146622,
                                              0.09453597664833069,
                                              1460.126708984375,
                                              0.09449618309736252,
                                              0.09447479248046875,
                                              0.0946631208062172,
                                              0.09447914361953735,
                                              0.09451417624950409,
                                              0.09457054734230042,
                                              578.6171264648438,
                                              0.09508994966745377,
                                              0.09456158429384232,
                                              0.09453702718019485,
                                              92.70061492919922,
                                              575.2883911132812,
                                              0.09449885785579681,
                                              0.09459429234266281,
                                              0.09453769028186798,
                                              0.09457365423440933,
                                              0.2661488950252533,
                                              0.5785210728645325,
                                              1.0697529315948486,
                                              9533.2490234375,
                                              36278084.0,
                                              0.09451647102832794,
                                              0.0945221334695816,
                                              0.0945245772600174,
                                              0.49862831830978394,
                                              0.1389312595129013,
                                              0.10536698997020721,
                                              0.6453412175178528,
                                              0.09668745845556259,
                                              7082514.5,
                                              0.09513116627931595,
                                              0.09418614208698273,
                                              0.09460048377513885,
                                              0.09451752156019211,
                                              0.09452079981565475,
                                              0.11058186739683151,
                                              0.09448027610778809,
                                              95075.0,
                                              0.09450844675302505,
                                              0.40205472707748413,
                                              0.09456601738929749,
                                              21503.685546875,
                                              25433.630859375,
                                              0.09459114074707031,
                                              1.827946662902832,
                                              0.09454144537448883,
                                              0.09408380091190338,
                                              0.09439128637313843,
                                              0.09441068768501282,
                                              75790.1328125,
                                              2222.14990234375,
                                              0.09460681676864624,
                                              0.09455598890781403,
                                              0.09462475776672363,
                                              0.09453312307596207,
                                              0.0945381447672844,
                                              0.09712633490562439,
                                              15.168710708618164,
                                              0.09447479993104935,
                                              0.09447994828224182,
                                              0.09455700218677521,
                                              1.1578611135482788,
                                              0.09453558921813965,
                                              0.0945739671587944,
                                              0.09440802037715912,
                                              7.872211933135986,
                                              0.09455535560846329,
                                              0.09453728795051575,
                                              0.09456964582204819,
                                              0.09449167549610138,
                                              403448.90625,
                                              0.11504217982292175,
                                              0.09461946040391922,
                                              0.09448207169771194,
                                              1.0811370611190796,
                                              0.09459654241800308,
                                              0.09441133588552475,
                                              0.0943896472454071,
                                              5106.0791015625,
                                              0.09468978643417358,
                                              0.09459315240383148,
                                              0.09458445757627487,
                                              12.510099411010742,
                                              0.09466719627380371,
                                              0.09455475211143494,
                                              20.623863220214844,
                                              0.09454236924648285,
                                              8.770496368408203,
                                              12.305490493774414,
                                              0.09461303800344467,
                                              0.09462690353393555,
                                              0.09455135464668274,
                                              0.09457971900701523,
                                              0.09463999420404434,
                                              0.09459368139505386,
                                              39.026275634765625,
                                              0.09459409862756729,
                                              0.14653317630290985,
                                              0.09447965025901794,
                                              0.09448003023862839,
                                              0.09420546144247055,
                                              0.10086872428655624,
                                              0.09402845054864883,
                                              0.09396755695343018,
                                              0.09385461360216141,
                                              0.7617712020874023,
                                              3.599776268005371,
                                              0.0940251424908638,
                                              0.09385236352682114,
                                              1.5394641160964966,
                                              555.6378784179688,
                                              21.09204864501953,
                                              0.09456101059913635,
                                              0.09457747638225555,
                                              0.20754975080490112,
                                              0.09456492215394974,
                                              0.09465780109167099,
                                              0.09462746232748032,
                                              0.09462113678455353,
                                              4181469.0,
                                              0.11877608299255371,
                                              0.09787335991859436,
                                              0.09376204758882523,
                                              0.09390174597501755,
                                              609.8927001953125,
                                              0.0945817232131958,
                                              56.260704040527344,
                                              0.09460467100143433,
                                              0.16732440888881683,
                                              0.09459473192691803,
                                              0.09458056092262268,
                                              0.09451653063297272,
                                              0.09456532448530197,
                                              0.09456890821456909,
                                              0.09472556412220001,
                                              0.09467514604330063,
                                              0.09467890858650208,
                                              0.09461469203233719,
                                              0.09461411088705063,
                                              0.09406077861785889,
                                              0.09436465054750443,
                                              6.517511367797852,
                                              0.09469257295131683,
                                              22.83505630493164,
                                              21.33751106262207,
                                              0.09463278949260712,
                                              1789.422119140625,
                                              435.6246337890625,
                                              0.24851779639720917,
                                              0.09462091326713562,
                                              614.681640625,
                                              0.09470755606889725,
                                              178.90574645996094,
                                              0.09464573115110397,
                                              0.09471963346004486,
                                              0.09465103596448898,
                                              0.6563721895217896,
                                              704190.0,
                                              0.09396739304065704,
                                              0.09447432309389114,
                                              0.09416063874959946,
                                              0.0944053903222084,
                                              65.62581634521484,
                                              0.09450186043977737,
                                              1024.021240234375,
                                              0.09452372044324875,
                                              0.09453118592500687,
                                              0.09463518112897873,
                                              0.09458576887845993,
                                              0.10458575189113617,
                                              47422.390625,
                                              0.09453989565372467,
                                              0.19003771245479584,
                                              0.09361455589532852,
                                              0.09875020384788513,
                                              0.10593827068805695,
                                              0.09450165182352066,
                                              2.2607531547546387,
                                              146.7675018310547,
                                              0.09460194408893585,
                                              0.09467209130525589,
                                              0.09463077038526535,
                                              0.09450284391641617,
                                              0.09454219788312912,
                                              0.09467772394418716,
                                              0.09461057186126709,
                                              301.4787902832031,
                                              0.0944533422589302,
                                              0.09459809958934784,
                                              0.09464976191520691,
                                              0.09458554536104202,
                                              0.09468982368707657,
                                              0.09466410428285599,
                                              0.5748986601829529,
                                              0.09391465783119202,
                                              0.09361892193555832,
                                              13.32568073272705,
                                              0.09446100890636444,
                                              77.79401397705078,
                                              2.8922955989837646,
                                              0.1703469306230545,
                                              7399.919921875,
                                              4709.5478515625,
                                              0.09465955197811127,
                                              0.09466127306222916,
                                              0.09468183666467667,
                                              0.09468203783035278,
                                              19.022647857666016,
                                              0.09639277309179306,
                                              0.09449823945760727,
                                              0.09443557262420654,
                                              0.09912924468517303,
                                              0.09453266113996506,
                                              0.09452731162309647,
                                              0.09462825953960419,
                                              0.09465084969997406,
                                              0.09445875138044357,
                                              9395.8203125,
                                              4131.25390625,
                                              0.09448602050542831,
                                              0.09457829594612122,
                                              0.09452693164348602,
                                              0.41866934299468994,
                                              0.09443027526140213,
                                              1.5612469911575317,
                                              0.09447776526212692,
                                              0.09464254975318909,
                                              0.0947151929140091,
                                              0.09461060166358948,
                                              1.6179649829864502,
                                              151.97926330566406,
                                              0.09670396894216537,
                                              0.09371296316385269,
                                              0.22721385955810547,
                                              0.09369426220655441,
                                              0.09369219094514847,
                                              0.0936376303434372,
                                              519.0321655273438,
                                              2733.38671875,
                                              0.09362778812646866,
                                              0.1038171648979187,
                                              0.36694493889808655,
                                              0.09366429597139359,
                                              0.09367946535348892,
                                              0.093511663377285,
                                              363061.71875,
                                              0.09354864805936813,
                                              0.09355273097753525,
                                              0.0940803587436676,
                                              0.09443733096122742,
                                              0.09446991235017776,
                                              0.09449158608913422,
                                              0.09442201256752014,
                                              0.09459392726421356,
                                              0.09461551159620285,
                                              3.2891151905059814,
                                              0.09446682780981064,
                                              214763.484375,
                                              0.0945248231291771,
                                              0.0945204347372055,
                                              1.0939825773239136,
                                              0.09466331452131271,
                                              0.09470713883638382,
                                              0.09462140500545502,
                                              0.0944676548242569,
                                              0.09462480992078781,
                                              0.0944826528429985,
                                              62.18036651611328,
                                              1.1048225164413452,
                                              7.396787166595459,
                                              0.09464084357023239,
                                              0.09469672292470932,
                                              0.09460265189409256,
                                              37.437835693359375,
                                              0.09448858350515366,
                                              13.001826286315918,
                                              45.448062896728516,
                                              0.0946521982550621,
                                              0.09455090761184692,
                                              0.2181532233953476,
                                              248.3521728515625,
                                              0.094516322016716,
                                              0.09450066834688187,
                                              0.09460214525461197,
                                              1419.5380859375,
                                              0.09455197304487228,
                                              89.7099380493164,
                                              0.09380219131708145,
                                              0.09361878782510757,
                                              0.09440010786056519,
                                              33.71955490112305,
                                              0.0946076437830925,
                                              0.09467274695634842,
                                              0.09467364847660065,
                                              6.568045139312744,
                                              0.0946686714887619,
                                              0.11860518157482147,
                                              144.5919952392578,
                                              0.0945608839392662,
                                              0.0977327898144722,
                                              0.09378885477781296,
                                              0.4567955732345581,
                                              0.28246936202049255,
                                              0.0937114804983139,
                                              3.4494714736938477,
                                              0.09347078949213028,
                                              1.9993853569030762,
                                              694.1502685546875,
                                              0.09461234509944916,
                                              0.09463471919298172,
                                              0.09461282938718796,
                                              0.09465289115905762,
                                              0.1086336076259613,
                                              0.09450510889291763,
                                              0.09465640783309937,
                                              0.0946459248661995,
                                              44.427127838134766,
                                              0.0945444405078888,
                                              0.09469397366046906,
                                              0.09456057101488113,
                                              0.09453918039798737,
                                              0.09460191428661346,
                                              16.45922088623047,
                                              0.09442133456468582,
                                              766.4246826171875,
                                              0.0944996252655983,
                                              57.03985595703125,
                                              0.09441376477479935,
                                              0.09458398073911667,
                                              4.6840386390686035,
                                              0.10124408453702927,
                                              0.09464626759290695,
                                              0.0945911556482315,
                                              34.607200622558594],
                       'val_accuracy_history': [1.736842105263158,
                                                2.210526315789474,
                                                0.05263157894736842,
                                                1.0,
                                                0.8947368421052632,
                                                0.9473684210526315,
                                                0.2631578947368421,
                                                0.6842105263157895,
                                                1.5789473684210527,
                                                0.7368421052631579,
                                                1.0,
                                                1.368421052631579,
                                                0.8947368421052632,
                                                0.0,
                                                0.15789473684210525,
                                                0.3684210526315789,
                                                1.5789473684210527,
                                                1.368421052631579,
                                                1.4736842105263157,
                                                0.2631578947368421,
                                                0.3684210526315789,
                                                1.631578947368421,
                                                0.47368421052631576,
                                                1.368421052631579,
                                                0.5789473684210527,
                                                0.42105263157894735,
                                                1.0,
                                                0.5789473684210527,
                                                0.6842105263157895,
                                                1.0,
                                                0.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                2.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                1.105263157894737,
                                                1.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                2.0,
                                                0.3684210526315789,
                                                0.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                4.0,
                                                0.0,
                                                0.0,
                                                3.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                3.0,
                                                1.0,
                                                0.0,
                                                2.0,
                                                0.0,
                                                4.0,
                                                0.0,
                                                1.0,
                                                1.0526315789473684,
                                                0.3157894736842105,
                                                0.0,
                                                2.0,
                                                1.0,
                                                1.4736842105263157,
                                                0.42105263157894735,
                                                1.0,
                                                2.0,
                                                1.0,
                                                0.0,
                                                2.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                2.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                1.4736842105263157,
                                                1.0,
                                                0.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                2.0,
                                                1.0,
                                                0.0,
                                                3.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                2.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                2.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                3.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                2.0,
                                                2.0,
                                                2.0,
                                                1.0,
                                                1.0,
                                                2.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                2.0,
                                                3.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.15789473684210525,
                                                0.0,
                                                2.0,
                                                2.0,
                                                0.0,
                                                2.0,
                                                0.0,
                                                3.0,
                                                1.0,
                                                1.0,
                                                1.894736842105263,
                                                1.0,
                                                0.0,
                                                1.0,
                                                2.0,
                                                4.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                2.0,
                                                1.0,
                                                0.8947368421052632,
                                                0.21052631578947367,
                                                1.5263157894736843,
                                                0.21052631578947367,
                                                0.5789473684210527,
                                                0.2631578947368421,
                                                0.8947368421052632,
                                                1.0526315789473684,
                                                1.0,
                                                2.0,
                                                2.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                0.6666666666666666,
                                                3.0,
                                                0.0,
                                                0.2222222222222222,
                                                1.0,
                                                0.5555555555555556,
                                                0.2222222222222222,
                                                0.2222222222222222,
                                                1.7777777777777777,
                                                1.8888888888888888,
                                                1.0,
                                                2.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                2.0,
                                                2.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                1.5555555555555556,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.8888888888888888,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                2.0,
                                                0.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                0.2222222222222222,
                                                1.7777777777777777,
                                                0.4444444444444444,
                                                0.0,
                                                0.4444444444444444,
                                                0.0,
                                                2.0,
                                                0.0,
                                                0.2222222222222222,
                                                0.0,
                                                1.0,
                                                0.0,
                                                2.0,
                                                1.0,
                                                1.0,
                                                3.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                0.7777777777777778,
                                                0.8888888888888888,
                                                1.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                2.0,
                                                3.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                5.0,
                                                1.0,
                                                2.0,
                                                1.0,
                                                2.0,
                                                4.0,
                                                4.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                2.0,
                                                2.0,
                                                1.0,
                                                3.0,
                                                1.0,
                                                2.263157894736842,
                                                3.0,
                                                2.0,
                                                1.6842105263157894,
                                                0.0,
                                                1.0,
                                                3.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                2.0,
                                                1.0,
                                                5.0,
                                                0.0,
                                                2.789473684210526,
                                                0.0,
                                                1.894736842105263,
                                                1.5789473684210527,
                                                1.0,
                                                2.0,
                                                1.0,
                                                2.0,
                                                4.0,
                                                2.0,
                                                1.0,
                                                3.0,
                                                2.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                2.0,
                                                5.0,
                                                4.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                1.894736842105263,
                                                1.0526315789473684,
                                                1.4736842105263157,
                                                0.0,
                                                0.0,
                                                3.0,
                                                1.894736842105263,
                                                3.0,
                                                2.0,
                                                1.0,
                                                2.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                0.631578947368421,
                                                1.0,
                                                2.0,
                                                3.0,
                                                2.0,
                                                2.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                3.0,
                                                1.0,
                                                0.0,
                                                5.0,
                                                2.0,
                                                0.0,
                                                3.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                2.0,
                                                2.0,
                                                0.0,
                                                1.6842105263157894,
                                                2.0526315789473686,
                                                1.6842105263157894,
                                                2.4210526315789473,
                                                1.894736842105263,
                                                2.526315789473684,
                                                1.7894736842105263,
                                                1.631578947368421,
                                                1.5789473684210527,
                                                1.631578947368421,
                                                0.7894736842105263,
                                                1.0,
                                                1.7894736842105263,
                                                0.8947368421052632,
                                                1.0526315789473684,
                                                2.6315789473684212,
                                                1.1578947368421053,
                                                3.0,
                                                1.0,
                                                3.0,
                                                2.0,
                                                2.0,
                                                2.0,
                                                3.0,
                                                0.0,
                                                2.0,
                                                2.0,
                                                2.0,
                                                2.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                1.0,
                                                2.0,
                                                2.0,
                                                4.0,
                                                1.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                2.0,
                                                1.0,
                                                0.0,
                                                3.0,
                                                4.0,
                                                1.0,
                                                2.0,
                                                3.0,
                                                1.1578947368421053,
                                                0.0,
                                                1.0,
                                                2.0,
                                                2.0,
                                                1.0,
                                                3.0,
                                                2.1052631578947367,
                                                3.9473684210526314,
                                                0.3157894736842105,
                                                1.0,
                                                1.0,
                                                1.0,
                                                3.0,
                                                3.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                2.0,
                                                0.9473684210526315,
                                                1.8421052631578947,
                                                1.631578947368421,
                                                2.1052631578947367,
                                                1.4736842105263157,
                                                1.1578947368421053,
                                                2.8421052631578947,
                                                3.0,
                                                0.21052631578947367,
                                                1.0,
                                                1.0,
                                                1.0,
                                                3.0,
                                                0.0,
                                                0.0,
                                                3.0,
                                                0.0,
                                                2.0,
                                                1.0,
                                                0.0,
                                                3.0,
                                                1.0,
                                                0.0,
                                                2.0,
                                                1.0,
                                                0.0,
                                                3.0,
                                                1.0,
                                                1.0,
                                                2.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                3.0,
                                                0.0],
                       'val_loss': 0.08975100517272949,
                       'val_loss_history': [0.12889361381530762,
                                            0.12675809860229492,
                                            929197312.0,
                                            0.12727877497673035,
                                            0.12555724382400513,
                                            1098212864.0,
                                            0.13582992553710938,
                                            0.1321955770254135,
                                            0.1356307566165924,
                                            1054040704.0,
                                            0.13556765019893646,
                                            0.14156989753246307,
                                            0.13816937804222107,
                                            268273632.0,
                                            0.13720349967479706,
                                            0.13542649149894714,
                                            0.1288645714521408,
                                            0.1226494163274765,
                                            0.12051358073949814,
                                            0.12420963495969772,
                                            0.12739074230194092,
                                            0.13060890138149261,
                                            0.1263168305158615,
                                            0.12670856714248657,
                                            0.11823848634958267,
                                            0.12219959497451782,
                                            0.11411118507385254,
                                            0.12208209931850433,
                                            0.12046002596616745,
                                            0.11428070068359375,
                                            0.11432527750730515,
                                            0.11463426798582077,
                                            0.1144825890660286,
                                            0.11444626748561859,
                                            0.11485874652862549,
                                            0.11457077413797379,
                                            0.1145758330821991,
                                            0.11442591995000839,
                                            0.1148984432220459,
                                            0.1149170994758606,
                                            0.11488619446754456,
                                            0.11486360430717468,
                                            0.114899180829525,
                                            0.1149524450302124,
                                            0.11508650332689285,
                                            0.1152985617518425,
                                            0.11511247605085373,
                                            223967184.0,
                                            0.1151353120803833,
                                            0.11522237956523895,
                                            0.11538894474506378,
                                            0.11536173522472382,
                                            0.11543510854244232,
                                            0.11553690582513809,
                                            0.11534439027309418,
                                            0.1152355745434761,
                                            0.11538137495517731,
                                            0.11679191887378693,
                                            0.11562331020832062,
                                            0.11570201814174652,
                                            0.11553406715393066,
                                            0.11560743302106857,
                                            0.11558980494737625,
                                            0.11557500809431076,
                                            0.11566941440105438,
                                            0.11569506675004959,
                                            0.1155826523900032,
                                            0.11563906818628311,
                                            0.11583434790372849,
                                            0.11583562195301056,
                                            0.11584048718214035,
                                            0.11579956114292145,
                                            0.1158096119761467,
                                            0.11577166616916656,
                                            0.11578544974327087,
                                            0.11576415598392487,
                                            0.11577217280864716,
                                            0.1157451793551445,
                                            0.1157572790980339,
                                            0.1158934161067009,
                                            0.11580001562833786,
                                            0.11593358963727951,
                                            0.11615775525569916,
                                            0.11587931215763092,
                                            0.1159670352935791,
                                            0.11585234105587006,
                                            0.11583954095840454,
                                            0.11609325557947159,
                                            0.11583058536052704,
                                            0.11599991470575333,
                                            0.11598140746355057,
                                            0.11592470854520798,
                                            0.11593374609947205,
                                            0.11602354794740677,
                                            0.11578560620546341,
                                            0.11599435657262802,
                                            0.11563146859407425,
                                            0.11705952137708664,
                                            0.11598608642816544,
                                            0.11604960262775421,
                                            0.11593892425298691,
                                            0.1176956295967102,
                                            0.12361403554677963,
                                            0.11607111245393753,
                                            0.11594823747873306,
                                            0.11593722552061081,
                                            0.11586553603410721,
                                            0.11618652194738388,
                                            0.11610959470272064,
                                            0.11593214422464371,
                                            0.11584636569023132,
                                            0.1159195527434349,
                                            0.11607397347688675,
                                            0.1159905418753624,
                                            0.1159055307507515,
                                            0.11624743789434433,
                                            0.11588098853826523,
                                            0.11602722853422165,
                                            0.11587972939014435,
                                            0.11616445332765579,
                                            0.11607798933982849,
                                            0.11569825559854507,
                                            0.11504572629928589,
                                            0.11600799113512039,
                                            0.11600035429000854,
                                            0.11612346768379211,
                                            0.11608055979013443,
                                            0.11566175520420074,
                                            0.1160079687833786,
                                            0.11582132428884506,
                                            0.11605935543775558,
                                            0.11587560176849365,
                                            0.11589808017015457,
                                            0.11591998487710953,
                                            0.11596931517124176,
                                            0.11618545651435852,
                                            0.11614586412906647,
                                            0.116184763610363,
                                            0.11593402177095413,
                                            0.11613350361585617,
                                            0.11606571078300476,
                                            0.11616233736276627,
                                            0.11608292907476425,
                                            0.11600427329540253,
                                            0.11627335101366043,
                                            0.11593248695135117,
                                            0.11602398753166199,
                                            0.11632940173149109,
                                            0.11617862433195114,
                                            0.11615864932537079,
                                            0.11609946191310883,
                                            0.11620981246232986,
                                            0.11623332649469376,
                                            0.11621405184268951,
                                            0.11617275327444077,
                                            0.11618194729089737,
                                            0.11618026345968246,
                                            0.11604853719472885,
                                            0.11610307544469833,
                                            0.11616066843271255,
                                            0.11609618365764618,
                                            0.11606502532958984,
                                            0.11607180535793304,
                                            0.11610879749059677,
                                            0.11615218967199326,
                                            0.11595457047224045,
                                            0.1159059926867485,
                                            0.11616579443216324,
                                            0.11522948741912842,
                                            0.11619925498962402,
                                            0.11620134115219116,
                                            0.11613330245018005,
                                            0.11608000099658966,
                                            0.11618129909038544,
                                            0.11634077876806259,
                                            0.11618518084287643,
                                            0.11617344617843628,
                                            0.11647146195173264,
                                            0.11618098616600037,
                                            0.11633583903312683,
                                            0.11608415842056274,
                                            0.11629339307546616,
                                            0.11636802554130554,
                                            0.11612612754106522,
                                            0.11618626117706299,
                                            0.11609125882387161,
                                            0.11604174226522446,
                                            0.11616165190935135,
                                            0.11629405617713928,
                                            0.12194935232400894,
                                            0.11632195115089417,
                                            0.1196022555232048,
                                            0.11585008352994919,
                                            0.12247772514820099,
                                            0.12818971276283264,
                                            0.12019037455320358,
                                            0.12467972934246063,
                                            0.11633002012968063,
                                            0.1162014976143837,
                                            0.11621222645044327,
                                            0.09137231856584549,
                                            0.09150426834821701,
                                            0.09157953411340714,
                                            0.09157821536064148,
                                            0.09164956212043762,
                                            0.09170088171958923,
                                            0.09174489974975586,
                                            0.09180380403995514,
                                            0.09149286895990372,
                                            0.09157092124223709,
                                            0.09140341728925705,
                                            0.09140137583017349,
                                            0.09056924283504486,
                                            0.09073775261640549,
                                            0.09158705919981003,
                                            0.0913533940911293,
                                            2.4094927310943604,
                                            0.09106162190437317,
                                            0.0903748944401741,
                                            0.09066660702228546,
                                            0.09095553308725357,
                                            0.09037058055400848,
                                            0.09065017104148865,
                                            0.09140996634960175,
                                            0.0915386751294136,
                                            0.09155571460723877,
                                            0.09167596697807312,
                                            0.09157241880893707,
                                            0.09159300476312637,
                                            0.09173470735549927,
                                            0.09168659150600433,
                                            0.09159538149833679,
                                            0.09157337993383408,
                                            0.09162420779466629,
                                            0.0916445180773735,
                                            0.09159063547849655,
                                            0.09151946753263474,
                                            0.09143755584955215,
                                            0.09167628735303879,
                                            0.09162566065788269,
                                            0.09144790470600128,
                                            0.09167981147766113,
                                            0.09183721989393234,
                                            0.09176892042160034,
                                            0.09148874878883362,
                                            0.09179455041885376,
                                            0.09149231016635895,
                                            0.09171196818351746,
                                            0.09166552871465683,
                                            0.09163936972618103,
                                            0.09166969358921051,
                                            0.09118418395519257,
                                            0.09167936444282532,
                                            0.09164978563785553,
                                            0.0916447639465332,
                                            0.091525137424469,
                                            0.0916038230061531,
                                            0.09174405038356781,
                                            0.09161648899316788,
                                            0.09172605723142624,
                                            0.09169495105743408,
                                            0.09160944074392319,
                                            0.09177764505147934,
                                            0.09175115078687668,
                                            0.09168726205825806,
                                            0.09168113023042679,
                                            0.0916929617524147,
                                            0.0916459709405899,
                                            0.09187834709882736,
                                            0.09167644381523132,
                                            0.0916120707988739,
                                            0.09165194630622864,
                                            0.09167271107435226,
                                            0.09164658933877945,
                                            0.09172767400741577,
                                            0.09166054427623749,
                                            0.09191282838582993,
                                            0.0919322669506073,
                                            0.09164082258939743,
                                            0.09164590388536453,
                                            0.09164344519376755,
                                            0.09161072224378586,
                                            0.09165661782026291,
                                            0.09178908169269562,
                                            0.09179839491844177,
                                            0.09184478968381882,
                                            0.09157416224479675,
                                            0.09172283858060837,
                                            0.09165951609611511,
                                            0.09154213964939117,
                                            0.09159321337938309,
                                            0.09175139665603638,
                                            0.0916604995727539,
                                            0.09152602404356003,
                                            0.09157373756170273,
                                            0.09160105139017105,
                                            0.09054151177406311,
                                            0.09138046205043793,
                                            0.0918545126914978,
                                            0.09164437651634216,
                                            0.09164205938577652,
                                            0.09161880612373352,
                                            0.09176848083734512,
                                            0.0915747582912445,
                                            0.09165892750024796,
                                            0.09155618399381638,
                                            0.09165655821561813,
                                            0.09176354855298996,
                                            0.0917825847864151,
                                            0.09167511016130447,
                                            0.09172207117080688,
                                            0.09178581088781357,
                                            0.09048595279455185,
                                            0.09156113862991333,
                                            0.09158486872911453,
                                            0.09151524305343628,
                                            0.09164683520793915,
                                            0.09167765825986862,
                                            0.09159915894269943,
                                            0.09176617115736008,
                                            0.09172510355710983,
                                            0.09160268306732178,
                                            0.09161321818828583,
                                            0.0915767028927803,
                                            0.0914856418967247,
                                            0.0916421189904213,
                                            0.09157934784889221,
                                            0.09156793355941772,
                                            0.09154218435287476,
                                            0.09154070168733597,
                                            134901216.0,
                                            0.09168075770139694,
                                            0.09144774079322815,
                                            0.09149137884378433,
                                            0.09155774861574173,
                                            0.09148041158914566,
                                            0.09169764071702957,
                                            0.0916762501001358,
                                            0.09158093482255936,
                                            0.09154773503541946,
                                            0.0916462242603302,
                                            0.0915762335062027,
                                            0.09153687953948975,
                                            0.09146840870380402,
                                            0.09179212152957916,
                                            0.0917789414525032,
                                            0.09176136553287506,
                                            0.09159605950117111,
                                            0.09162770956754684,
                                            0.09160606563091278,
                                            0.09176835417747498,
                                            0.09157654643058777,
                                            0.0916082113981247,
                                            0.09162461757659912,
                                            0.09182053804397583,
                                            0.09177893400192261,
                                            0.09165219217538834,
                                            0.09159030020236969,
                                            0.09154009819030762,
                                            0.09164740145206451,
                                            0.09176388382911682,
                                            0.0915454551577568,
                                            0.09177467226982117,
                                            0.09191010892391205,
                                            0.09156614542007446,
                                            0.09143801033496857,
                                            0.09114047884941101,
                                            0.09068604558706284,
                                            0.09070765227079391,
                                            0.09103754162788391,
                                            0.09052861481904984,
                                            0.09132906794548035,
                                            0.09160716831684113,
                                            0.09083017706871033,
                                            0.09090922772884369,
                                            0.09165212512016296,
                                            0.09168437123298645,
                                            0.09145098924636841,
                                            0.09153489023447037,
                                            8632.75390625,
                                            0.09173749387264252,
                                            0.09159638732671738,
                                            0.09155716747045517,
                                            0.09143556654453278,
                                            0.09150324761867523,
                                            0.09115756303071976,
                                            0.09095091372728348,
                                            0.09065255522727966,
                                            0.09077665954828262,
                                            0.09125294536352158,
                                            0.09146241843700409,
                                            0.0917263850569725,
                                            0.09158481657505035,
                                            0.09154310077428818,
                                            0.09155087172985077,
                                            0.09157835692167282,
                                            0.09175891429185867,
                                            0.09174496680498123,
                                            0.0915360376238823,
                                            0.0916609838604927,
                                            0.09006042033433914,
                                            0.09004908800125122,
                                            0.08979449421167374,
                                            0.0896245688199997,
                                            0.08980648964643478,
                                            0.0895053967833519,
                                            0.08965669572353363,
                                            0.09002640843391418,
                                            0.08996382355690002,
                                            0.08966709673404694,
                                            0.0897647812962532,
                                            0.08978300541639328,
                                            0.08981018513441086,
                                            0.0896415039896965,
                                            0.0901375487446785,
                                            0.089830681681633,
                                            0.09017007797956467,
                                            0.0898798406124115,
                                            0.09025020152330399,
                                            0.0899721086025238,
                                            0.08988645672798157,
                                            0.09003996104001999,
                                            0.08998358994722366,
                                            0.0893966481089592,
                                            0.08915132284164429,
                                            0.08969417959451675,
                                            0.08966122567653656,
                                            0.08971626311540604,
                                            0.08974547684192657,
                                            0.08988585323095322,
                                            0.09006890654563904,
                                            0.09015759825706482,
                                            0.08985879272222519,
                                            0.09011870622634888,
                                            0.089875727891922,
                                            0.08984020352363586,
                                            0.08999879658222198,
                                            0.08929476141929626,
                                            0.08917342871427536,
                                            0.0890253409743309,
                                            0.0890451967716217,
                                            0.08985283225774765,
                                            0.08977603167295456,
                                            0.08965098857879639,
                                            0.09000735729932785,
                                            0.0896926000714302,
                                            0.0897701233625412,
                                            0.08995763957500458,
                                            0.0898318812251091,
                                            0.08998756855726242,
                                            0.09007910639047623,
                                            0.09007957577705383,
                                            0.08997736871242523,
                                            0.08961615711450577,
                                            0.09001019597053528,
                                            0.08989803493022919,
                                            0.09013975411653519,
                                            0.0902264267206192,
                                            0.0899982675909996,
                                            0.08958622068166733,
                                            0.08898205310106277,
                                            0.08928899466991425,
                                            0.08966180682182312,
                                            0.0896264836192131,
                                            0.0899060070514679,
                                            165786832.0,
                                            0.08958649635314941,
                                            0.08978855609893799,
                                            0.0899762436747551,
                                            0.09000223875045776,
                                            0.08978422731161118,
                                            0.09043993800878525,
                                            0.09028058499097824,
                                            0.1005210429430008,
                                            0.08993940055370331,
                                            0.08960286527872086,
                                            0.08972615748643875,
                                            0.08975361287593842,
                                            0.08965158462524414,
                                            0.08964148908853531,
                                            0.08972785621881485,
                                            0.0897819921374321,
                                            0.08990558981895447,
                                            0.08987738937139511,
                                            0.09000679105520248,
                                            0.08997351676225662,
                                            0.08983514457941055,
                                            0.09013941884040833,
                                            0.09014367312192917,
                                            0.09003666788339615,
                                            0.09019093215465546,
                                            0.08984865993261337,
                                            0.08995110541582108,
                                            0.08969511836767197,
                                            0.08997870236635208,
                                            0.08957649022340775,
                                            0.08970191329717636,
                                            0.08933154493570328,
                                            0.08878691494464874,
                                            0.08927902579307556,
                                            0.08946020156145096,
                                            0.08946799486875534,
                                            0.08914709836244583,
                                            0.08887948840856552,
                                            0.08919765800237656,
                                            0.0895165354013443,
                                            0.08895201981067657,
                                            0.08957294374704361,
                                            0.08974824845790863,
                                            0.08876839280128479,
                                            0.08959838002920151,
                                            0.08914162218570709,
                                            0.08941791206598282,
                                            0.08913740515708923,
                                            0.08939765393733978,
                                            0.09013853222131729,
                                            0.08985241502523422,
                                            0.0897478461265564,
                                            0.08989280462265015,
                                            0.08953097462654114,
                                            0.08996637165546417,
                                            0.08999262005090714,
                                            0.09000848233699799,
                                            0.08972601592540741,
                                            0.08974424004554749,
                                            0.08952955156564713,
                                            0.08970942348241806,
                                            0.0896824523806572,
                                            0.0899357870221138,
                                            0.0900910422205925,
                                            0.08987075090408325,
                                            0.09012211114168167,
                                            0.0899343490600586,
                                            0.08979938924312592,
                                            0.08996124565601349,
                                            0.09016357362270355,
                                            0.09030314534902573,
                                            0.09023184329271317,
                                            0.09009277075529099,
                                            0.08956193923950195,
                                            0.09018462896347046,
                                            0.08991622179746628,
                                            0.08998739719390869,
                                            0.09010801464319229,
                                            0.08995979279279709,
                                            0.08951814472675323,
                                            0.09035605937242508,
                                            0.08999628573656082,
                                            0.08997727185487747,
                                            0.09033078700304031,
                                            0.08996672928333282,
                                            0.09028850495815277,
                                            0.0895623043179512,
                                            0.08894254267215729,
                                            0.09029853343963623,
                                            0.08997199684381485,
                                            0.08996271342039108,
                                            0.09009357541799545,
                                            0.09012578427791595,
                                            0.08983942866325378,
                                            0.08987858891487122,
                                            0.08974911272525787,
                                            0.08983664214611053,
                                            0.08988182246685028,
                                            0.0900752916932106,
                                            0.08942669630050659,
                                            0.08933917433023453,
                                            0.08779210597276688,
                                            0.08927730470895767,
                                            0.08909048140048981,
                                            0.0898263081908226,
                                            0.08931832760572433,
                                            0.08928386121988297,
                                            0.08991234749555588,
                                            0.08994374424219131,
                                            0.09000713378190994,
                                            0.08975329250097275,
                                            0.09016995877027512,
                                            0.08982622623443604,
                                            0.09043461084365845,
                                            0.09003646671772003,
                                            0.0898413360118866,
                                            0.08992080390453339,
                                            0.08976344019174576,
                                            0.09005337953567505,
                                            0.09002504497766495,
                                            0.08987294137477875,
                                            0.08990441262722015,
                                            0.08986706286668777,
                                            0.09007862210273743,
                                            0.08986128121614456,
                                            0.08975748717784882,
                                            0.0894976556301117,
                                            0.08977871388196945,
                                            0.08997070044279099,
                                            0.0896330401301384,
                                            0.09019531309604645,
                                            0.09008292108774185,
                                            0.09006659686565399,
                                            0.08975100517272949],
                       'weight_decay': 0.0005}],
 'models': {}}
2019-09-14 03:47:43,751 - training_jobs - DEBUG - test_multiple_models
2019-09-14 03:47:43,751 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-14 03:47:45,913 - training_jobs - ERROR - Error with trains/task_4.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 641, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 523, in training_dispatcher
    models_folder='models/gnn/')
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1793, in test_multiple_models
    testresult = testModel(bmodel, test_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1669, in testModel
    _, pred = model(data).max(dim=1)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 30, in forward
    x = self.ggnn(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 103, in forward
    h = self.rnn(m, h)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/rnn.py", line 800, in forward
    self.bias_ih, self.bias_hh,
RuntimeError: CUDA out of memory. Tried to allocate 716.38 MiB (GPU 0; 3.95 GiB total capacity; 2.32 GiB already allocated; 373.38 MiB free; 398.72 MiB cached)
2019-09-14 03:47:48,700 - training_jobs - DEBUG - training trains/task_2.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 30,
 'd2': 50,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max/',
 'epochs': 300,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-14 03:47:48,714 - training_jobs - DEBUG - training with: 
2019-09-14 03:47:48,715 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max/
2019-09-14 03:47:48,715 - training_jobs - DEBUG - GGNN1
2019-09-14 03:47:48,715 - training_jobs - DEBUG - 
2019-09-14 03:47:48,715 - training_jobs - DEBUG - ggnn training
2019-09-14 03:47:54,384 - training_jobs - DEBUG -  saving results to results/20190914_034754_ggnn_.json
2019-09-14 03:47:54,384 - training_jobs - DEBUG -  calling modelSelection
{'best_models': {'accuracy': {'accuracy': 2.0,
                              'batch_size': 32,
                              'cv_score': 0.017618830126676397,
                              'cv_val_accuracy': 0.7777777777777778,
                              'cv_val_loss': 0.09876688073078792,
                              'cv_val_macroF1': 0.017618830126676397,
                              'cv_val_microF1': 0.07673131407953461,
                              'epochs': 300,
                              'final_model': GGNN1(
  (ggnn): GatedGraphConv(30, num_layers=2)
  (fc1): Linear(in_features=30, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                              'kwargs': {'aggr_type': 'mean',
                                         'd1': 30,
                                         'd2': 50,
                                         'num_classes': 24,
                                         'num_layers': 2},
                              'learning_rate': 0.01,
                              'macroF1': 0.006748704489763125,
                              'microF1': 0.08414023372287145,
                              'model': <class 'TFM_graph_classification_models.GGNN1'>,
                              'model_instance': GGNN1(
  (ggnn): GatedGraphConv(30, num_layers=2)
  (fc1): Linear(in_features=30, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                              'score': 'f1_macro',
                              'time': 3923.382642507553,
                              'train_loss_history': [402.3915100097656,
                                                     0.627553403377533,
                                                     0.1610880345106125,
                                                     200590368.0,
                                                     127.40691375732422,
                                                     0.0838785469532013,
                                                     0.08383877575397491,
                                                     0.08405112475156784,
                                                     0.08400005102157593,
                                                     0.11704819649457932,
                                                     315.0480651855469,
                                                     8.739550590515137,
                                                     0.26758015155792236,
                                                     150.98406982421875,
                                                     1.1856788396835327,
                                                     1248861440.0,
                                                     2.6898114681243896,
                                                     0.08465886861085892,
                                                     0.08455833047628403,
                                                     0.08453904837369919,
                                                     0.08463430404663086,
                                                     0.08893854171037674,
                                                     2255.366943359375,
                                                     5255256.5,
                                                     0.0845518633723259,
                                                     0.08456744998693466,
                                                     0.0846351757645607,
                                                     0.08459139615297318,
                                                     0.0845951959490776,
                                                     0.08459458500146866,
                                                     741.9649047851562,
                                                     375.397216796875,
                                                     0.08453738689422607,
                                                     1314.5992431640625,
                                                     0.08459823578596115,
                                                     2760.76708984375,
                                                     0.08449413627386093,
                                                     0.0844498947262764,
                                                     0.08453848212957382,
                                                     0.08451221138238907,
                                                     0.08967375755310059,
                                                     0.21154144406318665,
                                                     6485.376953125,
                                                     123632.40625,
                                                     29165.78125,
                                                     38.000274658203125,
                                                     0.08432421088218689,
                                                     0.08440204709768295,
                                                     0.08437246084213257,
                                                     31.02018928527832,
                                                     0.08415640890598297,
                                                     4.811329364776611,
                                                     0.08450804650783539,
                                                     0.08431932330131531,
                                                     0.0844046100974083,
                                                     1.6224708557128906,
                                                     0.08452630043029785,
                                                     0.08453928679227829,
                                                     0.0845048725605011,
                                                     0.0844370573759079,
                                                     0.08444765210151672,
                                                     0.08450065553188324,
                                                     0.08445669710636139,
                                                     0.08438482880592346,
                                                     0.08415114134550095,
                                                     0.08441533893346786,
                                                     0.08437883108854294,
                                                     0.08448990434408188,
                                                     0.08448030054569244,
                                                     0.0844801515340805,
                                                     0.08438432216644287,
                                                     0.08433922380208969,
                                                     0.08432400226593018,
                                                     0.08451466262340546,
                                                     0.08452446758747101,
                                                     0.08453328907489777,
                                                     0.08448510617017746,
                                                     0.08447270840406418,
                                                     0.08449146896600723,
                                                     0.08447691053152084,
                                                     0.08443857729434967,
                                                     0.08449263125658035,
                                                     0.08442482352256775,
                                                     0.08449450135231018,
                                                     0.0844353586435318,
                                                     0.08452147245407104,
                                                     0.0844416618347168,
                                                     0.08446303755044937,
                                                     0.08447965979576111,
                                                     0.0843501091003418,
                                                     0.08422920107841492,
                                                     0.08444242179393768,
                                                     0.08447030931711197,
                                                     0.08448900282382965,
                                                     0.08450789749622345,
                                                     0.08435028791427612,
                                                     0.08434262871742249,
                                                     0.08449234813451767,
                                                     0.08449122309684753,
                                                     0.08449383825063705,
                                                     0.08452076464891434,
                                                     0.08449948579072952,
                                                     0.08440456539392471,
                                                     0.08449050039052963,
                                                     0.0841154232621193,
                                                     2.2428197860717773,
                                                     0.08384676277637482,
                                                     0.08394981920719147,
                                                     0.0838002935051918,
                                                     0.08447454124689102,
                                                     0.08348763734102249,
                                                     0.08416859805583954,
                                                     0.08417699486017227,
                                                     0.08438660949468613,
                                                     0.4162343144416809,
                                                     0.08450004458427429,
                                                     0.08446227014064789,
                                                     0.08444914221763611,
                                                     0.0843791589140892,
                                                     0.08446182310581207,
                                                     0.08455074578523636,
                                                     0.08446145057678223,
                                                     0.0836617723107338,
                                                     0.08338803797960281,
                                                     0.08333099633455276,
                                                     0.08318332582712173,
                                                     0.08332567662000656,
                                                     0.08302734792232513,
                                                     0.08371946960687637,
                                                     0.08449801802635193,
                                                     0.08449846506118774,
                                                     0.08426801860332489,
                                                     0.08432383090257645,
                                                     200335072.0,
                                                     0.08437470346689224,
                                                     0.08452057838439941,
                                                     0.08447948843240738,
                                                     0.08449156582355499,
                                                     0.08443012088537216,
                                                     0.0844721645116806,
                                                     0.08447398245334625,
                                                     0.08443788439035416,
                                                     0.08443618565797806,
                                                     0.08449234068393707,
                                                     0.10008276253938675,
                                                     0.08434968441724777,
                                                     0.08430284261703491,
                                                     0.08442266285419464,
                                                     0.08438229560852051,
                                                     1806.7781982421875,
                                                     0.08451782912015915,
                                                     0.0844816267490387,
                                                     13.898186683654785,
                                                     0.08445671200752258,
                                                     1.4685406684875488,
                                                     4.019253253936768,
                                                     302.15570068359375,
                                                     0.08452155441045761,
                                                     0.14563511312007904,
                                                     27.54737663269043,
                                                     3.7263739109039307,
                                                     78.2233657836914,
                                                     0.08451861888170242,
                                                     0.08448174595832825,
                                                     407.52105712890625,
                                                     19.42632484436035,
                                                     0.08444830775260925,
                                                     36449228.0,
                                                     7.751605033874512,
                                                     0.08446042984724045,
                                                     0.08437091112136841,
                                                     3167184.25,
                                                     0.08450077474117279,
                                                     0.08428910374641418,
                                                     0.08419448137283325,
                                                     0.08466444909572601,
                                                     5.597066402435303,
                                                     0.11012237519025803,
                                                     0.08447238802909851,
                                                     58.212467193603516,
                                                     5.655828952789307,
                                                     3588.997314453125,
                                                     0.409007728099823,
                                                     32.17847442626953,
                                                     0.08447328209877014,
                                                     0.0845048725605011,
                                                     0.08449171483516693,
                                                     0.08447052538394928,
                                                     0.08445976674556732,
                                                     0.08446221798658371,
                                                     0.08458263427019119,
                                                     0.08430005609989166,
                                                     8.114312171936035,
                                                     0.08432270586490631,
                                                     0.08445460349321365,
                                                     0.08443901687860489,
                                                     900.8036499023438,
                                                     0.08444087207317352,
                                                     0.08441809564828873,
                                                     0.4795038402080536,
                                                     0.08451033383607864,
                                                     0.08451204001903534,
                                                     0.08450926095247269,
                                                     0.08447223901748657,
                                                     0.08437959104776382,
                                                     0.08444824069738388,
                                                     0.08453644067049026,
                                                     7.690800666809082,
                                                     0.08448299020528793,
                                                     0.08449721336364746,
                                                     0.08445368707180023,
                                                     0.08438749611377716,
                                                     0.08451572060585022,
                                                     0.08452669531106949,
                                                     89.34236907958984,
                                                     0.08450479805469513,
                                                     30.75047492980957,
                                                     0.08451566100120544,
                                                     0.08449652045965195,
                                                     68.70880126953125,
                                                     0.08445856720209122,
                                                     12.459997177124023,
                                                     0.08449093997478485,
                                                     0.08446609228849411,
                                                     0.08448096364736557,
                                                     0.11521673947572708,
                                                     0.08448649197816849,
                                                     1010.2301025390625,
                                                     0.08451484888792038,
                                                     0.0844508707523346,
                                                     0.0843883827328682,
                                                     0.08444556593894958,
                                                     182.40513610839844,
                                                     0.08444695174694061,
                                                     304.5606689453125,
                                                     0.0844317376613617,
                                                     0.08447675406932831,
                                                     0.08446475118398666,
                                                     0.08447200804948807,
                                                     0.08442620933055878,
                                                     0.08441336452960968,
                                                     0.08443718403577805,
                                                     0.08444596081972122,
                                                     0.0845516175031662,
                                                     0.08445774763822556,
                                                     37.44003677368164,
                                                     0.0844288170337677,
                                                     0.08449657261371613,
                                                     0.08451525121927261,
                                                     0.08444896340370178,
                                                     0.08451040834188461,
                                                     0.08449771255254745,
                                                     0.0844845324754715,
                                                     0.08452331274747849,
                                                     0.08448804169893265,
                                                     0.08449441939592361,
                                                     0.08447355777025223,
                                                     217506.6875,
                                                     0.08448262512683868,
                                                     27566.361328125,
                                                     0.084529809653759,
                                                     60.93157958984375,
                                                     0.0844476968050003,
                                                     0.08442551642656326,
                                                     81.24128723144531,
                                                     0.08442741632461548,
                                                     0.08438073843717575,
                                                     0.08440481126308441,
                                                     0.08442452549934387,
                                                     0.08434814214706421,
                                                     11.347413063049316,
                                                     26.29210662841797,
                                                     0.23023147881031036,
                                                     0.08447755873203278,
                                                     0.08438970148563385,
                                                     0.08435468375682831,
                                                     0.08439872413873672,
                                                     0.08438412100076675,
                                                     0.08440207690000534,
                                                     0.0844302773475647,
                                                     11.261951446533203,
                                                     0.08435912430286407,
                                                     3.4124855995178223,
                                                     0.08436176180839539,
                                                     0.08430063724517822,
                                                     0.08941613137722015,
                                                     0.08446674793958664,
                                                     0.08433955907821655,
                                                     0.08443570137023926,
                                                     0.08435171097517014,
                                                     21675608.0,
                                                     0.08440378308296204,
                                                     0.08433499932289124,
                                                     0.40477755665779114,
                                                     0.08435443788766861,
                                                     0.08426611125469208,
                                                     13186.3408203125,
                                                     39600.96875,
                                                     0.08453357219696045,
                                                     0.9381150603294373,
                                                     0.09587060660123825,
                                                     0.514412522315979,
                                                     0.09367242455482483,
                                                     0.09441542625427246,
                                                     0.09452634304761887,
                                                     0.09454908967018127,
                                                     22.32537269592285,
                                                     32339.833984375,
                                                     0.09451416879892349,
                                                     0.09449652582406998,
                                                     0.09464240074157715,
                                                     0.09449930489063263,
                                                     1958.1798095703125,
                                                     0.09455957263708115,
                                                     0.09456851333379745,
                                                     1.2811931371688843,
                                                     0.09453321993350983,
                                                     0.09458591043949127,
                                                     0.09450304508209229,
                                                     5.49962043762207,
                                                     72.5165023803711,
                                                     0.09458471089601517,
                                                     0.0945429876446724,
                                                     0.09455682337284088,
                                                     0.09453796595335007,
                                                     0.09452159702777863,
                                                     2.5312438011169434,
                                                     0.0945226177573204,
                                                     0.0944872573018074,
                                                     5.223100185394287,
                                                     2137.731201171875,
                                                     0.142483189702034,
                                                     0.20935583114624023,
                                                     0.09459515661001205,
                                                     0.09455345571041107,
                                                     2812.721435546875,
                                                     0.09442364424467087,
                                                     0.1131102666258812,
                                                     0.09442222863435745,
                                                     0.09438570588827133,
                                                     0.09433407336473465,
                                                     0.13149750232696533,
                                                     0.09370430558919907,
                                                     0.09366568177938461,
                                                     0.09354622662067413,
                                                     0.09447423368692398,
                                                     0.6216050982475281,
                                                     0.09451036900281906,
                                                     0.09451115876436234,
                                                     1349.734619140625,
                                                     0.09453874081373215,
                                                     0.11682190001010895,
                                                     0.10219868272542953,
                                                     0.09455303847789764,
                                                     0.0945776104927063,
                                                     0.09455080330371857,
                                                     2.9681241512298584,
                                                     0.09452127665281296,
                                                     0.09452573955059052,
                                                     29.074121475219727,
                                                     14.531797409057617,
                                                     0.09461171925067902,
                                                     0.09457170218229294,
                                                     0.09453096240758896,
                                                     0.7046006321907043,
                                                     0.33546608686447144,
                                                     0.09435248374938965,
                                                     0.09443413466215134,
                                                     0.31541207432746887,
                                                     0.14433683454990387,
                                                     0.09446719288825989,
                                                     3715.66796875,
                                                     0.0944618359208107,
                                                     0.09454601258039474,
                                                     0.0945052057504654,
                                                     0.09456264227628708,
                                                     0.09456627815961838,
                                                     1.2233333587646484,
                                                     0.09454774111509323,
                                                     249.55780029296875,
                                                     181.83404541015625,
                                                     154.2233123779297,
                                                     0.09459660947322845,
                                                     0.09442275762557983,
                                                     0.09446389973163605,
                                                     0.09459325671195984,
                                                     0.09457375854253769,
                                                     0.0945519208908081,
                                                     0.09459152817726135,
                                                     0.0945110023021698,
                                                     0.7131268978118896,
                                                     0.09457242488861084,
                                                     13.699750900268555,
                                                     0.09455432742834091,
                                                     0.09460796415805817,
                                                     0.09450221061706543,
                                                     11696.2060546875,
                                                     0.09452937543392181,
                                                     0.09452711045742035,
                                                     0.09458191692829132,
                                                     9.455473899841309,
                                                     0.09451385587453842,
                                                     0.09448845684528351,
                                                     131.53732299804688,
                                                     4293.73291015625,
                                                     0.09409381449222565,
                                                     0.09368418902158737,
                                                     0.09371126443147659,
                                                     0.09354376047849655,
                                                     0.2533681094646454,
                                                     0.314374715089798,
                                                     0.09430520236492157,
                                                     0.09440848976373672,
                                                     0.094419464468956,
                                                     0.09437459707260132,
                                                     0.09439592063426971,
                                                     1767081.0,
                                                     594.7551879882812,
                                                     0.09452971816062927,
                                                     0.09449964016675949,
                                                     0.09450231492519379,
                                                     0.09459840506315231,
                                                     0.09455262124538422,
                                                     0.09447214007377625,
                                                     0.0944393202662468,
                                                     0.09440606832504272,
                                                     0.09447411447763443,
                                                     0.09437554329633713,
                                                     0.09433301538228989,
                                                     0.09440498054027557,
                                                     0.10780218243598938,
                                                     0.09436662495136261,
                                                     0.09431613236665726,
                                                     0.0945611521601677,
                                                     219.7027587890625,
                                                     0.4978674352169037,
                                                     0.0944586768746376,
                                                     0.30992811918258667,
                                                     1.8426533937454224,
                                                     0.5712193250656128,
                                                     0.09446409344673157,
                                                     0.09459622949361801,
                                                     0.09451809525489807,
                                                     87.38387298583984,
                                                     0.09456565976142883,
                                                     3497.284423828125,
                                                     0.09454844892024994,
                                                     0.09451253712177277,
                                                     0.09457314759492874,
                                                     0.09456212818622589,
                                                     0.15045373141765594,
                                                     0.14893552660942078,
                                                     562.9702758789062,
                                                     131.55374145507812,
                                                     0.09452853351831436,
                                                     0.09735272079706192,
                                                     0.09456931799650192,
                                                     0.6130989193916321,
                                                     0.09453856199979782,
                                                     0.09444250911474228,
                                                     0.09452873468399048,
                                                     0.09453690052032471,
                                                     1.229264736175537,
                                                     0.09449880570173264,
                                                     820.4354248046875,
                                                     1325.2156982421875,
                                                     0.09444205462932587,
                                                     0.0945630744099617,
                                                     0.09459412842988968,
                                                     0.0945231169462204,
                                                     513502.1875,
                                                     0.09456072747707367,
                                                     1.1679768562316895,
                                                     0.0945015400648117,
                                                     0.09457283467054367,
                                                     0.09455496817827225,
                                                     0.19079294800758362,
                                                     36.89104461669922,
                                                     0.0944361463189125,
                                                     0.09443370997905731,
                                                     0.09447237849235535,
                                                     20771922.0,
                                                     0.09442625194787979,
                                                     0.1135120838880539,
                                                     0.09445078670978546,
                                                     0.09434980154037476,
                                                     0.09435220062732697,
                                                     0.35446515679359436,
                                                     0.09654965996742249,
                                                     0.09453532844781876,
                                                     0.09456611424684525,
                                                     32323.8828125,
                                                     978.3565063476562,
                                                     0.09457312524318695,
                                                     0.0945013165473938,
                                                     0.09439229965209961,
                                                     4412095.5,
                                                     0.09448060393333435,
                                                     0.09440262615680695,
                                                     0.0943794921040535,
                                                     0.09443134814500809,
                                                     369.93389892578125,
                                                     0.14076443016529083,
                                                     0.09438452124595642,
                                                     0.09434284269809723,
                                                     0.10136646777391434,
                                                     0.09443105757236481,
                                                     0.09402314573526382,
                                                     26166.89453125,
                                                     0.09349802136421204,
                                                     3.321302652359009,
                                                     0.09364969283342361,
                                                     0.09496324509382248,
                                                     0.09372551739215851,
                                                     0.09436341375112534,
                                                     0.09432327002286911,
                                                     0.09436825662851334,
                                                     0.0944887325167656,
                                                     0.09451030939817429,
                                                     0.09454188495874405,
                                                     0.0945427343249321,
                                                     0.09454578906297684,
                                                     0.09454236924648285,
                                                     0.09450600296258926,
                                                     0.09452961385250092,
                                                     0.09452560544013977,
                                                     0.09452951699495316,
                                                     2834032.25,
                                                     0.09452395886182785,
                                                     10.720794677734375,
                                                     68.72327423095703,
                                                     8477249.0,
                                                     0.09440808743238449,
                                                     5520.7861328125,
                                                     0.09362620860338211,
                                                     0.09349146485328674,
                                                     0.09339573979377747,
                                                     149.35525512695312,
                                                     278.6841735839844,
                                                     0.0943543016910553,
                                                     37.89771270751953,
                                                     5100427.5,
                                                     0.09433344006538391,
                                                     6600393.5,
                                                     0.09443498402833939,
                                                     0.09448225051164627,
                                                     0.09439776837825775,
                                                     0.09439967572689056,
                                                     0.09428673982620239,
                                                     0.0945155993103981,
                                                     0.27247631549835205,
                                                     0.09454277157783508,
                                                     0.09456893801689148,
                                                     0.09452944248914719,
                                                     0.09454522281885147,
                                                     0.09451722353696823,
                                                     0.09460891038179398,
                                                     0.09455429017543793,
                                                     10244.599609375,
                                                     84.07907104492188,
                                                     0.0944337323307991,
                                                     0.09459497779607773,
                                                     0.09454827755689621,
                                                     0.09453362226486206,
                                                     0.09449025988578796,
                                                     0.09453912824392319,
                                                     0.3738631308078766,
                                                     0.09447181224822998,
                                                     0.09451087564229965,
                                                     0.09459631890058517,
                                                     0.09454839676618576,
                                                     0.09458976984024048,
                                                     0.09457769244909286,
                                                     0.09456919878721237,
                                                     3.4914751052856445,
                                                     1427.79345703125,
                                                     0.4578329920768738,
                                                     884.7360229492188,
                                                     0.09449641406536102,
                                                     0.09440287202596664,
                                                     510.2096862792969,
                                                     0.09450377523899078,
                                                     0.09451079368591309,
                                                     2.1641271114349365,
                                                     0.0944771096110344,
                                                     25.285322189331055,
                                                     0.09460421651601791,
                                                     0.09461460262537003,
                                                     21697.78125,
                                                     0.11157756298780441,
                                                     9.951555252075195,
                                                     0.09457164257764816,
                                                     0.09458516538143158,
                                                     770.6202392578125,
                                                     0.5892700552940369,
                                                     0.09393339604139328,
                                                     0.09381875395774841,
                                                     0.09371479600667953,
                                                     0.09378113597631454,
                                                     0.0938304141163826,
                                                     0.09431172162294388,
                                                     1034.4150390625,
                                                     538.9066772460938,
                                                     0.09464322030544281,
                                                     0.7673370242118835,
                                                     0.18193259835243225,
                                                     0.09461429715156555,
                                                     0.09462171792984009,
                                                     0.6443932056427002,
                                                     0.09465876966714859,
                                                     0.09471260756254196,
                                                     0.09465605765581131,
                                                     0.09452185034751892,
                                                     0.09450294822454453,
                                                     0.12810559570789337,
                                                     0.094684898853302,
                                                     8.992913246154785,
                                                     0.8569191694259644,
                                                     582.5096435546875,
                                                     0.09462642669677734,
                                                     0.09460807591676712,
                                                     0.09465175122022629,
                                                     0.0958004742860794,
                                                     0.09474059194326401,
                                                     0.09462853521108627,
                                                     657.1932373046875,
                                                     0.19906458258628845,
                                                     0.09392101317644119,
                                                     0.09379742294549942,
                                                     0.7946452498435974,
                                                     0.1725653111934662,
                                                     0.09447696059942245,
                                                     0.09447885304689407,
                                                     0.09469438344240189,
                                                     0.09465198963880539,
                                                     0.09460066258907318,
                                                     560.6574096679688,
                                                     0.09460125863552094,
                                                     0.0946880578994751,
                                                     0.09468343108892441,
                                                     0.09466951340436935,
                                                     7.846862316131592,
                                                     0.09464780241250992,
                                                     1.8931728601455688,
                                                     0.0939062237739563,
                                                     0.14714713394641876,
                                                     0.09371401369571686,
                                                     0.14943532645702362,
                                                     9.442521095275879,
                                                     0.09458307921886444,
                                                     0.09459754824638367,
                                                     0.19837377965450287,
                                                     0.18834809958934784,
                                                     0.09383810311555862,
                                                     0.09375286102294922,
                                                     0.0936644896864891,
                                                     0.09361999481916428,
                                                     0.0935748741030693,
                                                     1.445277214050293,
                                                     0.0944688618183136,
                                                     12.287859916687012,
                                                     0.09468797594308853,
                                                     0.09469488263130188,
                                                     0.09460695087909698,
                                                     0.7720686197280884,
                                                     0.0945553258061409,
                                                     0.09460161626338959,
                                                     0.09466303139925003,
                                                     0.09467338770627975,
                                                     0.09465459734201431,
                                                     0.09461504220962524,
                                                     0.0945778414607048,
                                                     0.09464619308710098,
                                                     0.09465964883565903,
                                                     0.09464212507009506,
                                                     229.66256713867188,
                                                     18.826688766479492,
                                                     0.09455174207687378,
                                                     0.09412211924791336,
                                                     17504.03515625,
                                                     0.09471108019351959,
                                                     0.09461493790149689,
                                                     10.089583396911621,
                                                     0.09461957961320877,
                                                     31.376590728759766,
                                                     0.09470537304878235,
                                                     0.09463783353567123,
                                                     5610218.0,
                                                     0.09464839100837708,
                                                     0.09463448077440262,
                                                     0.0942724272608757,
                                                     0.09408007562160492,
                                                     0.48262032866477966,
                                                     0.09465523064136505,
                                                     2.4015886783599854,
                                                     0.09462562948465347,
                                                     142063.546875,
                                                     0.09455953538417816,
                                                     1.974007487297058,
                                                     0.09464676678180695,
                                                     0.09465840458869934,
                                                     0.10140714049339294,
                                                     0.09454220533370972,
                                                     0.09462330490350723,
                                                     0.09451285749673843,
                                                     4.050822734832764,
                                                     0.09465648978948593,
                                                     0.09463375806808472,
                                                     0.09465823322534561,
                                                     0.09469760209321976,
                                                     0.09466546028852463,
                                                     932.041259765625,
                                                     0.09463147073984146,
                                                     3.409330129623413,
                                                     54.3207893371582,
                                                     0.09465781599283218,
                                                     0.09465416520833969,
                                                     1.7536137104034424,
                                                     0.09468229115009308,
                                                     84.98772430419922,
                                                     0.09475217014551163,
                                                     0.09401313960552216,
                                                     134.52792358398438,
                                                     0.09469401836395264,
                                                     0.23205254971981049,
                                                     0.0946466401219368,
                                                     0.09470997005701065,
                                                     0.09463036060333252,
                                                     0.09456641972064972,
                                                     0.09461867809295654,
                                                     0.9174342155456543,
                                                     0.09432287514209747,
                                                     3.919257879257202,
                                                     0.09378696233034134,
                                                     0.09345461428165436,
                                                     0.09351752698421478,
                                                     0.09323997050523758,
                                                     0.09312094748020172,
                                                     0.09307726472616196,
                                                     0.09321693331003189,
                                                     0.09316740185022354,
                                                     0.09299517422914505,
                                                     0.09285470098257065,
                                                     0.2572370171546936,
                                                     0.09311380237340927,
                                                     0.09293543547391891,
                                                     0.09303213655948639,
                                                     0.1493358463048935,
                                                     0.09310180693864822,
                                                     0.09321630746126175,
                                                     10.437809944152832,
                                                     0.0943022146821022,
                                                     0.09423695504665375,
                                                     2590.866943359375,
                                                     1.4054656028747559,
                                                     38.2252311706543,
                                                     19227.08203125,
                                                     0.09473899751901627,
                                                     0.09466087073087692,
                                                     0.0946071520447731,
                                                     830.8789672851562,
                                                     0.0945894792675972,
                                                     2.7011053562164307,
                                                     0.504011332988739,
                                                     0.09468135982751846,
                                                     0.09465712308883667,
                                                     0.0945027694106102,
                                                     3.204328775405884,
                                                     0.09469970315694809,
                                                     136.45835876464844,
                                                     0.0946570485830307,
                                                     276.604248046875,
                                                     0.12033631652593613,
                                                     0.09448137134313583,
                                                     1677058.875,
                                                     0.09450552612543106,
                                                     6.231417179107666,
                                                     0.4628032147884369,
                                                     0.12920531630516052,
                                                     0.5548384189605713,
                                                     0.09467286616563797,
                                                     0.09465939551591873,
                                                     0.09464029967784882,
                                                     0.09545759111642838,
                                                     0.09464362263679504,
                                                     1.4605083465576172,
                                                     0.6265268921852112,
                                                     0.13141100108623505,
                                                     0.09462108463048935,
                                                     0.09455641359090805,
                                                     5.9682111740112305,
                                                     0.09459859877824783,
                                                     32.384986877441406,
                                                     0.09444091469049454,
                                                     0.09422245621681213,
                                                     0.0940980538725853,
                                                     0.09372019022703171,
                                                     0.09348495304584503,
                                                     0.09494473040103912,
                                                     0.09471526741981506,
                                                     0.09466321766376495,
                                                     0.09463952481746674,
                                                     0.0946340337395668,
                                                     0.0946597084403038,
                                                     0.10840669274330139,
                                                     0.09467717260122299,
                                                     0.09460677951574326,
                                                     0.09988532215356827,
                                                     0.09422221034765244,
                                                     0.11676912754774094,
                                                     0.09440311044454575,
                                                     45.41225051879883,
                                                     507.40032958984375,
                                                     0.13748902082443237,
                                                     0.09470012038946152,
                                                     226.58145141601562,
                                                     0.4495183825492859,
                                                     0.09465526789426804,
                                                     0.09466543048620224,
                                                     61.091617584228516,
                                                     0.09468193352222443,
                                                     0.09465447813272476,
                                                     0.0945592001080513,
                                                     0.09467066824436188,
                                                     0.09464655071496964,
                                                     141.53970336914062,
                                                     116328.4609375,
                                                     0.09458193182945251,
                                                     0.12122631818056107,
                                                     4.387285232543945,
                                                     2681976.5,
                                                     0.09463132172822952,
                                                     0.0946061760187149,
                                                     0.0944783166050911,
                                                     0.09446098655462265,
                                                     0.09450376033782959,
                                                     0.09455794841051102,
                                                     0.09459281712770462,
                                                     0.09462811797857285,
                                                     6.4233717918396,
                                                     0.09450298547744751,
                                                     0.09456917643547058,
                                                     0.0934096947312355,
                                                     0.0934649258852005,
                                                     0.09316615760326385,
                                                     0.5173411965370178,
                                                     1.9697647094726562,
                                                     32.42613983154297,
                                                     0.09472211450338364,
                                                     0.09469816833734512,
                                                     0.0956822857260704,
                                                     0.09459832310676575,
                                                     0.0946735143661499,
                                                     0.09462333470582962,
                                                     0.09459586441516876,
                                                     1.9096038341522217,
                                                     0.0946708396077156,
                                                     0.0946536511182785,
                                                     0.09466943144798279,
                                                     0.09466858953237534,
                                                     0.09464891254901886,
                                                     0.09463982284069061,
                                                     0.09469345957040787,
                                                     0.0946459099650383,
                                                     0.0945952758193016,
                                                     122.37969970703125,
                                                     4.63157844543457,
                                                     56.718482971191406,
                                                     0.18920323252677917,
                                                     454.80084228515625,
                                                     4036.590087890625,
                                                     0.09464812278747559,
                                                     127179.6875,
                                                     0.09467849880456924,
                                                     0.09463875740766525,
                                                     0.09431662410497665,
                                                     665.6954956054688,
                                                     0.09449782967567444,
                                                     0.09459271281957626,
                                                     0.09455017000436783,
                                                     2.904323101043701,
                                                     0.09459733217954636,
                                                     0.0946681872010231,
                                                     0.09470684081315994,
                                                     3.52655029296875,
                                                     0.09464657306671143,
                                                     0.09463555365800858,
                                                     1.8135828971862793,
                                                     0.09465093165636063,
                                                     0.0946563184261322,
                                                     0.09455125033855438,
                                                     0.09466362744569778,
                                                     0.0946488231420517,
                                                     2.0649492740631104,
                                                     1.0428603887557983,
                                                     0.09464512020349503,
                                                     0.09463749080896378,
                                                     0.09463761001825333,
                                                     204.697509765625,
                                                     0.09467710554599762],
                              'val_accuracy_history': [1.7894736842105263,
                                                       1.368421052631579,
                                                       0.3157894736842105,
                                                       1.0526315789473684,
                                                       1.0,
                                                       0.42105263157894735,
                                                       0.6842105263157895,
                                                       1.4210526315789473,
                                                       0.10526315789473684,
                                                       0.3157894736842105,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       3.0,
                                                       0.0,
                                                       2.0,
                                                       1.0,
                                                       2.0,
                                                       1.0,
                                                       0.0,
                                                       2.0,
                                                       1.0,
                                                       2.0,
                                                       1.0,
                                                       2.0,
                                                       3.0,
                                                       1.0,
                                                       2.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       2.1578947368421053,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       3.0,
                                                       0.0,
                                                       1.0,
                                                       1.2105263157894737,
                                                       2.0,
                                                       1.0,
                                                       2.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       2.0,
                                                       1.0,
                                                       0.5263157894736842,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       2.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       2.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       4.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       2.0,
                                                       1.0,
                                                       3.0,
                                                       1.0,
                                                       0.631578947368421,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       1.631578947368421,
                                                       1.8421052631578947,
                                                       0.0,
                                                       2.0,
                                                       0.9473684210526315,
                                                       0.47368421052631576,
                                                       0.7894736842105263,
                                                       0.05263157894736842,
                                                       3.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       1.2105263157894737,
                                                       0.8421052631578947,
                                                       0.21052631578947367,
                                                       0.9473684210526315,
                                                       1.5263157894736843,
                                                       1.0526315789473684,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       2.0,
                                                       1.0,
                                                       2.0,
                                                       1.0,
                                                       2.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       3.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       1.6842105263157894,
                                                       1.9473684210526316,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       3.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       2.0,
                                                       2.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       3.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       3.0,
                                                       1.0,
                                                       1.0,
                                                       2.0,
                                                       1.0,
                                                       2.0,
                                                       3.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       2.0,
                                                       3.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       3.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.7777777777777778,
                                                       0.1111111111111111,
                                                       0.8888888888888888,
                                                       2.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       0.0,
                                                       2.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       0.7777777777777778,
                                                       0.7777777777777778,
                                                       0.7777777777777778,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       3.0,
                                                       1.0,
                                                       2.0,
                                                       2.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       2.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       2.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       2.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       0.1111111111111111,
                                                       0.4444444444444444,
                                                       0.8888888888888888,
                                                       0.2222222222222222,
                                                       0.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       2.0,
                                                       2.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       2.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       0.0,
                                                       3.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       2.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.8888888888888888,
                                                       0.5555555555555556,
                                                       0.7777777777777778,
                                                       1.0,
                                                       0.2222222222222222,
                                                       0.3333333333333333,
                                                       0.1111111111111111,
                                                       0.3333333333333333,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       2.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       2.0,
                                                       1.0,
                                                       0.0,
                                                       2.0,
                                                       1.0,
                                                       3.0,
                                                       1.0,
                                                       0.8888888888888888,
                                                       1.0,
                                                       0.8888888888888888,
                                                       0.0,
                                                       2.6666666666666665,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       1.7777777777777777,
                                                       0.0,
                                                       3.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       3.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       3.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       4.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.8888888888888888,
                                                       0.4444444444444444,
                                                       0.6666666666666666,
                                                       0.1111111111111111,
                                                       0.3333333333333333,
                                                       1.0,
                                                       3.0,
                                                       2.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       3.0,
                                                       0.0,
                                                       3.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       4.0,
                                                       2.0,
                                                       1.0,
                                                       0.0,
                                                       3.0,
                                                       3.0,
                                                       1.0,
                                                       1.0,
                                                       2.0526315789473686,
                                                       1.736842105263158,
                                                       2.9473684210526314,
                                                       1.894736842105263,
                                                       4.0,
                                                       2.0,
                                                       1.0,
                                                       2.0,
                                                       2.0,
                                                       1.0,
                                                       2.0,
                                                       3.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       2.0,
                                                       2.0,
                                                       0.3684210526315789,
                                                       2.6315789473684212,
                                                       2.526315789473684,
                                                       1.894736842105263,
                                                       4.0,
                                                       2.0,
                                                       3.0,
                                                       1.0,
                                                       2.0,
                                                       3.0526315789473686,
                                                       1.263157894736842,
                                                       1.105263157894737,
                                                       0.21052631578947367,
                                                       1.1578947368421053,
                                                       1.0,
                                                       2.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       3.0,
                                                       2.0,
                                                       2.0,
                                                       4.0,
                                                       3.0,
                                                       3.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       2.0,
                                                       2.0,
                                                       2.0,
                                                       0.0,
                                                       2.0,
                                                       3.210526315789474,
                                                       0.10526315789473684,
                                                       3.0,
                                                       0.0,
                                                       5.0,
                                                       2.0,
                                                       1.0,
                                                       3.0,
                                                       2.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       0.2631578947368421,
                                                       0.0,
                                                       2.0,
                                                       2.0,
                                                       1.0,
                                                       2.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       4.0,
                                                       0.0,
                                                       2.0,
                                                       2.0,
                                                       3.0,
                                                       4.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       3.0,
                                                       2.210526315789474,
                                                       3.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       0.10526315789473684,
                                                       4.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       2.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       1.8421052631578947,
                                                       2.4210526315789473,
                                                       1.4736842105263157,
                                                       0.3157894736842105,
                                                       0.7894736842105263,
                                                       1.6842105263157894,
                                                       1.631578947368421,
                                                       1.9473684210526316,
                                                       1.6842105263157894,
                                                       1.8421052631578947,
                                                       1.894736842105263,
                                                       0.6842105263157895,
                                                       1.0526315789473684,
                                                       0.8947368421052632,
                                                       2.0,
                                                       2.4210526315789473,
                                                       1.4210526315789473,
                                                       0.9473684210526315,
                                                       1.1578947368421053,
                                                       0.0,
                                                       0.9473684210526315,
                                                       1.0,
                                                       4.0,
                                                       4.0,
                                                       2.0,
                                                       3.0,
                                                       1.0,
                                                       2.0,
                                                       2.0,
                                                       0.0,
                                                       2.0,
                                                       3.0,
                                                       2.0,
                                                       2.0,
                                                       2.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       3.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       2.0,
                                                       2.0,
                                                       0.0,
                                                       1.0,
                                                       3.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       3.0,
                                                       3.0,
                                                       1.0,
                                                       0.0,
                                                       3.0,
                                                       2.0,
                                                       2.0,
                                                       0.0,
                                                       2.0,
                                                       0.8947368421052632,
                                                       1.5263157894736843,
                                                       1.6842105263157894,
                                                       0.2631578947368421,
                                                       1.0,
                                                       3.0,
                                                       5.0,
                                                       2.0,
                                                       3.0,
                                                       1.0,
                                                       2.0,
                                                       1.0,
                                                       2.0,
                                                       1.0,
                                                       2.789473684210526,
                                                       1.0,
                                                       0.0,
                                                       2.0,
                                                       1.0,
                                                       0.0,
                                                       2.0,
                                                       2.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       2.0,
                                                       1.0,
                                                       2.0,
                                                       4.0,
                                                       2.0,
                                                       1.0,
                                                       1.0,
                                                       4.0,
                                                       2.0,
                                                       3.0,
                                                       2.0,
                                                       4.0,
                                                       3.0,
                                                       2.0,
                                                       1.0,
                                                       1.0,
                                                       2.0,
                                                       2.0,
                                                       2.0,
                                                       1.0,
                                                       2.0,
                                                       1.631578947368421,
                                                       0.21052631578947367,
                                                       1.736842105263158,
                                                       0.9473684210526315,
                                                       1.4736842105263157,
                                                       1.0,
                                                       2.0,
                                                       2.0,
                                                       3.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       2.0,
                                                       2.0,
                                                       1.0,
                                                       4.0,
                                                       2.0,
                                                       3.0,
                                                       3.0,
                                                       4.0,
                                                       2.0,
                                                       3.0,
                                                       0.0,
                                                       2.0,
                                                       3.0,
                                                       1.0,
                                                       2.0,
                                                       3.0,
                                                       2.0,
                                                       2.0,
                                                       1.894736842105263,
                                                       2.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       2.0,
                                                       1.0,
                                                       5.0,
                                                       3.0,
                                                       3.0,
                                                       2.0,
                                                       2.0,
                                                       1.0,
                                                       1.0,
                                                       2.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       2.0],
                              'val_loss': 0.0900145098567009,
                              'val_loss_history': [0.13289986550807953,
                                                   30.159862518310547,
                                                   0.1306895911693573,
                                                   772.6478881835938,
                                                   0.12578943371772766,
                                                   0.1313501000404358,
                                                   0.12481862306594849,
                                                   0.12818337976932526,
                                                   0.12867261469364166,
                                                   0.13204559683799744,
                                                   0.1220577135682106,
                                                   0.11260901391506195,
                                                   0.11409381777048111,
                                                   0.11440904438495636,
                                                   0.11434926837682724,
                                                   0.11414065212011337,
                                                   0.11439044773578644,
                                                   0.11462827026844025,
                                                   0.11452741175889969,
                                                   0.11467263847589493,
                                                   0.11460597068071365,
                                                   0.11462745815515518,
                                                   0.11468517035245895,
                                                   0.11480430513620377,
                                                   0.1147732213139534,
                                                   0.11485739797353745,
                                                   0.11479106545448303,
                                                   0.1150507852435112,
                                                   0.11502186954021454,
                                                   0.11493460834026337,
                                                   0.11511021852493286,
                                                   0.11504624783992767,
                                                   0.1150892823934555,
                                                   0.11518291383981705,
                                                   64640324.0,
                                                   0.11498195677995682,
                                                   0.11846525222063065,
                                                   0.11501139402389526,
                                                   0.11516815423965454,
                                                   0.11503072828054428,
                                                   0.11535662412643433,
                                                   0.11514862626791,
                                                   0.11519207060337067,
                                                   0.11522386968135834,
                                                   0.11516549438238144,
                                                   0.11545438319444656,
                                                   0.11529026180505753,
                                                   0.11528454720973969,
                                                   0.11551813781261444,
                                                   0.11532633006572723,
                                                   0.1173066645860672,
                                                   0.11544819921255112,
                                                   0.11565966159105301,
                                                   0.1153019443154335,
                                                   0.11566291004419327,
                                                   0.11555980145931244,
                                                   0.11566717177629471,
                                                   0.11576888710260391,
                                                   0.11573272943496704,
                                                   0.11575499922037125,
                                                   0.11589116603136063,
                                                   0.11577892303466797,
                                                   0.11567796766757965,
                                                   0.11675383150577545,
                                                   0.11572939157485962,
                                                   0.11599055677652359,
                                                   0.11586335301399231,
                                                   0.11595223098993301,
                                                   0.115937240421772,
                                                   0.11586461216211319,
                                                   0.11587101221084595,
                                                   0.1159282699227333,
                                                   0.11588104814291,
                                                   0.11595000326633453,
                                                   0.11601797491312027,
                                                   0.11578642576932907,
                                                   0.1159607544541359,
                                                   0.11591330915689468,
                                                   0.1158139705657959,
                                                   0.1159902960062027,
                                                   0.1159883365035057,
                                                   0.1158863827586174,
                                                   0.11616058647632599,
                                                   0.116038978099823,
                                                   0.11605293303728104,
                                                   0.11608398705720901,
                                                   0.11597417294979095,
                                                   0.11598964035511017,
                                                   0.11528076976537704,
                                                   0.1164831668138504,
                                                   0.11611490696668625,
                                                   0.11596962809562683,
                                                   0.11598684638738632,
                                                   0.11591029167175293,
                                                   0.11605054140090942,
                                                   0.11600024253129959,
                                                   0.1159677505493164,
                                                   0.11592534184455872,
                                                   0.11612578481435776,
                                                   0.11618667840957642,
                                                   0.11620379984378815,
                                                   0.11618492752313614,
                                                   0.11606466770172119,
                                                   0.11596335470676422,
                                                   0.11628828197717667,
                                                   0.1161775216460228,
                                                   0.11668728291988373,
                                                   0.11626330763101578,
                                                   0.11897259205579758,
                                                   0.11721517890691757,
                                                   0.11805533617734909,
                                                   0.11683106422424316,
                                                   0.11628374457359314,
                                                   0.11647655814886093,
                                                   0.11625979095697403,
                                                   0.11615417152643204,
                                                   0.11611239612102509,
                                                   0.11597920954227448,
                                                   0.11626138538122177,
                                                   0.11619382351636887,
                                                   0.11614691466093063,
                                                   0.1158432811498642,
                                                   0.11844415962696075,
                                                   0.12012633681297302,
                                                   0.12201090902090073,
                                                   0.12300722301006317,
                                                   0.12179263681173325,
                                                   0.12022022157907486,
                                                   0.1161445826292038,
                                                   0.11631754785776138,
                                                   0.11599871516227722,
                                                   0.11595051735639572,
                                                   0.11614725738763809,
                                                   0.11601608991622925,
                                                   0.11616994440555573,
                                                   0.11606497317552567,
                                                   0.11608567833900452,
                                                   0.11612219363451004,
                                                   0.11595229059457779,
                                                   0.1160753071308136,
                                                   0.11604484170675278,
                                                   0.11610965430736542,
                                                   0.11596120893955231,
                                                   0.11606322973966599,
                                                   0.1158863827586174,
                                                   0.1162295863032341,
                                                   0.11610893160104752,
                                                   0.11617756634950638,
                                                   0.11616548895835876,
                                                   0.11614851653575897,
                                                   0.11626169085502625,
                                                   0.11611983180046082,
                                                   0.116350457072258,
                                                   0.1160777285695076,
                                                   0.11605817079544067,
                                                   0.1161554679274559,
                                                   0.1161135733127594,
                                                   0.11618547886610031,
                                                   0.11609655618667603,
                                                   0.11594346165657043,
                                                   0.11609303951263428,
                                                   0.11628188192844391,
                                                   0.11593768745660782,
                                                   0.116062231361866,
                                                   0.11615301668643951,
                                                   0.11612395197153091,
                                                   0.11609595268964767,
                                                   0.11606339365243912,
                                                   0.11621040850877762,
                                                   0.11611215770244598,
                                                   0.11631068587303162,
                                                   0.11604316532611847,
                                                   0.11610415577888489,
                                                   0.11617108434438705,
                                                   0.11737527698278427,
                                                   0.11636664718389511,
                                                   0.11643633991479874,
                                                   0.11641830205917358,
                                                   0.11623018234968185,
                                                   0.11627576500177383,
                                                   0.11618880182504654,
                                                   0.11590965837240219,
                                                   0.11600705981254578,
                                                   0.11603537946939468,
                                                   0.11614096164703369,
                                                   0.11609300225973129,
                                                   0.11620457470417023,
                                                   0.11623869836330414,
                                                   0.11612760275602341,
                                                   0.11619135737419128,
                                                   0.11633993685245514,
                                                   0.11595872789621353,
                                                   0.1160760223865509,
                                                   0.11613026261329651,
                                                   0.11629821360111237,
                                                   0.11606990545988083,
                                                   0.11618290096521378,
                                                   0.11590874940156937,
                                                   0.116066575050354,
                                                   0.1162467747926712,
                                                   0.11605139076709747,
                                                   0.11606994271278381,
                                                   0.11615034192800522,
                                                   0.11626721173524857,
                                                   0.1164214015007019,
                                                   0.11610067635774612,
                                                   0.11607576906681061,
                                                   0.11610443145036697,
                                                   0.11622398346662521,
                                                   0.11609799414873123,
                                                   0.11592576652765274,
                                                   0.11589066684246063,
                                                   0.11615613847970963,
                                                   0.11620646715164185,
                                                   0.11605507135391235,
                                                   0.11605215072631836,
                                                   0.11597687005996704,
                                                   0.11605720967054367,
                                                   0.11622954159975052,
                                                   0.11576541513204575,
                                                   0.11619725078344345,
                                                   0.11608881503343582,
                                                   0.11614267528057098,
                                                   0.11628157645463943,
                                                   0.11605657637119293,
                                                   0.11614761501550674,
                                                   0.11601652950048447,
                                                   0.11608336865901947,
                                                   0.1161508783698082,
                                                   0.11626416444778442,
                                                   0.11630623042583466,
                                                   0.1164545938372612,
                                                   0.11624109745025635,
                                                   0.11607788503170013,
                                                   0.1161070168018341,
                                                   0.11620175093412399,
                                                   0.11616086959838867,
                                                   0.11611688882112503,
                                                   0.11600390076637268,
                                                   0.11609793454408646,
                                                   0.11598715931177139,
                                                   0.11622306704521179,
                                                   0.11612720787525177,
                                                   0.11618942022323608,
                                                   0.11620881408452988,
                                                   0.11601428687572479,
                                                   0.11616939306259155,
                                                   0.11625801771879196,
                                                   0.11612837016582489,
                                                   0.11634919047355652,
                                                   0.11619079858064651,
                                                   0.11613990366458893,
                                                   0.11610959470272064,
                                                   0.1163247600197792,
                                                   0.11617305129766464,
                                                   0.11615300923585892,
                                                   0.11632908880710602,
                                                   0.11621703207492828,
                                                   0.11634662002325058,
                                                   0.11614250391721725,
                                                   0.11630403250455856,
                                                   0.11608672142028809,
                                                   0.11608536541461945,
                                                   31432.908203125,
                                                   0.1162625327706337,
                                                   0.11617450416088104,
                                                   0.11622246354818344,
                                                   0.11625657230615616,
                                                   0.11630494892597198,
                                                   0.1160629466176033,
                                                   0.11630386859178543,
                                                   0.11654241383075714,
                                                   0.11633019894361496,
                                                   0.11625935137271881,
                                                   0.1159849464893341,
                                                   0.11611940711736679,
                                                   0.11628160625696182,
                                                   0.11616548895835876,
                                                   0.1158863976597786,
                                                   0.11616910994052887,
                                                   0.11640705168247223,
                                                   57.9826545715332,
                                                   0.11621317267417908,
                                                   0.1162533313035965,
                                                   0.11624367535114288,
                                                   0.11624933034181595,
                                                   0.11617560684680939,
                                                   0.11601812392473221,
                                                   0.11616231501102448,
                                                   0.1162579134106636,
                                                   0.1163044273853302,
                                                   0.11610657721757889,
                                                   0.11615923047065735,
                                                   0.11604088544845581,
                                                   0.11613348871469498,
                                                   0.1160733699798584,
                                                   0.11620483547449112,
                                                   0.11624867469072342,
                                                   0.11615971475839615,
                                                   0.11607415229082108,
                                                   75018216.0,
                                                   0.09041483700275421,
                                                   0.09068240970373154,
                                                   0.09158043563365936,
                                                   0.09161237627267838,
                                                   0.09168776124715805,
                                                   0.09167324006557465,
                                                   0.09174589067697525,
                                                   0.09159623086452484,
                                                   0.091584213078022,
                                                   0.09181859344244003,
                                                   0.09148017317056656,
                                                   0.09162292629480362,
                                                   0.09164972603321075,
                                                   0.09172000735998154,
                                                   0.09156201779842377,
                                                   0.09159772098064423,
                                                   0.09161564707756042,
                                                   0.09180383384227753,
                                                   0.09165830910205841,
                                                   0.09175550937652588,
                                                   0.09164596349000931,
                                                   0.09165975451469421,
                                                   0.09162546694278717,
                                                   0.09173918515443802,
                                                   0.09151586145162582,
                                                   0.09154339134693146,
                                                   0.09142778068780899,
                                                   0.09155946969985962,
                                                   0.0915718749165535,
                                                   0.09148793667554855,
                                                   0.09153666347265244,
                                                   0.0915401503443718,
                                                   0.0918232873082161,
                                                   0.09156225621700287,
                                                   0.09150882065296173,
                                                   0.09154771268367767,
                                                   0.09136240184307098,
                                                   0.09136436134576797,
                                                   0.09138547629117966,
                                                   0.09111592918634415,
                                                   0.09049556404352188,
                                                   0.09093768894672394,
                                                   0.09028400480747223,
                                                   0.0910341814160347,
                                                   0.09143580496311188,
                                                   0.09173454344272614,
                                                   0.09146896004676819,
                                                   0.09174791723489761,
                                                   0.09165650606155396,
                                                   0.09164563566446304,
                                                   0.09150607138872147,
                                                   0.09162599593400955,
                                                   0.09160754829645157,
                                                   0.0916544646024704,
                                                   0.09154925495386124,
                                                   0.09158218652009964,
                                                   0.09168332815170288,
                                                   0.09178471565246582,
                                                   0.09159593284130096,
                                                   0.09167377650737762,
                                                   0.09158945083618164,
                                                   0.09157179296016693,
                                                   0.09173416346311569,
                                                   0.09171082079410553,
                                                   0.0915522649884224,
                                                   0.09138037264347076,
                                                   6550871.0,
                                                   0.0917968600988388,
                                                   0.09176656603813171,
                                                   0.09189419448375702,
                                                   0.09189261496067047,
                                                   0.09148921072483063,
                                                   0.0916379913687706,
                                                   0.09164125472307205,
                                                   0.09168343245983124,
                                                   0.0915914997458458,
                                                   0.0916556641459465,
                                                   0.09176559001207352,
                                                   0.09119798988103867,
                                                   0.09149183332920074,
                                                   0.09167253226041794,
                                                   0.09157314896583557,
                                                   0.09136173874139786,
                                                   0.09172654151916504,
                                                   0.09172135591506958,
                                                   0.0916282907128334,
                                                   0.09151092171669006,
                                                   0.09152401983737946,
                                                   0.09146179258823395,
                                                   0.09152831137180328,
                                                   0.09171596169471741,
                                                   0.09146741032600403,
                                                   0.09163735806941986,
                                                   0.09172643721103668,
                                                   0.09151304513216019,
                                                   0.09167574346065521,
                                                   0.09155893325805664,
                                                   0.09151120483875275,
                                                   0.09154047071933746,
                                                   0.09145689010620117,
                                                   0.09163273870944977,
                                                   0.09142628312110901,
                                                   0.09159869700670242,
                                                   0.09154431521892548,
                                                   0.09110464155673981,
                                                   0.09078102558851242,
                                                   0.0907980427145958,
                                                   0.09076491743326187,
                                                   0.09136105328798294,
                                                   0.09158964455127716,
                                                   0.09161243587732315,
                                                   0.09146032482385635,
                                                   0.09154172241687775,
                                                   0.0915302038192749,
                                                   0.09143901616334915,
                                                   0.09158524125814438,
                                                   0.09163603186607361,
                                                   0.09191957861185074,
                                                   0.09158330410718918,
                                                   0.0916702076792717,
                                                   0.09173353016376495,
                                                   0.09168414771556854,
                                                   0.09165389090776443,
                                                   0.09153973311185837,
                                                   0.09151528030633926,
                                                   0.09149136394262314,
                                                   0.09165231138467789,
                                                   0.09150249511003494,
                                                   0.09171602129936218,
                                                   0.09170891344547272,
                                                   0.09149736166000366,
                                                   0.09154205024242401,
                                                   0.09144985675811768,
                                                   0.09173857420682907,
                                                   0.09168737381696701,
                                                   0.09159638732671738,
                                                   0.09164682775735855,
                                                   0.09182272851467133,
                                                   0.09188602864742279,
                                                   0.0916704535484314,
                                                   0.09181439131498337,
                                                   0.0916689857840538,
                                                   0.0916949138045311,
                                                   0.0918634682893753,
                                                   0.091732919216156,
                                                   0.09177081286907196,
                                                   0.09180489182472229,
                                                   0.09170784801244736,
                                                   0.09166619181632996,
                                                   0.09147465229034424,
                                                   0.09065674990415573,
                                                   0.09165985137224197,
                                                   0.0915789008140564,
                                                   0.09145854413509369,
                                                   0.0916421115398407,
                                                   0.09175586700439453,
                                                   0.0916069895029068,
                                                   0.091783806681633,
                                                   0.09182631224393845,
                                                   0.0917629525065422,
                                                   0.09174057096242905,
                                                   0.09179218113422394,
                                                   0.09166530519723892,
                                                   0.09162689000368118,
                                                   0.09167832881212234,
                                                   0.09155615419149399,
                                                   0.09177278727293015,
                                                   0.09168699383735657,
                                                   0.09166309982538223,
                                                   0.09175059199333191,
                                                   0.09156449139118195,
                                                   0.09154761582612991,
                                                   0.09163499623537064,
                                                   0.09183468669652939,
                                                   0.09155910462141037,
                                                   0.09153896570205688,
                                                   0.09173861145973206,
                                                   0.09150022268295288,
                                                   0.09156262874603271,
                                                   0.09158863872289658,
                                                   0.091577909886837,
                                                   0.0914931669831276,
                                                   0.09152447432279587,
                                                   0.09160035848617554,
                                                   0.0915502980351448,
                                                   0.09158534556627274,
                                                   0.09145323932170868,
                                                   0.09149032831192017,
                                                   0.09140226989984512,
                                                   0.09157698601484299,
                                                   0.09184463322162628,
                                                   0.09163165837526321,
                                                   0.09175490587949753,
                                                   0.09173326194286346,
                                                   0.09146177023649216,
                                                   0.09170445799827576,
                                                   0.09182441979646683,
                                                   0.09142305701971054,
                                                   0.09153810143470764,
                                                   0.09144371002912521,
                                                   0.09164082258939743,
                                                   0.09147138893604279,
                                                   0.09169746935367584,
                                                   0.09146449714899063,
                                                   0.0915309488773346,
                                                   0.09153302758932114,
                                                   0.09078874439001083,
                                                   0.09040026366710663,
                                                   0.0903313085436821,
                                                   0.09044385701417923,
                                                   0.09064793586730957,
                                                   0.09086998552083969,
                                                   0.0905841663479805,
                                                   0.09128729999065399,
                                                   0.09156367182731628,
                                                   0.09157119691371918,
                                                   0.09156181663274765,
                                                   0.091734878718853,
                                                   0.09174881130456924,
                                                   0.09162548929452896,
                                                   0.09173157066106796,
                                                   0.09165001660585403,
                                                   0.09184536337852478,
                                                   0.09174542874097824,
                                                   0.09148374944925308,
                                                   0.0918145477771759,
                                                   0.09172198921442032,
                                                   0.09163372218608856,
                                                   0.09175238013267517,
                                                   0.09166068583726883,
                                                   0.09152699261903763,
                                                   0.09570552408695221,
                                                   0.09075891226530075,
                                                   0.09028211236000061,
                                                   0.09057898819446564,
                                                   0.09034465253353119,
                                                   0.091373510658741,
                                                   0.09163247048854828,
                                                   0.0916837826371193,
                                                   623378176.0,
                                                   0.09151039272546768,
                                                   0.09154417365789413,
                                                   0.09182267636060715,
                                                   0.0917121171951294,
                                                   0.09159564971923828,
                                                   0.09143423289060593,
                                                   0.09164773672819138,
                                                   0.09148409217596054,
                                                   0.0917004719376564,
                                                   0.0917796865105629,
                                                   0.0917053371667862,
                                                   0.09163802117109299,
                                                   597140.375,
                                                   0.0918307676911354,
                                                   0.09166297316551208,
                                                   0.09173108637332916,
                                                   0.09177133440971375,
                                                   0.09167598187923431,
                                                   0.09167742729187012,
                                                   0.09176761656999588,
                                                   0.09161846339702606,
                                                   0.09151538461446762,
                                                   0.09162520617246628,
                                                   0.09155528992414474,
                                                   0.09133844822645187,
                                                   0.09172411262989044,
                                                   0.09168177098035812,
                                                   0.09152256697416306,
                                                   0.09153567254543304,
                                                   0.09174582362174988,
                                                   0.09146131575107574,
                                                   0.09169001132249832,
                                                   0.09183165431022644,
                                                   0.09155264496803284,
                                                   0.09179975837469101,
                                                   0.09141802042722702,
                                                   0.09178242087364197,
                                                   0.09155882894992828,
                                                   0.09155619144439697,
                                                   0.09172812104225159,
                                                   0.09160418808460236,
                                                   0.09162043035030365,
                                                   0.0916271060705185,
                                                   0.0916542038321495,
                                                   0.09182074666023254,
                                                   0.091661237180233,
                                                   0.09168948233127594,
                                                   0.09161534905433655,
                                                   0.09176906198263168,
                                                   0.09174256771802902,
                                                   0.09184432774782181,
                                                   0.09175481647253036,
                                                   0.09163566678762436,
                                                   0.09110815823078156,
                                                   0.09047967940568924,
                                                   0.09134313464164734,
                                                   0.09058742970228195,
                                                   0.09057310223579407,
                                                   0.09021198004484177,
                                                   0.08950533717870712,
                                                   0.08991483598947525,
                                                   0.09000581502914429,
                                                   0.08975820243358612,
                                                   0.08989927172660828,
                                                   0.08977536857128143,
                                                   0.09046599268913269,
                                                   0.09019088745117188,
                                                   0.08985491842031479,
                                                   0.09006495773792267,
                                                   0.09006132930517197,
                                                   0.08998249471187592,
                                                   0.08991481363773346,
                                                   0.09035725891590118,
                                                   0.09026259183883667,
                                                   0.09033244848251343,
                                                   0.09021812677383423,
                                                   0.08977342396974564,
                                                   0.08995147794485092,
                                                   0.09018651396036148,
                                                   0.0902789831161499,
                                                   0.09030649065971375,
                                                   0.09018151462078094,
                                                   0.08963830769062042,
                                                   0.09024237841367722,
                                                   0.08991577476263046,
                                                   0.09044059365987778,
                                                   0.08928724378347397,
                                                   0.08972622454166412,
                                                   0.08882772922515869,
                                                   0.08975745737552643,
                                                   0.09016703814268112,
                                                   0.08963004499673843,
                                                   0.0898420587182045,
                                                   0.08989901095628738,
                                                   0.09010357409715652,
                                                   0.0901118665933609,
                                                   0.09002404659986496,
                                                   0.09008162468671799,
                                                   0.08998696506023407,
                                                   0.09019099175930023,
                                                   0.09039027988910675,
                                                   0.09014302492141724,
                                                   0.09035284072160721,
                                                   0.090223528444767,
                                                   0.08909245580434799,
                                                   0.08939402550458908,
                                                   0.08942470699548721,
                                                   0.08992207795381546,
                                                   0.08974660187959671,
                                                   0.09004580974578857,
                                                   0.09003043174743652,
                                                   0.08954887092113495,
                                                   0.08958173543214798,
                                                   0.0888582319021225,
                                                   0.08949612826108932,
                                                   0.0890330970287323,
                                                   0.08879958093166351,
                                                   0.08949175477027893,
                                                   0.09019462019205093,
                                                   0.08992094546556473,
                                                   0.08964835852384567,
                                                   0.08991514146327972,
                                                   0.08982381969690323,
                                                   0.08980073034763336,
                                                   0.0896264985203743,
                                                   0.08992141485214233,
                                                   0.09008410573005676,
                                                   0.08989385515451431,
                                                   0.09021643549203873,
                                                   0.09013224393129349,
                                                   0.0900585800409317,
                                                   0.08978967368602753,
                                                   0.09012910723686218,
                                                   0.08994796127080917,
                                                   0.09000711888074875,
                                                   0.08993218839168549,
                                                   0.09006299823522568,
                                                   0.08917942643165588,
                                                   0.08967818319797516,
                                                   0.08993396908044815,
                                                   0.09023486822843552,
                                                   0.0900617465376854,
                                                   0.09012560546398163,
                                                   0.08974074572324753,
                                                   0.09009005129337311,
                                                   0.08979517966508865,
                                                   0.08997286111116409,
                                                   0.09035619348287582,
                                                   0.09010802954435349,
                                                   0.09086864441633224,
                                                   0.0897199809551239,
                                                   0.08955328911542892,
                                                   0.08973503112792969,
                                                   0.0901649221777916,
                                                   0.08986207097768784,
                                                   0.09005796909332275,
                                                   0.08977393060922623,
                                                   0.09000321477651596,
                                                   0.08998708426952362,
                                                   0.08971834927797318,
                                                   0.0897836983203888,
                                                   0.09028778970241547,
                                                   0.08969186991453171,
                                                   0.09012392163276672,
                                                   0.08985482901334763,
                                                   0.09004218131303787,
                                                   0.09037664532661438,
                                                   0.09028197079896927,
                                                   0.09011863172054291,
                                                   0.0896797850728035,
                                                   0.0896434634923935,
                                                   0.0899115651845932,
                                                   0.08928757160902023,
                                                   0.08990449458360672,
                                                   0.08990985155105591,
                                                   0.09001431614160538,
                                                   0.0895829126238823,
                                                   0.09010770171880722,
                                                   0.08968362212181091,
                                                   0.09000500291585922,
                                                   0.0898640975356102,
                                                   0.08954513818025589,
                                                   123260808.0,
                                                   0.09009537845849991,
                                                   0.08999543637037277,
                                                   0.09009146690368652,
                                                   0.08993417024612427,
                                                   0.08979174494743347,
                                                   0.09024576097726822,
                                                   0.08982154726982117,
                                                   0.09024430066347122,
                                                   0.08962235599756241,
                                                   0.08915040642023087,
                                                   0.08879758417606354,
                                                   0.08888117223978043,
                                                   0.08869042247533798,
                                                   0.08853305876255035,
                                                   0.08868934214115143,
                                                   0.08893421292304993,
                                                   0.0886133685708046,
                                                   0.08817695081233978,
                                                   0.0881962701678276,
                                                   0.08811213821172714,
                                                   0.0891452506184578,
                                                   0.08935011923313141,
                                                   0.08934961259365082,
                                                   0.08890455216169357,
                                                   0.08886080235242844,
                                                   0.0891081690788269,
                                                   0.08976045250892639,
                                                   0.08954772353172302,
                                                   0.08939021825790405,
                                                   0.08975906670093536,
                                                   0.08977613598108292,
                                                   0.0896851047873497,
                                                   0.08977438509464264,
                                                   0.09006603807210922,
                                                   0.08980820327997208,
                                                   0.08988718688488007,
                                                   0.08971526473760605,
                                                   0.09004899114370346,
                                                   0.0901557132601738,
                                                   0.08969920128583908,
                                                   0.09032884240150452,
                                                   0.08998294174671173,
                                                   0.08977649360895157,
                                                   0.09020925313234329,
                                                   0.09029025584459305,
                                                   0.08976597338914871,
                                                   0.08999000489711761,
                                                   0.08985752612352371,
                                                   0.09009544551372528,
                                                   0.08979445695877075,
                                                   0.08981890231370926,
                                                   0.08993826806545258,
                                                   0.09029476344585419,
                                                   0.0901898518204689,
                                                   0.0904805064201355,
                                                   0.0902039036154747,
                                                   0.09018836915493011,
                                                   0.09021788835525513,
                                                   13.298559188842773,
                                                   0.09035386145114899,
                                                   0.08994783461093903,
                                                   0.08987398445606232,
                                                   0.0898030549287796,
                                                   0.09018076211214066,
                                                   0.08989060670137405,
                                                   0.0900542363524437,
                                                   0.08998247236013412,
                                                   0.08996569365262985,
                                                   0.08956349641084671,
                                                   0.0899026170372963,
                                                   0.08934273570775986,
                                                   0.08953327685594559,
                                                   0.08896757662296295,
                                                   40.889766693115234,
                                                   0.0892421156167984,
                                                   0.09008168429136276,
                                                   0.0901041328907013,
                                                   0.08966265618801117,
                                                   0.08987640589475632,
                                                   0.09025770425796509,
                                                   0.0899430364370346,
                                                   0.09005750715732574,
                                                   0.09002253413200378,
                                                   0.08991013467311859,
                                                   0.08955400437116623,
                                                   0.09024599194526672,
                                                   0.08989398926496506,
                                                   0.09036830812692642,
                                                   0.09003657847642899,
                                                   0.09000864624977112,
                                                   0.09005343914031982,
                                                   0.09015601873397827,
                                                   0.09009037166833878,
                                                   0.09033283591270447,
                                                   0.08988620340824127,
                                                   0.09028229117393494,
                                                   0.09019381552934647,
                                                   0.0902322456240654,
                                                   0.09032434225082397,
                                                   0.09011644124984741,
                                                   0.08976441621780396,
                                                   0.08991662412881851,
                                                   0.08978656679391861,
                                                   0.08997219800949097,
                                                   0.09036137908697128,
                                                   0.08971432596445084,
                                                   0.08993349224328995,
                                                   0.08978254348039627,
                                                   0.09011665731668472,
                                                   0.0903627872467041,
                                                   0.09019304066896439,
                                                   0.08973104506731033,
                                                   0.09010143578052521,
                                                   0.09001858532428741,
                                                   0.08998378366231918,
                                                   0.09022297710180283,
                                                   0.09093927592039108,
                                                   0.08909867703914642,
                                                   0.08914345502853394,
                                                   0.08835233747959137,
                                                   0.0882631167769432,
                                                   0.08950014412403107,
                                                   0.08979093283414841,
                                                   0.09012936055660248,
                                                   0.08974586427211761,
                                                   0.09011997282505035,
                                                   0.08974523842334747,
                                                   0.09000830352306366,
                                                   0.09017462283372879,
                                                   0.09004103392362595,
                                                   0.08963128179311752,
                                                   0.08985885232686996,
                                                   0.08983511477708817,
                                                   0.09008574485778809,
                                                   0.09016025811433792,
                                                   0.08972913771867752,
                                                   0.08963147550821304,
                                                   0.08982467651367188,
                                                   0.08994393795728683,
                                                   0.08989205211400986,
                                                   0.08967658132314682,
                                                   0.08987260609865189,
                                                   0.09007982909679413,
                                                   0.09010133147239685,
                                                   0.09020086377859116,
                                                   0.09046005457639694,
                                                   0.08994335681200027,
                                                   0.09018530696630478,
                                                   0.08992722630500793,
                                                   0.09044454246759415,
                                                   0.09027785807847977,
                                                   0.08971437811851501,
                                                   0.08930660039186478,
                                                   0.09013891965150833,
                                                   0.08999187499284744,
                                                   0.08977315574884415,
                                                   0.08965734392404556,
                                                   0.08979973196983337,
                                                   0.09008533507585526,
                                                   0.08993113040924072,
                                                   0.08979012817144394,
                                                   0.08988290280103683,
                                                   0.09004655480384827,
                                                   0.09017106890678406,
                                                   0.089727021753788,
                                                   0.08970198780298233,
                                                   0.09002815186977386,
                                                   0.08984025567770004,
                                                   0.09001448005437851,
                                                   0.08957063406705856,
                                                   0.08992192894220352,
                                                   0.09023972600698471,
                                                   0.09008713066577911,
                                                   0.0900142714381218,
                                                   0.08985602855682373,
                                                   0.0900145098567009],
                              'weight_decay': 0.0005},
                 'loss': {'accuracy': 2.0,
                          'batch_size': 32,
                          'cv_score': 0.017618830126676397,
                          'cv_val_accuracy': 0.7777777777777778,
                          'cv_val_loss': 0.09876688073078792,
                          'cv_val_macroF1': 0.017618830126676397,
                          'cv_val_microF1': 0.07673131407953461,
                          'epochs': 300,
                          'final_model': GGNN1(
  (ggnn): GatedGraphConv(30, num_layers=2)
  (fc1): Linear(in_features=30, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                          'kwargs': {'aggr_type': 'mean',
                                     'd1': 30,
                                     'd2': 50,
                                     'num_classes': 24,
                                     'num_layers': 2},
                          'learning_rate': 0.01,
                          'macroF1': 0.006748704489763125,
                          'microF1': 0.08414023372287145,
                          'model': <class 'TFM_graph_classification_models.GGNN1'>,
                          'model_instance': GGNN1(
  (ggnn): GatedGraphConv(30, num_layers=2)
  (fc1): Linear(in_features=30, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                          'score': 'f1_macro',
                          'time': 3923.382642507553,
                          'train_loss_history': [402.3915100097656,
                                                 0.627553403377533,
                                                 0.1610880345106125,
                                                 200590368.0,
                                                 127.40691375732422,
                                                 0.0838785469532013,
                                                 0.08383877575397491,
                                                 0.08405112475156784,
                                                 0.08400005102157593,
                                                 0.11704819649457932,
                                                 315.0480651855469,
                                                 8.739550590515137,
                                                 0.26758015155792236,
                                                 150.98406982421875,
                                                 1.1856788396835327,
                                                 1248861440.0,
                                                 2.6898114681243896,
                                                 0.08465886861085892,
                                                 0.08455833047628403,
                                                 0.08453904837369919,
                                                 0.08463430404663086,
                                                 0.08893854171037674,
                                                 2255.366943359375,
                                                 5255256.5,
                                                 0.0845518633723259,
                                                 0.08456744998693466,
                                                 0.0846351757645607,
                                                 0.08459139615297318,
                                                 0.0845951959490776,
                                                 0.08459458500146866,
                                                 741.9649047851562,
                                                 375.397216796875,
                                                 0.08453738689422607,
                                                 1314.5992431640625,
                                                 0.08459823578596115,
                                                 2760.76708984375,
                                                 0.08449413627386093,
                                                 0.0844498947262764,
                                                 0.08453848212957382,
                                                 0.08451221138238907,
                                                 0.08967375755310059,
                                                 0.21154144406318665,
                                                 6485.376953125,
                                                 123632.40625,
                                                 29165.78125,
                                                 38.000274658203125,
                                                 0.08432421088218689,
                                                 0.08440204709768295,
                                                 0.08437246084213257,
                                                 31.02018928527832,
                                                 0.08415640890598297,
                                                 4.811329364776611,
                                                 0.08450804650783539,
                                                 0.08431932330131531,
                                                 0.0844046100974083,
                                                 1.6224708557128906,
                                                 0.08452630043029785,
                                                 0.08453928679227829,
                                                 0.0845048725605011,
                                                 0.0844370573759079,
                                                 0.08444765210151672,
                                                 0.08450065553188324,
                                                 0.08445669710636139,
                                                 0.08438482880592346,
                                                 0.08415114134550095,
                                                 0.08441533893346786,
                                                 0.08437883108854294,
                                                 0.08448990434408188,
                                                 0.08448030054569244,
                                                 0.0844801515340805,
                                                 0.08438432216644287,
                                                 0.08433922380208969,
                                                 0.08432400226593018,
                                                 0.08451466262340546,
                                                 0.08452446758747101,
                                                 0.08453328907489777,
                                                 0.08448510617017746,
                                                 0.08447270840406418,
                                                 0.08449146896600723,
                                                 0.08447691053152084,
                                                 0.08443857729434967,
                                                 0.08449263125658035,
                                                 0.08442482352256775,
                                                 0.08449450135231018,
                                                 0.0844353586435318,
                                                 0.08452147245407104,
                                                 0.0844416618347168,
                                                 0.08446303755044937,
                                                 0.08447965979576111,
                                                 0.0843501091003418,
                                                 0.08422920107841492,
                                                 0.08444242179393768,
                                                 0.08447030931711197,
                                                 0.08448900282382965,
                                                 0.08450789749622345,
                                                 0.08435028791427612,
                                                 0.08434262871742249,
                                                 0.08449234813451767,
                                                 0.08449122309684753,
                                                 0.08449383825063705,
                                                 0.08452076464891434,
                                                 0.08449948579072952,
                                                 0.08440456539392471,
                                                 0.08449050039052963,
                                                 0.0841154232621193,
                                                 2.2428197860717773,
                                                 0.08384676277637482,
                                                 0.08394981920719147,
                                                 0.0838002935051918,
                                                 0.08447454124689102,
                                                 0.08348763734102249,
                                                 0.08416859805583954,
                                                 0.08417699486017227,
                                                 0.08438660949468613,
                                                 0.4162343144416809,
                                                 0.08450004458427429,
                                                 0.08446227014064789,
                                                 0.08444914221763611,
                                                 0.0843791589140892,
                                                 0.08446182310581207,
                                                 0.08455074578523636,
                                                 0.08446145057678223,
                                                 0.0836617723107338,
                                                 0.08338803797960281,
                                                 0.08333099633455276,
                                                 0.08318332582712173,
                                                 0.08332567662000656,
                                                 0.08302734792232513,
                                                 0.08371946960687637,
                                                 0.08449801802635193,
                                                 0.08449846506118774,
                                                 0.08426801860332489,
                                                 0.08432383090257645,
                                                 200335072.0,
                                                 0.08437470346689224,
                                                 0.08452057838439941,
                                                 0.08447948843240738,
                                                 0.08449156582355499,
                                                 0.08443012088537216,
                                                 0.0844721645116806,
                                                 0.08447398245334625,
                                                 0.08443788439035416,
                                                 0.08443618565797806,
                                                 0.08449234068393707,
                                                 0.10008276253938675,
                                                 0.08434968441724777,
                                                 0.08430284261703491,
                                                 0.08442266285419464,
                                                 0.08438229560852051,
                                                 1806.7781982421875,
                                                 0.08451782912015915,
                                                 0.0844816267490387,
                                                 13.898186683654785,
                                                 0.08445671200752258,
                                                 1.4685406684875488,
                                                 4.019253253936768,
                                                 302.15570068359375,
                                                 0.08452155441045761,
                                                 0.14563511312007904,
                                                 27.54737663269043,
                                                 3.7263739109039307,
                                                 78.2233657836914,
                                                 0.08451861888170242,
                                                 0.08448174595832825,
                                                 407.52105712890625,
                                                 19.42632484436035,
                                                 0.08444830775260925,
                                                 36449228.0,
                                                 7.751605033874512,
                                                 0.08446042984724045,
                                                 0.08437091112136841,
                                                 3167184.25,
                                                 0.08450077474117279,
                                                 0.08428910374641418,
                                                 0.08419448137283325,
                                                 0.08466444909572601,
                                                 5.597066402435303,
                                                 0.11012237519025803,
                                                 0.08447238802909851,
                                                 58.212467193603516,
                                                 5.655828952789307,
                                                 3588.997314453125,
                                                 0.409007728099823,
                                                 32.17847442626953,
                                                 0.08447328209877014,
                                                 0.0845048725605011,
                                                 0.08449171483516693,
                                                 0.08447052538394928,
                                                 0.08445976674556732,
                                                 0.08446221798658371,
                                                 0.08458263427019119,
                                                 0.08430005609989166,
                                                 8.114312171936035,
                                                 0.08432270586490631,
                                                 0.08445460349321365,
                                                 0.08443901687860489,
                                                 900.8036499023438,
                                                 0.08444087207317352,
                                                 0.08441809564828873,
                                                 0.4795038402080536,
                                                 0.08451033383607864,
                                                 0.08451204001903534,
                                                 0.08450926095247269,
                                                 0.08447223901748657,
                                                 0.08437959104776382,
                                                 0.08444824069738388,
                                                 0.08453644067049026,
                                                 7.690800666809082,
                                                 0.08448299020528793,
                                                 0.08449721336364746,
                                                 0.08445368707180023,
                                                 0.08438749611377716,
                                                 0.08451572060585022,
                                                 0.08452669531106949,
                                                 89.34236907958984,
                                                 0.08450479805469513,
                                                 30.75047492980957,
                                                 0.08451566100120544,
                                                 0.08449652045965195,
                                                 68.70880126953125,
                                                 0.08445856720209122,
                                                 12.459997177124023,
                                                 0.08449093997478485,
                                                 0.08446609228849411,
                                                 0.08448096364736557,
                                                 0.11521673947572708,
                                                 0.08448649197816849,
                                                 1010.2301025390625,
                                                 0.08451484888792038,
                                                 0.0844508707523346,
                                                 0.0843883827328682,
                                                 0.08444556593894958,
                                                 182.40513610839844,
                                                 0.08444695174694061,
                                                 304.5606689453125,
                                                 0.0844317376613617,
                                                 0.08447675406932831,
                                                 0.08446475118398666,
                                                 0.08447200804948807,
                                                 0.08442620933055878,
                                                 0.08441336452960968,
                                                 0.08443718403577805,
                                                 0.08444596081972122,
                                                 0.0845516175031662,
                                                 0.08445774763822556,
                                                 37.44003677368164,
                                                 0.0844288170337677,
                                                 0.08449657261371613,
                                                 0.08451525121927261,
                                                 0.08444896340370178,
                                                 0.08451040834188461,
                                                 0.08449771255254745,
                                                 0.0844845324754715,
                                                 0.08452331274747849,
                                                 0.08448804169893265,
                                                 0.08449441939592361,
                                                 0.08447355777025223,
                                                 217506.6875,
                                                 0.08448262512683868,
                                                 27566.361328125,
                                                 0.084529809653759,
                                                 60.93157958984375,
                                                 0.0844476968050003,
                                                 0.08442551642656326,
                                                 81.24128723144531,
                                                 0.08442741632461548,
                                                 0.08438073843717575,
                                                 0.08440481126308441,
                                                 0.08442452549934387,
                                                 0.08434814214706421,
                                                 11.347413063049316,
                                                 26.29210662841797,
                                                 0.23023147881031036,
                                                 0.08447755873203278,
                                                 0.08438970148563385,
                                                 0.08435468375682831,
                                                 0.08439872413873672,
                                                 0.08438412100076675,
                                                 0.08440207690000534,
                                                 0.0844302773475647,
                                                 11.261951446533203,
                                                 0.08435912430286407,
                                                 3.4124855995178223,
                                                 0.08436176180839539,
                                                 0.08430063724517822,
                                                 0.08941613137722015,
                                                 0.08446674793958664,
                                                 0.08433955907821655,
                                                 0.08443570137023926,
                                                 0.08435171097517014,
                                                 21675608.0,
                                                 0.08440378308296204,
                                                 0.08433499932289124,
                                                 0.40477755665779114,
                                                 0.08435443788766861,
                                                 0.08426611125469208,
                                                 13186.3408203125,
                                                 39600.96875,
                                                 0.08453357219696045,
                                                 0.9381150603294373,
                                                 0.09587060660123825,
                                                 0.514412522315979,
                                                 0.09367242455482483,
                                                 0.09441542625427246,
                                                 0.09452634304761887,
                                                 0.09454908967018127,
                                                 22.32537269592285,
                                                 32339.833984375,
                                                 0.09451416879892349,
                                                 0.09449652582406998,
                                                 0.09464240074157715,
                                                 0.09449930489063263,
                                                 1958.1798095703125,
                                                 0.09455957263708115,
                                                 0.09456851333379745,
                                                 1.2811931371688843,
                                                 0.09453321993350983,
                                                 0.09458591043949127,
                                                 0.09450304508209229,
                                                 5.49962043762207,
                                                 72.5165023803711,
                                                 0.09458471089601517,
                                                 0.0945429876446724,
                                                 0.09455682337284088,
                                                 0.09453796595335007,
                                                 0.09452159702777863,
                                                 2.5312438011169434,
                                                 0.0945226177573204,
                                                 0.0944872573018074,
                                                 5.223100185394287,
                                                 2137.731201171875,
                                                 0.142483189702034,
                                                 0.20935583114624023,
                                                 0.09459515661001205,
                                                 0.09455345571041107,
                                                 2812.721435546875,
                                                 0.09442364424467087,
                                                 0.1131102666258812,
                                                 0.09442222863435745,
                                                 0.09438570588827133,
                                                 0.09433407336473465,
                                                 0.13149750232696533,
                                                 0.09370430558919907,
                                                 0.09366568177938461,
                                                 0.09354622662067413,
                                                 0.09447423368692398,
                                                 0.6216050982475281,
                                                 0.09451036900281906,
                                                 0.09451115876436234,
                                                 1349.734619140625,
                                                 0.09453874081373215,
                                                 0.11682190001010895,
                                                 0.10219868272542953,
                                                 0.09455303847789764,
                                                 0.0945776104927063,
                                                 0.09455080330371857,
                                                 2.9681241512298584,
                                                 0.09452127665281296,
                                                 0.09452573955059052,
                                                 29.074121475219727,
                                                 14.531797409057617,
                                                 0.09461171925067902,
                                                 0.09457170218229294,
                                                 0.09453096240758896,
                                                 0.7046006321907043,
                                                 0.33546608686447144,
                                                 0.09435248374938965,
                                                 0.09443413466215134,
                                                 0.31541207432746887,
                                                 0.14433683454990387,
                                                 0.09446719288825989,
                                                 3715.66796875,
                                                 0.0944618359208107,
                                                 0.09454601258039474,
                                                 0.0945052057504654,
                                                 0.09456264227628708,
                                                 0.09456627815961838,
                                                 1.2233333587646484,
                                                 0.09454774111509323,
                                                 249.55780029296875,
                                                 181.83404541015625,
                                                 154.2233123779297,
                                                 0.09459660947322845,
                                                 0.09442275762557983,
                                                 0.09446389973163605,
                                                 0.09459325671195984,
                                                 0.09457375854253769,
                                                 0.0945519208908081,
                                                 0.09459152817726135,
                                                 0.0945110023021698,
                                                 0.7131268978118896,
                                                 0.09457242488861084,
                                                 13.699750900268555,
                                                 0.09455432742834091,
                                                 0.09460796415805817,
                                                 0.09450221061706543,
                                                 11696.2060546875,
                                                 0.09452937543392181,
                                                 0.09452711045742035,
                                                 0.09458191692829132,
                                                 9.455473899841309,
                                                 0.09451385587453842,
                                                 0.09448845684528351,
                                                 131.53732299804688,
                                                 4293.73291015625,
                                                 0.09409381449222565,
                                                 0.09368418902158737,
                                                 0.09371126443147659,
                                                 0.09354376047849655,
                                                 0.2533681094646454,
                                                 0.314374715089798,
                                                 0.09430520236492157,
                                                 0.09440848976373672,
                                                 0.094419464468956,
                                                 0.09437459707260132,
                                                 0.09439592063426971,
                                                 1767081.0,
                                                 594.7551879882812,
                                                 0.09452971816062927,
                                                 0.09449964016675949,
                                                 0.09450231492519379,
                                                 0.09459840506315231,
                                                 0.09455262124538422,
                                                 0.09447214007377625,
                                                 0.0944393202662468,
                                                 0.09440606832504272,
                                                 0.09447411447763443,
                                                 0.09437554329633713,
                                                 0.09433301538228989,
                                                 0.09440498054027557,
                                                 0.10780218243598938,
                                                 0.09436662495136261,
                                                 0.09431613236665726,
                                                 0.0945611521601677,
                                                 219.7027587890625,
                                                 0.4978674352169037,
                                                 0.0944586768746376,
                                                 0.30992811918258667,
                                                 1.8426533937454224,
                                                 0.5712193250656128,
                                                 0.09446409344673157,
                                                 0.09459622949361801,
                                                 0.09451809525489807,
                                                 87.38387298583984,
                                                 0.09456565976142883,
                                                 3497.284423828125,
                                                 0.09454844892024994,
                                                 0.09451253712177277,
                                                 0.09457314759492874,
                                                 0.09456212818622589,
                                                 0.15045373141765594,
                                                 0.14893552660942078,
                                                 562.9702758789062,
                                                 131.55374145507812,
                                                 0.09452853351831436,
                                                 0.09735272079706192,
                                                 0.09456931799650192,
                                                 0.6130989193916321,
                                                 0.09453856199979782,
                                                 0.09444250911474228,
                                                 0.09452873468399048,
                                                 0.09453690052032471,
                                                 1.229264736175537,
                                                 0.09449880570173264,
                                                 820.4354248046875,
                                                 1325.2156982421875,
                                                 0.09444205462932587,
                                                 0.0945630744099617,
                                                 0.09459412842988968,
                                                 0.0945231169462204,
                                                 513502.1875,
                                                 0.09456072747707367,
                                                 1.1679768562316895,
                                                 0.0945015400648117,
                                                 0.09457283467054367,
                                                 0.09455496817827225,
                                                 0.19079294800758362,
                                                 36.89104461669922,
                                                 0.0944361463189125,
                                                 0.09443370997905731,
                                                 0.09447237849235535,
                                                 20771922.0,
                                                 0.09442625194787979,
                                                 0.1135120838880539,
                                                 0.09445078670978546,
                                                 0.09434980154037476,
                                                 0.09435220062732697,
                                                 0.35446515679359436,
                                                 0.09654965996742249,
                                                 0.09453532844781876,
                                                 0.09456611424684525,
                                                 32323.8828125,
                                                 978.3565063476562,
                                                 0.09457312524318695,
                                                 0.0945013165473938,
                                                 0.09439229965209961,
                                                 4412095.5,
                                                 0.09448060393333435,
                                                 0.09440262615680695,
                                                 0.0943794921040535,
                                                 0.09443134814500809,
                                                 369.93389892578125,
                                                 0.14076443016529083,
                                                 0.09438452124595642,
                                                 0.09434284269809723,
                                                 0.10136646777391434,
                                                 0.09443105757236481,
                                                 0.09402314573526382,
                                                 26166.89453125,
                                                 0.09349802136421204,
                                                 3.321302652359009,
                                                 0.09364969283342361,
                                                 0.09496324509382248,
                                                 0.09372551739215851,
                                                 0.09436341375112534,
                                                 0.09432327002286911,
                                                 0.09436825662851334,
                                                 0.0944887325167656,
                                                 0.09451030939817429,
                                                 0.09454188495874405,
                                                 0.0945427343249321,
                                                 0.09454578906297684,
                                                 0.09454236924648285,
                                                 0.09450600296258926,
                                                 0.09452961385250092,
                                                 0.09452560544013977,
                                                 0.09452951699495316,
                                                 2834032.25,
                                                 0.09452395886182785,
                                                 10.720794677734375,
                                                 68.72327423095703,
                                                 8477249.0,
                                                 0.09440808743238449,
                                                 5520.7861328125,
                                                 0.09362620860338211,
                                                 0.09349146485328674,
                                                 0.09339573979377747,
                                                 149.35525512695312,
                                                 278.6841735839844,
                                                 0.0943543016910553,
                                                 37.89771270751953,
                                                 5100427.5,
                                                 0.09433344006538391,
                                                 6600393.5,
                                                 0.09443498402833939,
                                                 0.09448225051164627,
                                                 0.09439776837825775,
                                                 0.09439967572689056,
                                                 0.09428673982620239,
                                                 0.0945155993103981,
                                                 0.27247631549835205,
                                                 0.09454277157783508,
                                                 0.09456893801689148,
                                                 0.09452944248914719,
                                                 0.09454522281885147,
                                                 0.09451722353696823,
                                                 0.09460891038179398,
                                                 0.09455429017543793,
                                                 10244.599609375,
                                                 84.07907104492188,
                                                 0.0944337323307991,
                                                 0.09459497779607773,
                                                 0.09454827755689621,
                                                 0.09453362226486206,
                                                 0.09449025988578796,
                                                 0.09453912824392319,
                                                 0.3738631308078766,
                                                 0.09447181224822998,
                                                 0.09451087564229965,
                                                 0.09459631890058517,
                                                 0.09454839676618576,
                                                 0.09458976984024048,
                                                 0.09457769244909286,
                                                 0.09456919878721237,
                                                 3.4914751052856445,
                                                 1427.79345703125,
                                                 0.4578329920768738,
                                                 884.7360229492188,
                                                 0.09449641406536102,
                                                 0.09440287202596664,
                                                 510.2096862792969,
                                                 0.09450377523899078,
                                                 0.09451079368591309,
                                                 2.1641271114349365,
                                                 0.0944771096110344,
                                                 25.285322189331055,
                                                 0.09460421651601791,
                                                 0.09461460262537003,
                                                 21697.78125,
                                                 0.11157756298780441,
                                                 9.951555252075195,
                                                 0.09457164257764816,
                                                 0.09458516538143158,
                                                 770.6202392578125,
                                                 0.5892700552940369,
                                                 0.09393339604139328,
                                                 0.09381875395774841,
                                                 0.09371479600667953,
                                                 0.09378113597631454,
                                                 0.0938304141163826,
                                                 0.09431172162294388,
                                                 1034.4150390625,
                                                 538.9066772460938,
                                                 0.09464322030544281,
                                                 0.7673370242118835,
                                                 0.18193259835243225,
                                                 0.09461429715156555,
                                                 0.09462171792984009,
                                                 0.6443932056427002,
                                                 0.09465876966714859,
                                                 0.09471260756254196,
                                                 0.09465605765581131,
                                                 0.09452185034751892,
                                                 0.09450294822454453,
                                                 0.12810559570789337,
                                                 0.094684898853302,
                                                 8.992913246154785,
                                                 0.8569191694259644,
                                                 582.5096435546875,
                                                 0.09462642669677734,
                                                 0.09460807591676712,
                                                 0.09465175122022629,
                                                 0.0958004742860794,
                                                 0.09474059194326401,
                                                 0.09462853521108627,
                                                 657.1932373046875,
                                                 0.19906458258628845,
                                                 0.09392101317644119,
                                                 0.09379742294549942,
                                                 0.7946452498435974,
                                                 0.1725653111934662,
                                                 0.09447696059942245,
                                                 0.09447885304689407,
                                                 0.09469438344240189,
                                                 0.09465198963880539,
                                                 0.09460066258907318,
                                                 560.6574096679688,
                                                 0.09460125863552094,
                                                 0.0946880578994751,
                                                 0.09468343108892441,
                                                 0.09466951340436935,
                                                 7.846862316131592,
                                                 0.09464780241250992,
                                                 1.8931728601455688,
                                                 0.0939062237739563,
                                                 0.14714713394641876,
                                                 0.09371401369571686,
                                                 0.14943532645702362,
                                                 9.442521095275879,
                                                 0.09458307921886444,
                                                 0.09459754824638367,
                                                 0.19837377965450287,
                                                 0.18834809958934784,
                                                 0.09383810311555862,
                                                 0.09375286102294922,
                                                 0.0936644896864891,
                                                 0.09361999481916428,
                                                 0.0935748741030693,
                                                 1.445277214050293,
                                                 0.0944688618183136,
                                                 12.287859916687012,
                                                 0.09468797594308853,
                                                 0.09469488263130188,
                                                 0.09460695087909698,
                                                 0.7720686197280884,
                                                 0.0945553258061409,
                                                 0.09460161626338959,
                                                 0.09466303139925003,
                                                 0.09467338770627975,
                                                 0.09465459734201431,
                                                 0.09461504220962524,
                                                 0.0945778414607048,
                                                 0.09464619308710098,
                                                 0.09465964883565903,
                                                 0.09464212507009506,
                                                 229.66256713867188,
                                                 18.826688766479492,
                                                 0.09455174207687378,
                                                 0.09412211924791336,
                                                 17504.03515625,
                                                 0.09471108019351959,
                                                 0.09461493790149689,
                                                 10.089583396911621,
                                                 0.09461957961320877,
                                                 31.376590728759766,
                                                 0.09470537304878235,
                                                 0.09463783353567123,
                                                 5610218.0,
                                                 0.09464839100837708,
                                                 0.09463448077440262,
                                                 0.0942724272608757,
                                                 0.09408007562160492,
                                                 0.48262032866477966,
                                                 0.09465523064136505,
                                                 2.4015886783599854,
                                                 0.09462562948465347,
                                                 142063.546875,
                                                 0.09455953538417816,
                                                 1.974007487297058,
                                                 0.09464676678180695,
                                                 0.09465840458869934,
                                                 0.10140714049339294,
                                                 0.09454220533370972,
                                                 0.09462330490350723,
                                                 0.09451285749673843,
                                                 4.050822734832764,
                                                 0.09465648978948593,
                                                 0.09463375806808472,
                                                 0.09465823322534561,
                                                 0.09469760209321976,
                                                 0.09466546028852463,
                                                 932.041259765625,
                                                 0.09463147073984146,
                                                 3.409330129623413,
                                                 54.3207893371582,
                                                 0.09465781599283218,
                                                 0.09465416520833969,
                                                 1.7536137104034424,
                                                 0.09468229115009308,
                                                 84.98772430419922,
                                                 0.09475217014551163,
                                                 0.09401313960552216,
                                                 134.52792358398438,
                                                 0.09469401836395264,
                                                 0.23205254971981049,
                                                 0.0946466401219368,
                                                 0.09470997005701065,
                                                 0.09463036060333252,
                                                 0.09456641972064972,
                                                 0.09461867809295654,
                                                 0.9174342155456543,
                                                 0.09432287514209747,
                                                 3.919257879257202,
                                                 0.09378696233034134,
                                                 0.09345461428165436,
                                                 0.09351752698421478,
                                                 0.09323997050523758,
                                                 0.09312094748020172,
                                                 0.09307726472616196,
                                                 0.09321693331003189,
                                                 0.09316740185022354,
                                                 0.09299517422914505,
                                                 0.09285470098257065,
                                                 0.2572370171546936,
                                                 0.09311380237340927,
                                                 0.09293543547391891,
                                                 0.09303213655948639,
                                                 0.1493358463048935,
                                                 0.09310180693864822,
                                                 0.09321630746126175,
                                                 10.437809944152832,
                                                 0.0943022146821022,
                                                 0.09423695504665375,
                                                 2590.866943359375,
                                                 1.4054656028747559,
                                                 38.2252311706543,
                                                 19227.08203125,
                                                 0.09473899751901627,
                                                 0.09466087073087692,
                                                 0.0946071520447731,
                                                 830.8789672851562,
                                                 0.0945894792675972,
                                                 2.7011053562164307,
                                                 0.504011332988739,
                                                 0.09468135982751846,
                                                 0.09465712308883667,
                                                 0.0945027694106102,
                                                 3.204328775405884,
                                                 0.09469970315694809,
                                                 136.45835876464844,
                                                 0.0946570485830307,
                                                 276.604248046875,
                                                 0.12033631652593613,
                                                 0.09448137134313583,
                                                 1677058.875,
                                                 0.09450552612543106,
                                                 6.231417179107666,
                                                 0.4628032147884369,
                                                 0.12920531630516052,
                                                 0.5548384189605713,
                                                 0.09467286616563797,
                                                 0.09465939551591873,
                                                 0.09464029967784882,
                                                 0.09545759111642838,
                                                 0.09464362263679504,
                                                 1.4605083465576172,
                                                 0.6265268921852112,
                                                 0.13141100108623505,
                                                 0.09462108463048935,
                                                 0.09455641359090805,
                                                 5.9682111740112305,
                                                 0.09459859877824783,
                                                 32.384986877441406,
                                                 0.09444091469049454,
                                                 0.09422245621681213,
                                                 0.0940980538725853,
                                                 0.09372019022703171,
                                                 0.09348495304584503,
                                                 0.09494473040103912,
                                                 0.09471526741981506,
                                                 0.09466321766376495,
                                                 0.09463952481746674,
                                                 0.0946340337395668,
                                                 0.0946597084403038,
                                                 0.10840669274330139,
                                                 0.09467717260122299,
                                                 0.09460677951574326,
                                                 0.09988532215356827,
                                                 0.09422221034765244,
                                                 0.11676912754774094,
                                                 0.09440311044454575,
                                                 45.41225051879883,
                                                 507.40032958984375,
                                                 0.13748902082443237,
                                                 0.09470012038946152,
                                                 226.58145141601562,
                                                 0.4495183825492859,
                                                 0.09465526789426804,
                                                 0.09466543048620224,
                                                 61.091617584228516,
                                                 0.09468193352222443,
                                                 0.09465447813272476,
                                                 0.0945592001080513,
                                                 0.09467066824436188,
                                                 0.09464655071496964,
                                                 141.53970336914062,
                                                 116328.4609375,
                                                 0.09458193182945251,
                                                 0.12122631818056107,
                                                 4.387285232543945,
                                                 2681976.5,
                                                 0.09463132172822952,
                                                 0.0946061760187149,
                                                 0.0944783166050911,
                                                 0.09446098655462265,
                                                 0.09450376033782959,
                                                 0.09455794841051102,
                                                 0.09459281712770462,
                                                 0.09462811797857285,
                                                 6.4233717918396,
                                                 0.09450298547744751,
                                                 0.09456917643547058,
                                                 0.0934096947312355,
                                                 0.0934649258852005,
                                                 0.09316615760326385,
                                                 0.5173411965370178,
                                                 1.9697647094726562,
                                                 32.42613983154297,
                                                 0.09472211450338364,
                                                 0.09469816833734512,
                                                 0.0956822857260704,
                                                 0.09459832310676575,
                                                 0.0946735143661499,
                                                 0.09462333470582962,
                                                 0.09459586441516876,
                                                 1.9096038341522217,
                                                 0.0946708396077156,
                                                 0.0946536511182785,
                                                 0.09466943144798279,
                                                 0.09466858953237534,
                                                 0.09464891254901886,
                                                 0.09463982284069061,
                                                 0.09469345957040787,
                                                 0.0946459099650383,
                                                 0.0945952758193016,
                                                 122.37969970703125,
                                                 4.63157844543457,
                                                 56.718482971191406,
                                                 0.18920323252677917,
                                                 454.80084228515625,
                                                 4036.590087890625,
                                                 0.09464812278747559,
                                                 127179.6875,
                                                 0.09467849880456924,
                                                 0.09463875740766525,
                                                 0.09431662410497665,
                                                 665.6954956054688,
                                                 0.09449782967567444,
                                                 0.09459271281957626,
                                                 0.09455017000436783,
                                                 2.904323101043701,
                                                 0.09459733217954636,
                                                 0.0946681872010231,
                                                 0.09470684081315994,
                                                 3.52655029296875,
                                                 0.09464657306671143,
                                                 0.09463555365800858,
                                                 1.8135828971862793,
                                                 0.09465093165636063,
                                                 0.0946563184261322,
                                                 0.09455125033855438,
                                                 0.09466362744569778,
                                                 0.0946488231420517,
                                                 2.0649492740631104,
                                                 1.0428603887557983,
                                                 0.09464512020349503,
                                                 0.09463749080896378,
                                                 0.09463761001825333,
                                                 204.697509765625,
                                                 0.09467710554599762],
                          'val_accuracy_history': [1.7894736842105263,
                                                   1.368421052631579,
                                                   0.3157894736842105,
                                                   1.0526315789473684,
                                                   1.0,
                                                   0.42105263157894735,
                                                   0.6842105263157895,
                                                   1.4210526315789473,
                                                   0.10526315789473684,
                                                   0.3157894736842105,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   3.0,
                                                   0.0,
                                                   2.0,
                                                   1.0,
                                                   2.0,
                                                   1.0,
                                                   0.0,
                                                   2.0,
                                                   1.0,
                                                   2.0,
                                                   1.0,
                                                   2.0,
                                                   3.0,
                                                   1.0,
                                                   2.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   2.1578947368421053,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   3.0,
                                                   0.0,
                                                   1.0,
                                                   1.2105263157894737,
                                                   2.0,
                                                   1.0,
                                                   2.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   2.0,
                                                   1.0,
                                                   0.5263157894736842,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   2.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   2.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   4.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   2.0,
                                                   1.0,
                                                   3.0,
                                                   1.0,
                                                   0.631578947368421,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   1.631578947368421,
                                                   1.8421052631578947,
                                                   0.0,
                                                   2.0,
                                                   0.9473684210526315,
                                                   0.47368421052631576,
                                                   0.7894736842105263,
                                                   0.05263157894736842,
                                                   3.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   1.2105263157894737,
                                                   0.8421052631578947,
                                                   0.21052631578947367,
                                                   0.9473684210526315,
                                                   1.5263157894736843,
                                                   1.0526315789473684,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   2.0,
                                                   1.0,
                                                   2.0,
                                                   1.0,
                                                   2.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   3.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   1.6842105263157894,
                                                   1.9473684210526316,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   3.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   2.0,
                                                   2.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   3.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   3.0,
                                                   1.0,
                                                   1.0,
                                                   2.0,
                                                   1.0,
                                                   2.0,
                                                   3.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   2.0,
                                                   3.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   3.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.7777777777777778,
                                                   0.1111111111111111,
                                                   0.8888888888888888,
                                                   2.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   0.0,
                                                   2.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   0.7777777777777778,
                                                   0.7777777777777778,
                                                   0.7777777777777778,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   3.0,
                                                   1.0,
                                                   2.0,
                                                   2.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   2.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   2.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   2.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   0.1111111111111111,
                                                   0.4444444444444444,
                                                   0.8888888888888888,
                                                   0.2222222222222222,
                                                   0.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   2.0,
                                                   2.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   2.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   0.0,
                                                   3.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   2.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.8888888888888888,
                                                   0.5555555555555556,
                                                   0.7777777777777778,
                                                   1.0,
                                                   0.2222222222222222,
                                                   0.3333333333333333,
                                                   0.1111111111111111,
                                                   0.3333333333333333,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   2.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   2.0,
                                                   1.0,
                                                   0.0,
                                                   2.0,
                                                   1.0,
                                                   3.0,
                                                   1.0,
                                                   0.8888888888888888,
                                                   1.0,
                                                   0.8888888888888888,
                                                   0.0,
                                                   2.6666666666666665,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   1.7777777777777777,
                                                   0.0,
                                                   3.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   3.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   3.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   4.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.8888888888888888,
                                                   0.4444444444444444,
                                                   0.6666666666666666,
                                                   0.1111111111111111,
                                                   0.3333333333333333,
                                                   1.0,
                                                   3.0,
                                                   2.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   3.0,
                                                   0.0,
                                                   3.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   4.0,
                                                   2.0,
                                                   1.0,
                                                   0.0,
                                                   3.0,
                                                   3.0,
                                                   1.0,
                                                   1.0,
                                                   2.0526315789473686,
                                                   1.736842105263158,
                                                   2.9473684210526314,
                                                   1.894736842105263,
                                                   4.0,
                                                   2.0,
                                                   1.0,
                                                   2.0,
                                                   2.0,
                                                   1.0,
                                                   2.0,
                                                   3.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   2.0,
                                                   2.0,
                                                   0.3684210526315789,
                                                   2.6315789473684212,
                                                   2.526315789473684,
                                                   1.894736842105263,
                                                   4.0,
                                                   2.0,
                                                   3.0,
                                                   1.0,
                                                   2.0,
                                                   3.0526315789473686,
                                                   1.263157894736842,
                                                   1.105263157894737,
                                                   0.21052631578947367,
                                                   1.1578947368421053,
                                                   1.0,
                                                   2.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   3.0,
                                                   2.0,
                                                   2.0,
                                                   4.0,
                                                   3.0,
                                                   3.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   2.0,
                                                   2.0,
                                                   2.0,
                                                   0.0,
                                                   2.0,
                                                   3.210526315789474,
                                                   0.10526315789473684,
                                                   3.0,
                                                   0.0,
                                                   5.0,
                                                   2.0,
                                                   1.0,
                                                   3.0,
                                                   2.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   0.2631578947368421,
                                                   0.0,
                                                   2.0,
                                                   2.0,
                                                   1.0,
                                                   2.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   4.0,
                                                   0.0,
                                                   2.0,
                                                   2.0,
                                                   3.0,
                                                   4.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   3.0,
                                                   2.210526315789474,
                                                   3.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   0.10526315789473684,
                                                   4.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   2.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   1.8421052631578947,
                                                   2.4210526315789473,
                                                   1.4736842105263157,
                                                   0.3157894736842105,
                                                   0.7894736842105263,
                                                   1.6842105263157894,
                                                   1.631578947368421,
                                                   1.9473684210526316,
                                                   1.6842105263157894,
                                                   1.8421052631578947,
                                                   1.894736842105263,
                                                   0.6842105263157895,
                                                   1.0526315789473684,
                                                   0.8947368421052632,
                                                   2.0,
                                                   2.4210526315789473,
                                                   1.4210526315789473,
                                                   0.9473684210526315,
                                                   1.1578947368421053,
                                                   0.0,
                                                   0.9473684210526315,
                                                   1.0,
                                                   4.0,
                                                   4.0,
                                                   2.0,
                                                   3.0,
                                                   1.0,
                                                   2.0,
                                                   2.0,
                                                   0.0,
                                                   2.0,
                                                   3.0,
                                                   2.0,
                                                   2.0,
                                                   2.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   3.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   2.0,
                                                   2.0,
                                                   0.0,
                                                   1.0,
                                                   3.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   3.0,
                                                   3.0,
                                                   1.0,
                                                   0.0,
                                                   3.0,
                                                   2.0,
                                                   2.0,
                                                   0.0,
                                                   2.0,
                                                   0.8947368421052632,
                                                   1.5263157894736843,
                                                   1.6842105263157894,
                                                   0.2631578947368421,
                                                   1.0,
                                                   3.0,
                                                   5.0,
                                                   2.0,
                                                   3.0,
                                                   1.0,
                                                   2.0,
                                                   1.0,
                                                   2.0,
                                                   1.0,
                                                   2.789473684210526,
                                                   1.0,
                                                   0.0,
                                                   2.0,
                                                   1.0,
                                                   0.0,
                                                   2.0,
                                                   2.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   2.0,
                                                   1.0,
                                                   2.0,
                                                   4.0,
                                                   2.0,
                                                   1.0,
                                                   1.0,
                                                   4.0,
                                                   2.0,
                                                   3.0,
                                                   2.0,
                                                   4.0,
                                                   3.0,
                                                   2.0,
                                                   1.0,
                                                   1.0,
                                                   2.0,
                                                   2.0,
                                                   2.0,
                                                   1.0,
                                                   2.0,
                                                   1.631578947368421,
                                                   0.21052631578947367,
                                                   1.736842105263158,
                                                   0.9473684210526315,
                                                   1.4736842105263157,
                                                   1.0,
                                                   2.0,
                                                   2.0,
                                                   3.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   2.0,
                                                   2.0,
                                                   1.0,
                                                   4.0,
                                                   2.0,
                                                   3.0,
                                                   3.0,
                                                   4.0,
                                                   2.0,
                                                   3.0,
                                                   0.0,
                                                   2.0,
                                                   3.0,
                                                   1.0,
                                                   2.0,
                                                   3.0,
                                                   2.0,
                                                   2.0,
                                                   1.894736842105263,
                                                   2.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   2.0,
                                                   1.0,
                                                   5.0,
                                                   3.0,
                                                   3.0,
                                                   2.0,
                                                   2.0,
                                                   1.0,
                                                   1.0,
                                                   2.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   2.0],
                          'val_loss': 0.0900145098567009,
                          'val_loss_history': [0.13289986550807953,
                                               30.159862518310547,
                                               0.1306895911693573,
                                               772.6478881835938,
                                               0.12578943371772766,
                                               0.1313501000404358,
                                               0.12481862306594849,
                                               0.12818337976932526,
                                               0.12867261469364166,
                                               0.13204559683799744,
                                               0.1220577135682106,
                                               0.11260901391506195,
                                               0.11409381777048111,
                                               0.11440904438495636,
                                               0.11434926837682724,
                                               0.11414065212011337,
                                               0.11439044773578644,
                                               0.11462827026844025,
                                               0.11452741175889969,
                                               0.11467263847589493,
                                               0.11460597068071365,
                                               0.11462745815515518,
                                               0.11468517035245895,
                                               0.11480430513620377,
                                               0.1147732213139534,
                                               0.11485739797353745,
                                               0.11479106545448303,
                                               0.1150507852435112,
                                               0.11502186954021454,
                                               0.11493460834026337,
                                               0.11511021852493286,
                                               0.11504624783992767,
                                               0.1150892823934555,
                                               0.11518291383981705,
                                               64640324.0,
                                               0.11498195677995682,
                                               0.11846525222063065,
                                               0.11501139402389526,
                                               0.11516815423965454,
                                               0.11503072828054428,
                                               0.11535662412643433,
                                               0.11514862626791,
                                               0.11519207060337067,
                                               0.11522386968135834,
                                               0.11516549438238144,
                                               0.11545438319444656,
                                               0.11529026180505753,
                                               0.11528454720973969,
                                               0.11551813781261444,
                                               0.11532633006572723,
                                               0.1173066645860672,
                                               0.11544819921255112,
                                               0.11565966159105301,
                                               0.1153019443154335,
                                               0.11566291004419327,
                                               0.11555980145931244,
                                               0.11566717177629471,
                                               0.11576888710260391,
                                               0.11573272943496704,
                                               0.11575499922037125,
                                               0.11589116603136063,
                                               0.11577892303466797,
                                               0.11567796766757965,
                                               0.11675383150577545,
                                               0.11572939157485962,
                                               0.11599055677652359,
                                               0.11586335301399231,
                                               0.11595223098993301,
                                               0.115937240421772,
                                               0.11586461216211319,
                                               0.11587101221084595,
                                               0.1159282699227333,
                                               0.11588104814291,
                                               0.11595000326633453,
                                               0.11601797491312027,
                                               0.11578642576932907,
                                               0.1159607544541359,
                                               0.11591330915689468,
                                               0.1158139705657959,
                                               0.1159902960062027,
                                               0.1159883365035057,
                                               0.1158863827586174,
                                               0.11616058647632599,
                                               0.116038978099823,
                                               0.11605293303728104,
                                               0.11608398705720901,
                                               0.11597417294979095,
                                               0.11598964035511017,
                                               0.11528076976537704,
                                               0.1164831668138504,
                                               0.11611490696668625,
                                               0.11596962809562683,
                                               0.11598684638738632,
                                               0.11591029167175293,
                                               0.11605054140090942,
                                               0.11600024253129959,
                                               0.1159677505493164,
                                               0.11592534184455872,
                                               0.11612578481435776,
                                               0.11618667840957642,
                                               0.11620379984378815,
                                               0.11618492752313614,
                                               0.11606466770172119,
                                               0.11596335470676422,
                                               0.11628828197717667,
                                               0.1161775216460228,
                                               0.11668728291988373,
                                               0.11626330763101578,
                                               0.11897259205579758,
                                               0.11721517890691757,
                                               0.11805533617734909,
                                               0.11683106422424316,
                                               0.11628374457359314,
                                               0.11647655814886093,
                                               0.11625979095697403,
                                               0.11615417152643204,
                                               0.11611239612102509,
                                               0.11597920954227448,
                                               0.11626138538122177,
                                               0.11619382351636887,
                                               0.11614691466093063,
                                               0.1158432811498642,
                                               0.11844415962696075,
                                               0.12012633681297302,
                                               0.12201090902090073,
                                               0.12300722301006317,
                                               0.12179263681173325,
                                               0.12022022157907486,
                                               0.1161445826292038,
                                               0.11631754785776138,
                                               0.11599871516227722,
                                               0.11595051735639572,
                                               0.11614725738763809,
                                               0.11601608991622925,
                                               0.11616994440555573,
                                               0.11606497317552567,
                                               0.11608567833900452,
                                               0.11612219363451004,
                                               0.11595229059457779,
                                               0.1160753071308136,
                                               0.11604484170675278,
                                               0.11610965430736542,
                                               0.11596120893955231,
                                               0.11606322973966599,
                                               0.1158863827586174,
                                               0.1162295863032341,
                                               0.11610893160104752,
                                               0.11617756634950638,
                                               0.11616548895835876,
                                               0.11614851653575897,
                                               0.11626169085502625,
                                               0.11611983180046082,
                                               0.116350457072258,
                                               0.1160777285695076,
                                               0.11605817079544067,
                                               0.1161554679274559,
                                               0.1161135733127594,
                                               0.11618547886610031,
                                               0.11609655618667603,
                                               0.11594346165657043,
                                               0.11609303951263428,
                                               0.11628188192844391,
                                               0.11593768745660782,
                                               0.116062231361866,
                                               0.11615301668643951,
                                               0.11612395197153091,
                                               0.11609595268964767,
                                               0.11606339365243912,
                                               0.11621040850877762,
                                               0.11611215770244598,
                                               0.11631068587303162,
                                               0.11604316532611847,
                                               0.11610415577888489,
                                               0.11617108434438705,
                                               0.11737527698278427,
                                               0.11636664718389511,
                                               0.11643633991479874,
                                               0.11641830205917358,
                                               0.11623018234968185,
                                               0.11627576500177383,
                                               0.11618880182504654,
                                               0.11590965837240219,
                                               0.11600705981254578,
                                               0.11603537946939468,
                                               0.11614096164703369,
                                               0.11609300225973129,
                                               0.11620457470417023,
                                               0.11623869836330414,
                                               0.11612760275602341,
                                               0.11619135737419128,
                                               0.11633993685245514,
                                               0.11595872789621353,
                                               0.1160760223865509,
                                               0.11613026261329651,
                                               0.11629821360111237,
                                               0.11606990545988083,
                                               0.11618290096521378,
                                               0.11590874940156937,
                                               0.116066575050354,
                                               0.1162467747926712,
                                               0.11605139076709747,
                                               0.11606994271278381,
                                               0.11615034192800522,
                                               0.11626721173524857,
                                               0.1164214015007019,
                                               0.11610067635774612,
                                               0.11607576906681061,
                                               0.11610443145036697,
                                               0.11622398346662521,
                                               0.11609799414873123,
                                               0.11592576652765274,
                                               0.11589066684246063,
                                               0.11615613847970963,
                                               0.11620646715164185,
                                               0.11605507135391235,
                                               0.11605215072631836,
                                               0.11597687005996704,
                                               0.11605720967054367,
                                               0.11622954159975052,
                                               0.11576541513204575,
                                               0.11619725078344345,
                                               0.11608881503343582,
                                               0.11614267528057098,
                                               0.11628157645463943,
                                               0.11605657637119293,
                                               0.11614761501550674,
                                               0.11601652950048447,
                                               0.11608336865901947,
                                               0.1161508783698082,
                                               0.11626416444778442,
                                               0.11630623042583466,
                                               0.1164545938372612,
                                               0.11624109745025635,
                                               0.11607788503170013,
                                               0.1161070168018341,
                                               0.11620175093412399,
                                               0.11616086959838867,
                                               0.11611688882112503,
                                               0.11600390076637268,
                                               0.11609793454408646,
                                               0.11598715931177139,
                                               0.11622306704521179,
                                               0.11612720787525177,
                                               0.11618942022323608,
                                               0.11620881408452988,
                                               0.11601428687572479,
                                               0.11616939306259155,
                                               0.11625801771879196,
                                               0.11612837016582489,
                                               0.11634919047355652,
                                               0.11619079858064651,
                                               0.11613990366458893,
                                               0.11610959470272064,
                                               0.1163247600197792,
                                               0.11617305129766464,
                                               0.11615300923585892,
                                               0.11632908880710602,
                                               0.11621703207492828,
                                               0.11634662002325058,
                                               0.11614250391721725,
                                               0.11630403250455856,
                                               0.11608672142028809,
                                               0.11608536541461945,
                                               31432.908203125,
                                               0.1162625327706337,
                                               0.11617450416088104,
                                               0.11622246354818344,
                                               0.11625657230615616,
                                               0.11630494892597198,
                                               0.1160629466176033,
                                               0.11630386859178543,
                                               0.11654241383075714,
                                               0.11633019894361496,
                                               0.11625935137271881,
                                               0.1159849464893341,
                                               0.11611940711736679,
                                               0.11628160625696182,
                                               0.11616548895835876,
                                               0.1158863976597786,
                                               0.11616910994052887,
                                               0.11640705168247223,
                                               57.9826545715332,
                                               0.11621317267417908,
                                               0.1162533313035965,
                                               0.11624367535114288,
                                               0.11624933034181595,
                                               0.11617560684680939,
                                               0.11601812392473221,
                                               0.11616231501102448,
                                               0.1162579134106636,
                                               0.1163044273853302,
                                               0.11610657721757889,
                                               0.11615923047065735,
                                               0.11604088544845581,
                                               0.11613348871469498,
                                               0.1160733699798584,
                                               0.11620483547449112,
                                               0.11624867469072342,
                                               0.11615971475839615,
                                               0.11607415229082108,
                                               75018216.0,
                                               0.09041483700275421,
                                               0.09068240970373154,
                                               0.09158043563365936,
                                               0.09161237627267838,
                                               0.09168776124715805,
                                               0.09167324006557465,
                                               0.09174589067697525,
                                               0.09159623086452484,
                                               0.091584213078022,
                                               0.09181859344244003,
                                               0.09148017317056656,
                                               0.09162292629480362,
                                               0.09164972603321075,
                                               0.09172000735998154,
                                               0.09156201779842377,
                                               0.09159772098064423,
                                               0.09161564707756042,
                                               0.09180383384227753,
                                               0.09165830910205841,
                                               0.09175550937652588,
                                               0.09164596349000931,
                                               0.09165975451469421,
                                               0.09162546694278717,
                                               0.09173918515443802,
                                               0.09151586145162582,
                                               0.09154339134693146,
                                               0.09142778068780899,
                                               0.09155946969985962,
                                               0.0915718749165535,
                                               0.09148793667554855,
                                               0.09153666347265244,
                                               0.0915401503443718,
                                               0.0918232873082161,
                                               0.09156225621700287,
                                               0.09150882065296173,
                                               0.09154771268367767,
                                               0.09136240184307098,
                                               0.09136436134576797,
                                               0.09138547629117966,
                                               0.09111592918634415,
                                               0.09049556404352188,
                                               0.09093768894672394,
                                               0.09028400480747223,
                                               0.0910341814160347,
                                               0.09143580496311188,
                                               0.09173454344272614,
                                               0.09146896004676819,
                                               0.09174791723489761,
                                               0.09165650606155396,
                                               0.09164563566446304,
                                               0.09150607138872147,
                                               0.09162599593400955,
                                               0.09160754829645157,
                                               0.0916544646024704,
                                               0.09154925495386124,
                                               0.09158218652009964,
                                               0.09168332815170288,
                                               0.09178471565246582,
                                               0.09159593284130096,
                                               0.09167377650737762,
                                               0.09158945083618164,
                                               0.09157179296016693,
                                               0.09173416346311569,
                                               0.09171082079410553,
                                               0.0915522649884224,
                                               0.09138037264347076,
                                               6550871.0,
                                               0.0917968600988388,
                                               0.09176656603813171,
                                               0.09189419448375702,
                                               0.09189261496067047,
                                               0.09148921072483063,
                                               0.0916379913687706,
                                               0.09164125472307205,
                                               0.09168343245983124,
                                               0.0915914997458458,
                                               0.0916556641459465,
                                               0.09176559001207352,
                                               0.09119798988103867,
                                               0.09149183332920074,
                                               0.09167253226041794,
                                               0.09157314896583557,
                                               0.09136173874139786,
                                               0.09172654151916504,
                                               0.09172135591506958,
                                               0.0916282907128334,
                                               0.09151092171669006,
                                               0.09152401983737946,
                                               0.09146179258823395,
                                               0.09152831137180328,
                                               0.09171596169471741,
                                               0.09146741032600403,
                                               0.09163735806941986,
                                               0.09172643721103668,
                                               0.09151304513216019,
                                               0.09167574346065521,
                                               0.09155893325805664,
                                               0.09151120483875275,
                                               0.09154047071933746,
                                               0.09145689010620117,
                                               0.09163273870944977,
                                               0.09142628312110901,
                                               0.09159869700670242,
                                               0.09154431521892548,
                                               0.09110464155673981,
                                               0.09078102558851242,
                                               0.0907980427145958,
                                               0.09076491743326187,
                                               0.09136105328798294,
                                               0.09158964455127716,
                                               0.09161243587732315,
                                               0.09146032482385635,
                                               0.09154172241687775,
                                               0.0915302038192749,
                                               0.09143901616334915,
                                               0.09158524125814438,
                                               0.09163603186607361,
                                               0.09191957861185074,
                                               0.09158330410718918,
                                               0.0916702076792717,
                                               0.09173353016376495,
                                               0.09168414771556854,
                                               0.09165389090776443,
                                               0.09153973311185837,
                                               0.09151528030633926,
                                               0.09149136394262314,
                                               0.09165231138467789,
                                               0.09150249511003494,
                                               0.09171602129936218,
                                               0.09170891344547272,
                                               0.09149736166000366,
                                               0.09154205024242401,
                                               0.09144985675811768,
                                               0.09173857420682907,
                                               0.09168737381696701,
                                               0.09159638732671738,
                                               0.09164682775735855,
                                               0.09182272851467133,
                                               0.09188602864742279,
                                               0.0916704535484314,
                                               0.09181439131498337,
                                               0.0916689857840538,
                                               0.0916949138045311,
                                               0.0918634682893753,
                                               0.091732919216156,
                                               0.09177081286907196,
                                               0.09180489182472229,
                                               0.09170784801244736,
                                               0.09166619181632996,
                                               0.09147465229034424,
                                               0.09065674990415573,
                                               0.09165985137224197,
                                               0.0915789008140564,
                                               0.09145854413509369,
                                               0.0916421115398407,
                                               0.09175586700439453,
                                               0.0916069895029068,
                                               0.091783806681633,
                                               0.09182631224393845,
                                               0.0917629525065422,
                                               0.09174057096242905,
                                               0.09179218113422394,
                                               0.09166530519723892,
                                               0.09162689000368118,
                                               0.09167832881212234,
                                               0.09155615419149399,
                                               0.09177278727293015,
                                               0.09168699383735657,
                                               0.09166309982538223,
                                               0.09175059199333191,
                                               0.09156449139118195,
                                               0.09154761582612991,
                                               0.09163499623537064,
                                               0.09183468669652939,
                                               0.09155910462141037,
                                               0.09153896570205688,
                                               0.09173861145973206,
                                               0.09150022268295288,
                                               0.09156262874603271,
                                               0.09158863872289658,
                                               0.091577909886837,
                                               0.0914931669831276,
                                               0.09152447432279587,
                                               0.09160035848617554,
                                               0.0915502980351448,
                                               0.09158534556627274,
                                               0.09145323932170868,
                                               0.09149032831192017,
                                               0.09140226989984512,
                                               0.09157698601484299,
                                               0.09184463322162628,
                                               0.09163165837526321,
                                               0.09175490587949753,
                                               0.09173326194286346,
                                               0.09146177023649216,
                                               0.09170445799827576,
                                               0.09182441979646683,
                                               0.09142305701971054,
                                               0.09153810143470764,
                                               0.09144371002912521,
                                               0.09164082258939743,
                                               0.09147138893604279,
                                               0.09169746935367584,
                                               0.09146449714899063,
                                               0.0915309488773346,
                                               0.09153302758932114,
                                               0.09078874439001083,
                                               0.09040026366710663,
                                               0.0903313085436821,
                                               0.09044385701417923,
                                               0.09064793586730957,
                                               0.09086998552083969,
                                               0.0905841663479805,
                                               0.09128729999065399,
                                               0.09156367182731628,
                                               0.09157119691371918,
                                               0.09156181663274765,
                                               0.091734878718853,
                                               0.09174881130456924,
                                               0.09162548929452896,
                                               0.09173157066106796,
                                               0.09165001660585403,
                                               0.09184536337852478,
                                               0.09174542874097824,
                                               0.09148374944925308,
                                               0.0918145477771759,
                                               0.09172198921442032,
                                               0.09163372218608856,
                                               0.09175238013267517,
                                               0.09166068583726883,
                                               0.09152699261903763,
                                               0.09570552408695221,
                                               0.09075891226530075,
                                               0.09028211236000061,
                                               0.09057898819446564,
                                               0.09034465253353119,
                                               0.091373510658741,
                                               0.09163247048854828,
                                               0.0916837826371193,
                                               623378176.0,
                                               0.09151039272546768,
                                               0.09154417365789413,
                                               0.09182267636060715,
                                               0.0917121171951294,
                                               0.09159564971923828,
                                               0.09143423289060593,
                                               0.09164773672819138,
                                               0.09148409217596054,
                                               0.0917004719376564,
                                               0.0917796865105629,
                                               0.0917053371667862,
                                               0.09163802117109299,
                                               597140.375,
                                               0.0918307676911354,
                                               0.09166297316551208,
                                               0.09173108637332916,
                                               0.09177133440971375,
                                               0.09167598187923431,
                                               0.09167742729187012,
                                               0.09176761656999588,
                                               0.09161846339702606,
                                               0.09151538461446762,
                                               0.09162520617246628,
                                               0.09155528992414474,
                                               0.09133844822645187,
                                               0.09172411262989044,
                                               0.09168177098035812,
                                               0.09152256697416306,
                                               0.09153567254543304,
                                               0.09174582362174988,
                                               0.09146131575107574,
                                               0.09169001132249832,
                                               0.09183165431022644,
                                               0.09155264496803284,
                                               0.09179975837469101,
                                               0.09141802042722702,
                                               0.09178242087364197,
                                               0.09155882894992828,
                                               0.09155619144439697,
                                               0.09172812104225159,
                                               0.09160418808460236,
                                               0.09162043035030365,
                                               0.0916271060705185,
                                               0.0916542038321495,
                                               0.09182074666023254,
                                               0.091661237180233,
                                               0.09168948233127594,
                                               0.09161534905433655,
                                               0.09176906198263168,
                                               0.09174256771802902,
                                               0.09184432774782181,
                                               0.09175481647253036,
                                               0.09163566678762436,
                                               0.09110815823078156,
                                               0.09047967940568924,
                                               0.09134313464164734,
                                               0.09058742970228195,
                                               0.09057310223579407,
                                               0.09021198004484177,
                                               0.08950533717870712,
                                               0.08991483598947525,
                                               0.09000581502914429,
                                               0.08975820243358612,
                                               0.08989927172660828,
                                               0.08977536857128143,
                                               0.09046599268913269,
                                               0.09019088745117188,
                                               0.08985491842031479,
                                               0.09006495773792267,
                                               0.09006132930517197,
                                               0.08998249471187592,
                                               0.08991481363773346,
                                               0.09035725891590118,
                                               0.09026259183883667,
                                               0.09033244848251343,
                                               0.09021812677383423,
                                               0.08977342396974564,
                                               0.08995147794485092,
                                               0.09018651396036148,
                                               0.0902789831161499,
                                               0.09030649065971375,
                                               0.09018151462078094,
                                               0.08963830769062042,
                                               0.09024237841367722,
                                               0.08991577476263046,
                                               0.09044059365987778,
                                               0.08928724378347397,
                                               0.08972622454166412,
                                               0.08882772922515869,
                                               0.08975745737552643,
                                               0.09016703814268112,
                                               0.08963004499673843,
                                               0.0898420587182045,
                                               0.08989901095628738,
                                               0.09010357409715652,
                                               0.0901118665933609,
                                               0.09002404659986496,
                                               0.09008162468671799,
                                               0.08998696506023407,
                                               0.09019099175930023,
                                               0.09039027988910675,
                                               0.09014302492141724,
                                               0.09035284072160721,
                                               0.090223528444767,
                                               0.08909245580434799,
                                               0.08939402550458908,
                                               0.08942470699548721,
                                               0.08992207795381546,
                                               0.08974660187959671,
                                               0.09004580974578857,
                                               0.09003043174743652,
                                               0.08954887092113495,
                                               0.08958173543214798,
                                               0.0888582319021225,
                                               0.08949612826108932,
                                               0.0890330970287323,
                                               0.08879958093166351,
                                               0.08949175477027893,
                                               0.09019462019205093,
                                               0.08992094546556473,
                                               0.08964835852384567,
                                               0.08991514146327972,
                                               0.08982381969690323,
                                               0.08980073034763336,
                                               0.0896264985203743,
                                               0.08992141485214233,
                                               0.09008410573005676,
                                               0.08989385515451431,
                                               0.09021643549203873,
                                               0.09013224393129349,
                                               0.0900585800409317,
                                               0.08978967368602753,
                                               0.09012910723686218,
                                               0.08994796127080917,
                                               0.09000711888074875,
                                               0.08993218839168549,
                                               0.09006299823522568,
                                               0.08917942643165588,
                                               0.08967818319797516,
                                               0.08993396908044815,
                                               0.09023486822843552,
                                               0.0900617465376854,
                                               0.09012560546398163,
                                               0.08974074572324753,
                                               0.09009005129337311,
                                               0.08979517966508865,
                                               0.08997286111116409,
                                               0.09035619348287582,
                                               0.09010802954435349,
                                               0.09086864441633224,
                                               0.0897199809551239,
                                               0.08955328911542892,
                                               0.08973503112792969,
                                               0.0901649221777916,
                                               0.08986207097768784,
                                               0.09005796909332275,
                                               0.08977393060922623,
                                               0.09000321477651596,
                                               0.08998708426952362,
                                               0.08971834927797318,
                                               0.0897836983203888,
                                               0.09028778970241547,
                                               0.08969186991453171,
                                               0.09012392163276672,
                                               0.08985482901334763,
                                               0.09004218131303787,
                                               0.09037664532661438,
                                               0.09028197079896927,
                                               0.09011863172054291,
                                               0.0896797850728035,
                                               0.0896434634923935,
                                               0.0899115651845932,
                                               0.08928757160902023,
                                               0.08990449458360672,
                                               0.08990985155105591,
                                               0.09001431614160538,
                                               0.0895829126238823,
                                               0.09010770171880722,
                                               0.08968362212181091,
                                               0.09000500291585922,
                                               0.0898640975356102,
                                               0.08954513818025589,
                                               123260808.0,
                                               0.09009537845849991,
                                               0.08999543637037277,
                                               0.09009146690368652,
                                               0.08993417024612427,
                                               0.08979174494743347,
                                               0.09024576097726822,
                                               0.08982154726982117,
                                               0.09024430066347122,
                                               0.08962235599756241,
                                               0.08915040642023087,
                                               0.08879758417606354,
                                               0.08888117223978043,
                                               0.08869042247533798,
                                               0.08853305876255035,
                                               0.08868934214115143,
                                               0.08893421292304993,
                                               0.0886133685708046,
                                               0.08817695081233978,
                                               0.0881962701678276,
                                               0.08811213821172714,
                                               0.0891452506184578,
                                               0.08935011923313141,
                                               0.08934961259365082,
                                               0.08890455216169357,
                                               0.08886080235242844,
                                               0.0891081690788269,
                                               0.08976045250892639,
                                               0.08954772353172302,
                                               0.08939021825790405,
                                               0.08975906670093536,
                                               0.08977613598108292,
                                               0.0896851047873497,
                                               0.08977438509464264,
                                               0.09006603807210922,
                                               0.08980820327997208,
                                               0.08988718688488007,
                                               0.08971526473760605,
                                               0.09004899114370346,
                                               0.0901557132601738,
                                               0.08969920128583908,
                                               0.09032884240150452,
                                               0.08998294174671173,
                                               0.08977649360895157,
                                               0.09020925313234329,
                                               0.09029025584459305,
                                               0.08976597338914871,
                                               0.08999000489711761,
                                               0.08985752612352371,
                                               0.09009544551372528,
                                               0.08979445695877075,
                                               0.08981890231370926,
                                               0.08993826806545258,
                                               0.09029476344585419,
                                               0.0901898518204689,
                                               0.0904805064201355,
                                               0.0902039036154747,
                                               0.09018836915493011,
                                               0.09021788835525513,
                                               13.298559188842773,
                                               0.09035386145114899,
                                               0.08994783461093903,
                                               0.08987398445606232,
                                               0.0898030549287796,
                                               0.09018076211214066,
                                               0.08989060670137405,
                                               0.0900542363524437,
                                               0.08998247236013412,
                                               0.08996569365262985,
                                               0.08956349641084671,
                                               0.0899026170372963,
                                               0.08934273570775986,
                                               0.08953327685594559,
                                               0.08896757662296295,
                                               40.889766693115234,
                                               0.0892421156167984,
                                               0.09008168429136276,
                                               0.0901041328907013,
                                               0.08966265618801117,
                                               0.08987640589475632,
                                               0.09025770425796509,
                                               0.0899430364370346,
                                               0.09005750715732574,
                                               0.09002253413200378,
                                               0.08991013467311859,
                                               0.08955400437116623,
                                               0.09024599194526672,
                                               0.08989398926496506,
                                               0.09036830812692642,
                                               0.09003657847642899,
                                               0.09000864624977112,
                                               0.09005343914031982,
                                               0.09015601873397827,
                                               0.09009037166833878,
                                               0.09033283591270447,
                                               0.08988620340824127,
                                               0.09028229117393494,
                                               0.09019381552934647,
                                               0.0902322456240654,
                                               0.09032434225082397,
                                               0.09011644124984741,
                                               0.08976441621780396,
                                               0.08991662412881851,
                                               0.08978656679391861,
                                               0.08997219800949097,
                                               0.09036137908697128,
                                               0.08971432596445084,
                                               0.08993349224328995,
                                               0.08978254348039627,
                                               0.09011665731668472,
                                               0.0903627872467041,
                                               0.09019304066896439,
                                               0.08973104506731033,
                                               0.09010143578052521,
                                               0.09001858532428741,
                                               0.08998378366231918,
                                               0.09022297710180283,
                                               0.09093927592039108,
                                               0.08909867703914642,
                                               0.08914345502853394,
                                               0.08835233747959137,
                                               0.0882631167769432,
                                               0.08950014412403107,
                                               0.08979093283414841,
                                               0.09012936055660248,
                                               0.08974586427211761,
                                               0.09011997282505035,
                                               0.08974523842334747,
                                               0.09000830352306366,
                                               0.09017462283372879,
                                               0.09004103392362595,
                                               0.08963128179311752,
                                               0.08985885232686996,
                                               0.08983511477708817,
                                               0.09008574485778809,
                                               0.09016025811433792,
                                               0.08972913771867752,
                                               0.08963147550821304,
                                               0.08982467651367188,
                                               0.08994393795728683,
                                               0.08989205211400986,
                                               0.08967658132314682,
                                               0.08987260609865189,
                                               0.09007982909679413,
                                               0.09010133147239685,
                                               0.09020086377859116,
                                               0.09046005457639694,
                                               0.08994335681200027,
                                               0.09018530696630478,
                                               0.08992722630500793,
                                               0.09044454246759415,
                                               0.09027785807847977,
                                               0.08971437811851501,
                                               0.08930660039186478,
                                               0.09013891965150833,
                                               0.08999187499284744,
                                               0.08977315574884415,
                                               0.08965734392404556,
                                               0.08979973196983337,
                                               0.09008533507585526,
                                               0.08993113040924072,
                                               0.08979012817144394,
                                               0.08988290280103683,
                                               0.09004655480384827,
                                               0.09017106890678406,
                                               0.089727021753788,
                                               0.08970198780298233,
                                               0.09002815186977386,
                                               0.08984025567770004,
                                               0.09001448005437851,
                                               0.08957063406705856,
                                               0.08992192894220352,
                                               0.09023972600698471,
                                               0.09008713066577911,
                                               0.0900142714381218,
                                               0.08985602855682373,
                                               0.0900145098567009],
                          'weight_decay': 0.0005},
                 'macroF1': {'accuracy': 2.0,
                             'batch_size': 32,
                             'cv_score': 0.017618830126676397,
                             'cv_val_accuracy': 0.7777777777777778,
                             'cv_val_loss': 0.09876688073078792,
                             'cv_val_macroF1': 0.017618830126676397,
                             'cv_val_microF1': 0.07673131407953461,
                             'epochs': 300,
                             'final_model': GGNN1(
  (ggnn): GatedGraphConv(30, num_layers=2)
  (fc1): Linear(in_features=30, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                             'kwargs': {'aggr_type': 'mean',
                                        'd1': 30,
                                        'd2': 50,
                                        'num_classes': 24,
                                        'num_layers': 2},
                             'learning_rate': 0.01,
                             'macroF1': 0.006748704489763125,
                             'microF1': 0.08414023372287145,
                             'model': <class 'TFM_graph_classification_models.GGNN1'>,
                             'model_instance': GGNN1(
  (ggnn): GatedGraphConv(30, num_layers=2)
  (fc1): Linear(in_features=30, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                             'score': 'f1_macro',
                             'time': 3923.382642507553,
                             'train_loss_history': [402.3915100097656,
                                                    0.627553403377533,
                                                    0.1610880345106125,
                                                    200590368.0,
                                                    127.40691375732422,
                                                    0.0838785469532013,
                                                    0.08383877575397491,
                                                    0.08405112475156784,
                                                    0.08400005102157593,
                                                    0.11704819649457932,
                                                    315.0480651855469,
                                                    8.739550590515137,
                                                    0.26758015155792236,
                                                    150.98406982421875,
                                                    1.1856788396835327,
                                                    1248861440.0,
                                                    2.6898114681243896,
                                                    0.08465886861085892,
                                                    0.08455833047628403,
                                                    0.08453904837369919,
                                                    0.08463430404663086,
                                                    0.08893854171037674,
                                                    2255.366943359375,
                                                    5255256.5,
                                                    0.0845518633723259,
                                                    0.08456744998693466,
                                                    0.0846351757645607,
                                                    0.08459139615297318,
                                                    0.0845951959490776,
                                                    0.08459458500146866,
                                                    741.9649047851562,
                                                    375.397216796875,
                                                    0.08453738689422607,
                                                    1314.5992431640625,
                                                    0.08459823578596115,
                                                    2760.76708984375,
                                                    0.08449413627386093,
                                                    0.0844498947262764,
                                                    0.08453848212957382,
                                                    0.08451221138238907,
                                                    0.08967375755310059,
                                                    0.21154144406318665,
                                                    6485.376953125,
                                                    123632.40625,
                                                    29165.78125,
                                                    38.000274658203125,
                                                    0.08432421088218689,
                                                    0.08440204709768295,
                                                    0.08437246084213257,
                                                    31.02018928527832,
                                                    0.08415640890598297,
                                                    4.811329364776611,
                                                    0.08450804650783539,
                                                    0.08431932330131531,
                                                    0.0844046100974083,
                                                    1.6224708557128906,
                                                    0.08452630043029785,
                                                    0.08453928679227829,
                                                    0.0845048725605011,
                                                    0.0844370573759079,
                                                    0.08444765210151672,
                                                    0.08450065553188324,
                                                    0.08445669710636139,
                                                    0.08438482880592346,
                                                    0.08415114134550095,
                                                    0.08441533893346786,
                                                    0.08437883108854294,
                                                    0.08448990434408188,
                                                    0.08448030054569244,
                                                    0.0844801515340805,
                                                    0.08438432216644287,
                                                    0.08433922380208969,
                                                    0.08432400226593018,
                                                    0.08451466262340546,
                                                    0.08452446758747101,
                                                    0.08453328907489777,
                                                    0.08448510617017746,
                                                    0.08447270840406418,
                                                    0.08449146896600723,
                                                    0.08447691053152084,
                                                    0.08443857729434967,
                                                    0.08449263125658035,
                                                    0.08442482352256775,
                                                    0.08449450135231018,
                                                    0.0844353586435318,
                                                    0.08452147245407104,
                                                    0.0844416618347168,
                                                    0.08446303755044937,
                                                    0.08447965979576111,
                                                    0.0843501091003418,
                                                    0.08422920107841492,
                                                    0.08444242179393768,
                                                    0.08447030931711197,
                                                    0.08448900282382965,
                                                    0.08450789749622345,
                                                    0.08435028791427612,
                                                    0.08434262871742249,
                                                    0.08449234813451767,
                                                    0.08449122309684753,
                                                    0.08449383825063705,
                                                    0.08452076464891434,
                                                    0.08449948579072952,
                                                    0.08440456539392471,
                                                    0.08449050039052963,
                                                    0.0841154232621193,
                                                    2.2428197860717773,
                                                    0.08384676277637482,
                                                    0.08394981920719147,
                                                    0.0838002935051918,
                                                    0.08447454124689102,
                                                    0.08348763734102249,
                                                    0.08416859805583954,
                                                    0.08417699486017227,
                                                    0.08438660949468613,
                                                    0.4162343144416809,
                                                    0.08450004458427429,
                                                    0.08446227014064789,
                                                    0.08444914221763611,
                                                    0.0843791589140892,
                                                    0.08446182310581207,
                                                    0.08455074578523636,
                                                    0.08446145057678223,
                                                    0.0836617723107338,
                                                    0.08338803797960281,
                                                    0.08333099633455276,
                                                    0.08318332582712173,
                                                    0.08332567662000656,
                                                    0.08302734792232513,
                                                    0.08371946960687637,
                                                    0.08449801802635193,
                                                    0.08449846506118774,
                                                    0.08426801860332489,
                                                    0.08432383090257645,
                                                    200335072.0,
                                                    0.08437470346689224,
                                                    0.08452057838439941,
                                                    0.08447948843240738,
                                                    0.08449156582355499,
                                                    0.08443012088537216,
                                                    0.0844721645116806,
                                                    0.08447398245334625,
                                                    0.08443788439035416,
                                                    0.08443618565797806,
                                                    0.08449234068393707,
                                                    0.10008276253938675,
                                                    0.08434968441724777,
                                                    0.08430284261703491,
                                                    0.08442266285419464,
                                                    0.08438229560852051,
                                                    1806.7781982421875,
                                                    0.08451782912015915,
                                                    0.0844816267490387,
                                                    13.898186683654785,
                                                    0.08445671200752258,
                                                    1.4685406684875488,
                                                    4.019253253936768,
                                                    302.15570068359375,
                                                    0.08452155441045761,
                                                    0.14563511312007904,
                                                    27.54737663269043,
                                                    3.7263739109039307,
                                                    78.2233657836914,
                                                    0.08451861888170242,
                                                    0.08448174595832825,
                                                    407.52105712890625,
                                                    19.42632484436035,
                                                    0.08444830775260925,
                                                    36449228.0,
                                                    7.751605033874512,
                                                    0.08446042984724045,
                                                    0.08437091112136841,
                                                    3167184.25,
                                                    0.08450077474117279,
                                                    0.08428910374641418,
                                                    0.08419448137283325,
                                                    0.08466444909572601,
                                                    5.597066402435303,
                                                    0.11012237519025803,
                                                    0.08447238802909851,
                                                    58.212467193603516,
                                                    5.655828952789307,
                                                    3588.997314453125,
                                                    0.409007728099823,
                                                    32.17847442626953,
                                                    0.08447328209877014,
                                                    0.0845048725605011,
                                                    0.08449171483516693,
                                                    0.08447052538394928,
                                                    0.08445976674556732,
                                                    0.08446221798658371,
                                                    0.08458263427019119,
                                                    0.08430005609989166,
                                                    8.114312171936035,
                                                    0.08432270586490631,
                                                    0.08445460349321365,
                                                    0.08443901687860489,
                                                    900.8036499023438,
                                                    0.08444087207317352,
                                                    0.08441809564828873,
                                                    0.4795038402080536,
                                                    0.08451033383607864,
                                                    0.08451204001903534,
                                                    0.08450926095247269,
                                                    0.08447223901748657,
                                                    0.08437959104776382,
                                                    0.08444824069738388,
                                                    0.08453644067049026,
                                                    7.690800666809082,
                                                    0.08448299020528793,
                                                    0.08449721336364746,
                                                    0.08445368707180023,
                                                    0.08438749611377716,
                                                    0.08451572060585022,
                                                    0.08452669531106949,
                                                    89.34236907958984,
                                                    0.08450479805469513,
                                                    30.75047492980957,
                                                    0.08451566100120544,
                                                    0.08449652045965195,
                                                    68.70880126953125,
                                                    0.08445856720209122,
                                                    12.459997177124023,
                                                    0.08449093997478485,
                                                    0.08446609228849411,
                                                    0.08448096364736557,
                                                    0.11521673947572708,
                                                    0.08448649197816849,
                                                    1010.2301025390625,
                                                    0.08451484888792038,
                                                    0.0844508707523346,
                                                    0.0843883827328682,
                                                    0.08444556593894958,
                                                    182.40513610839844,
                                                    0.08444695174694061,
                                                    304.5606689453125,
                                                    0.0844317376613617,
                                                    0.08447675406932831,
                                                    0.08446475118398666,
                                                    0.08447200804948807,
                                                    0.08442620933055878,
                                                    0.08441336452960968,
                                                    0.08443718403577805,
                                                    0.08444596081972122,
                                                    0.0845516175031662,
                                                    0.08445774763822556,
                                                    37.44003677368164,
                                                    0.0844288170337677,
                                                    0.08449657261371613,
                                                    0.08451525121927261,
                                                    0.08444896340370178,
                                                    0.08451040834188461,
                                                    0.08449771255254745,
                                                    0.0844845324754715,
                                                    0.08452331274747849,
                                                    0.08448804169893265,
                                                    0.08449441939592361,
                                                    0.08447355777025223,
                                                    217506.6875,
                                                    0.08448262512683868,
                                                    27566.361328125,
                                                    0.084529809653759,
                                                    60.93157958984375,
                                                    0.0844476968050003,
                                                    0.08442551642656326,
                                                    81.24128723144531,
                                                    0.08442741632461548,
                                                    0.08438073843717575,
                                                    0.08440481126308441,
                                                    0.08442452549934387,
                                                    0.08434814214706421,
                                                    11.347413063049316,
                                                    26.29210662841797,
                                                    0.23023147881031036,
                                                    0.08447755873203278,
                                                    0.08438970148563385,
                                                    0.08435468375682831,
                                                    0.08439872413873672,
                                                    0.08438412100076675,
                                                    0.08440207690000534,
                                                    0.0844302773475647,
                                                    11.261951446533203,
                                                    0.08435912430286407,
                                                    3.4124855995178223,
                                                    0.08436176180839539,
                                                    0.08430063724517822,
                                                    0.08941613137722015,
                                                    0.08446674793958664,
                                                    0.08433955907821655,
                                                    0.08443570137023926,
                                                    0.08435171097517014,
                                                    21675608.0,
                                                    0.08440378308296204,
                                                    0.08433499932289124,
                                                    0.40477755665779114,
                                                    0.08435443788766861,
                                                    0.08426611125469208,
                                                    13186.3408203125,
                                                    39600.96875,
                                                    0.08453357219696045,
                                                    0.9381150603294373,
                                                    0.09587060660123825,
                                                    0.514412522315979,
                                                    0.09367242455482483,
                                                    0.09441542625427246,
                                                    0.09452634304761887,
                                                    0.09454908967018127,
                                                    22.32537269592285,
                                                    32339.833984375,
                                                    0.09451416879892349,
                                                    0.09449652582406998,
                                                    0.09464240074157715,
                                                    0.09449930489063263,
                                                    1958.1798095703125,
                                                    0.09455957263708115,
                                                    0.09456851333379745,
                                                    1.2811931371688843,
                                                    0.09453321993350983,
                                                    0.09458591043949127,
                                                    0.09450304508209229,
                                                    5.49962043762207,
                                                    72.5165023803711,
                                                    0.09458471089601517,
                                                    0.0945429876446724,
                                                    0.09455682337284088,
                                                    0.09453796595335007,
                                                    0.09452159702777863,
                                                    2.5312438011169434,
                                                    0.0945226177573204,
                                                    0.0944872573018074,
                                                    5.223100185394287,
                                                    2137.731201171875,
                                                    0.142483189702034,
                                                    0.20935583114624023,
                                                    0.09459515661001205,
                                                    0.09455345571041107,
                                                    2812.721435546875,
                                                    0.09442364424467087,
                                                    0.1131102666258812,
                                                    0.09442222863435745,
                                                    0.09438570588827133,
                                                    0.09433407336473465,
                                                    0.13149750232696533,
                                                    0.09370430558919907,
                                                    0.09366568177938461,
                                                    0.09354622662067413,
                                                    0.09447423368692398,
                                                    0.6216050982475281,
                                                    0.09451036900281906,
                                                    0.09451115876436234,
                                                    1349.734619140625,
                                                    0.09453874081373215,
                                                    0.11682190001010895,
                                                    0.10219868272542953,
                                                    0.09455303847789764,
                                                    0.0945776104927063,
                                                    0.09455080330371857,
                                                    2.9681241512298584,
                                                    0.09452127665281296,
                                                    0.09452573955059052,
                                                    29.074121475219727,
                                                    14.531797409057617,
                                                    0.09461171925067902,
                                                    0.09457170218229294,
                                                    0.09453096240758896,
                                                    0.7046006321907043,
                                                    0.33546608686447144,
                                                    0.09435248374938965,
                                                    0.09443413466215134,
                                                    0.31541207432746887,
                                                    0.14433683454990387,
                                                    0.09446719288825989,
                                                    3715.66796875,
                                                    0.0944618359208107,
                                                    0.09454601258039474,
                                                    0.0945052057504654,
                                                    0.09456264227628708,
                                                    0.09456627815961838,
                                                    1.2233333587646484,
                                                    0.09454774111509323,
                                                    249.55780029296875,
                                                    181.83404541015625,
                                                    154.2233123779297,
                                                    0.09459660947322845,
                                                    0.09442275762557983,
                                                    0.09446389973163605,
                                                    0.09459325671195984,
                                                    0.09457375854253769,
                                                    0.0945519208908081,
                                                    0.09459152817726135,
                                                    0.0945110023021698,
                                                    0.7131268978118896,
                                                    0.09457242488861084,
                                                    13.699750900268555,
                                                    0.09455432742834091,
                                                    0.09460796415805817,
                                                    0.09450221061706543,
                                                    11696.2060546875,
                                                    0.09452937543392181,
                                                    0.09452711045742035,
                                                    0.09458191692829132,
                                                    9.455473899841309,
                                                    0.09451385587453842,
                                                    0.09448845684528351,
                                                    131.53732299804688,
                                                    4293.73291015625,
                                                    0.09409381449222565,
                                                    0.09368418902158737,
                                                    0.09371126443147659,
                                                    0.09354376047849655,
                                                    0.2533681094646454,
                                                    0.314374715089798,
                                                    0.09430520236492157,
                                                    0.09440848976373672,
                                                    0.094419464468956,
                                                    0.09437459707260132,
                                                    0.09439592063426971,
                                                    1767081.0,
                                                    594.7551879882812,
                                                    0.09452971816062927,
                                                    0.09449964016675949,
                                                    0.09450231492519379,
                                                    0.09459840506315231,
                                                    0.09455262124538422,
                                                    0.09447214007377625,
                                                    0.0944393202662468,
                                                    0.09440606832504272,
                                                    0.09447411447763443,
                                                    0.09437554329633713,
                                                    0.09433301538228989,
                                                    0.09440498054027557,
                                                    0.10780218243598938,
                                                    0.09436662495136261,
                                                    0.09431613236665726,
                                                    0.0945611521601677,
                                                    219.7027587890625,
                                                    0.4978674352169037,
                                                    0.0944586768746376,
                                                    0.30992811918258667,
                                                    1.8426533937454224,
                                                    0.5712193250656128,
                                                    0.09446409344673157,
                                                    0.09459622949361801,
                                                    0.09451809525489807,
                                                    87.38387298583984,
                                                    0.09456565976142883,
                                                    3497.284423828125,
                                                    0.09454844892024994,
                                                    0.09451253712177277,
                                                    0.09457314759492874,
                                                    0.09456212818622589,
                                                    0.15045373141765594,
                                                    0.14893552660942078,
                                                    562.9702758789062,
                                                    131.55374145507812,
                                                    0.09452853351831436,
                                                    0.09735272079706192,
                                                    0.09456931799650192,
                                                    0.6130989193916321,
                                                    0.09453856199979782,
                                                    0.09444250911474228,
                                                    0.09452873468399048,
                                                    0.09453690052032471,
                                                    1.229264736175537,
                                                    0.09449880570173264,
                                                    820.4354248046875,
                                                    1325.2156982421875,
                                                    0.09444205462932587,
                                                    0.0945630744099617,
                                                    0.09459412842988968,
                                                    0.0945231169462204,
                                                    513502.1875,
                                                    0.09456072747707367,
                                                    1.1679768562316895,
                                                    0.0945015400648117,
                                                    0.09457283467054367,
                                                    0.09455496817827225,
                                                    0.19079294800758362,
                                                    36.89104461669922,
                                                    0.0944361463189125,
                                                    0.09443370997905731,
                                                    0.09447237849235535,
                                                    20771922.0,
                                                    0.09442625194787979,
                                                    0.1135120838880539,
                                                    0.09445078670978546,
                                                    0.09434980154037476,
                                                    0.09435220062732697,
                                                    0.35446515679359436,
                                                    0.09654965996742249,
                                                    0.09453532844781876,
                                                    0.09456611424684525,
                                                    32323.8828125,
                                                    978.3565063476562,
                                                    0.09457312524318695,
                                                    0.0945013165473938,
                                                    0.09439229965209961,
                                                    4412095.5,
                                                    0.09448060393333435,
                                                    0.09440262615680695,
                                                    0.0943794921040535,
                                                    0.09443134814500809,
                                                    369.93389892578125,
                                                    0.14076443016529083,
                                                    0.09438452124595642,
                                                    0.09434284269809723,
                                                    0.10136646777391434,
                                                    0.09443105757236481,
                                                    0.09402314573526382,
                                                    26166.89453125,
                                                    0.09349802136421204,
                                                    3.321302652359009,
                                                    0.09364969283342361,
                                                    0.09496324509382248,
                                                    0.09372551739215851,
                                                    0.09436341375112534,
                                                    0.09432327002286911,
                                                    0.09436825662851334,
                                                    0.0944887325167656,
                                                    0.09451030939817429,
                                                    0.09454188495874405,
                                                    0.0945427343249321,
                                                    0.09454578906297684,
                                                    0.09454236924648285,
                                                    0.09450600296258926,
                                                    0.09452961385250092,
                                                    0.09452560544013977,
                                                    0.09452951699495316,
                                                    2834032.25,
                                                    0.09452395886182785,
                                                    10.720794677734375,
                                                    68.72327423095703,
                                                    8477249.0,
                                                    0.09440808743238449,
                                                    5520.7861328125,
                                                    0.09362620860338211,
                                                    0.09349146485328674,
                                                    0.09339573979377747,
                                                    149.35525512695312,
                                                    278.6841735839844,
                                                    0.0943543016910553,
                                                    37.89771270751953,
                                                    5100427.5,
                                                    0.09433344006538391,
                                                    6600393.5,
                                                    0.09443498402833939,
                                                    0.09448225051164627,
                                                    0.09439776837825775,
                                                    0.09439967572689056,
                                                    0.09428673982620239,
                                                    0.0945155993103981,
                                                    0.27247631549835205,
                                                    0.09454277157783508,
                                                    0.09456893801689148,
                                                    0.09452944248914719,
                                                    0.09454522281885147,
                                                    0.09451722353696823,
                                                    0.09460891038179398,
                                                    0.09455429017543793,
                                                    10244.599609375,
                                                    84.07907104492188,
                                                    0.0944337323307991,
                                                    0.09459497779607773,
                                                    0.09454827755689621,
                                                    0.09453362226486206,
                                                    0.09449025988578796,
                                                    0.09453912824392319,
                                                    0.3738631308078766,
                                                    0.09447181224822998,
                                                    0.09451087564229965,
                                                    0.09459631890058517,
                                                    0.09454839676618576,
                                                    0.09458976984024048,
                                                    0.09457769244909286,
                                                    0.09456919878721237,
                                                    3.4914751052856445,
                                                    1427.79345703125,
                                                    0.4578329920768738,
                                                    884.7360229492188,
                                                    0.09449641406536102,
                                                    0.09440287202596664,
                                                    510.2096862792969,
                                                    0.09450377523899078,
                                                    0.09451079368591309,
                                                    2.1641271114349365,
                                                    0.0944771096110344,
                                                    25.285322189331055,
                                                    0.09460421651601791,
                                                    0.09461460262537003,
                                                    21697.78125,
                                                    0.11157756298780441,
                                                    9.951555252075195,
                                                    0.09457164257764816,
                                                    0.09458516538143158,
                                                    770.6202392578125,
                                                    0.5892700552940369,
                                                    0.09393339604139328,
                                                    0.09381875395774841,
                                                    0.09371479600667953,
                                                    0.09378113597631454,
                                                    0.0938304141163826,
                                                    0.09431172162294388,
                                                    1034.4150390625,
                                                    538.9066772460938,
                                                    0.09464322030544281,
                                                    0.7673370242118835,
                                                    0.18193259835243225,
                                                    0.09461429715156555,
                                                    0.09462171792984009,
                                                    0.6443932056427002,
                                                    0.09465876966714859,
                                                    0.09471260756254196,
                                                    0.09465605765581131,
                                                    0.09452185034751892,
                                                    0.09450294822454453,
                                                    0.12810559570789337,
                                                    0.094684898853302,
                                                    8.992913246154785,
                                                    0.8569191694259644,
                                                    582.5096435546875,
                                                    0.09462642669677734,
                                                    0.09460807591676712,
                                                    0.09465175122022629,
                                                    0.0958004742860794,
                                                    0.09474059194326401,
                                                    0.09462853521108627,
                                                    657.1932373046875,
                                                    0.19906458258628845,
                                                    0.09392101317644119,
                                                    0.09379742294549942,
                                                    0.7946452498435974,
                                                    0.1725653111934662,
                                                    0.09447696059942245,
                                                    0.09447885304689407,
                                                    0.09469438344240189,
                                                    0.09465198963880539,
                                                    0.09460066258907318,
                                                    560.6574096679688,
                                                    0.09460125863552094,
                                                    0.0946880578994751,
                                                    0.09468343108892441,
                                                    0.09466951340436935,
                                                    7.846862316131592,
                                                    0.09464780241250992,
                                                    1.8931728601455688,
                                                    0.0939062237739563,
                                                    0.14714713394641876,
                                                    0.09371401369571686,
                                                    0.14943532645702362,
                                                    9.442521095275879,
                                                    0.09458307921886444,
                                                    0.09459754824638367,
                                                    0.19837377965450287,
                                                    0.18834809958934784,
                                                    0.09383810311555862,
                                                    0.09375286102294922,
                                                    0.0936644896864891,
                                                    0.09361999481916428,
                                                    0.0935748741030693,
                                                    1.445277214050293,
                                                    0.0944688618183136,
                                                    12.287859916687012,
                                                    0.09468797594308853,
                                                    0.09469488263130188,
                                                    0.09460695087909698,
                                                    0.7720686197280884,
                                                    0.0945553258061409,
                                                    0.09460161626338959,
                                                    0.09466303139925003,
                                                    0.09467338770627975,
                                                    0.09465459734201431,
                                                    0.09461504220962524,
                                                    0.0945778414607048,
                                                    0.09464619308710098,
                                                    0.09465964883565903,
                                                    0.09464212507009506,
                                                    229.66256713867188,
                                                    18.826688766479492,
                                                    0.09455174207687378,
                                                    0.09412211924791336,
                                                    17504.03515625,
                                                    0.09471108019351959,
                                                    0.09461493790149689,
                                                    10.089583396911621,
                                                    0.09461957961320877,
                                                    31.376590728759766,
                                                    0.09470537304878235,
                                                    0.09463783353567123,
                                                    5610218.0,
                                                    0.09464839100837708,
                                                    0.09463448077440262,
                                                    0.0942724272608757,
                                                    0.09408007562160492,
                                                    0.48262032866477966,
                                                    0.09465523064136505,
                                                    2.4015886783599854,
                                                    0.09462562948465347,
                                                    142063.546875,
                                                    0.09455953538417816,
                                                    1.974007487297058,
                                                    0.09464676678180695,
                                                    0.09465840458869934,
                                                    0.10140714049339294,
                                                    0.09454220533370972,
                                                    0.09462330490350723,
                                                    0.09451285749673843,
                                                    4.050822734832764,
                                                    0.09465648978948593,
                                                    0.09463375806808472,
                                                    0.09465823322534561,
                                                    0.09469760209321976,
                                                    0.09466546028852463,
                                                    932.041259765625,
                                                    0.09463147073984146,
                                                    3.409330129623413,
                                                    54.3207893371582,
                                                    0.09465781599283218,
                                                    0.09465416520833969,
                                                    1.7536137104034424,
                                                    0.09468229115009308,
                                                    84.98772430419922,
                                                    0.09475217014551163,
                                                    0.09401313960552216,
                                                    134.52792358398438,
                                                    0.09469401836395264,
                                                    0.23205254971981049,
                                                    0.0946466401219368,
                                                    0.09470997005701065,
                                                    0.09463036060333252,
                                                    0.09456641972064972,
                                                    0.09461867809295654,
                                                    0.9174342155456543,
                                                    0.09432287514209747,
                                                    3.919257879257202,
                                                    0.09378696233034134,
                                                    0.09345461428165436,
                                                    0.09351752698421478,
                                                    0.09323997050523758,
                                                    0.09312094748020172,
                                                    0.09307726472616196,
                                                    0.09321693331003189,
                                                    0.09316740185022354,
                                                    0.09299517422914505,
                                                    0.09285470098257065,
                                                    0.2572370171546936,
                                                    0.09311380237340927,
                                                    0.09293543547391891,
                                                    0.09303213655948639,
                                                    0.1493358463048935,
                                                    0.09310180693864822,
                                                    0.09321630746126175,
                                                    10.437809944152832,
                                                    0.0943022146821022,
                                                    0.09423695504665375,
                                                    2590.866943359375,
                                                    1.4054656028747559,
                                                    38.2252311706543,
                                                    19227.08203125,
                                                    0.09473899751901627,
                                                    0.09466087073087692,
                                                    0.0946071520447731,
                                                    830.8789672851562,
                                                    0.0945894792675972,
                                                    2.7011053562164307,
                                                    0.504011332988739,
                                                    0.09468135982751846,
                                                    0.09465712308883667,
                                                    0.0945027694106102,
                                                    3.204328775405884,
                                                    0.09469970315694809,
                                                    136.45835876464844,
                                                    0.0946570485830307,
                                                    276.604248046875,
                                                    0.12033631652593613,
                                                    0.09448137134313583,
                                                    1677058.875,
                                                    0.09450552612543106,
                                                    6.231417179107666,
                                                    0.4628032147884369,
                                                    0.12920531630516052,
                                                    0.5548384189605713,
                                                    0.09467286616563797,
                                                    0.09465939551591873,
                                                    0.09464029967784882,
                                                    0.09545759111642838,
                                                    0.09464362263679504,
                                                    1.4605083465576172,
                                                    0.6265268921852112,
                                                    0.13141100108623505,
                                                    0.09462108463048935,
                                                    0.09455641359090805,
                                                    5.9682111740112305,
                                                    0.09459859877824783,
                                                    32.384986877441406,
                                                    0.09444091469049454,
                                                    0.09422245621681213,
                                                    0.0940980538725853,
                                                    0.09372019022703171,
                                                    0.09348495304584503,
                                                    0.09494473040103912,
                                                    0.09471526741981506,
                                                    0.09466321766376495,
                                                    0.09463952481746674,
                                                    0.0946340337395668,
                                                    0.0946597084403038,
                                                    0.10840669274330139,
                                                    0.09467717260122299,
                                                    0.09460677951574326,
                                                    0.09988532215356827,
                                                    0.09422221034765244,
                                                    0.11676912754774094,
                                                    0.09440311044454575,
                                                    45.41225051879883,
                                                    507.40032958984375,
                                                    0.13748902082443237,
                                                    0.09470012038946152,
                                                    226.58145141601562,
                                                    0.4495183825492859,
                                                    0.09465526789426804,
                                                    0.09466543048620224,
                                                    61.091617584228516,
                                                    0.09468193352222443,
                                                    0.09465447813272476,
                                                    0.0945592001080513,
                                                    0.09467066824436188,
                                                    0.09464655071496964,
                                                    141.53970336914062,
                                                    116328.4609375,
                                                    0.09458193182945251,
                                                    0.12122631818056107,
                                                    4.387285232543945,
                                                    2681976.5,
                                                    0.09463132172822952,
                                                    0.0946061760187149,
                                                    0.0944783166050911,
                                                    0.09446098655462265,
                                                    0.09450376033782959,
                                                    0.09455794841051102,
                                                    0.09459281712770462,
                                                    0.09462811797857285,
                                                    6.4233717918396,
                                                    0.09450298547744751,
                                                    0.09456917643547058,
                                                    0.0934096947312355,
                                                    0.0934649258852005,
                                                    0.09316615760326385,
                                                    0.5173411965370178,
                                                    1.9697647094726562,
                                                    32.42613983154297,
                                                    0.09472211450338364,
                                                    0.09469816833734512,
                                                    0.0956822857260704,
                                                    0.09459832310676575,
                                                    0.0946735143661499,
                                                    0.09462333470582962,
                                                    0.09459586441516876,
                                                    1.9096038341522217,
                                                    0.0946708396077156,
                                                    0.0946536511182785,
                                                    0.09466943144798279,
                                                    0.09466858953237534,
                                                    0.09464891254901886,
                                                    0.09463982284069061,
                                                    0.09469345957040787,
                                                    0.0946459099650383,
                                                    0.0945952758193016,
                                                    122.37969970703125,
                                                    4.63157844543457,
                                                    56.718482971191406,
                                                    0.18920323252677917,
                                                    454.80084228515625,
                                                    4036.590087890625,
                                                    0.09464812278747559,
                                                    127179.6875,
                                                    0.09467849880456924,
                                                    0.09463875740766525,
                                                    0.09431662410497665,
                                                    665.6954956054688,
                                                    0.09449782967567444,
                                                    0.09459271281957626,
                                                    0.09455017000436783,
                                                    2.904323101043701,
                                                    0.09459733217954636,
                                                    0.0946681872010231,
                                                    0.09470684081315994,
                                                    3.52655029296875,
                                                    0.09464657306671143,
                                                    0.09463555365800858,
                                                    1.8135828971862793,
                                                    0.09465093165636063,
                                                    0.0946563184261322,
                                                    0.09455125033855438,
                                                    0.09466362744569778,
                                                    0.0946488231420517,
                                                    2.0649492740631104,
                                                    1.0428603887557983,
                                                    0.09464512020349503,
                                                    0.09463749080896378,
                                                    0.09463761001825333,
                                                    204.697509765625,
                                                    0.09467710554599762],
                             'val_accuracy_history': [1.7894736842105263,
                                                      1.368421052631579,
                                                      0.3157894736842105,
                                                      1.0526315789473684,
                                                      1.0,
                                                      0.42105263157894735,
                                                      0.6842105263157895,
                                                      1.4210526315789473,
                                                      0.10526315789473684,
                                                      0.3157894736842105,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      3.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      3.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      2.1578947368421053,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      3.0,
                                                      0.0,
                                                      1.0,
                                                      1.2105263157894737,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      0.5263157894736842,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      4.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      3.0,
                                                      1.0,
                                                      0.631578947368421,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      1.631578947368421,
                                                      1.8421052631578947,
                                                      0.0,
                                                      2.0,
                                                      0.9473684210526315,
                                                      0.47368421052631576,
                                                      0.7894736842105263,
                                                      0.05263157894736842,
                                                      3.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.2105263157894737,
                                                      0.8421052631578947,
                                                      0.21052631578947367,
                                                      0.9473684210526315,
                                                      1.5263157894736843,
                                                      1.0526315789473684,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      3.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.6842105263157894,
                                                      1.9473684210526316,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      3.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      3.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      3.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      3.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      3.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      3.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.7777777777777778,
                                                      0.1111111111111111,
                                                      0.8888888888888888,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.7777777777777778,
                                                      0.7777777777777778,
                                                      0.7777777777777778,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      3.0,
                                                      1.0,
                                                      2.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.1111111111111111,
                                                      0.4444444444444444,
                                                      0.8888888888888888,
                                                      0.2222222222222222,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      2.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      3.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.8888888888888888,
                                                      0.5555555555555556,
                                                      0.7777777777777778,
                                                      1.0,
                                                      0.2222222222222222,
                                                      0.3333333333333333,
                                                      0.1111111111111111,
                                                      0.3333333333333333,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      3.0,
                                                      1.0,
                                                      0.8888888888888888,
                                                      1.0,
                                                      0.8888888888888888,
                                                      0.0,
                                                      2.6666666666666665,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      1.7777777777777777,
                                                      0.0,
                                                      3.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      3.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      3.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      4.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.8888888888888888,
                                                      0.4444444444444444,
                                                      0.6666666666666666,
                                                      0.1111111111111111,
                                                      0.3333333333333333,
                                                      1.0,
                                                      3.0,
                                                      2.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      3.0,
                                                      0.0,
                                                      3.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      4.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      3.0,
                                                      3.0,
                                                      1.0,
                                                      1.0,
                                                      2.0526315789473686,
                                                      1.736842105263158,
                                                      2.9473684210526314,
                                                      1.894736842105263,
                                                      4.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      3.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      2.0,
                                                      0.3684210526315789,
                                                      2.6315789473684212,
                                                      2.526315789473684,
                                                      1.894736842105263,
                                                      4.0,
                                                      2.0,
                                                      3.0,
                                                      1.0,
                                                      2.0,
                                                      3.0526315789473686,
                                                      1.263157894736842,
                                                      1.105263157894737,
                                                      0.21052631578947367,
                                                      1.1578947368421053,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      3.0,
                                                      2.0,
                                                      2.0,
                                                      4.0,
                                                      3.0,
                                                      3.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      2.0,
                                                      2.0,
                                                      0.0,
                                                      2.0,
                                                      3.210526315789474,
                                                      0.10526315789473684,
                                                      3.0,
                                                      0.0,
                                                      5.0,
                                                      2.0,
                                                      1.0,
                                                      3.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.2631578947368421,
                                                      0.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      4.0,
                                                      0.0,
                                                      2.0,
                                                      2.0,
                                                      3.0,
                                                      4.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      3.0,
                                                      2.210526315789474,
                                                      3.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.10526315789473684,
                                                      4.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.8421052631578947,
                                                      2.4210526315789473,
                                                      1.4736842105263157,
                                                      0.3157894736842105,
                                                      0.7894736842105263,
                                                      1.6842105263157894,
                                                      1.631578947368421,
                                                      1.9473684210526316,
                                                      1.6842105263157894,
                                                      1.8421052631578947,
                                                      1.894736842105263,
                                                      0.6842105263157895,
                                                      1.0526315789473684,
                                                      0.8947368421052632,
                                                      2.0,
                                                      2.4210526315789473,
                                                      1.4210526315789473,
                                                      0.9473684210526315,
                                                      1.1578947368421053,
                                                      0.0,
                                                      0.9473684210526315,
                                                      1.0,
                                                      4.0,
                                                      4.0,
                                                      2.0,
                                                      3.0,
                                                      1.0,
                                                      2.0,
                                                      2.0,
                                                      0.0,
                                                      2.0,
                                                      3.0,
                                                      2.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      3.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      3.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      3.0,
                                                      3.0,
                                                      1.0,
                                                      0.0,
                                                      3.0,
                                                      2.0,
                                                      2.0,
                                                      0.0,
                                                      2.0,
                                                      0.8947368421052632,
                                                      1.5263157894736843,
                                                      1.6842105263157894,
                                                      0.2631578947368421,
                                                      1.0,
                                                      3.0,
                                                      5.0,
                                                      2.0,
                                                      3.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      2.789473684210526,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      4.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      4.0,
                                                      2.0,
                                                      3.0,
                                                      2.0,
                                                      4.0,
                                                      3.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      1.631578947368421,
                                                      0.21052631578947367,
                                                      1.736842105263158,
                                                      0.9473684210526315,
                                                      1.4736842105263157,
                                                      1.0,
                                                      2.0,
                                                      2.0,
                                                      3.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      4.0,
                                                      2.0,
                                                      3.0,
                                                      3.0,
                                                      4.0,
                                                      2.0,
                                                      3.0,
                                                      0.0,
                                                      2.0,
                                                      3.0,
                                                      1.0,
                                                      2.0,
                                                      3.0,
                                                      2.0,
                                                      2.0,
                                                      1.894736842105263,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      5.0,
                                                      3.0,
                                                      3.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      2.0],
                             'val_loss': 0.0900145098567009,
                             'val_loss_history': [0.13289986550807953,
                                                  30.159862518310547,
                                                  0.1306895911693573,
                                                  772.6478881835938,
                                                  0.12578943371772766,
                                                  0.1313501000404358,
                                                  0.12481862306594849,
                                                  0.12818337976932526,
                                                  0.12867261469364166,
                                                  0.13204559683799744,
                                                  0.1220577135682106,
                                                  0.11260901391506195,
                                                  0.11409381777048111,
                                                  0.11440904438495636,
                                                  0.11434926837682724,
                                                  0.11414065212011337,
                                                  0.11439044773578644,
                                                  0.11462827026844025,
                                                  0.11452741175889969,
                                                  0.11467263847589493,
                                                  0.11460597068071365,
                                                  0.11462745815515518,
                                                  0.11468517035245895,
                                                  0.11480430513620377,
                                                  0.1147732213139534,
                                                  0.11485739797353745,
                                                  0.11479106545448303,
                                                  0.1150507852435112,
                                                  0.11502186954021454,
                                                  0.11493460834026337,
                                                  0.11511021852493286,
                                                  0.11504624783992767,
                                                  0.1150892823934555,
                                                  0.11518291383981705,
                                                  64640324.0,
                                                  0.11498195677995682,
                                                  0.11846525222063065,
                                                  0.11501139402389526,
                                                  0.11516815423965454,
                                                  0.11503072828054428,
                                                  0.11535662412643433,
                                                  0.11514862626791,
                                                  0.11519207060337067,
                                                  0.11522386968135834,
                                                  0.11516549438238144,
                                                  0.11545438319444656,
                                                  0.11529026180505753,
                                                  0.11528454720973969,
                                                  0.11551813781261444,
                                                  0.11532633006572723,
                                                  0.1173066645860672,
                                                  0.11544819921255112,
                                                  0.11565966159105301,
                                                  0.1153019443154335,
                                                  0.11566291004419327,
                                                  0.11555980145931244,
                                                  0.11566717177629471,
                                                  0.11576888710260391,
                                                  0.11573272943496704,
                                                  0.11575499922037125,
                                                  0.11589116603136063,
                                                  0.11577892303466797,
                                                  0.11567796766757965,
                                                  0.11675383150577545,
                                                  0.11572939157485962,
                                                  0.11599055677652359,
                                                  0.11586335301399231,
                                                  0.11595223098993301,
                                                  0.115937240421772,
                                                  0.11586461216211319,
                                                  0.11587101221084595,
                                                  0.1159282699227333,
                                                  0.11588104814291,
                                                  0.11595000326633453,
                                                  0.11601797491312027,
                                                  0.11578642576932907,
                                                  0.1159607544541359,
                                                  0.11591330915689468,
                                                  0.1158139705657959,
                                                  0.1159902960062027,
                                                  0.1159883365035057,
                                                  0.1158863827586174,
                                                  0.11616058647632599,
                                                  0.116038978099823,
                                                  0.11605293303728104,
                                                  0.11608398705720901,
                                                  0.11597417294979095,
                                                  0.11598964035511017,
                                                  0.11528076976537704,
                                                  0.1164831668138504,
                                                  0.11611490696668625,
                                                  0.11596962809562683,
                                                  0.11598684638738632,
                                                  0.11591029167175293,
                                                  0.11605054140090942,
                                                  0.11600024253129959,
                                                  0.1159677505493164,
                                                  0.11592534184455872,
                                                  0.11612578481435776,
                                                  0.11618667840957642,
                                                  0.11620379984378815,
                                                  0.11618492752313614,
                                                  0.11606466770172119,
                                                  0.11596335470676422,
                                                  0.11628828197717667,
                                                  0.1161775216460228,
                                                  0.11668728291988373,
                                                  0.11626330763101578,
                                                  0.11897259205579758,
                                                  0.11721517890691757,
                                                  0.11805533617734909,
                                                  0.11683106422424316,
                                                  0.11628374457359314,
                                                  0.11647655814886093,
                                                  0.11625979095697403,
                                                  0.11615417152643204,
                                                  0.11611239612102509,
                                                  0.11597920954227448,
                                                  0.11626138538122177,
                                                  0.11619382351636887,
                                                  0.11614691466093063,
                                                  0.1158432811498642,
                                                  0.11844415962696075,
                                                  0.12012633681297302,
                                                  0.12201090902090073,
                                                  0.12300722301006317,
                                                  0.12179263681173325,
                                                  0.12022022157907486,
                                                  0.1161445826292038,
                                                  0.11631754785776138,
                                                  0.11599871516227722,
                                                  0.11595051735639572,
                                                  0.11614725738763809,
                                                  0.11601608991622925,
                                                  0.11616994440555573,
                                                  0.11606497317552567,
                                                  0.11608567833900452,
                                                  0.11612219363451004,
                                                  0.11595229059457779,
                                                  0.1160753071308136,
                                                  0.11604484170675278,
                                                  0.11610965430736542,
                                                  0.11596120893955231,
                                                  0.11606322973966599,
                                                  0.1158863827586174,
                                                  0.1162295863032341,
                                                  0.11610893160104752,
                                                  0.11617756634950638,
                                                  0.11616548895835876,
                                                  0.11614851653575897,
                                                  0.11626169085502625,
                                                  0.11611983180046082,
                                                  0.116350457072258,
                                                  0.1160777285695076,
                                                  0.11605817079544067,
                                                  0.1161554679274559,
                                                  0.1161135733127594,
                                                  0.11618547886610031,
                                                  0.11609655618667603,
                                                  0.11594346165657043,
                                                  0.11609303951263428,
                                                  0.11628188192844391,
                                                  0.11593768745660782,
                                                  0.116062231361866,
                                                  0.11615301668643951,
                                                  0.11612395197153091,
                                                  0.11609595268964767,
                                                  0.11606339365243912,
                                                  0.11621040850877762,
                                                  0.11611215770244598,
                                                  0.11631068587303162,
                                                  0.11604316532611847,
                                                  0.11610415577888489,
                                                  0.11617108434438705,
                                                  0.11737527698278427,
                                                  0.11636664718389511,
                                                  0.11643633991479874,
                                                  0.11641830205917358,
                                                  0.11623018234968185,
                                                  0.11627576500177383,
                                                  0.11618880182504654,
                                                  0.11590965837240219,
                                                  0.11600705981254578,
                                                  0.11603537946939468,
                                                  0.11614096164703369,
                                                  0.11609300225973129,
                                                  0.11620457470417023,
                                                  0.11623869836330414,
                                                  0.11612760275602341,
                                                  0.11619135737419128,
                                                  0.11633993685245514,
                                                  0.11595872789621353,
                                                  0.1160760223865509,
                                                  0.11613026261329651,
                                                  0.11629821360111237,
                                                  0.11606990545988083,
                                                  0.11618290096521378,
                                                  0.11590874940156937,
                                                  0.116066575050354,
                                                  0.1162467747926712,
                                                  0.11605139076709747,
                                                  0.11606994271278381,
                                                  0.11615034192800522,
                                                  0.11626721173524857,
                                                  0.1164214015007019,
                                                  0.11610067635774612,
                                                  0.11607576906681061,
                                                  0.11610443145036697,
                                                  0.11622398346662521,
                                                  0.11609799414873123,
                                                  0.11592576652765274,
                                                  0.11589066684246063,
                                                  0.11615613847970963,
                                                  0.11620646715164185,
                                                  0.11605507135391235,
                                                  0.11605215072631836,
                                                  0.11597687005996704,
                                                  0.11605720967054367,
                                                  0.11622954159975052,
                                                  0.11576541513204575,
                                                  0.11619725078344345,
                                                  0.11608881503343582,
                                                  0.11614267528057098,
                                                  0.11628157645463943,
                                                  0.11605657637119293,
                                                  0.11614761501550674,
                                                  0.11601652950048447,
                                                  0.11608336865901947,
                                                  0.1161508783698082,
                                                  0.11626416444778442,
                                                  0.11630623042583466,
                                                  0.1164545938372612,
                                                  0.11624109745025635,
                                                  0.11607788503170013,
                                                  0.1161070168018341,
                                                  0.11620175093412399,
                                                  0.11616086959838867,
                                                  0.11611688882112503,
                                                  0.11600390076637268,
                                                  0.11609793454408646,
                                                  0.11598715931177139,
                                                  0.11622306704521179,
                                                  0.11612720787525177,
                                                  0.11618942022323608,
                                                  0.11620881408452988,
                                                  0.11601428687572479,
                                                  0.11616939306259155,
                                                  0.11625801771879196,
                                                  0.11612837016582489,
                                                  0.11634919047355652,
                                                  0.11619079858064651,
                                                  0.11613990366458893,
                                                  0.11610959470272064,
                                                  0.1163247600197792,
                                                  0.11617305129766464,
                                                  0.11615300923585892,
                                                  0.11632908880710602,
                                                  0.11621703207492828,
                                                  0.11634662002325058,
                                                  0.11614250391721725,
                                                  0.11630403250455856,
                                                  0.11608672142028809,
                                                  0.11608536541461945,
                                                  31432.908203125,
                                                  0.1162625327706337,
                                                  0.11617450416088104,
                                                  0.11622246354818344,
                                                  0.11625657230615616,
                                                  0.11630494892597198,
                                                  0.1160629466176033,
                                                  0.11630386859178543,
                                                  0.11654241383075714,
                                                  0.11633019894361496,
                                                  0.11625935137271881,
                                                  0.1159849464893341,
                                                  0.11611940711736679,
                                                  0.11628160625696182,
                                                  0.11616548895835876,
                                                  0.1158863976597786,
                                                  0.11616910994052887,
                                                  0.11640705168247223,
                                                  57.9826545715332,
                                                  0.11621317267417908,
                                                  0.1162533313035965,
                                                  0.11624367535114288,
                                                  0.11624933034181595,
                                                  0.11617560684680939,
                                                  0.11601812392473221,
                                                  0.11616231501102448,
                                                  0.1162579134106636,
                                                  0.1163044273853302,
                                                  0.11610657721757889,
                                                  0.11615923047065735,
                                                  0.11604088544845581,
                                                  0.11613348871469498,
                                                  0.1160733699798584,
                                                  0.11620483547449112,
                                                  0.11624867469072342,
                                                  0.11615971475839615,
                                                  0.11607415229082108,
                                                  75018216.0,
                                                  0.09041483700275421,
                                                  0.09068240970373154,
                                                  0.09158043563365936,
                                                  0.09161237627267838,
                                                  0.09168776124715805,
                                                  0.09167324006557465,
                                                  0.09174589067697525,
                                                  0.09159623086452484,
                                                  0.091584213078022,
                                                  0.09181859344244003,
                                                  0.09148017317056656,
                                                  0.09162292629480362,
                                                  0.09164972603321075,
                                                  0.09172000735998154,
                                                  0.09156201779842377,
                                                  0.09159772098064423,
                                                  0.09161564707756042,
                                                  0.09180383384227753,
                                                  0.09165830910205841,
                                                  0.09175550937652588,
                                                  0.09164596349000931,
                                                  0.09165975451469421,
                                                  0.09162546694278717,
                                                  0.09173918515443802,
                                                  0.09151586145162582,
                                                  0.09154339134693146,
                                                  0.09142778068780899,
                                                  0.09155946969985962,
                                                  0.0915718749165535,
                                                  0.09148793667554855,
                                                  0.09153666347265244,
                                                  0.0915401503443718,
                                                  0.0918232873082161,
                                                  0.09156225621700287,
                                                  0.09150882065296173,
                                                  0.09154771268367767,
                                                  0.09136240184307098,
                                                  0.09136436134576797,
                                                  0.09138547629117966,
                                                  0.09111592918634415,
                                                  0.09049556404352188,
                                                  0.09093768894672394,
                                                  0.09028400480747223,
                                                  0.0910341814160347,
                                                  0.09143580496311188,
                                                  0.09173454344272614,
                                                  0.09146896004676819,
                                                  0.09174791723489761,
                                                  0.09165650606155396,
                                                  0.09164563566446304,
                                                  0.09150607138872147,
                                                  0.09162599593400955,
                                                  0.09160754829645157,
                                                  0.0916544646024704,
                                                  0.09154925495386124,
                                                  0.09158218652009964,
                                                  0.09168332815170288,
                                                  0.09178471565246582,
                                                  0.09159593284130096,
                                                  0.09167377650737762,
                                                  0.09158945083618164,
                                                  0.09157179296016693,
                                                  0.09173416346311569,
                                                  0.09171082079410553,
                                                  0.0915522649884224,
                                                  0.09138037264347076,
                                                  6550871.0,
                                                  0.0917968600988388,
                                                  0.09176656603813171,
                                                  0.09189419448375702,
                                                  0.09189261496067047,
                                                  0.09148921072483063,
                                                  0.0916379913687706,
                                                  0.09164125472307205,
                                                  0.09168343245983124,
                                                  0.0915914997458458,
                                                  0.0916556641459465,
                                                  0.09176559001207352,
                                                  0.09119798988103867,
                                                  0.09149183332920074,
                                                  0.09167253226041794,
                                                  0.09157314896583557,
                                                  0.09136173874139786,
                                                  0.09172654151916504,
                                                  0.09172135591506958,
                                                  0.0916282907128334,
                                                  0.09151092171669006,
                                                  0.09152401983737946,
                                                  0.09146179258823395,
                                                  0.09152831137180328,
                                                  0.09171596169471741,
                                                  0.09146741032600403,
                                                  0.09163735806941986,
                                                  0.09172643721103668,
                                                  0.09151304513216019,
                                                  0.09167574346065521,
                                                  0.09155893325805664,
                                                  0.09151120483875275,
                                                  0.09154047071933746,
                                                  0.09145689010620117,
                                                  0.09163273870944977,
                                                  0.09142628312110901,
                                                  0.09159869700670242,
                                                  0.09154431521892548,
                                                  0.09110464155673981,
                                                  0.09078102558851242,
                                                  0.0907980427145958,
                                                  0.09076491743326187,
                                                  0.09136105328798294,
                                                  0.09158964455127716,
                                                  0.09161243587732315,
                                                  0.09146032482385635,
                                                  0.09154172241687775,
                                                  0.0915302038192749,
                                                  0.09143901616334915,
                                                  0.09158524125814438,
                                                  0.09163603186607361,
                                                  0.09191957861185074,
                                                  0.09158330410718918,
                                                  0.0916702076792717,
                                                  0.09173353016376495,
                                                  0.09168414771556854,
                                                  0.09165389090776443,
                                                  0.09153973311185837,
                                                  0.09151528030633926,
                                                  0.09149136394262314,
                                                  0.09165231138467789,
                                                  0.09150249511003494,
                                                  0.09171602129936218,
                                                  0.09170891344547272,
                                                  0.09149736166000366,
                                                  0.09154205024242401,
                                                  0.09144985675811768,
                                                  0.09173857420682907,
                                                  0.09168737381696701,
                                                  0.09159638732671738,
                                                  0.09164682775735855,
                                                  0.09182272851467133,
                                                  0.09188602864742279,
                                                  0.0916704535484314,
                                                  0.09181439131498337,
                                                  0.0916689857840538,
                                                  0.0916949138045311,
                                                  0.0918634682893753,
                                                  0.091732919216156,
                                                  0.09177081286907196,
                                                  0.09180489182472229,
                                                  0.09170784801244736,
                                                  0.09166619181632996,
                                                  0.09147465229034424,
                                                  0.09065674990415573,
                                                  0.09165985137224197,
                                                  0.0915789008140564,
                                                  0.09145854413509369,
                                                  0.0916421115398407,
                                                  0.09175586700439453,
                                                  0.0916069895029068,
                                                  0.091783806681633,
                                                  0.09182631224393845,
                                                  0.0917629525065422,
                                                  0.09174057096242905,
                                                  0.09179218113422394,
                                                  0.09166530519723892,
                                                  0.09162689000368118,
                                                  0.09167832881212234,
                                                  0.09155615419149399,
                                                  0.09177278727293015,
                                                  0.09168699383735657,
                                                  0.09166309982538223,
                                                  0.09175059199333191,
                                                  0.09156449139118195,
                                                  0.09154761582612991,
                                                  0.09163499623537064,
                                                  0.09183468669652939,
                                                  0.09155910462141037,
                                                  0.09153896570205688,
                                                  0.09173861145973206,
                                                  0.09150022268295288,
                                                  0.09156262874603271,
                                                  0.09158863872289658,
                                                  0.091577909886837,
                                                  0.0914931669831276,
                                                  0.09152447432279587,
                                                  0.09160035848617554,
                                                  0.0915502980351448,
                                                  0.09158534556627274,
                                                  0.09145323932170868,
                                                  0.09149032831192017,
                                                  0.09140226989984512,
                                                  0.09157698601484299,
                                                  0.09184463322162628,
                                                  0.09163165837526321,
                                                  0.09175490587949753,
                                                  0.09173326194286346,
                                                  0.09146177023649216,
                                                  0.09170445799827576,
                                                  0.09182441979646683,
                                                  0.09142305701971054,
                                                  0.09153810143470764,
                                                  0.09144371002912521,
                                                  0.09164082258939743,
                                                  0.09147138893604279,
                                                  0.09169746935367584,
                                                  0.09146449714899063,
                                                  0.0915309488773346,
                                                  0.09153302758932114,
                                                  0.09078874439001083,
                                                  0.09040026366710663,
                                                  0.0903313085436821,
                                                  0.09044385701417923,
                                                  0.09064793586730957,
                                                  0.09086998552083969,
                                                  0.0905841663479805,
                                                  0.09128729999065399,
                                                  0.09156367182731628,
                                                  0.09157119691371918,
                                                  0.09156181663274765,
                                                  0.091734878718853,
                                                  0.09174881130456924,
                                                  0.09162548929452896,
                                                  0.09173157066106796,
                                                  0.09165001660585403,
                                                  0.09184536337852478,
                                                  0.09174542874097824,
                                                  0.09148374944925308,
                                                  0.0918145477771759,
                                                  0.09172198921442032,
                                                  0.09163372218608856,
                                                  0.09175238013267517,
                                                  0.09166068583726883,
                                                  0.09152699261903763,
                                                  0.09570552408695221,
                                                  0.09075891226530075,
                                                  0.09028211236000061,
                                                  0.09057898819446564,
                                                  0.09034465253353119,
                                                  0.091373510658741,
                                                  0.09163247048854828,
                                                  0.0916837826371193,
                                                  623378176.0,
                                                  0.09151039272546768,
                                                  0.09154417365789413,
                                                  0.09182267636060715,
                                                  0.0917121171951294,
                                                  0.09159564971923828,
                                                  0.09143423289060593,
                                                  0.09164773672819138,
                                                  0.09148409217596054,
                                                  0.0917004719376564,
                                                  0.0917796865105629,
                                                  0.0917053371667862,
                                                  0.09163802117109299,
                                                  597140.375,
                                                  0.0918307676911354,
                                                  0.09166297316551208,
                                                  0.09173108637332916,
                                                  0.09177133440971375,
                                                  0.09167598187923431,
                                                  0.09167742729187012,
                                                  0.09176761656999588,
                                                  0.09161846339702606,
                                                  0.09151538461446762,
                                                  0.09162520617246628,
                                                  0.09155528992414474,
                                                  0.09133844822645187,
                                                  0.09172411262989044,
                                                  0.09168177098035812,
                                                  0.09152256697416306,
                                                  0.09153567254543304,
                                                  0.09174582362174988,
                                                  0.09146131575107574,
                                                  0.09169001132249832,
                                                  0.09183165431022644,
                                                  0.09155264496803284,
                                                  0.09179975837469101,
                                                  0.09141802042722702,
                                                  0.09178242087364197,
                                                  0.09155882894992828,
                                                  0.09155619144439697,
                                                  0.09172812104225159,
                                                  0.09160418808460236,
                                                  0.09162043035030365,
                                                  0.0916271060705185,
                                                  0.0916542038321495,
                                                  0.09182074666023254,
                                                  0.091661237180233,
                                                  0.09168948233127594,
                                                  0.09161534905433655,
                                                  0.09176906198263168,
                                                  0.09174256771802902,
                                                  0.09184432774782181,
                                                  0.09175481647253036,
                                                  0.09163566678762436,
                                                  0.09110815823078156,
                                                  0.09047967940568924,
                                                  0.09134313464164734,
                                                  0.09058742970228195,
                                                  0.09057310223579407,
                                                  0.09021198004484177,
                                                  0.08950533717870712,
                                                  0.08991483598947525,
                                                  0.09000581502914429,
                                                  0.08975820243358612,
                                                  0.08989927172660828,
                                                  0.08977536857128143,
                                                  0.09046599268913269,
                                                  0.09019088745117188,
                                                  0.08985491842031479,
                                                  0.09006495773792267,
                                                  0.09006132930517197,
                                                  0.08998249471187592,
                                                  0.08991481363773346,
                                                  0.09035725891590118,
                                                  0.09026259183883667,
                                                  0.09033244848251343,
                                                  0.09021812677383423,
                                                  0.08977342396974564,
                                                  0.08995147794485092,
                                                  0.09018651396036148,
                                                  0.0902789831161499,
                                                  0.09030649065971375,
                                                  0.09018151462078094,
                                                  0.08963830769062042,
                                                  0.09024237841367722,
                                                  0.08991577476263046,
                                                  0.09044059365987778,
                                                  0.08928724378347397,
                                                  0.08972622454166412,
                                                  0.08882772922515869,
                                                  0.08975745737552643,
                                                  0.09016703814268112,
                                                  0.08963004499673843,
                                                  0.0898420587182045,
                                                  0.08989901095628738,
                                                  0.09010357409715652,
                                                  0.0901118665933609,
                                                  0.09002404659986496,
                                                  0.09008162468671799,
                                                  0.08998696506023407,
                                                  0.09019099175930023,
                                                  0.09039027988910675,
                                                  0.09014302492141724,
                                                  0.09035284072160721,
                                                  0.090223528444767,
                                                  0.08909245580434799,
                                                  0.08939402550458908,
                                                  0.08942470699548721,
                                                  0.08992207795381546,
                                                  0.08974660187959671,
                                                  0.09004580974578857,
                                                  0.09003043174743652,
                                                  0.08954887092113495,
                                                  0.08958173543214798,
                                                  0.0888582319021225,
                                                  0.08949612826108932,
                                                  0.0890330970287323,
                                                  0.08879958093166351,
                                                  0.08949175477027893,
                                                  0.09019462019205093,
                                                  0.08992094546556473,
                                                  0.08964835852384567,
                                                  0.08991514146327972,
                                                  0.08982381969690323,
                                                  0.08980073034763336,
                                                  0.0896264985203743,
                                                  0.08992141485214233,
                                                  0.09008410573005676,
                                                  0.08989385515451431,
                                                  0.09021643549203873,
                                                  0.09013224393129349,
                                                  0.0900585800409317,
                                                  0.08978967368602753,
                                                  0.09012910723686218,
                                                  0.08994796127080917,
                                                  0.09000711888074875,
                                                  0.08993218839168549,
                                                  0.09006299823522568,
                                                  0.08917942643165588,
                                                  0.08967818319797516,
                                                  0.08993396908044815,
                                                  0.09023486822843552,
                                                  0.0900617465376854,
                                                  0.09012560546398163,
                                                  0.08974074572324753,
                                                  0.09009005129337311,
                                                  0.08979517966508865,
                                                  0.08997286111116409,
                                                  0.09035619348287582,
                                                  0.09010802954435349,
                                                  0.09086864441633224,
                                                  0.0897199809551239,
                                                  0.08955328911542892,
                                                  0.08973503112792969,
                                                  0.0901649221777916,
                                                  0.08986207097768784,
                                                  0.09005796909332275,
                                                  0.08977393060922623,
                                                  0.09000321477651596,
                                                  0.08998708426952362,
                                                  0.08971834927797318,
                                                  0.0897836983203888,
                                                  0.09028778970241547,
                                                  0.08969186991453171,
                                                  0.09012392163276672,
                                                  0.08985482901334763,
                                                  0.09004218131303787,
                                                  0.09037664532661438,
                                                  0.09028197079896927,
                                                  0.09011863172054291,
                                                  0.0896797850728035,
                                                  0.0896434634923935,
                                                  0.0899115651845932,
                                                  0.08928757160902023,
                                                  0.08990449458360672,
                                                  0.08990985155105591,
                                                  0.09001431614160538,
                                                  0.0895829126238823,
                                                  0.09010770171880722,
                                                  0.08968362212181091,
                                                  0.09000500291585922,
                                                  0.0898640975356102,
                                                  0.08954513818025589,
                                                  123260808.0,
                                                  0.09009537845849991,
                                                  0.08999543637037277,
                                                  0.09009146690368652,
                                                  0.08993417024612427,
                                                  0.08979174494743347,
                                                  0.09024576097726822,
                                                  0.08982154726982117,
                                                  0.09024430066347122,
                                                  0.08962235599756241,
                                                  0.08915040642023087,
                                                  0.08879758417606354,
                                                  0.08888117223978043,
                                                  0.08869042247533798,
                                                  0.08853305876255035,
                                                  0.08868934214115143,
                                                  0.08893421292304993,
                                                  0.0886133685708046,
                                                  0.08817695081233978,
                                                  0.0881962701678276,
                                                  0.08811213821172714,
                                                  0.0891452506184578,
                                                  0.08935011923313141,
                                                  0.08934961259365082,
                                                  0.08890455216169357,
                                                  0.08886080235242844,
                                                  0.0891081690788269,
                                                  0.08976045250892639,
                                                  0.08954772353172302,
                                                  0.08939021825790405,
                                                  0.08975906670093536,
                                                  0.08977613598108292,
                                                  0.0896851047873497,
                                                  0.08977438509464264,
                                                  0.09006603807210922,
                                                  0.08980820327997208,
                                                  0.08988718688488007,
                                                  0.08971526473760605,
                                                  0.09004899114370346,
                                                  0.0901557132601738,
                                                  0.08969920128583908,
                                                  0.09032884240150452,
                                                  0.08998294174671173,
                                                  0.08977649360895157,
                                                  0.09020925313234329,
                                                  0.09029025584459305,
                                                  0.08976597338914871,
                                                  0.08999000489711761,
                                                  0.08985752612352371,
                                                  0.09009544551372528,
                                                  0.08979445695877075,
                                                  0.08981890231370926,
                                                  0.08993826806545258,
                                                  0.09029476344585419,
                                                  0.0901898518204689,
                                                  0.0904805064201355,
                                                  0.0902039036154747,
                                                  0.09018836915493011,
                                                  0.09021788835525513,
                                                  13.298559188842773,
                                                  0.09035386145114899,
                                                  0.08994783461093903,
                                                  0.08987398445606232,
                                                  0.0898030549287796,
                                                  0.09018076211214066,
                                                  0.08989060670137405,
                                                  0.0900542363524437,
                                                  0.08998247236013412,
                                                  0.08996569365262985,
                                                  0.08956349641084671,
                                                  0.0899026170372963,
                                                  0.08934273570775986,
                                                  0.08953327685594559,
                                                  0.08896757662296295,
                                                  40.889766693115234,
                                                  0.0892421156167984,
                                                  0.09008168429136276,
                                                  0.0901041328907013,
                                                  0.08966265618801117,
                                                  0.08987640589475632,
                                                  0.09025770425796509,
                                                  0.0899430364370346,
                                                  0.09005750715732574,
                                                  0.09002253413200378,
                                                  0.08991013467311859,
                                                  0.08955400437116623,
                                                  0.09024599194526672,
                                                  0.08989398926496506,
                                                  0.09036830812692642,
                                                  0.09003657847642899,
                                                  0.09000864624977112,
                                                  0.09005343914031982,
                                                  0.09015601873397827,
                                                  0.09009037166833878,
                                                  0.09033283591270447,
                                                  0.08988620340824127,
                                                  0.09028229117393494,
                                                  0.09019381552934647,
                                                  0.0902322456240654,
                                                  0.09032434225082397,
                                                  0.09011644124984741,
                                                  0.08976441621780396,
                                                  0.08991662412881851,
                                                  0.08978656679391861,
                                                  0.08997219800949097,
                                                  0.09036137908697128,
                                                  0.08971432596445084,
                                                  0.08993349224328995,
                                                  0.08978254348039627,
                                                  0.09011665731668472,
                                                  0.0903627872467041,
                                                  0.09019304066896439,
                                                  0.08973104506731033,
                                                  0.09010143578052521,
                                                  0.09001858532428741,
                                                  0.08998378366231918,
                                                  0.09022297710180283,
                                                  0.09093927592039108,
                                                  0.08909867703914642,
                                                  0.08914345502853394,
                                                  0.08835233747959137,
                                                  0.0882631167769432,
                                                  0.08950014412403107,
                                                  0.08979093283414841,
                                                  0.09012936055660248,
                                                  0.08974586427211761,
                                                  0.09011997282505035,
                                                  0.08974523842334747,
                                                  0.09000830352306366,
                                                  0.09017462283372879,
                                                  0.09004103392362595,
                                                  0.08963128179311752,
                                                  0.08985885232686996,
                                                  0.08983511477708817,
                                                  0.09008574485778809,
                                                  0.09016025811433792,
                                                  0.08972913771867752,
                                                  0.08963147550821304,
                                                  0.08982467651367188,
                                                  0.08994393795728683,
                                                  0.08989205211400986,
                                                  0.08967658132314682,
                                                  0.08987260609865189,
                                                  0.09007982909679413,
                                                  0.09010133147239685,
                                                  0.09020086377859116,
                                                  0.09046005457639694,
                                                  0.08994335681200027,
                                                  0.09018530696630478,
                                                  0.08992722630500793,
                                                  0.09044454246759415,
                                                  0.09027785807847977,
                                                  0.08971437811851501,
                                                  0.08930660039186478,
                                                  0.09013891965150833,
                                                  0.08999187499284744,
                                                  0.08977315574884415,
                                                  0.08965734392404556,
                                                  0.08979973196983337,
                                                  0.09008533507585526,
                                                  0.08993113040924072,
                                                  0.08979012817144394,
                                                  0.08988290280103683,
                                                  0.09004655480384827,
                                                  0.09017106890678406,
                                                  0.089727021753788,
                                                  0.08970198780298233,
                                                  0.09002815186977386,
                                                  0.08984025567770004,
                                                  0.09001448005437851,
                                                  0.08957063406705856,
                                                  0.08992192894220352,
                                                  0.09023972600698471,
                                                  0.09008713066577911,
                                                  0.0900142714381218,
                                                  0.08985602855682373,
                                                  0.0900145098567009],
                             'weight_decay': 0.0005},
                 'microF1': {'accuracy': 2.0,
                             'batch_size': 32,
                             'cv_score': 0.017618830126676397,
                             'cv_val_accuracy': 0.7777777777777778,
                             'cv_val_loss': 0.09876688073078792,
                             'cv_val_macroF1': 0.017618830126676397,
                             'cv_val_microF1': 0.07673131407953461,
                             'epochs': 300,
                             'final_model': GGNN1(
  (ggnn): GatedGraphConv(30, num_layers=2)
  (fc1): Linear(in_features=30, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                             'kwargs': {'aggr_type': 'mean',
                                        'd1': 30,
                                        'd2': 50,
                                        'num_classes': 24,
                                        'num_layers': 2},
                             'learning_rate': 0.01,
                             'macroF1': 0.006748704489763125,
                             'microF1': 0.08414023372287145,
                             'model': <class 'TFM_graph_classification_models.GGNN1'>,
                             'model_instance': GGNN1(
  (ggnn): GatedGraphConv(30, num_layers=2)
  (fc1): Linear(in_features=30, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                             'score': 'f1_macro',
                             'time': 3923.382642507553,
                             'train_loss_history': [402.3915100097656,
                                                    0.627553403377533,
                                                    0.1610880345106125,
                                                    200590368.0,
                                                    127.40691375732422,
                                                    0.0838785469532013,
                                                    0.08383877575397491,
                                                    0.08405112475156784,
                                                    0.08400005102157593,
                                                    0.11704819649457932,
                                                    315.0480651855469,
                                                    8.739550590515137,
                                                    0.26758015155792236,
                                                    150.98406982421875,
                                                    1.1856788396835327,
                                                    1248861440.0,
                                                    2.6898114681243896,
                                                    0.08465886861085892,
                                                    0.08455833047628403,
                                                    0.08453904837369919,
                                                    0.08463430404663086,
                                                    0.08893854171037674,
                                                    2255.366943359375,
                                                    5255256.5,
                                                    0.0845518633723259,
                                                    0.08456744998693466,
                                                    0.0846351757645607,
                                                    0.08459139615297318,
                                                    0.0845951959490776,
                                                    0.08459458500146866,
                                                    741.9649047851562,
                                                    375.397216796875,
                                                    0.08453738689422607,
                                                    1314.5992431640625,
                                                    0.08459823578596115,
                                                    2760.76708984375,
                                                    0.08449413627386093,
                                                    0.0844498947262764,
                                                    0.08453848212957382,
                                                    0.08451221138238907,
                                                    0.08967375755310059,
                                                    0.21154144406318665,
                                                    6485.376953125,
                                                    123632.40625,
                                                    29165.78125,
                                                    38.000274658203125,
                                                    0.08432421088218689,
                                                    0.08440204709768295,
                                                    0.08437246084213257,
                                                    31.02018928527832,
                                                    0.08415640890598297,
                                                    4.811329364776611,
                                                    0.08450804650783539,
                                                    0.08431932330131531,
                                                    0.0844046100974083,
                                                    1.6224708557128906,
                                                    0.08452630043029785,
                                                    0.08453928679227829,
                                                    0.0845048725605011,
                                                    0.0844370573759079,
                                                    0.08444765210151672,
                                                    0.08450065553188324,
                                                    0.08445669710636139,
                                                    0.08438482880592346,
                                                    0.08415114134550095,
                                                    0.08441533893346786,
                                                    0.08437883108854294,
                                                    0.08448990434408188,
                                                    0.08448030054569244,
                                                    0.0844801515340805,
                                                    0.08438432216644287,
                                                    0.08433922380208969,
                                                    0.08432400226593018,
                                                    0.08451466262340546,
                                                    0.08452446758747101,
                                                    0.08453328907489777,
                                                    0.08448510617017746,
                                                    0.08447270840406418,
                                                    0.08449146896600723,
                                                    0.08447691053152084,
                                                    0.08443857729434967,
                                                    0.08449263125658035,
                                                    0.08442482352256775,
                                                    0.08449450135231018,
                                                    0.0844353586435318,
                                                    0.08452147245407104,
                                                    0.0844416618347168,
                                                    0.08446303755044937,
                                                    0.08447965979576111,
                                                    0.0843501091003418,
                                                    0.08422920107841492,
                                                    0.08444242179393768,
                                                    0.08447030931711197,
                                                    0.08448900282382965,
                                                    0.08450789749622345,
                                                    0.08435028791427612,
                                                    0.08434262871742249,
                                                    0.08449234813451767,
                                                    0.08449122309684753,
                                                    0.08449383825063705,
                                                    0.08452076464891434,
                                                    0.08449948579072952,
                                                    0.08440456539392471,
                                                    0.08449050039052963,
                                                    0.0841154232621193,
                                                    2.2428197860717773,
                                                    0.08384676277637482,
                                                    0.08394981920719147,
                                                    0.0838002935051918,
                                                    0.08447454124689102,
                                                    0.08348763734102249,
                                                    0.08416859805583954,
                                                    0.08417699486017227,
                                                    0.08438660949468613,
                                                    0.4162343144416809,
                                                    0.08450004458427429,
                                                    0.08446227014064789,
                                                    0.08444914221763611,
                                                    0.0843791589140892,
                                                    0.08446182310581207,
                                                    0.08455074578523636,
                                                    0.08446145057678223,
                                                    0.0836617723107338,
                                                    0.08338803797960281,
                                                    0.08333099633455276,
                                                    0.08318332582712173,
                                                    0.08332567662000656,
                                                    0.08302734792232513,
                                                    0.08371946960687637,
                                                    0.08449801802635193,
                                                    0.08449846506118774,
                                                    0.08426801860332489,
                                                    0.08432383090257645,
                                                    200335072.0,
                                                    0.08437470346689224,
                                                    0.08452057838439941,
                                                    0.08447948843240738,
                                                    0.08449156582355499,
                                                    0.08443012088537216,
                                                    0.0844721645116806,
                                                    0.08447398245334625,
                                                    0.08443788439035416,
                                                    0.08443618565797806,
                                                    0.08449234068393707,
                                                    0.10008276253938675,
                                                    0.08434968441724777,
                                                    0.08430284261703491,
                                                    0.08442266285419464,
                                                    0.08438229560852051,
                                                    1806.7781982421875,
                                                    0.08451782912015915,
                                                    0.0844816267490387,
                                                    13.898186683654785,
                                                    0.08445671200752258,
                                                    1.4685406684875488,
                                                    4.019253253936768,
                                                    302.15570068359375,
                                                    0.08452155441045761,
                                                    0.14563511312007904,
                                                    27.54737663269043,
                                                    3.7263739109039307,
                                                    78.2233657836914,
                                                    0.08451861888170242,
                                                    0.08448174595832825,
                                                    407.52105712890625,
                                                    19.42632484436035,
                                                    0.08444830775260925,
                                                    36449228.0,
                                                    7.751605033874512,
                                                    0.08446042984724045,
                                                    0.08437091112136841,
                                                    3167184.25,
                                                    0.08450077474117279,
                                                    0.08428910374641418,
                                                    0.08419448137283325,
                                                    0.08466444909572601,
                                                    5.597066402435303,
                                                    0.11012237519025803,
                                                    0.08447238802909851,
                                                    58.212467193603516,
                                                    5.655828952789307,
                                                    3588.997314453125,
                                                    0.409007728099823,
                                                    32.17847442626953,
                                                    0.08447328209877014,
                                                    0.0845048725605011,
                                                    0.08449171483516693,
                                                    0.08447052538394928,
                                                    0.08445976674556732,
                                                    0.08446221798658371,
                                                    0.08458263427019119,
                                                    0.08430005609989166,
                                                    8.114312171936035,
                                                    0.08432270586490631,
                                                    0.08445460349321365,
                                                    0.08443901687860489,
                                                    900.8036499023438,
                                                    0.08444087207317352,
                                                    0.08441809564828873,
                                                    0.4795038402080536,
                                                    0.08451033383607864,
                                                    0.08451204001903534,
                                                    0.08450926095247269,
                                                    0.08447223901748657,
                                                    0.08437959104776382,
                                                    0.08444824069738388,
                                                    0.08453644067049026,
                                                    7.690800666809082,
                                                    0.08448299020528793,
                                                    0.08449721336364746,
                                                    0.08445368707180023,
                                                    0.08438749611377716,
                                                    0.08451572060585022,
                                                    0.08452669531106949,
                                                    89.34236907958984,
                                                    0.08450479805469513,
                                                    30.75047492980957,
                                                    0.08451566100120544,
                                                    0.08449652045965195,
                                                    68.70880126953125,
                                                    0.08445856720209122,
                                                    12.459997177124023,
                                                    0.08449093997478485,
                                                    0.08446609228849411,
                                                    0.08448096364736557,
                                                    0.11521673947572708,
                                                    0.08448649197816849,
                                                    1010.2301025390625,
                                                    0.08451484888792038,
                                                    0.0844508707523346,
                                                    0.0843883827328682,
                                                    0.08444556593894958,
                                                    182.40513610839844,
                                                    0.08444695174694061,
                                                    304.5606689453125,
                                                    0.0844317376613617,
                                                    0.08447675406932831,
                                                    0.08446475118398666,
                                                    0.08447200804948807,
                                                    0.08442620933055878,
                                                    0.08441336452960968,
                                                    0.08443718403577805,
                                                    0.08444596081972122,
                                                    0.0845516175031662,
                                                    0.08445774763822556,
                                                    37.44003677368164,
                                                    0.0844288170337677,
                                                    0.08449657261371613,
                                                    0.08451525121927261,
                                                    0.08444896340370178,
                                                    0.08451040834188461,
                                                    0.08449771255254745,
                                                    0.0844845324754715,
                                                    0.08452331274747849,
                                                    0.08448804169893265,
                                                    0.08449441939592361,
                                                    0.08447355777025223,
                                                    217506.6875,
                                                    0.08448262512683868,
                                                    27566.361328125,
                                                    0.084529809653759,
                                                    60.93157958984375,
                                                    0.0844476968050003,
                                                    0.08442551642656326,
                                                    81.24128723144531,
                                                    0.08442741632461548,
                                                    0.08438073843717575,
                                                    0.08440481126308441,
                                                    0.08442452549934387,
                                                    0.08434814214706421,
                                                    11.347413063049316,
                                                    26.29210662841797,
                                                    0.23023147881031036,
                                                    0.08447755873203278,
                                                    0.08438970148563385,
                                                    0.08435468375682831,
                                                    0.08439872413873672,
                                                    0.08438412100076675,
                                                    0.08440207690000534,
                                                    0.0844302773475647,
                                                    11.261951446533203,
                                                    0.08435912430286407,
                                                    3.4124855995178223,
                                                    0.08436176180839539,
                                                    0.08430063724517822,
                                                    0.08941613137722015,
                                                    0.08446674793958664,
                                                    0.08433955907821655,
                                                    0.08443570137023926,
                                                    0.08435171097517014,
                                                    21675608.0,
                                                    0.08440378308296204,
                                                    0.08433499932289124,
                                                    0.40477755665779114,
                                                    0.08435443788766861,
                                                    0.08426611125469208,
                                                    13186.3408203125,
                                                    39600.96875,
                                                    0.08453357219696045,
                                                    0.9381150603294373,
                                                    0.09587060660123825,
                                                    0.514412522315979,
                                                    0.09367242455482483,
                                                    0.09441542625427246,
                                                    0.09452634304761887,
                                                    0.09454908967018127,
                                                    22.32537269592285,
                                                    32339.833984375,
                                                    0.09451416879892349,
                                                    0.09449652582406998,
                                                    0.09464240074157715,
                                                    0.09449930489063263,
                                                    1958.1798095703125,
                                                    0.09455957263708115,
                                                    0.09456851333379745,
                                                    1.2811931371688843,
                                                    0.09453321993350983,
                                                    0.09458591043949127,
                                                    0.09450304508209229,
                                                    5.49962043762207,
                                                    72.5165023803711,
                                                    0.09458471089601517,
                                                    0.0945429876446724,
                                                    0.09455682337284088,
                                                    0.09453796595335007,
                                                    0.09452159702777863,
                                                    2.5312438011169434,
                                                    0.0945226177573204,
                                                    0.0944872573018074,
                                                    5.223100185394287,
                                                    2137.731201171875,
                                                    0.142483189702034,
                                                    0.20935583114624023,
                                                    0.09459515661001205,
                                                    0.09455345571041107,
                                                    2812.721435546875,
                                                    0.09442364424467087,
                                                    0.1131102666258812,
                                                    0.09442222863435745,
                                                    0.09438570588827133,
                                                    0.09433407336473465,
                                                    0.13149750232696533,
                                                    0.09370430558919907,
                                                    0.09366568177938461,
                                                    0.09354622662067413,
                                                    0.09447423368692398,
                                                    0.6216050982475281,
                                                    0.09451036900281906,
                                                    0.09451115876436234,
                                                    1349.734619140625,
                                                    0.09453874081373215,
                                                    0.11682190001010895,
                                                    0.10219868272542953,
                                                    0.09455303847789764,
                                                    0.0945776104927063,
                                                    0.09455080330371857,
                                                    2.9681241512298584,
                                                    0.09452127665281296,
                                                    0.09452573955059052,
                                                    29.074121475219727,
                                                    14.531797409057617,
                                                    0.09461171925067902,
                                                    0.09457170218229294,
                                                    0.09453096240758896,
                                                    0.7046006321907043,
                                                    0.33546608686447144,
                                                    0.09435248374938965,
                                                    0.09443413466215134,
                                                    0.31541207432746887,
                                                    0.14433683454990387,
                                                    0.09446719288825989,
                                                    3715.66796875,
                                                    0.0944618359208107,
                                                    0.09454601258039474,
                                                    0.0945052057504654,
                                                    0.09456264227628708,
                                                    0.09456627815961838,
                                                    1.2233333587646484,
                                                    0.09454774111509323,
                                                    249.55780029296875,
                                                    181.83404541015625,
                                                    154.2233123779297,
                                                    0.09459660947322845,
                                                    0.09442275762557983,
                                                    0.09446389973163605,
                                                    0.09459325671195984,
                                                    0.09457375854253769,
                                                    0.0945519208908081,
                                                    0.09459152817726135,
                                                    0.0945110023021698,
                                                    0.7131268978118896,
                                                    0.09457242488861084,
                                                    13.699750900268555,
                                                    0.09455432742834091,
                                                    0.09460796415805817,
                                                    0.09450221061706543,
                                                    11696.2060546875,
                                                    0.09452937543392181,
                                                    0.09452711045742035,
                                                    0.09458191692829132,
                                                    9.455473899841309,
                                                    0.09451385587453842,
                                                    0.09448845684528351,
                                                    131.53732299804688,
                                                    4293.73291015625,
                                                    0.09409381449222565,
                                                    0.09368418902158737,
                                                    0.09371126443147659,
                                                    0.09354376047849655,
                                                    0.2533681094646454,
                                                    0.314374715089798,
                                                    0.09430520236492157,
                                                    0.09440848976373672,
                                                    0.094419464468956,
                                                    0.09437459707260132,
                                                    0.09439592063426971,
                                                    1767081.0,
                                                    594.7551879882812,
                                                    0.09452971816062927,
                                                    0.09449964016675949,
                                                    0.09450231492519379,
                                                    0.09459840506315231,
                                                    0.09455262124538422,
                                                    0.09447214007377625,
                                                    0.0944393202662468,
                                                    0.09440606832504272,
                                                    0.09447411447763443,
                                                    0.09437554329633713,
                                                    0.09433301538228989,
                                                    0.09440498054027557,
                                                    0.10780218243598938,
                                                    0.09436662495136261,
                                                    0.09431613236665726,
                                                    0.0945611521601677,
                                                    219.7027587890625,
                                                    0.4978674352169037,
                                                    0.0944586768746376,
                                                    0.30992811918258667,
                                                    1.8426533937454224,
                                                    0.5712193250656128,
                                                    0.09446409344673157,
                                                    0.09459622949361801,
                                                    0.09451809525489807,
                                                    87.38387298583984,
                                                    0.09456565976142883,
                                                    3497.284423828125,
                                                    0.09454844892024994,
                                                    0.09451253712177277,
                                                    0.09457314759492874,
                                                    0.09456212818622589,
                                                    0.15045373141765594,
                                                    0.14893552660942078,
                                                    562.9702758789062,
                                                    131.55374145507812,
                                                    0.09452853351831436,
                                                    0.09735272079706192,
                                                    0.09456931799650192,
                                                    0.6130989193916321,
                                                    0.09453856199979782,
                                                    0.09444250911474228,
                                                    0.09452873468399048,
                                                    0.09453690052032471,
                                                    1.229264736175537,
                                                    0.09449880570173264,
                                                    820.4354248046875,
                                                    1325.2156982421875,
                                                    0.09444205462932587,
                                                    0.0945630744099617,
                                                    0.09459412842988968,
                                                    0.0945231169462204,
                                                    513502.1875,
                                                    0.09456072747707367,
                                                    1.1679768562316895,
                                                    0.0945015400648117,
                                                    0.09457283467054367,
                                                    0.09455496817827225,
                                                    0.19079294800758362,
                                                    36.89104461669922,
                                                    0.0944361463189125,
                                                    0.09443370997905731,
                                                    0.09447237849235535,
                                                    20771922.0,
                                                    0.09442625194787979,
                                                    0.1135120838880539,
                                                    0.09445078670978546,
                                                    0.09434980154037476,
                                                    0.09435220062732697,
                                                    0.35446515679359436,
                                                    0.09654965996742249,
                                                    0.09453532844781876,
                                                    0.09456611424684525,
                                                    32323.8828125,
                                                    978.3565063476562,
                                                    0.09457312524318695,
                                                    0.0945013165473938,
                                                    0.09439229965209961,
                                                    4412095.5,
                                                    0.09448060393333435,
                                                    0.09440262615680695,
                                                    0.0943794921040535,
                                                    0.09443134814500809,
                                                    369.93389892578125,
                                                    0.14076443016529083,
                                                    0.09438452124595642,
                                                    0.09434284269809723,
                                                    0.10136646777391434,
                                                    0.09443105757236481,
                                                    0.09402314573526382,
                                                    26166.89453125,
                                                    0.09349802136421204,
                                                    3.321302652359009,
                                                    0.09364969283342361,
                                                    0.09496324509382248,
                                                    0.09372551739215851,
                                                    0.09436341375112534,
                                                    0.09432327002286911,
                                                    0.09436825662851334,
                                                    0.0944887325167656,
                                                    0.09451030939817429,
                                                    0.09454188495874405,
                                                    0.0945427343249321,
                                                    0.09454578906297684,
                                                    0.09454236924648285,
                                                    0.09450600296258926,
                                                    0.09452961385250092,
                                                    0.09452560544013977,
                                                    0.09452951699495316,
                                                    2834032.25,
                                                    0.09452395886182785,
                                                    10.720794677734375,
                                                    68.72327423095703,
                                                    8477249.0,
                                                    0.09440808743238449,
                                                    5520.7861328125,
                                                    0.09362620860338211,
                                                    0.09349146485328674,
                                                    0.09339573979377747,
                                                    149.35525512695312,
                                                    278.6841735839844,
                                                    0.0943543016910553,
                                                    37.89771270751953,
                                                    5100427.5,
                                                    0.09433344006538391,
                                                    6600393.5,
                                                    0.09443498402833939,
                                                    0.09448225051164627,
                                                    0.09439776837825775,
                                                    0.09439967572689056,
                                                    0.09428673982620239,
                                                    0.0945155993103981,
                                                    0.27247631549835205,
                                                    0.09454277157783508,
                                                    0.09456893801689148,
                                                    0.09452944248914719,
                                                    0.09454522281885147,
                                                    0.09451722353696823,
                                                    0.09460891038179398,
                                                    0.09455429017543793,
                                                    10244.599609375,
                                                    84.07907104492188,
                                                    0.0944337323307991,
                                                    0.09459497779607773,
                                                    0.09454827755689621,
                                                    0.09453362226486206,
                                                    0.09449025988578796,
                                                    0.09453912824392319,
                                                    0.3738631308078766,
                                                    0.09447181224822998,
                                                    0.09451087564229965,
                                                    0.09459631890058517,
                                                    0.09454839676618576,
                                                    0.09458976984024048,
                                                    0.09457769244909286,
                                                    0.09456919878721237,
                                                    3.4914751052856445,
                                                    1427.79345703125,
                                                    0.4578329920768738,
                                                    884.7360229492188,
                                                    0.09449641406536102,
                                                    0.09440287202596664,
                                                    510.2096862792969,
                                                    0.09450377523899078,
                                                    0.09451079368591309,
                                                    2.1641271114349365,
                                                    0.0944771096110344,
                                                    25.285322189331055,
                                                    0.09460421651601791,
                                                    0.09461460262537003,
                                                    21697.78125,
                                                    0.11157756298780441,
                                                    9.951555252075195,
                                                    0.09457164257764816,
                                                    0.09458516538143158,
                                                    770.6202392578125,
                                                    0.5892700552940369,
                                                    0.09393339604139328,
                                                    0.09381875395774841,
                                                    0.09371479600667953,
                                                    0.09378113597631454,
                                                    0.0938304141163826,
                                                    0.09431172162294388,
                                                    1034.4150390625,
                                                    538.9066772460938,
                                                    0.09464322030544281,
                                                    0.7673370242118835,
                                                    0.18193259835243225,
                                                    0.09461429715156555,
                                                    0.09462171792984009,
                                                    0.6443932056427002,
                                                    0.09465876966714859,
                                                    0.09471260756254196,
                                                    0.09465605765581131,
                                                    0.09452185034751892,
                                                    0.09450294822454453,
                                                    0.12810559570789337,
                                                    0.094684898853302,
                                                    8.992913246154785,
                                                    0.8569191694259644,
                                                    582.5096435546875,
                                                    0.09462642669677734,
                                                    0.09460807591676712,
                                                    0.09465175122022629,
                                                    0.0958004742860794,
                                                    0.09474059194326401,
                                                    0.09462853521108627,
                                                    657.1932373046875,
                                                    0.19906458258628845,
                                                    0.09392101317644119,
                                                    0.09379742294549942,
                                                    0.7946452498435974,
                                                    0.1725653111934662,
                                                    0.09447696059942245,
                                                    0.09447885304689407,
                                                    0.09469438344240189,
                                                    0.09465198963880539,
                                                    0.09460066258907318,
                                                    560.6574096679688,
                                                    0.09460125863552094,
                                                    0.0946880578994751,
                                                    0.09468343108892441,
                                                    0.09466951340436935,
                                                    7.846862316131592,
                                                    0.09464780241250992,
                                                    1.8931728601455688,
                                                    0.0939062237739563,
                                                    0.14714713394641876,
                                                    0.09371401369571686,
                                                    0.14943532645702362,
                                                    9.442521095275879,
                                                    0.09458307921886444,
                                                    0.09459754824638367,
                                                    0.19837377965450287,
                                                    0.18834809958934784,
                                                    0.09383810311555862,
                                                    0.09375286102294922,
                                                    0.0936644896864891,
                                                    0.09361999481916428,
                                                    0.0935748741030693,
                                                    1.445277214050293,
                                                    0.0944688618183136,
                                                    12.287859916687012,
                                                    0.09468797594308853,
                                                    0.09469488263130188,
                                                    0.09460695087909698,
                                                    0.7720686197280884,
                                                    0.0945553258061409,
                                                    0.09460161626338959,
                                                    0.09466303139925003,
                                                    0.09467338770627975,
                                                    0.09465459734201431,
                                                    0.09461504220962524,
                                                    0.0945778414607048,
                                                    0.09464619308710098,
                                                    0.09465964883565903,
                                                    0.09464212507009506,
                                                    229.66256713867188,
                                                    18.826688766479492,
                                                    0.09455174207687378,
                                                    0.09412211924791336,
                                                    17504.03515625,
                                                    0.09471108019351959,
                                                    0.09461493790149689,
                                                    10.089583396911621,
                                                    0.09461957961320877,
                                                    31.376590728759766,
                                                    0.09470537304878235,
                                                    0.09463783353567123,
                                                    5610218.0,
                                                    0.09464839100837708,
                                                    0.09463448077440262,
                                                    0.0942724272608757,
                                                    0.09408007562160492,
                                                    0.48262032866477966,
                                                    0.09465523064136505,
                                                    2.4015886783599854,
                                                    0.09462562948465347,
                                                    142063.546875,
                                                    0.09455953538417816,
                                                    1.974007487297058,
                                                    0.09464676678180695,
                                                    0.09465840458869934,
                                                    0.10140714049339294,
                                                    0.09454220533370972,
                                                    0.09462330490350723,
                                                    0.09451285749673843,
                                                    4.050822734832764,
                                                    0.09465648978948593,
                                                    0.09463375806808472,
                                                    0.09465823322534561,
                                                    0.09469760209321976,
                                                    0.09466546028852463,
                                                    932.041259765625,
                                                    0.09463147073984146,
                                                    3.409330129623413,
                                                    54.3207893371582,
                                                    0.09465781599283218,
                                                    0.09465416520833969,
                                                    1.7536137104034424,
                                                    0.09468229115009308,
                                                    84.98772430419922,
                                                    0.09475217014551163,
                                                    0.09401313960552216,
                                                    134.52792358398438,
                                                    0.09469401836395264,
                                                    0.23205254971981049,
                                                    0.0946466401219368,
                                                    0.09470997005701065,
                                                    0.09463036060333252,
                                                    0.09456641972064972,
                                                    0.09461867809295654,
                                                    0.9174342155456543,
                                                    0.09432287514209747,
                                                    3.919257879257202,
                                                    0.09378696233034134,
                                                    0.09345461428165436,
                                                    0.09351752698421478,
                                                    0.09323997050523758,
                                                    0.09312094748020172,
                                                    0.09307726472616196,
                                                    0.09321693331003189,
                                                    0.09316740185022354,
                                                    0.09299517422914505,
                                                    0.09285470098257065,
                                                    0.2572370171546936,
                                                    0.09311380237340927,
                                                    0.09293543547391891,
                                                    0.09303213655948639,
                                                    0.1493358463048935,
                                                    0.09310180693864822,
                                                    0.09321630746126175,
                                                    10.437809944152832,
                                                    0.0943022146821022,
                                                    0.09423695504665375,
                                                    2590.866943359375,
                                                    1.4054656028747559,
                                                    38.2252311706543,
                                                    19227.08203125,
                                                    0.09473899751901627,
                                                    0.09466087073087692,
                                                    0.0946071520447731,
                                                    830.8789672851562,
                                                    0.0945894792675972,
                                                    2.7011053562164307,
                                                    0.504011332988739,
                                                    0.09468135982751846,
                                                    0.09465712308883667,
                                                    0.0945027694106102,
                                                    3.204328775405884,
                                                    0.09469970315694809,
                                                    136.45835876464844,
                                                    0.0946570485830307,
                                                    276.604248046875,
                                                    0.12033631652593613,
                                                    0.09448137134313583,
                                                    1677058.875,
                                                    0.09450552612543106,
                                                    6.231417179107666,
                                                    0.4628032147884369,
                                                    0.12920531630516052,
                                                    0.5548384189605713,
                                                    0.09467286616563797,
                                                    0.09465939551591873,
                                                    0.09464029967784882,
                                                    0.09545759111642838,
                                                    0.09464362263679504,
                                                    1.4605083465576172,
                                                    0.6265268921852112,
                                                    0.13141100108623505,
                                                    0.09462108463048935,
                                                    0.09455641359090805,
                                                    5.9682111740112305,
                                                    0.09459859877824783,
                                                    32.384986877441406,
                                                    0.09444091469049454,
                                                    0.09422245621681213,
                                                    0.0940980538725853,
                                                    0.09372019022703171,
                                                    0.09348495304584503,
                                                    0.09494473040103912,
                                                    0.09471526741981506,
                                                    0.09466321766376495,
                                                    0.09463952481746674,
                                                    0.0946340337395668,
                                                    0.0946597084403038,
                                                    0.10840669274330139,
                                                    0.09467717260122299,
                                                    0.09460677951574326,
                                                    0.09988532215356827,
                                                    0.09422221034765244,
                                                    0.11676912754774094,
                                                    0.09440311044454575,
                                                    45.41225051879883,
                                                    507.40032958984375,
                                                    0.13748902082443237,
                                                    0.09470012038946152,
                                                    226.58145141601562,
                                                    0.4495183825492859,
                                                    0.09465526789426804,
                                                    0.09466543048620224,
                                                    61.091617584228516,
                                                    0.09468193352222443,
                                                    0.09465447813272476,
                                                    0.0945592001080513,
                                                    0.09467066824436188,
                                                    0.09464655071496964,
                                                    141.53970336914062,
                                                    116328.4609375,
                                                    0.09458193182945251,
                                                    0.12122631818056107,
                                                    4.387285232543945,
                                                    2681976.5,
                                                    0.09463132172822952,
                                                    0.0946061760187149,
                                                    0.0944783166050911,
                                                    0.09446098655462265,
                                                    0.09450376033782959,
                                                    0.09455794841051102,
                                                    0.09459281712770462,
                                                    0.09462811797857285,
                                                    6.4233717918396,
                                                    0.09450298547744751,
                                                    0.09456917643547058,
                                                    0.0934096947312355,
                                                    0.0934649258852005,
                                                    0.09316615760326385,
                                                    0.5173411965370178,
                                                    1.9697647094726562,
                                                    32.42613983154297,
                                                    0.09472211450338364,
                                                    0.09469816833734512,
                                                    0.0956822857260704,
                                                    0.09459832310676575,
                                                    0.0946735143661499,
                                                    0.09462333470582962,
                                                    0.09459586441516876,
                                                    1.9096038341522217,
                                                    0.0946708396077156,
                                                    0.0946536511182785,
                                                    0.09466943144798279,
                                                    0.09466858953237534,
                                                    0.09464891254901886,
                                                    0.09463982284069061,
                                                    0.09469345957040787,
                                                    0.0946459099650383,
                                                    0.0945952758193016,
                                                    122.37969970703125,
                                                    4.63157844543457,
                                                    56.718482971191406,
                                                    0.18920323252677917,
                                                    454.80084228515625,
                                                    4036.590087890625,
                                                    0.09464812278747559,
                                                    127179.6875,
                                                    0.09467849880456924,
                                                    0.09463875740766525,
                                                    0.09431662410497665,
                                                    665.6954956054688,
                                                    0.09449782967567444,
                                                    0.09459271281957626,
                                                    0.09455017000436783,
                                                    2.904323101043701,
                                                    0.09459733217954636,
                                                    0.0946681872010231,
                                                    0.09470684081315994,
                                                    3.52655029296875,
                                                    0.09464657306671143,
                                                    0.09463555365800858,
                                                    1.8135828971862793,
                                                    0.09465093165636063,
                                                    0.0946563184261322,
                                                    0.09455125033855438,
                                                    0.09466362744569778,
                                                    0.0946488231420517,
                                                    2.0649492740631104,
                                                    1.0428603887557983,
                                                    0.09464512020349503,
                                                    0.09463749080896378,
                                                    0.09463761001825333,
                                                    204.697509765625,
                                                    0.09467710554599762],
                             'val_accuracy_history': [1.7894736842105263,
                                                      1.368421052631579,
                                                      0.3157894736842105,
                                                      1.0526315789473684,
                                                      1.0,
                                                      0.42105263157894735,
                                                      0.6842105263157895,
                                                      1.4210526315789473,
                                                      0.10526315789473684,
                                                      0.3157894736842105,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      3.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      3.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      2.1578947368421053,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      3.0,
                                                      0.0,
                                                      1.0,
                                                      1.2105263157894737,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      0.5263157894736842,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      4.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      3.0,
                                                      1.0,
                                                      0.631578947368421,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      1.631578947368421,
                                                      1.8421052631578947,
                                                      0.0,
                                                      2.0,
                                                      0.9473684210526315,
                                                      0.47368421052631576,
                                                      0.7894736842105263,
                                                      0.05263157894736842,
                                                      3.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.2105263157894737,
                                                      0.8421052631578947,
                                                      0.21052631578947367,
                                                      0.9473684210526315,
                                                      1.5263157894736843,
                                                      1.0526315789473684,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      3.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.6842105263157894,
                                                      1.9473684210526316,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      3.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      3.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      3.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      3.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      3.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      3.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.7777777777777778,
                                                      0.1111111111111111,
                                                      0.8888888888888888,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.7777777777777778,
                                                      0.7777777777777778,
                                                      0.7777777777777778,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      3.0,
                                                      1.0,
                                                      2.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.1111111111111111,
                                                      0.4444444444444444,
                                                      0.8888888888888888,
                                                      0.2222222222222222,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      2.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      3.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.8888888888888888,
                                                      0.5555555555555556,
                                                      0.7777777777777778,
                                                      1.0,
                                                      0.2222222222222222,
                                                      0.3333333333333333,
                                                      0.1111111111111111,
                                                      0.3333333333333333,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      3.0,
                                                      1.0,
                                                      0.8888888888888888,
                                                      1.0,
                                                      0.8888888888888888,
                                                      0.0,
                                                      2.6666666666666665,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      1.7777777777777777,
                                                      0.0,
                                                      3.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      3.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      3.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      4.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.8888888888888888,
                                                      0.4444444444444444,
                                                      0.6666666666666666,
                                                      0.1111111111111111,
                                                      0.3333333333333333,
                                                      1.0,
                                                      3.0,
                                                      2.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      3.0,
                                                      0.0,
                                                      3.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      4.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      3.0,
                                                      3.0,
                                                      1.0,
                                                      1.0,
                                                      2.0526315789473686,
                                                      1.736842105263158,
                                                      2.9473684210526314,
                                                      1.894736842105263,
                                                      4.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      3.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      2.0,
                                                      0.3684210526315789,
                                                      2.6315789473684212,
                                                      2.526315789473684,
                                                      1.894736842105263,
                                                      4.0,
                                                      2.0,
                                                      3.0,
                                                      1.0,
                                                      2.0,
                                                      3.0526315789473686,
                                                      1.263157894736842,
                                                      1.105263157894737,
                                                      0.21052631578947367,
                                                      1.1578947368421053,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      3.0,
                                                      2.0,
                                                      2.0,
                                                      4.0,
                                                      3.0,
                                                      3.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      2.0,
                                                      2.0,
                                                      0.0,
                                                      2.0,
                                                      3.210526315789474,
                                                      0.10526315789473684,
                                                      3.0,
                                                      0.0,
                                                      5.0,
                                                      2.0,
                                                      1.0,
                                                      3.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.2631578947368421,
                                                      0.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      4.0,
                                                      0.0,
                                                      2.0,
                                                      2.0,
                                                      3.0,
                                                      4.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      3.0,
                                                      2.210526315789474,
                                                      3.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.10526315789473684,
                                                      4.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.8421052631578947,
                                                      2.4210526315789473,
                                                      1.4736842105263157,
                                                      0.3157894736842105,
                                                      0.7894736842105263,
                                                      1.6842105263157894,
                                                      1.631578947368421,
                                                      1.9473684210526316,
                                                      1.6842105263157894,
                                                      1.8421052631578947,
                                                      1.894736842105263,
                                                      0.6842105263157895,
                                                      1.0526315789473684,
                                                      0.8947368421052632,
                                                      2.0,
                                                      2.4210526315789473,
                                                      1.4210526315789473,
                                                      0.9473684210526315,
                                                      1.1578947368421053,
                                                      0.0,
                                                      0.9473684210526315,
                                                      1.0,
                                                      4.0,
                                                      4.0,
                                                      2.0,
                                                      3.0,
                                                      1.0,
                                                      2.0,
                                                      2.0,
                                                      0.0,
                                                      2.0,
                                                      3.0,
                                                      2.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      3.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      3.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      3.0,
                                                      3.0,
                                                      1.0,
                                                      0.0,
                                                      3.0,
                                                      2.0,
                                                      2.0,
                                                      0.0,
                                                      2.0,
                                                      0.8947368421052632,
                                                      1.5263157894736843,
                                                      1.6842105263157894,
                                                      0.2631578947368421,
                                                      1.0,
                                                      3.0,
                                                      5.0,
                                                      2.0,
                                                      3.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      2.789473684210526,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      4.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      4.0,
                                                      2.0,
                                                      3.0,
                                                      2.0,
                                                      4.0,
                                                      3.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      1.631578947368421,
                                                      0.21052631578947367,
                                                      1.736842105263158,
                                                      0.9473684210526315,
                                                      1.4736842105263157,
                                                      1.0,
                                                      2.0,
                                                      2.0,
                                                      3.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      4.0,
                                                      2.0,
                                                      3.0,
                                                      3.0,
                                                      4.0,
                                                      2.0,
                                                      3.0,
                                                      0.0,
                                                      2.0,
                                                      3.0,
                                                      1.0,
                                                      2.0,
                                                      3.0,
                                                      2.0,
                                                      2.0,
                                                      1.894736842105263,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      5.0,
                                                      3.0,
                                                      3.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      2.0],
                             'val_loss': 0.0900145098567009,
                             'val_loss_history': [0.13289986550807953,
                                                  30.159862518310547,
                                                  0.1306895911693573,
                                                  772.6478881835938,
                                                  0.12578943371772766,
                                                  0.1313501000404358,
                                                  0.12481862306594849,
                                                  0.12818337976932526,
                                                  0.12867261469364166,
                                                  0.13204559683799744,
                                                  0.1220577135682106,
                                                  0.11260901391506195,
                                                  0.11409381777048111,
                                                  0.11440904438495636,
                                                  0.11434926837682724,
                                                  0.11414065212011337,
                                                  0.11439044773578644,
                                                  0.11462827026844025,
                                                  0.11452741175889969,
                                                  0.11467263847589493,
                                                  0.11460597068071365,
                                                  0.11462745815515518,
                                                  0.11468517035245895,
                                                  0.11480430513620377,
                                                  0.1147732213139534,
                                                  0.11485739797353745,
                                                  0.11479106545448303,
                                                  0.1150507852435112,
                                                  0.11502186954021454,
                                                  0.11493460834026337,
                                                  0.11511021852493286,
                                                  0.11504624783992767,
                                                  0.1150892823934555,
                                                  0.11518291383981705,
                                                  64640324.0,
                                                  0.11498195677995682,
                                                  0.11846525222063065,
                                                  0.11501139402389526,
                                                  0.11516815423965454,
                                                  0.11503072828054428,
                                                  0.11535662412643433,
                                                  0.11514862626791,
                                                  0.11519207060337067,
                                                  0.11522386968135834,
                                                  0.11516549438238144,
                                                  0.11545438319444656,
                                                  0.11529026180505753,
                                                  0.11528454720973969,
                                                  0.11551813781261444,
                                                  0.11532633006572723,
                                                  0.1173066645860672,
                                                  0.11544819921255112,
                                                  0.11565966159105301,
                                                  0.1153019443154335,
                                                  0.11566291004419327,
                                                  0.11555980145931244,
                                                  0.11566717177629471,
                                                  0.11576888710260391,
                                                  0.11573272943496704,
                                                  0.11575499922037125,
                                                  0.11589116603136063,
                                                  0.11577892303466797,
                                                  0.11567796766757965,
                                                  0.11675383150577545,
                                                  0.11572939157485962,
                                                  0.11599055677652359,
                                                  0.11586335301399231,
                                                  0.11595223098993301,
                                                  0.115937240421772,
                                                  0.11586461216211319,
                                                  0.11587101221084595,
                                                  0.1159282699227333,
                                                  0.11588104814291,
                                                  0.11595000326633453,
                                                  0.11601797491312027,
                                                  0.11578642576932907,
                                                  0.1159607544541359,
                                                  0.11591330915689468,
                                                  0.1158139705657959,
                                                  0.1159902960062027,
                                                  0.1159883365035057,
                                                  0.1158863827586174,
                                                  0.11616058647632599,
                                                  0.116038978099823,
                                                  0.11605293303728104,
                                                  0.11608398705720901,
                                                  0.11597417294979095,
                                                  0.11598964035511017,
                                                  0.11528076976537704,
                                                  0.1164831668138504,
                                                  0.11611490696668625,
                                                  0.11596962809562683,
                                                  0.11598684638738632,
                                                  0.11591029167175293,
                                                  0.11605054140090942,
                                                  0.11600024253129959,
                                                  0.1159677505493164,
                                                  0.11592534184455872,
                                                  0.11612578481435776,
                                                  0.11618667840957642,
                                                  0.11620379984378815,
                                                  0.11618492752313614,
                                                  0.11606466770172119,
                                                  0.11596335470676422,
                                                  0.11628828197717667,
                                                  0.1161775216460228,
                                                  0.11668728291988373,
                                                  0.11626330763101578,
                                                  0.11897259205579758,
                                                  0.11721517890691757,
                                                  0.11805533617734909,
                                                  0.11683106422424316,
                                                  0.11628374457359314,
                                                  0.11647655814886093,
                                                  0.11625979095697403,
                                                  0.11615417152643204,
                                                  0.11611239612102509,
                                                  0.11597920954227448,
                                                  0.11626138538122177,
                                                  0.11619382351636887,
                                                  0.11614691466093063,
                                                  0.1158432811498642,
                                                  0.11844415962696075,
                                                  0.12012633681297302,
                                                  0.12201090902090073,
                                                  0.12300722301006317,
                                                  0.12179263681173325,
                                                  0.12022022157907486,
                                                  0.1161445826292038,
                                                  0.11631754785776138,
                                                  0.11599871516227722,
                                                  0.11595051735639572,
                                                  0.11614725738763809,
                                                  0.11601608991622925,
                                                  0.11616994440555573,
                                                  0.11606497317552567,
                                                  0.11608567833900452,
                                                  0.11612219363451004,
                                                  0.11595229059457779,
                                                  0.1160753071308136,
                                                  0.11604484170675278,
                                                  0.11610965430736542,
                                                  0.11596120893955231,
                                                  0.11606322973966599,
                                                  0.1158863827586174,
                                                  0.1162295863032341,
                                                  0.11610893160104752,
                                                  0.11617756634950638,
                                                  0.11616548895835876,
                                                  0.11614851653575897,
                                                  0.11626169085502625,
                                                  0.11611983180046082,
                                                  0.116350457072258,
                                                  0.1160777285695076,
                                                  0.11605817079544067,
                                                  0.1161554679274559,
                                                  0.1161135733127594,
                                                  0.11618547886610031,
                                                  0.11609655618667603,
                                                  0.11594346165657043,
                                                  0.11609303951263428,
                                                  0.11628188192844391,
                                                  0.11593768745660782,
                                                  0.116062231361866,
                                                  0.11615301668643951,
                                                  0.11612395197153091,
                                                  0.11609595268964767,
                                                  0.11606339365243912,
                                                  0.11621040850877762,
                                                  0.11611215770244598,
                                                  0.11631068587303162,
                                                  0.11604316532611847,
                                                  0.11610415577888489,
                                                  0.11617108434438705,
                                                  0.11737527698278427,
                                                  0.11636664718389511,
                                                  0.11643633991479874,
                                                  0.11641830205917358,
                                                  0.11623018234968185,
                                                  0.11627576500177383,
                                                  0.11618880182504654,
                                                  0.11590965837240219,
                                                  0.11600705981254578,
                                                  0.11603537946939468,
                                                  0.11614096164703369,
                                                  0.11609300225973129,
                                                  0.11620457470417023,
                                                  0.11623869836330414,
                                                  0.11612760275602341,
                                                  0.11619135737419128,
                                                  0.11633993685245514,
                                                  0.11595872789621353,
                                                  0.1160760223865509,
                                                  0.11613026261329651,
                                                  0.11629821360111237,
                                                  0.11606990545988083,
                                                  0.11618290096521378,
                                                  0.11590874940156937,
                                                  0.116066575050354,
                                                  0.1162467747926712,
                                                  0.11605139076709747,
                                                  0.11606994271278381,
                                                  0.11615034192800522,
                                                  0.11626721173524857,
                                                  0.1164214015007019,
                                                  0.11610067635774612,
                                                  0.11607576906681061,
                                                  0.11610443145036697,
                                                  0.11622398346662521,
                                                  0.11609799414873123,
                                                  0.11592576652765274,
                                                  0.11589066684246063,
                                                  0.11615613847970963,
                                                  0.11620646715164185,
                                                  0.11605507135391235,
                                                  0.11605215072631836,
                                                  0.11597687005996704,
                                                  0.11605720967054367,
                                                  0.11622954159975052,
                                                  0.11576541513204575,
                                                  0.11619725078344345,
                                                  0.11608881503343582,
                                                  0.11614267528057098,
                                                  0.11628157645463943,
                                                  0.11605657637119293,
                                                  0.11614761501550674,
                                                  0.11601652950048447,
                                                  0.11608336865901947,
                                                  0.1161508783698082,
                                                  0.11626416444778442,
                                                  0.11630623042583466,
                                                  0.1164545938372612,
                                                  0.11624109745025635,
                                                  0.11607788503170013,
                                                  0.1161070168018341,
                                                  0.11620175093412399,
                                                  0.11616086959838867,
                                                  0.11611688882112503,
                                                  0.11600390076637268,
                                                  0.11609793454408646,
                                                  0.11598715931177139,
                                                  0.11622306704521179,
                                                  0.11612720787525177,
                                                  0.11618942022323608,
                                                  0.11620881408452988,
                                                  0.11601428687572479,
                                                  0.11616939306259155,
                                                  0.11625801771879196,
                                                  0.11612837016582489,
                                                  0.11634919047355652,
                                                  0.11619079858064651,
                                                  0.11613990366458893,
                                                  0.11610959470272064,
                                                  0.1163247600197792,
                                                  0.11617305129766464,
                                                  0.11615300923585892,
                                                  0.11632908880710602,
                                                  0.11621703207492828,
                                                  0.11634662002325058,
                                                  0.11614250391721725,
                                                  0.11630403250455856,
                                                  0.11608672142028809,
                                                  0.11608536541461945,
                                                  31432.908203125,
                                                  0.1162625327706337,
                                                  0.11617450416088104,
                                                  0.11622246354818344,
                                                  0.11625657230615616,
                                                  0.11630494892597198,
                                                  0.1160629466176033,
                                                  0.11630386859178543,
                                                  0.11654241383075714,
                                                  0.11633019894361496,
                                                  0.11625935137271881,
                                                  0.1159849464893341,
                                                  0.11611940711736679,
                                                  0.11628160625696182,
                                                  0.11616548895835876,
                                                  0.1158863976597786,
                                                  0.11616910994052887,
                                                  0.11640705168247223,
                                                  57.9826545715332,
                                                  0.11621317267417908,
                                                  0.1162533313035965,
                                                  0.11624367535114288,
                                                  0.11624933034181595,
                                                  0.11617560684680939,
                                                  0.11601812392473221,
                                                  0.11616231501102448,
                                                  0.1162579134106636,
                                                  0.1163044273853302,
                                                  0.11610657721757889,
                                                  0.11615923047065735,
                                                  0.11604088544845581,
                                                  0.11613348871469498,
                                                  0.1160733699798584,
                                                  0.11620483547449112,
                                                  0.11624867469072342,
                                                  0.11615971475839615,
                                                  0.11607415229082108,
                                                  75018216.0,
                                                  0.09041483700275421,
                                                  0.09068240970373154,
                                                  0.09158043563365936,
                                                  0.09161237627267838,
                                                  0.09168776124715805,
                                                  0.09167324006557465,
                                                  0.09174589067697525,
                                                  0.09159623086452484,
                                                  0.091584213078022,
                                                  0.09181859344244003,
                                                  0.09148017317056656,
                                                  0.09162292629480362,
                                                  0.09164972603321075,
                                                  0.09172000735998154,
                                                  0.09156201779842377,
                                                  0.09159772098064423,
                                                  0.09161564707756042,
                                                  0.09180383384227753,
                                                  0.09165830910205841,
                                                  0.09175550937652588,
                                                  0.09164596349000931,
                                                  0.09165975451469421,
                                                  0.09162546694278717,
                                                  0.09173918515443802,
                                                  0.09151586145162582,
                                                  0.09154339134693146,
                                                  0.09142778068780899,
                                                  0.09155946969985962,
                                                  0.0915718749165535,
                                                  0.09148793667554855,
                                                  0.09153666347265244,
                                                  0.0915401503443718,
                                                  0.0918232873082161,
                                                  0.09156225621700287,
                                                  0.09150882065296173,
                                                  0.09154771268367767,
                                                  0.09136240184307098,
                                                  0.09136436134576797,
                                                  0.09138547629117966,
                                                  0.09111592918634415,
                                                  0.09049556404352188,
                                                  0.09093768894672394,
                                                  0.09028400480747223,
                                                  0.0910341814160347,
                                                  0.09143580496311188,
                                                  0.09173454344272614,
                                                  0.09146896004676819,
                                                  0.09174791723489761,
                                                  0.09165650606155396,
                                                  0.09164563566446304,
                                                  0.09150607138872147,
                                                  0.09162599593400955,
                                                  0.09160754829645157,
                                                  0.0916544646024704,
                                                  0.09154925495386124,
                                                  0.09158218652009964,
                                                  0.09168332815170288,
                                                  0.09178471565246582,
                                                  0.09159593284130096,
                                                  0.09167377650737762,
                                                  0.09158945083618164,
                                                  0.09157179296016693,
                                                  0.09173416346311569,
                                                  0.09171082079410553,
                                                  0.0915522649884224,
                                                  0.09138037264347076,
                                                  6550871.0,
                                                  0.0917968600988388,
                                                  0.09176656603813171,
                                                  0.09189419448375702,
                                                  0.09189261496067047,
                                                  0.09148921072483063,
                                                  0.0916379913687706,
                                                  0.09164125472307205,
                                                  0.09168343245983124,
                                                  0.0915914997458458,
                                                  0.0916556641459465,
                                                  0.09176559001207352,
                                                  0.09119798988103867,
                                                  0.09149183332920074,
                                                  0.09167253226041794,
                                                  0.09157314896583557,
                                                  0.09136173874139786,
                                                  0.09172654151916504,
                                                  0.09172135591506958,
                                                  0.0916282907128334,
                                                  0.09151092171669006,
                                                  0.09152401983737946,
                                                  0.09146179258823395,
                                                  0.09152831137180328,
                                                  0.09171596169471741,
                                                  0.09146741032600403,
                                                  0.09163735806941986,
                                                  0.09172643721103668,
                                                  0.09151304513216019,
                                                  0.09167574346065521,
                                                  0.09155893325805664,
                                                  0.09151120483875275,
                                                  0.09154047071933746,
                                                  0.09145689010620117,
                                                  0.09163273870944977,
                                                  0.09142628312110901,
                                                  0.09159869700670242,
                                                  0.09154431521892548,
                                                  0.09110464155673981,
                                                  0.09078102558851242,
                                                  0.0907980427145958,
                                                  0.09076491743326187,
                                                  0.09136105328798294,
                                                  0.09158964455127716,
                                                  0.09161243587732315,
                                                  0.09146032482385635,
                                                  0.09154172241687775,
                                                  0.0915302038192749,
                                                  0.09143901616334915,
                                                  0.09158524125814438,
                                                  0.09163603186607361,
                                                  0.09191957861185074,
                                                  0.09158330410718918,
                                                  0.0916702076792717,
                                                  0.09173353016376495,
                                                  0.09168414771556854,
                                                  0.09165389090776443,
                                                  0.09153973311185837,
                                                  0.09151528030633926,
                                                  0.09149136394262314,
                                                  0.09165231138467789,
                                                  0.09150249511003494,
                                                  0.09171602129936218,
                                                  0.09170891344547272,
                                                  0.09149736166000366,
                                                  0.09154205024242401,
                                                  0.09144985675811768,
                                                  0.09173857420682907,
                                                  0.09168737381696701,
                                                  0.09159638732671738,
                                                  0.09164682775735855,
                                                  0.09182272851467133,
                                                  0.09188602864742279,
                                                  0.0916704535484314,
                                                  0.09181439131498337,
                                                  0.0916689857840538,
                                                  0.0916949138045311,
                                                  0.0918634682893753,
                                                  0.091732919216156,
                                                  0.09177081286907196,
                                                  0.09180489182472229,
                                                  0.09170784801244736,
                                                  0.09166619181632996,
                                                  0.09147465229034424,
                                                  0.09065674990415573,
                                                  0.09165985137224197,
                                                  0.0915789008140564,
                                                  0.09145854413509369,
                                                  0.0916421115398407,
                                                  0.09175586700439453,
                                                  0.0916069895029068,
                                                  0.091783806681633,
                                                  0.09182631224393845,
                                                  0.0917629525065422,
                                                  0.09174057096242905,
                                                  0.09179218113422394,
                                                  0.09166530519723892,
                                                  0.09162689000368118,
                                                  0.09167832881212234,
                                                  0.09155615419149399,
                                                  0.09177278727293015,
                                                  0.09168699383735657,
                                                  0.09166309982538223,
                                                  0.09175059199333191,
                                                  0.09156449139118195,
                                                  0.09154761582612991,
                                                  0.09163499623537064,
                                                  0.09183468669652939,
                                                  0.09155910462141037,
                                                  0.09153896570205688,
                                                  0.09173861145973206,
                                                  0.09150022268295288,
                                                  0.09156262874603271,
                                                  0.09158863872289658,
                                                  0.091577909886837,
                                                  0.0914931669831276,
                                                  0.09152447432279587,
                                                  0.09160035848617554,
                                                  0.0915502980351448,
                                                  0.09158534556627274,
                                                  0.09145323932170868,
                                                  0.09149032831192017,
                                                  0.09140226989984512,
                                                  0.09157698601484299,
                                                  0.09184463322162628,
                                                  0.09163165837526321,
                                                  0.09175490587949753,
                                                  0.09173326194286346,
                                                  0.09146177023649216,
                                                  0.09170445799827576,
                                                  0.09182441979646683,
                                                  0.09142305701971054,
                                                  0.09153810143470764,
                                                  0.09144371002912521,
                                                  0.09164082258939743,
                                                  0.09147138893604279,
                                                  0.09169746935367584,
                                                  0.09146449714899063,
                                                  0.0915309488773346,
                                                  0.09153302758932114,
                                                  0.09078874439001083,
                                                  0.09040026366710663,
                                                  0.0903313085436821,
                                                  0.09044385701417923,
                                                  0.09064793586730957,
                                                  0.09086998552083969,
                                                  0.0905841663479805,
                                                  0.09128729999065399,
                                                  0.09156367182731628,
                                                  0.09157119691371918,
                                                  0.09156181663274765,
                                                  0.091734878718853,
                                                  0.09174881130456924,
                                                  0.09162548929452896,
                                                  0.09173157066106796,
                                                  0.09165001660585403,
                                                  0.09184536337852478,
                                                  0.09174542874097824,
                                                  0.09148374944925308,
                                                  0.0918145477771759,
                                                  0.09172198921442032,
                                                  0.09163372218608856,
                                                  0.09175238013267517,
                                                  0.09166068583726883,
                                                  0.09152699261903763,
                                                  0.09570552408695221,
                                                  0.09075891226530075,
                                                  0.09028211236000061,
                                                  0.09057898819446564,
                                                  0.09034465253353119,
                                                  0.091373510658741,
                                                  0.09163247048854828,
                                                  0.0916837826371193,
                                                  623378176.0,
                                                  0.09151039272546768,
                                                  0.09154417365789413,
                                                  0.09182267636060715,
                                                  0.0917121171951294,
                                                  0.09159564971923828,
                                                  0.09143423289060593,
                                                  0.09164773672819138,
                                                  0.09148409217596054,
                                                  0.0917004719376564,
                                                  0.0917796865105629,
                                                  0.0917053371667862,
                                                  0.09163802117109299,
                                                  597140.375,
                                                  0.0918307676911354,
                                                  0.09166297316551208,
                                                  0.09173108637332916,
                                                  0.09177133440971375,
                                                  0.09167598187923431,
                                                  0.09167742729187012,
                                                  0.09176761656999588,
                                                  0.09161846339702606,
                                                  0.09151538461446762,
                                                  0.09162520617246628,
                                                  0.09155528992414474,
                                                  0.09133844822645187,
                                                  0.09172411262989044,
                                                  0.09168177098035812,
                                                  0.09152256697416306,
                                                  0.09153567254543304,
                                                  0.09174582362174988,
                                                  0.09146131575107574,
                                                  0.09169001132249832,
                                                  0.09183165431022644,
                                                  0.09155264496803284,
                                                  0.09179975837469101,
                                                  0.09141802042722702,
                                                  0.09178242087364197,
                                                  0.09155882894992828,
                                                  0.09155619144439697,
                                                  0.09172812104225159,
                                                  0.09160418808460236,
                                                  0.09162043035030365,
                                                  0.0916271060705185,
                                                  0.0916542038321495,
                                                  0.09182074666023254,
                                                  0.091661237180233,
                                                  0.09168948233127594,
                                                  0.09161534905433655,
                                                  0.09176906198263168,
                                                  0.09174256771802902,
                                                  0.09184432774782181,
                                                  0.09175481647253036,
                                                  0.09163566678762436,
                                                  0.09110815823078156,
                                                  0.09047967940568924,
                                                  0.09134313464164734,
                                                  0.09058742970228195,
                                                  0.09057310223579407,
                                                  0.09021198004484177,
                                                  0.08950533717870712,
                                                  0.08991483598947525,
                                                  0.09000581502914429,
                                                  0.08975820243358612,
                                                  0.08989927172660828,
                                                  0.08977536857128143,
                                                  0.09046599268913269,
                                                  0.09019088745117188,
                                                  0.08985491842031479,
                                                  0.09006495773792267,
                                                  0.09006132930517197,
                                                  0.08998249471187592,
                                                  0.08991481363773346,
                                                  0.09035725891590118,
                                                  0.09026259183883667,
                                                  0.09033244848251343,
                                                  0.09021812677383423,
                                                  0.08977342396974564,
                                                  0.08995147794485092,
                                                  0.09018651396036148,
                                                  0.0902789831161499,
                                                  0.09030649065971375,
                                                  0.09018151462078094,
                                                  0.08963830769062042,
                                                  0.09024237841367722,
                                                  0.08991577476263046,
                                                  0.09044059365987778,
                                                  0.08928724378347397,
                                                  0.08972622454166412,
                                                  0.08882772922515869,
                                                  0.08975745737552643,
                                                  0.09016703814268112,
                                                  0.08963004499673843,
                                                  0.0898420587182045,
                                                  0.08989901095628738,
                                                  0.09010357409715652,
                                                  0.0901118665933609,
                                                  0.09002404659986496,
                                                  0.09008162468671799,
                                                  0.08998696506023407,
                                                  0.09019099175930023,
                                                  0.09039027988910675,
                                                  0.09014302492141724,
                                                  0.09035284072160721,
                                                  0.090223528444767,
                                                  0.08909245580434799,
                                                  0.08939402550458908,
                                                  0.08942470699548721,
                                                  0.08992207795381546,
                                                  0.08974660187959671,
                                                  0.09004580974578857,
                                                  0.09003043174743652,
                                                  0.08954887092113495,
                                                  0.08958173543214798,
                                                  0.0888582319021225,
                                                  0.08949612826108932,
                                                  0.0890330970287323,
                                                  0.08879958093166351,
                                                  0.08949175477027893,
                                                  0.09019462019205093,
                                                  0.08992094546556473,
                                                  0.08964835852384567,
                                                  0.08991514146327972,
                                                  0.08982381969690323,
                                                  0.08980073034763336,
                                                  0.0896264985203743,
                                                  0.08992141485214233,
                                                  0.09008410573005676,
                                                  0.08989385515451431,
                                                  0.09021643549203873,
                                                  0.09013224393129349,
                                                  0.0900585800409317,
                                                  0.08978967368602753,
                                                  0.09012910723686218,
                                                  0.08994796127080917,
                                                  0.09000711888074875,
                                                  0.08993218839168549,
                                                  0.09006299823522568,
                                                  0.08917942643165588,
                                                  0.08967818319797516,
                                                  0.08993396908044815,
                                                  0.09023486822843552,
                                                  0.0900617465376854,
                                                  0.09012560546398163,
                                                  0.08974074572324753,
                                                  0.09009005129337311,
                                                  0.08979517966508865,
                                                  0.08997286111116409,
                                                  0.09035619348287582,
                                                  0.09010802954435349,
                                                  0.09086864441633224,
                                                  0.0897199809551239,
                                                  0.08955328911542892,
                                                  0.08973503112792969,
                                                  0.0901649221777916,
                                                  0.08986207097768784,
                                                  0.09005796909332275,
                                                  0.08977393060922623,
                                                  0.09000321477651596,
                                                  0.08998708426952362,
                                                  0.08971834927797318,
                                                  0.0897836983203888,
                                                  0.09028778970241547,
                                                  0.08969186991453171,
                                                  0.09012392163276672,
                                                  0.08985482901334763,
                                                  0.09004218131303787,
                                                  0.09037664532661438,
                                                  0.09028197079896927,
                                                  0.09011863172054291,
                                                  0.0896797850728035,
                                                  0.0896434634923935,
                                                  0.0899115651845932,
                                                  0.08928757160902023,
                                                  0.08990449458360672,
                                                  0.08990985155105591,
                                                  0.09001431614160538,
                                                  0.0895829126238823,
                                                  0.09010770171880722,
                                                  0.08968362212181091,
                                                  0.09000500291585922,
                                                  0.0898640975356102,
                                                  0.08954513818025589,
                                                  123260808.0,
                                                  0.09009537845849991,
                                                  0.08999543637037277,
                                                  0.09009146690368652,
                                                  0.08993417024612427,
                                                  0.08979174494743347,
                                                  0.09024576097726822,
                                                  0.08982154726982117,
                                                  0.09024430066347122,
                                                  0.08962235599756241,
                                                  0.08915040642023087,
                                                  0.08879758417606354,
                                                  0.08888117223978043,
                                                  0.08869042247533798,
                                                  0.08853305876255035,
                                                  0.08868934214115143,
                                                  0.08893421292304993,
                                                  0.0886133685708046,
                                                  0.08817695081233978,
                                                  0.0881962701678276,
                                                  0.08811213821172714,
                                                  0.0891452506184578,
                                                  0.08935011923313141,
                                                  0.08934961259365082,
                                                  0.08890455216169357,
                                                  0.08886080235242844,
                                                  0.0891081690788269,
                                                  0.08976045250892639,
                                                  0.08954772353172302,
                                                  0.08939021825790405,
                                                  0.08975906670093536,
                                                  0.08977613598108292,
                                                  0.0896851047873497,
                                                  0.08977438509464264,
                                                  0.09006603807210922,
                                                  0.08980820327997208,
                                                  0.08988718688488007,
                                                  0.08971526473760605,
                                                  0.09004899114370346,
                                                  0.0901557132601738,
                                                  0.08969920128583908,
                                                  0.09032884240150452,
                                                  0.08998294174671173,
                                                  0.08977649360895157,
                                                  0.09020925313234329,
                                                  0.09029025584459305,
                                                  0.08976597338914871,
                                                  0.08999000489711761,
                                                  0.08985752612352371,
                                                  0.09009544551372528,
                                                  0.08979445695877075,
                                                  0.08981890231370926,
                                                  0.08993826806545258,
                                                  0.09029476344585419,
                                                  0.0901898518204689,
                                                  0.0904805064201355,
                                                  0.0902039036154747,
                                                  0.09018836915493011,
                                                  0.09021788835525513,
                                                  13.298559188842773,
                                                  0.09035386145114899,
                                                  0.08994783461093903,
                                                  0.08987398445606232,
                                                  0.0898030549287796,
                                                  0.09018076211214066,
                                                  0.08989060670137405,
                                                  0.0900542363524437,
                                                  0.08998247236013412,
                                                  0.08996569365262985,
                                                  0.08956349641084671,
                                                  0.0899026170372963,
                                                  0.08934273570775986,
                                                  0.08953327685594559,
                                                  0.08896757662296295,
                                                  40.889766693115234,
                                                  0.0892421156167984,
                                                  0.09008168429136276,
                                                  0.0901041328907013,
                                                  0.08966265618801117,
                                                  0.08987640589475632,
                                                  0.09025770425796509,
                                                  0.0899430364370346,
                                                  0.09005750715732574,
                                                  0.09002253413200378,
                                                  0.08991013467311859,
                                                  0.08955400437116623,
                                                  0.09024599194526672,
                                                  0.08989398926496506,
                                                  0.09036830812692642,
                                                  0.09003657847642899,
                                                  0.09000864624977112,
                                                  0.09005343914031982,
                                                  0.09015601873397827,
                                                  0.09009037166833878,
                                                  0.09033283591270447,
                                                  0.08988620340824127,
                                                  0.09028229117393494,
                                                  0.09019381552934647,
                                                  0.0902322456240654,
                                                  0.09032434225082397,
                                                  0.09011644124984741,
                                                  0.08976441621780396,
                                                  0.08991662412881851,
                                                  0.08978656679391861,
                                                  0.08997219800949097,
                                                  0.09036137908697128,
                                                  0.08971432596445084,
                                                  0.08993349224328995,
                                                  0.08978254348039627,
                                                  0.09011665731668472,
                                                  0.0903627872467041,
                                                  0.09019304066896439,
                                                  0.08973104506731033,
                                                  0.09010143578052521,
                                                  0.09001858532428741,
                                                  0.08998378366231918,
                                                  0.09022297710180283,
                                                  0.09093927592039108,
                                                  0.08909867703914642,
                                                  0.08914345502853394,
                                                  0.08835233747959137,
                                                  0.0882631167769432,
                                                  0.08950014412403107,
                                                  0.08979093283414841,
                                                  0.09012936055660248,
                                                  0.08974586427211761,
                                                  0.09011997282505035,
                                                  0.08974523842334747,
                                                  0.09000830352306366,
                                                  0.09017462283372879,
                                                  0.09004103392362595,
                                                  0.08963128179311752,
                                                  0.08985885232686996,
                                                  0.08983511477708817,
                                                  0.09008574485778809,
                                                  0.09016025811433792,
                                                  0.08972913771867752,
                                                  0.08963147550821304,
                                                  0.08982467651367188,
                                                  0.08994393795728683,
                                                  0.08989205211400986,
                                                  0.08967658132314682,
                                                  0.08987260609865189,
                                                  0.09007982909679413,
                                                  0.09010133147239685,
                                                  0.09020086377859116,
                                                  0.09046005457639694,
                                                  0.08994335681200027,
                                                  0.09018530696630478,
                                                  0.08992722630500793,
                                                  0.09044454246759415,
                                                  0.09027785807847977,
                                                  0.08971437811851501,
                                                  0.08930660039186478,
                                                  0.09013891965150833,
                                                  0.08999187499284744,
                                                  0.08977315574884415,
                                                  0.08965734392404556,
                                                  0.08979973196983337,
                                                  0.09008533507585526,
                                                  0.08993113040924072,
                                                  0.08979012817144394,
                                                  0.08988290280103683,
                                                  0.09004655480384827,
                                                  0.09017106890678406,
                                                  0.089727021753788,
                                                  0.08970198780298233,
                                                  0.09002815186977386,
                                                  0.08984025567770004,
                                                  0.09001448005437851,
                                                  0.08957063406705856,
                                                  0.08992192894220352,
                                                  0.09023972600698471,
                                                  0.09008713066577911,
                                                  0.0900142714381218,
                                                  0.08985602855682373,
                                                  0.0900145098567009],
                             'weight_decay': 0.0005}},
 'models': [{'accuracy': 2.0,
             'batch_size': 32,
             'cv_score': 0.017618830126676397,
             'cv_val_accuracy': 0.7777777777777778,
             'cv_val_loss': 0.09876688073078792,
             'cv_val_macroF1': 0.017618830126676397,
             'cv_val_microF1': 0.07673131407953461,
             'epochs': 300,
             'final_model': GGNN1(
  (ggnn): GatedGraphConv(30, num_layers=2)
  (fc1): Linear(in_features=30, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
             'kwargs': {'aggr_type': 'mean',
                        'd1': 30,
                        'd2': 50,
                        'num_classes': 24,
                        'num_layers': 2},
             'learning_rate': 0.01,
             'macroF1': 0.006748704489763125,
             'microF1': 0.08414023372287145,
             'model': <class 'TFM_graph_classification_models.GGNN1'>,
             'model_instance': GGNN1(
  (ggnn): GatedGraphConv(30, num_layers=2)
  (fc1): Linear(in_features=30, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
             'score': 'f1_macro',
             'time': 3923.382642507553,
             'train_loss_history': [402.3915100097656,
                                    0.627553403377533,
                                    0.1610880345106125,
                                    200590368.0,
                                    127.40691375732422,
                                    0.0838785469532013,
                                    0.08383877575397491,
                                    0.08405112475156784,
                                    0.08400005102157593,
                                    0.11704819649457932,
                                    315.0480651855469,
                                    8.739550590515137,
                                    0.26758015155792236,
                                    150.98406982421875,
                                    1.1856788396835327,
                                    1248861440.0,
                                    2.6898114681243896,
                                    0.08465886861085892,
                                    0.08455833047628403,
                                    0.08453904837369919,
                                    0.08463430404663086,
                                    0.08893854171037674,
                                    2255.366943359375,
                                    5255256.5,
                                    0.0845518633723259,
                                    0.08456744998693466,
                                    0.0846351757645607,
                                    0.08459139615297318,
                                    0.0845951959490776,
                                    0.08459458500146866,
                                    741.9649047851562,
                                    375.397216796875,
                                    0.08453738689422607,
                                    1314.5992431640625,
                                    0.08459823578596115,
                                    2760.76708984375,
                                    0.08449413627386093,
                                    0.0844498947262764,
                                    0.08453848212957382,
                                    0.08451221138238907,
                                    0.08967375755310059,
                                    0.21154144406318665,
                                    6485.376953125,
                                    123632.40625,
                                    29165.78125,
                                    38.000274658203125,
                                    0.08432421088218689,
                                    0.08440204709768295,
                                    0.08437246084213257,
                                    31.02018928527832,
                                    0.08415640890598297,
                                    4.811329364776611,
                                    0.08450804650783539,
                                    0.08431932330131531,
                                    0.0844046100974083,
                                    1.6224708557128906,
                                    0.08452630043029785,
                                    0.08453928679227829,
                                    0.0845048725605011,
                                    0.0844370573759079,
                                    0.08444765210151672,
                                    0.08450065553188324,
                                    0.08445669710636139,
                                    0.08438482880592346,
                                    0.08415114134550095,
                                    0.08441533893346786,
                                    0.08437883108854294,
                                    0.08448990434408188,
                                    0.08448030054569244,
                                    0.0844801515340805,
                                    0.08438432216644287,
                                    0.08433922380208969,
                                    0.08432400226593018,
                                    0.08451466262340546,
                                    0.08452446758747101,
                                    0.08453328907489777,
                                    0.08448510617017746,
                                    0.08447270840406418,
                                    0.08449146896600723,
                                    0.08447691053152084,
                                    0.08443857729434967,
                                    0.08449263125658035,
                                    0.08442482352256775,
                                    0.08449450135231018,
                                    0.0844353586435318,
                                    0.08452147245407104,
                                    0.0844416618347168,
                                    0.08446303755044937,
                                    0.08447965979576111,
                                    0.0843501091003418,
                                    0.08422920107841492,
                                    0.08444242179393768,
                                    0.08447030931711197,
                                    0.08448900282382965,
                                    0.08450789749622345,
                                    0.08435028791427612,
                                    0.08434262871742249,
                                    0.08449234813451767,
                                    0.08449122309684753,
                                    0.08449383825063705,
                                    0.08452076464891434,
                                    0.08449948579072952,
                                    0.08440456539392471,
                                    0.08449050039052963,
                                    0.0841154232621193,
                                    2.2428197860717773,
                                    0.08384676277637482,
                                    0.08394981920719147,
                                    0.0838002935051918,
                                    0.08447454124689102,
                                    0.08348763734102249,
                                    0.08416859805583954,
                                    0.08417699486017227,
                                    0.08438660949468613,
                                    0.4162343144416809,
                                    0.08450004458427429,
                                    0.08446227014064789,
                                    0.08444914221763611,
                                    0.0843791589140892,
                                    0.08446182310581207,
                                    0.08455074578523636,
                                    0.08446145057678223,
                                    0.0836617723107338,
                                    0.08338803797960281,
                                    0.08333099633455276,
                                    0.08318332582712173,
                                    0.08332567662000656,
                                    0.08302734792232513,
                                    0.08371946960687637,
                                    0.08449801802635193,
                                    0.08449846506118774,
                                    0.08426801860332489,
                                    0.08432383090257645,
                                    200335072.0,
                                    0.08437470346689224,
                                    0.08452057838439941,
                                    0.08447948843240738,
                                    0.08449156582355499,
                                    0.08443012088537216,
                                    0.0844721645116806,
                                    0.08447398245334625,
                                    0.08443788439035416,
                                    0.08443618565797806,
                                    0.08449234068393707,
                                    0.10008276253938675,
                                    0.08434968441724777,
                                    0.08430284261703491,
                                    0.08442266285419464,
                                    0.08438229560852051,
                                    1806.7781982421875,
                                    0.08451782912015915,
                                    0.0844816267490387,
                                    13.898186683654785,
                                    0.08445671200752258,
                                    1.4685406684875488,
                                    4.019253253936768,
                                    302.15570068359375,
                                    0.08452155441045761,
                                    0.14563511312007904,
                                    27.54737663269043,
                                    3.7263739109039307,
                                    78.2233657836914,
                                    0.08451861888170242,
                                    0.08448174595832825,
                                    407.52105712890625,
                                    19.42632484436035,
                                    0.08444830775260925,
                                    36449228.0,
                                    7.751605033874512,
                                    0.08446042984724045,
                                    0.08437091112136841,
                                    3167184.25,
                                    0.08450077474117279,
                                    0.08428910374641418,
                                    0.08419448137283325,
                                    0.08466444909572601,
                                    5.597066402435303,
                                    0.11012237519025803,
                                    0.08447238802909851,
                                    58.212467193603516,
                                    5.655828952789307,
                                    3588.997314453125,
                                    0.409007728099823,
                                    32.17847442626953,
                                    0.08447328209877014,
                                    0.0845048725605011,
                                    0.08449171483516693,
                                    0.08447052538394928,
                                    0.08445976674556732,
                                    0.08446221798658371,
                                    0.08458263427019119,
                                    0.08430005609989166,
                                    8.114312171936035,
                                    0.08432270586490631,
                                    0.08445460349321365,
                                    0.08443901687860489,
                                    900.8036499023438,
                                    0.08444087207317352,
                                    0.08441809564828873,
                                    0.4795038402080536,
                                    0.08451033383607864,
                                    0.08451204001903534,
                                    0.08450926095247269,
                                    0.08447223901748657,
                                    0.08437959104776382,
                                    0.08444824069738388,
                                    0.08453644067049026,
                                    7.690800666809082,
                                    0.08448299020528793,
                                    0.08449721336364746,
                                    0.08445368707180023,
                                    0.08438749611377716,
                                    0.08451572060585022,
                                    0.08452669531106949,
                                    89.34236907958984,
                                    0.08450479805469513,
                                    30.75047492980957,
                                    0.08451566100120544,
                                    0.08449652045965195,
                                    68.70880126953125,
                                    0.08445856720209122,
                                    12.459997177124023,
                                    0.08449093997478485,
                                    0.08446609228849411,
                                    0.08448096364736557,
                                    0.11521673947572708,
                                    0.08448649197816849,
                                    1010.2301025390625,
                                    0.08451484888792038,
                                    0.0844508707523346,
                                    0.0843883827328682,
                                    0.08444556593894958,
                                    182.40513610839844,
                                    0.08444695174694061,
                                    304.5606689453125,
                                    0.0844317376613617,
                                    0.08447675406932831,
                                    0.08446475118398666,
                                    0.08447200804948807,
                                    0.08442620933055878,
                                    0.08441336452960968,
                                    0.08443718403577805,
                                    0.08444596081972122,
                                    0.0845516175031662,
                                    0.08445774763822556,
                                    37.44003677368164,
                                    0.0844288170337677,
                                    0.08449657261371613,
                                    0.08451525121927261,
                                    0.08444896340370178,
                                    0.08451040834188461,
                                    0.08449771255254745,
                                    0.0844845324754715,
                                    0.08452331274747849,
                                    0.08448804169893265,
                                    0.08449441939592361,
                                    0.08447355777025223,
                                    217506.6875,
                                    0.08448262512683868,
                                    27566.361328125,
                                    0.084529809653759,
                                    60.93157958984375,
                                    0.0844476968050003,
                                    0.08442551642656326,
                                    81.24128723144531,
                                    0.08442741632461548,
                                    0.08438073843717575,
                                    0.08440481126308441,
                                    0.08442452549934387,
                                    0.08434814214706421,
                                    11.347413063049316,
                                    26.29210662841797,
                                    0.23023147881031036,
                                    0.08447755873203278,
                                    0.08438970148563385,
                                    0.08435468375682831,
                                    0.08439872413873672,
                                    0.08438412100076675,
                                    0.08440207690000534,
                                    0.0844302773475647,
                                    11.261951446533203,
                                    0.08435912430286407,
                                    3.4124855995178223,
                                    0.08436176180839539,
                                    0.08430063724517822,
                                    0.08941613137722015,
                                    0.08446674793958664,
                                    0.08433955907821655,
                                    0.08443570137023926,
                                    0.08435171097517014,
                                    21675608.0,
                                    0.08440378308296204,
                                    0.08433499932289124,
                                    0.40477755665779114,
                                    0.08435443788766861,
                                    0.08426611125469208,
                                    13186.3408203125,
                                    39600.96875,
                                    0.08453357219696045,
                                    0.9381150603294373,
                                    0.09587060660123825,
                                    0.514412522315979,
                                    0.09367242455482483,
                                    0.09441542625427246,
                                    0.09452634304761887,
                                    0.09454908967018127,
                                    22.32537269592285,
                                    32339.833984375,
                                    0.09451416879892349,
                                    0.09449652582406998,
                                    0.09464240074157715,
                                    0.09449930489063263,
                                    1958.1798095703125,
                                    0.09455957263708115,
                                    0.09456851333379745,
                                    1.2811931371688843,
                                    0.09453321993350983,
                                    0.09458591043949127,
                                    0.09450304508209229,
                                    5.49962043762207,
                                    72.5165023803711,
                                    0.09458471089601517,
                                    0.0945429876446724,
                                    0.09455682337284088,
                                    0.09453796595335007,
                                    0.09452159702777863,
                                    2.5312438011169434,
                                    0.0945226177573204,
                                    0.0944872573018074,
                                    5.223100185394287,
                                    2137.731201171875,
                                    0.142483189702034,
                                    0.20935583114624023,
                                    0.09459515661001205,
                                    0.09455345571041107,
                                    2812.721435546875,
                                    0.09442364424467087,
                                    0.1131102666258812,
                                    0.09442222863435745,
                                    0.09438570588827133,
                                    0.09433407336473465,
                                    0.13149750232696533,
                                    0.09370430558919907,
                                    0.09366568177938461,
                                    0.09354622662067413,
                                    0.09447423368692398,
                                    0.6216050982475281,
                                    0.09451036900281906,
                                    0.09451115876436234,
                                    1349.734619140625,
                                    0.09453874081373215,
                                    0.11682190001010895,
                                    0.10219868272542953,
                                    0.09455303847789764,
                                    0.0945776104927063,
                                    0.09455080330371857,
                                    2.9681241512298584,
                                    0.09452127665281296,
                                    0.09452573955059052,
                                    29.074121475219727,
                                    14.531797409057617,
                                    0.09461171925067902,
                                    0.09457170218229294,
                                    0.09453096240758896,
                                    0.7046006321907043,
                                    0.33546608686447144,
                                    0.09435248374938965,
                                    0.09443413466215134,
                                    0.31541207432746887,
                                    0.14433683454990387,
                                    0.09446719288825989,
                                    3715.66796875,
                                    0.0944618359208107,
                                    0.09454601258039474,
                                    0.0945052057504654,
                                    0.09456264227628708,
                                    0.09456627815961838,
                                    1.2233333587646484,
                                    0.09454774111509323,
                                    249.55780029296875,
                                    181.83404541015625,
                                    154.2233123779297,
                                    0.09459660947322845,
                                    0.09442275762557983,
                                    0.09446389973163605,
                                    0.09459325671195984,
                                    0.09457375854253769,
                                    0.0945519208908081,
                                    0.09459152817726135,
                                    0.0945110023021698,
                                    0.7131268978118896,
                                    0.09457242488861084,
                                    13.699750900268555,
                                    0.09455432742834091,
                                    0.09460796415805817,
                                    0.09450221061706543,
                                    11696.2060546875,
                                    0.09452937543392181,
                                    0.09452711045742035,
                                    0.09458191692829132,
                                    9.455473899841309,
                                    0.09451385587453842,
                                    0.09448845684528351,
                                    131.53732299804688,
                                    4293.73291015625,
                                    0.09409381449222565,
                                    0.09368418902158737,
                                    0.09371126443147659,
                                    0.09354376047849655,
                                    0.2533681094646454,
                                    0.314374715089798,
                                    0.09430520236492157,
                                    0.09440848976373672,
                                    0.094419464468956,
                                    0.09437459707260132,
                                    0.09439592063426971,
                                    1767081.0,
                                    594.7551879882812,
                                    0.09452971816062927,
                                    0.09449964016675949,
                                    0.09450231492519379,
                                    0.09459840506315231,
                                    0.09455262124538422,
                                    0.09447214007377625,
                                    0.0944393202662468,
                                    0.09440606832504272,
                                    0.09447411447763443,
                                    0.09437554329633713,
                                    0.09433301538228989,
                                    0.09440498054027557,
                                    0.10780218243598938,
                                    0.09436662495136261,
                                    0.09431613236665726,
                                    0.0945611521601677,
                                    219.7027587890625,
                                    0.4978674352169037,
                                    0.0944586768746376,
                                    0.30992811918258667,
                                    1.8426533937454224,
                                    0.5712193250656128,
                                    0.09446409344673157,
                                    0.09459622949361801,
                                    0.09451809525489807,
                                    87.38387298583984,
                                    0.09456565976142883,
                                    3497.284423828125,
                                    0.09454844892024994,
                                    0.09451253712177277,
                                    0.09457314759492874,
                                    0.09456212818622589,
                                    0.15045373141765594,
                                    0.14893552660942078,
                                    562.9702758789062,
                                    131.55374145507812,
                                    0.09452853351831436,
                                    0.09735272079706192,
                                    0.09456931799650192,
                                    0.6130989193916321,
                                    0.09453856199979782,
                                    0.09444250911474228,
                                    0.09452873468399048,
                                    0.09453690052032471,
                                    1.229264736175537,
                                    0.09449880570173264,
                                    820.4354248046875,
                                    1325.2156982421875,
                                    0.09444205462932587,
                                    0.0945630744099617,
                                    0.09459412842988968,
                                    0.0945231169462204,
                                    513502.1875,
                                    0.09456072747707367,
                                    1.1679768562316895,
                                    0.0945015400648117,
                                    0.09457283467054367,
                                    0.09455496817827225,
                                    0.19079294800758362,
                                    36.89104461669922,
                                    0.0944361463189125,
                                    0.09443370997905731,
                                    0.09447237849235535,
                                    20771922.0,
                                    0.09442625194787979,
                                    0.1135120838880539,
                                    0.09445078670978546,
                                    0.09434980154037476,
                                    0.09435220062732697,
                                    0.35446515679359436,
                                    0.09654965996742249,
                                    0.09453532844781876,
                                    0.09456611424684525,
                                    32323.8828125,
                                    978.3565063476562,
                                    0.09457312524318695,
                                    0.0945013165473938,
                                    0.09439229965209961,
                                    4412095.5,
                                    0.09448060393333435,
                                    0.09440262615680695,
                                    0.0943794921040535,
                                    0.09443134814500809,
                                    369.93389892578125,
                                    0.14076443016529083,
                                    0.09438452124595642,
                                    0.09434284269809723,
                                    0.10136646777391434,
                                    0.09443105757236481,
                                    0.09402314573526382,
                                    26166.89453125,
                                    0.09349802136421204,
                                    3.321302652359009,
                                    0.09364969283342361,
                                    0.09496324509382248,
                                    0.09372551739215851,
                                    0.09436341375112534,
                                    0.09432327002286911,
                                    0.09436825662851334,
                                    0.0944887325167656,
                                    0.09451030939817429,
                                    0.09454188495874405,
                                    0.0945427343249321,
                                    0.09454578906297684,
                                    0.09454236924648285,
                                    0.09450600296258926,
                                    0.09452961385250092,
                                    0.09452560544013977,
                                    0.09452951699495316,
                                    2834032.25,
                                    0.09452395886182785,
                                    10.720794677734375,
                                    68.72327423095703,
                                    8477249.0,
                                    0.09440808743238449,
                                    5520.7861328125,
                                    0.09362620860338211,
                                    0.09349146485328674,
                                    0.09339573979377747,
                                    149.35525512695312,
                                    278.6841735839844,
                                    0.0943543016910553,
                                    37.89771270751953,
                                    5100427.5,
                                    0.09433344006538391,
                                    6600393.5,
                                    0.09443498402833939,
                                    0.09448225051164627,
                                    0.09439776837825775,
                                    0.09439967572689056,
                                    0.09428673982620239,
                                    0.0945155993103981,
                                    0.27247631549835205,
                                    0.09454277157783508,
                                    0.09456893801689148,
                                    0.09452944248914719,
                                    0.09454522281885147,
                                    0.09451722353696823,
                                    0.09460891038179398,
                                    0.09455429017543793,
                                    10244.599609375,
                                    84.07907104492188,
                                    0.0944337323307991,
                                    0.09459497779607773,
                                    0.09454827755689621,
                                    0.09453362226486206,
                                    0.09449025988578796,
                                    0.09453912824392319,
                                    0.3738631308078766,
                                    0.09447181224822998,
                                    0.09451087564229965,
                                    0.09459631890058517,
                                    0.09454839676618576,
                                    0.09458976984024048,
                                    0.09457769244909286,
                                    0.09456919878721237,
                                    3.4914751052856445,
                                    1427.79345703125,
                                    0.4578329920768738,
                                    884.7360229492188,
                                    0.09449641406536102,
                                    0.09440287202596664,
                                    510.2096862792969,
                                    0.09450377523899078,
                                    0.09451079368591309,
                                    2.1641271114349365,
                                    0.0944771096110344,
                                    25.285322189331055,
                                    0.09460421651601791,
                                    0.09461460262537003,
                                    21697.78125,
                                    0.11157756298780441,
                                    9.951555252075195,
                                    0.09457164257764816,
                                    0.09458516538143158,
                                    770.6202392578125,
                                    0.5892700552940369,
                                    0.09393339604139328,
                                    0.09381875395774841,
                                    0.09371479600667953,
                                    0.09378113597631454,
                                    0.0938304141163826,
                                    0.09431172162294388,
                                    1034.4150390625,
                                    538.9066772460938,
                                    0.09464322030544281,
                                    0.7673370242118835,
                                    0.18193259835243225,
                                    0.09461429715156555,
                                    0.09462171792984009,
                                    0.6443932056427002,
                                    0.09465876966714859,
                                    0.09471260756254196,
                                    0.09465605765581131,
                                    0.09452185034751892,
                                    0.09450294822454453,
                                    0.12810559570789337,
                                    0.094684898853302,
                                    8.992913246154785,
                                    0.8569191694259644,
                                    582.5096435546875,
                                    0.09462642669677734,
                                    0.09460807591676712,
                                    0.09465175122022629,
                                    0.0958004742860794,
                                    0.09474059194326401,
                                    0.09462853521108627,
                                    657.1932373046875,
                                    0.19906458258628845,
                                    0.09392101317644119,
                                    0.09379742294549942,
                                    0.7946452498435974,
                                    0.1725653111934662,
                                    0.09447696059942245,
                                    0.09447885304689407,
                                    0.09469438344240189,
                                    0.09465198963880539,
                                    0.09460066258907318,
                                    560.6574096679688,
                                    0.09460125863552094,
                                    0.0946880578994751,
                                    0.09468343108892441,
                                    0.09466951340436935,
                                    7.846862316131592,
                                    0.09464780241250992,
                                    1.8931728601455688,
                                    0.0939062237739563,
                                    0.14714713394641876,
                                    0.09371401369571686,
                                    0.14943532645702362,
                                    9.442521095275879,
                                    0.09458307921886444,
                                    0.09459754824638367,
                                    0.19837377965450287,
                                    0.18834809958934784,
                                    0.09383810311555862,
                                    0.09375286102294922,
                                    0.0936644896864891,
                                    0.09361999481916428,
                                    0.0935748741030693,
                                    1.445277214050293,
                                    0.0944688618183136,
                                    12.287859916687012,
                                    0.09468797594308853,
                                    0.09469488263130188,
                                    0.09460695087909698,
                                    0.7720686197280884,
                                    0.0945553258061409,
                                    0.09460161626338959,
                                    0.09466303139925003,
                                    0.09467338770627975,
                                    0.09465459734201431,
                                    0.09461504220962524,
                                    0.0945778414607048,
                                    0.09464619308710098,
                                    0.09465964883565903,
                                    0.09464212507009506,
                                    229.66256713867188,
                                    18.826688766479492,
                                    0.09455174207687378,
                                    0.09412211924791336,
                                    17504.03515625,
                                    0.09471108019351959,
                                    0.09461493790149689,
                                    10.089583396911621,
                                    0.09461957961320877,
                                    31.376590728759766,
                                    0.09470537304878235,
                                    0.09463783353567123,
                                    5610218.0,
                                    0.09464839100837708,
                                    0.09463448077440262,
                                    0.0942724272608757,
                                    0.09408007562160492,
                                    0.48262032866477966,
                                    0.09465523064136505,
                                    2.4015886783599854,
                                    0.09462562948465347,
                                    142063.546875,
                                    0.09455953538417816,
                                    1.974007487297058,
                                    0.09464676678180695,
                                    0.09465840458869934,
                                    0.10140714049339294,
                                    0.09454220533370972,
                                    0.09462330490350723,
                                    0.09451285749673843,
                                    4.050822734832764,
                                    0.09465648978948593,
                                    0.09463375806808472,
                                    0.09465823322534561,
                                    0.09469760209321976,
                                    0.09466546028852463,
                                    932.041259765625,
                                    0.09463147073984146,
                                    3.409330129623413,
                                    54.3207893371582,
                                    0.09465781599283218,
                                    0.09465416520833969,
                                    1.7536137104034424,
                                    0.09468229115009308,
                                    84.98772430419922,
                                    0.09475217014551163,
                                    0.09401313960552216,
                                    134.52792358398438,
                                    0.09469401836395264,
                                    0.23205254971981049,
                                    0.0946466401219368,
                                    0.09470997005701065,
                                    0.09463036060333252,
                                    0.09456641972064972,
                                    0.09461867809295654,
                                    0.9174342155456543,
                                    0.09432287514209747,
                                    3.919257879257202,
                                    0.09378696233034134,
                                    0.09345461428165436,
                                    0.09351752698421478,
                                    0.09323997050523758,
                                    0.09312094748020172,
                                    0.09307726472616196,
                                    0.09321693331003189,
                                    0.09316740185022354,
                                    0.09299517422914505,
                                    0.09285470098257065,
                                    0.2572370171546936,
                                    0.09311380237340927,
                                    0.09293543547391891,
                                    0.09303213655948639,
                                    0.1493358463048935,
                                    0.09310180693864822,
                                    0.09321630746126175,
                                    10.437809944152832,
                                    0.0943022146821022,
                                    0.09423695504665375,
                                    2590.866943359375,
                                    1.4054656028747559,
                                    38.2252311706543,
                                    19227.08203125,
                                    0.09473899751901627,
                                    0.09466087073087692,
                                    0.0946071520447731,
                                    830.8789672851562,
                                    0.0945894792675972,
                                    2.7011053562164307,
                                    0.504011332988739,
                                    0.09468135982751846,
                                    0.09465712308883667,
                                    0.0945027694106102,
                                    3.204328775405884,
                                    0.09469970315694809,
                                    136.45835876464844,
                                    0.0946570485830307,
                                    276.604248046875,
                                    0.12033631652593613,
                                    0.09448137134313583,
                                    1677058.875,
                                    0.09450552612543106,
                                    6.231417179107666,
                                    0.4628032147884369,
                                    0.12920531630516052,
                                    0.5548384189605713,
                                    0.09467286616563797,
                                    0.09465939551591873,
                                    0.09464029967784882,
                                    0.09545759111642838,
                                    0.09464362263679504,
                                    1.4605083465576172,
                                    0.6265268921852112,
                                    0.13141100108623505,
                                    0.09462108463048935,
                                    0.09455641359090805,
                                    5.9682111740112305,
                                    0.09459859877824783,
                                    32.384986877441406,
                                    0.09444091469049454,
                                    0.09422245621681213,
                                    0.0940980538725853,
                                    0.09372019022703171,
                                    0.09348495304584503,
                                    0.09494473040103912,
                                    0.09471526741981506,
                                    0.09466321766376495,
                                    0.09463952481746674,
                                    0.0946340337395668,
                                    0.0946597084403038,
                                    0.10840669274330139,
                                    0.09467717260122299,
                                    0.09460677951574326,
                                    0.09988532215356827,
                                    0.09422221034765244,
                                    0.11676912754774094,
                                    0.09440311044454575,
                                    45.41225051879883,
                                    507.40032958984375,
                                    0.13748902082443237,
                                    0.09470012038946152,
                                    226.58145141601562,
                                    0.4495183825492859,
                                    0.09465526789426804,
                                    0.09466543048620224,
                                    61.091617584228516,
                                    0.09468193352222443,
                                    0.09465447813272476,
                                    0.0945592001080513,
                                    0.09467066824436188,
                                    0.09464655071496964,
                                    141.53970336914062,
                                    116328.4609375,
                                    0.09458193182945251,
                                    0.12122631818056107,
                                    4.387285232543945,
                                    2681976.5,
                                    0.09463132172822952,
                                    0.0946061760187149,
                                    0.0944783166050911,
                                    0.09446098655462265,
                                    0.09450376033782959,
                                    0.09455794841051102,
                                    0.09459281712770462,
                                    0.09462811797857285,
                                    6.4233717918396,
                                    0.09450298547744751,
                                    0.09456917643547058,
                                    0.0934096947312355,
                                    0.0934649258852005,
                                    0.09316615760326385,
                                    0.5173411965370178,
                                    1.9697647094726562,
                                    32.42613983154297,
                                    0.09472211450338364,
                                    0.09469816833734512,
                                    0.0956822857260704,
                                    0.09459832310676575,
                                    0.0946735143661499,
                                    0.09462333470582962,
                                    0.09459586441516876,
                                    1.9096038341522217,
                                    0.0946708396077156,
                                    0.0946536511182785,
                                    0.09466943144798279,
                                    0.09466858953237534,
                                    0.09464891254901886,
                                    0.09463982284069061,
                                    0.09469345957040787,
                                    0.0946459099650383,
                                    0.0945952758193016,
                                    122.37969970703125,
                                    4.63157844543457,
                                    56.718482971191406,
                                    0.18920323252677917,
                                    454.80084228515625,
                                    4036.590087890625,
                                    0.09464812278747559,
                                    127179.6875,
                                    0.09467849880456924,
                                    0.09463875740766525,
                                    0.09431662410497665,
                                    665.6954956054688,
                                    0.09449782967567444,
                                    0.09459271281957626,
                                    0.09455017000436783,
                                    2.904323101043701,
                                    0.09459733217954636,
                                    0.0946681872010231,
                                    0.09470684081315994,
                                    3.52655029296875,
                                    0.09464657306671143,
                                    0.09463555365800858,
                                    1.8135828971862793,
                                    0.09465093165636063,
                                    0.0946563184261322,
                                    0.09455125033855438,
                                    0.09466362744569778,
                                    0.0946488231420517,
                                    2.0649492740631104,
                                    1.0428603887557983,
                                    0.09464512020349503,
                                    0.09463749080896378,
                                    0.09463761001825333,
                                    204.697509765625,
                                    0.09467710554599762],
             'val_accuracy_history': [1.7894736842105263,
                                      1.368421052631579,
                                      0.3157894736842105,
                                      1.0526315789473684,
                                      1.0,
                                      0.42105263157894735,
                                      0.6842105263157895,
                                      1.4210526315789473,
                                      0.10526315789473684,
                                      0.3157894736842105,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      3.0,
                                      0.0,
                                      2.0,
                                      1.0,
                                      2.0,
                                      1.0,
                                      0.0,
                                      2.0,
                                      1.0,
                                      2.0,
                                      1.0,
                                      2.0,
                                      3.0,
                                      1.0,
                                      2.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      2.1578947368421053,
                                      1.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      3.0,
                                      0.0,
                                      1.0,
                                      1.2105263157894737,
                                      2.0,
                                      1.0,
                                      2.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      2.0,
                                      1.0,
                                      0.5263157894736842,
                                      0.0,
                                      1.0,
                                      0.0,
                                      2.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      2.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      4.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      2.0,
                                      1.0,
                                      3.0,
                                      1.0,
                                      0.631578947368421,
                                      1.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      1.631578947368421,
                                      1.8421052631578947,
                                      0.0,
                                      2.0,
                                      0.9473684210526315,
                                      0.47368421052631576,
                                      0.7894736842105263,
                                      0.05263157894736842,
                                      3.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      1.2105263157894737,
                                      0.8421052631578947,
                                      0.21052631578947367,
                                      0.9473684210526315,
                                      1.5263157894736843,
                                      1.0526315789473684,
                                      1.0,
                                      0.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      2.0,
                                      1.0,
                                      2.0,
                                      1.0,
                                      2.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      3.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      1.6842105263157894,
                                      1.9473684210526316,
                                      0.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      3.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      2.0,
                                      2.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      3.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      3.0,
                                      1.0,
                                      1.0,
                                      2.0,
                                      1.0,
                                      2.0,
                                      3.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      2.0,
                                      3.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      3.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.7777777777777778,
                                      0.1111111111111111,
                                      0.8888888888888888,
                                      2.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      0.0,
                                      2.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      0.7777777777777778,
                                      0.7777777777777778,
                                      0.7777777777777778,
                                      1.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      3.0,
                                      1.0,
                                      2.0,
                                      2.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      2.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      2.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      2.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      0.1111111111111111,
                                      0.4444444444444444,
                                      0.8888888888888888,
                                      0.2222222222222222,
                                      0.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      2.0,
                                      2.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      2.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      0.0,
                                      3.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      2.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.8888888888888888,
                                      0.5555555555555556,
                                      0.7777777777777778,
                                      1.0,
                                      0.2222222222222222,
                                      0.3333333333333333,
                                      0.1111111111111111,
                                      0.3333333333333333,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      2.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      2.0,
                                      1.0,
                                      0.0,
                                      2.0,
                                      1.0,
                                      3.0,
                                      1.0,
                                      0.8888888888888888,
                                      1.0,
                                      0.8888888888888888,
                                      0.0,
                                      2.6666666666666665,
                                      1.0,
                                      2.0,
                                      0.0,
                                      1.7777777777777777,
                                      0.0,
                                      3.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      3.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      3.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      4.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.8888888888888888,
                                      0.4444444444444444,
                                      0.6666666666666666,
                                      0.1111111111111111,
                                      0.3333333333333333,
                                      1.0,
                                      3.0,
                                      2.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      3.0,
                                      0.0,
                                      3.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      4.0,
                                      2.0,
                                      1.0,
                                      0.0,
                                      3.0,
                                      3.0,
                                      1.0,
                                      1.0,
                                      2.0526315789473686,
                                      1.736842105263158,
                                      2.9473684210526314,
                                      1.894736842105263,
                                      4.0,
                                      2.0,
                                      1.0,
                                      2.0,
                                      2.0,
                                      1.0,
                                      2.0,
                                      3.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      2.0,
                                      2.0,
                                      0.3684210526315789,
                                      2.6315789473684212,
                                      2.526315789473684,
                                      1.894736842105263,
                                      4.0,
                                      2.0,
                                      3.0,
                                      1.0,
                                      2.0,
                                      3.0526315789473686,
                                      1.263157894736842,
                                      1.105263157894737,
                                      0.21052631578947367,
                                      1.1578947368421053,
                                      1.0,
                                      2.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      3.0,
                                      2.0,
                                      2.0,
                                      4.0,
                                      3.0,
                                      3.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      2.0,
                                      2.0,
                                      2.0,
                                      0.0,
                                      2.0,
                                      3.210526315789474,
                                      0.10526315789473684,
                                      3.0,
                                      0.0,
                                      5.0,
                                      2.0,
                                      1.0,
                                      3.0,
                                      2.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      0.2631578947368421,
                                      0.0,
                                      2.0,
                                      2.0,
                                      1.0,
                                      2.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      4.0,
                                      0.0,
                                      2.0,
                                      2.0,
                                      3.0,
                                      4.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      3.0,
                                      2.210526315789474,
                                      3.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      0.10526315789473684,
                                      4.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      2.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      1.8421052631578947,
                                      2.4210526315789473,
                                      1.4736842105263157,
                                      0.3157894736842105,
                                      0.7894736842105263,
                                      1.6842105263157894,
                                      1.631578947368421,
                                      1.9473684210526316,
                                      1.6842105263157894,
                                      1.8421052631578947,
                                      1.894736842105263,
                                      0.6842105263157895,
                                      1.0526315789473684,
                                      0.8947368421052632,
                                      2.0,
                                      2.4210526315789473,
                                      1.4210526315789473,
                                      0.9473684210526315,
                                      1.1578947368421053,
                                      0.0,
                                      0.9473684210526315,
                                      1.0,
                                      4.0,
                                      4.0,
                                      2.0,
                                      3.0,
                                      1.0,
                                      2.0,
                                      2.0,
                                      0.0,
                                      2.0,
                                      3.0,
                                      2.0,
                                      2.0,
                                      2.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      3.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      2.0,
                                      2.0,
                                      0.0,
                                      1.0,
                                      3.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      3.0,
                                      3.0,
                                      1.0,
                                      0.0,
                                      3.0,
                                      2.0,
                                      2.0,
                                      0.0,
                                      2.0,
                                      0.8947368421052632,
                                      1.5263157894736843,
                                      1.6842105263157894,
                                      0.2631578947368421,
                                      1.0,
                                      3.0,
                                      5.0,
                                      2.0,
                                      3.0,
                                      1.0,
                                      2.0,
                                      1.0,
                                      2.0,
                                      1.0,
                                      2.789473684210526,
                                      1.0,
                                      0.0,
                                      2.0,
                                      1.0,
                                      0.0,
                                      2.0,
                                      2.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      2.0,
                                      1.0,
                                      2.0,
                                      4.0,
                                      2.0,
                                      1.0,
                                      1.0,
                                      4.0,
                                      2.0,
                                      3.0,
                                      2.0,
                                      4.0,
                                      3.0,
                                      2.0,
                                      1.0,
                                      1.0,
                                      2.0,
                                      2.0,
                                      2.0,
                                      1.0,
                                      2.0,
                                      1.631578947368421,
                                      0.21052631578947367,
                                      1.736842105263158,
                                      0.9473684210526315,
                                      1.4736842105263157,
                                      1.0,
                                      2.0,
                                      2.0,
                                      3.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      2.0,
                                      2.0,
                                      1.0,
                                      4.0,
                                      2.0,
                                      3.0,
                                      3.0,
                                      4.0,
                                      2.0,
                                      3.0,
                                      0.0,
                                      2.0,
                                      3.0,
                                      1.0,
                                      2.0,
                                      3.0,
                                      2.0,
                                      2.0,
                                      1.894736842105263,
                                      2.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      2.0,
                                      1.0,
                                      5.0,
                                      3.0,
                                      3.0,
                                      2.0,
                                      2.0,
                                      1.0,
                                      1.0,
                                      2.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      2.0],
             'val_loss': 0.0900145098567009,
             'val_loss_history': [0.13289986550807953,
                                  30.159862518310547,
                                  0.1306895911693573,
                                  772.6478881835938,
                                  0.12578943371772766,
                                  0.1313501000404358,
                                  0.12481862306594849,
                                  0.12818337976932526,
                                  0.12867261469364166,
                                  0.13204559683799744,
                                  0.1220577135682106,
                                  0.11260901391506195,
                                  0.11409381777048111,
                                  0.11440904438495636,
                                  0.11434926837682724,
                                  0.11414065212011337,
                                  0.11439044773578644,
                                  0.11462827026844025,
                                  0.11452741175889969,
                                  0.11467263847589493,
                                  0.11460597068071365,
                                  0.11462745815515518,
                                  0.11468517035245895,
                                  0.11480430513620377,
                                  0.1147732213139534,
                                  0.11485739797353745,
                                  0.11479106545448303,
                                  0.1150507852435112,
                                  0.11502186954021454,
                                  0.11493460834026337,
                                  0.11511021852493286,
                                  0.11504624783992767,
                                  0.1150892823934555,
                                  0.11518291383981705,
                                  64640324.0,
                                  0.11498195677995682,
                                  0.11846525222063065,
                                  0.11501139402389526,
                                  0.11516815423965454,
                                  0.11503072828054428,
                                  0.11535662412643433,
                                  0.11514862626791,
                                  0.11519207060337067,
                                  0.11522386968135834,
                                  0.11516549438238144,
                                  0.11545438319444656,
                                  0.11529026180505753,
                                  0.11528454720973969,
                                  0.11551813781261444,
                                  0.11532633006572723,
                                  0.1173066645860672,
                                  0.11544819921255112,
                                  0.11565966159105301,
                                  0.1153019443154335,
                                  0.11566291004419327,
                                  0.11555980145931244,
                                  0.11566717177629471,
                                  0.11576888710260391,
                                  0.11573272943496704,
                                  0.11575499922037125,
                                  0.11589116603136063,
                                  0.11577892303466797,
                                  0.11567796766757965,
                                  0.11675383150577545,
                                  0.11572939157485962,
                                  0.11599055677652359,
                                  0.11586335301399231,
                                  0.11595223098993301,
                                  0.115937240421772,
                                  0.11586461216211319,
                                  0.11587101221084595,
                                  0.1159282699227333,
                                  0.11588104814291,
                                  0.11595000326633453,
                                  0.11601797491312027,
                                  0.11578642576932907,
                                  0.1159607544541359,
                                  0.11591330915689468,
                                  0.1158139705657959,
                                  0.1159902960062027,
                                  0.1159883365035057,
                                  0.1158863827586174,
                                  0.11616058647632599,
                                  0.116038978099823,
                                  0.11605293303728104,
                                  0.11608398705720901,
                                  0.11597417294979095,
                                  0.11598964035511017,
                                  0.11528076976537704,
                                  0.1164831668138504,
                                  0.11611490696668625,
                                  0.11596962809562683,
                                  0.11598684638738632,
                                  0.11591029167175293,
                                  0.11605054140090942,
                                  0.11600024253129959,
                                  0.1159677505493164,
                                  0.11592534184455872,
                                  0.11612578481435776,
                                  0.11618667840957642,
                                  0.11620379984378815,
                                  0.11618492752313614,
                                  0.11606466770172119,
                                  0.11596335470676422,
                                  0.11628828197717667,
                                  0.1161775216460228,
                                  0.11668728291988373,
                                  0.11626330763101578,
                                  0.11897259205579758,
                                  0.11721517890691757,
                                  0.11805533617734909,
                                  0.11683106422424316,
                                  0.11628374457359314,
                                  0.11647655814886093,
                                  0.11625979095697403,
                                  0.11615417152643204,
                                  0.11611239612102509,
                                  0.11597920954227448,
                                  0.11626138538122177,
                                  0.11619382351636887,
                                  0.11614691466093063,
                                  0.1158432811498642,
                                  0.11844415962696075,
                                  0.12012633681297302,
                                  0.12201090902090073,
                                  0.12300722301006317,
                                  0.12179263681173325,
                                  0.12022022157907486,
                                  0.1161445826292038,
                                  0.11631754785776138,
                                  0.11599871516227722,
                                  0.11595051735639572,
                                  0.11614725738763809,
                                  0.11601608991622925,
                                  0.11616994440555573,
                                  0.11606497317552567,
                                  0.11608567833900452,
                                  0.11612219363451004,
                                  0.11595229059457779,
                                  0.1160753071308136,
                                  0.11604484170675278,
                                  0.11610965430736542,
                                  0.11596120893955231,
                                  0.11606322973966599,
                                  0.1158863827586174,
                                  0.1162295863032341,
                                  0.11610893160104752,
                                  0.11617756634950638,
                                  0.11616548895835876,
                                  0.11614851653575897,
                                  0.11626169085502625,
                                  0.11611983180046082,
                                  0.116350457072258,
                                  0.1160777285695076,
                                  0.11605817079544067,
                                  0.1161554679274559,
                                  0.1161135733127594,
                                  0.11618547886610031,
                                  0.11609655618667603,
                                  0.11594346165657043,
                                  0.11609303951263428,
                                  0.11628188192844391,
                                  0.11593768745660782,
                                  0.116062231361866,
                                  0.11615301668643951,
                                  0.11612395197153091,
                                  0.11609595268964767,
                                  0.11606339365243912,
                                  0.11621040850877762,
                                  0.11611215770244598,
                                  0.11631068587303162,
                                  0.11604316532611847,
                                  0.11610415577888489,
                                  0.11617108434438705,
                                  0.11737527698278427,
                                  0.11636664718389511,
                                  0.11643633991479874,
                                  0.11641830205917358,
                                  0.11623018234968185,
                                  0.11627576500177383,
                                  0.11618880182504654,
                                  0.11590965837240219,
                                  0.11600705981254578,
                                  0.11603537946939468,
                                  0.11614096164703369,
                                  0.11609300225973129,
                                  0.11620457470417023,
                                  0.11623869836330414,
                                  0.11612760275602341,
                                  0.11619135737419128,
                                  0.11633993685245514,
                                  0.11595872789621353,
                                  0.1160760223865509,
                                  0.11613026261329651,
                                  0.11629821360111237,
                                  0.11606990545988083,
                                  0.11618290096521378,
                                  0.11590874940156937,
                                  0.116066575050354,
                                  0.1162467747926712,
                                  0.11605139076709747,
                                  0.11606994271278381,
                                  0.11615034192800522,
                                  0.11626721173524857,
                                  0.1164214015007019,
                                  0.11610067635774612,
                                  0.11607576906681061,
                                  0.11610443145036697,
                                  0.11622398346662521,
                                  0.11609799414873123,
                                  0.11592576652765274,
                                  0.11589066684246063,
                                  0.11615613847970963,
                                  0.11620646715164185,
                                  0.11605507135391235,
                                  0.11605215072631836,
                                  0.11597687005996704,
                                  0.11605720967054367,
                                  0.11622954159975052,
                                  0.11576541513204575,
                                  0.11619725078344345,
                                  0.11608881503343582,
                                  0.11614267528057098,
                                  0.11628157645463943,
                                  0.11605657637119293,
                                  0.11614761501550674,
                                  0.11601652950048447,
                                  0.11608336865901947,
                                  0.1161508783698082,
                                  0.11626416444778442,
                                  0.11630623042583466,
                                  0.1164545938372612,
                                  0.11624109745025635,
                                  0.11607788503170013,
                                  0.1161070168018341,
                                  0.11620175093412399,
                                  0.11616086959838867,
                                  0.11611688882112503,
                                  0.11600390076637268,
                                  0.11609793454408646,
                                  0.11598715931177139,
                                  0.11622306704521179,
                                  0.11612720787525177,
                                  0.11618942022323608,
                                  0.11620881408452988,
                                  0.11601428687572479,
                                  0.11616939306259155,
                                  0.11625801771879196,
                                  0.11612837016582489,
                                  0.11634919047355652,
                                  0.11619079858064651,
                                  0.11613990366458893,
                                  0.11610959470272064,
                                  0.1163247600197792,
                                  0.11617305129766464,
                                  0.11615300923585892,
                                  0.11632908880710602,
                                  0.11621703207492828,
                                  0.11634662002325058,
                                  0.11614250391721725,
                                  0.11630403250455856,
                                  0.11608672142028809,
                                  0.11608536541461945,
                                  31432.908203125,
                                  0.1162625327706337,
                                  0.11617450416088104,
                                  0.11622246354818344,
                                  0.11625657230615616,
                                  0.11630494892597198,
                                  0.1160629466176033,
                                  0.11630386859178543,
                                  0.11654241383075714,
                                  0.11633019894361496,
                                  0.11625935137271881,
                                  0.1159849464893341,
                                  0.11611940711736679,
                                  0.11628160625696182,
                                  0.11616548895835876,
                                  0.1158863976597786,
                                  0.11616910994052887,
                                  0.11640705168247223,
                                  57.9826545715332,
                                  0.11621317267417908,
                                  0.1162533313035965,
                                  0.11624367535114288,
                                  0.11624933034181595,
                                  0.11617560684680939,
                                  0.11601812392473221,
                                  0.11616231501102448,
                                  0.1162579134106636,
                                  0.1163044273853302,
                                  0.11610657721757889,
                                  0.11615923047065735,
                                  0.11604088544845581,
                                  0.11613348871469498,
                                  0.1160733699798584,
                                  0.11620483547449112,
                                  0.11624867469072342,
                                  0.11615971475839615,
                                  0.11607415229082108,
                                  75018216.0,
                                  0.09041483700275421,
                                  0.09068240970373154,
                                  0.09158043563365936,
                                  0.09161237627267838,
                                  0.09168776124715805,
                                  0.09167324006557465,
                                  0.09174589067697525,
                                  0.09159623086452484,
                                  0.091584213078022,
                                  0.09181859344244003,
                                  0.09148017317056656,
                                  0.09162292629480362,
                                  0.09164972603321075,
                                  0.09172000735998154,
                                  0.09156201779842377,
                                  0.09159772098064423,
                                  0.09161564707756042,
                                  0.09180383384227753,
                                  0.09165830910205841,
                                  0.09175550937652588,
                                  0.09164596349000931,
                                  0.09165975451469421,
                                  0.09162546694278717,
                                  0.09173918515443802,
                                  0.09151586145162582,
                                  0.09154339134693146,
                                  0.09142778068780899,
                                  0.09155946969985962,
                                  0.0915718749165535,
                                  0.09148793667554855,
                                  0.09153666347265244,
                                  0.0915401503443718,
                                  0.0918232873082161,
                                  0.09156225621700287,
                                  0.09150882065296173,
                                  0.09154771268367767,
                                  0.09136240184307098,
                                  0.09136436134576797,
                                  0.09138547629117966,
                                  0.09111592918634415,
                                  0.09049556404352188,
                                  0.09093768894672394,
                                  0.09028400480747223,
                                  0.0910341814160347,
                                  0.09143580496311188,
                                  0.09173454344272614,
                                  0.09146896004676819,
                                  0.09174791723489761,
                                  0.09165650606155396,
                                  0.09164563566446304,
                                  0.09150607138872147,
                                  0.09162599593400955,
                                  0.09160754829645157,
                                  0.0916544646024704,
                                  0.09154925495386124,
                                  0.09158218652009964,
                                  0.09168332815170288,
                                  0.09178471565246582,
                                  0.09159593284130096,
                                  0.09167377650737762,
                                  0.09158945083618164,
                                  0.09157179296016693,
                                  0.09173416346311569,
                                  0.09171082079410553,
                                  0.0915522649884224,
                                  0.09138037264347076,
                                  6550871.0,
                                  0.0917968600988388,
                                  0.09176656603813171,
                                  0.09189419448375702,
                                  0.09189261496067047,
                                  0.09148921072483063,
                                  0.0916379913687706,
                                  0.09164125472307205,
                                  0.09168343245983124,
                                  0.0915914997458458,
                                  0.0916556641459465,
                                  0.09176559001207352,
                                  0.09119798988103867,
                                  0.09149183332920074,
                                  0.09167253226041794,
                                  0.09157314896583557,
                                  0.09136173874139786,
                                  0.09172654151916504,
                                  0.09172135591506958,
                                  0.0916282907128334,
                                  0.09151092171669006,
                                  0.09152401983737946,
                                  0.09146179258823395,
                                  0.09152831137180328,
                                  0.09171596169471741,
                                  0.09146741032600403,
                                  0.09163735806941986,
                                  0.09172643721103668,
                                  0.09151304513216019,
                                  0.09167574346065521,
                                  0.09155893325805664,
                                  0.09151120483875275,
                                  0.09154047071933746,
                                  0.09145689010620117,
                                  0.09163273870944977,
                                  0.09142628312110901,
                                  0.09159869700670242,
                                  0.09154431521892548,
                                  0.09110464155673981,
                                  0.09078102558851242,
                                  0.0907980427145958,
                                  0.09076491743326187,
                                  0.09136105328798294,
                                  0.09158964455127716,
                                  0.09161243587732315,
                                  0.09146032482385635,
                                  0.09154172241687775,
                                  0.0915302038192749,
                                  0.09143901616334915,
                                  0.09158524125814438,
                                  0.09163603186607361,
                                  0.09191957861185074,
                                  0.09158330410718918,
                                  0.0916702076792717,
                                  0.09173353016376495,
                                  0.09168414771556854,
                                  0.09165389090776443,
                                  0.09153973311185837,
                                  0.09151528030633926,
                                  0.09149136394262314,
                                  0.09165231138467789,
                                  0.09150249511003494,
                                  0.09171602129936218,
                                  0.09170891344547272,
                                  0.09149736166000366,
                                  0.09154205024242401,
                                  0.09144985675811768,
                                  0.09173857420682907,
                                  0.09168737381696701,
                                  0.09159638732671738,
                                  0.09164682775735855,
                                  0.09182272851467133,
                                  0.09188602864742279,
                                  0.0916704535484314,
                                  0.09181439131498337,
                                  0.0916689857840538,
                                  0.0916949138045311,
                                  0.0918634682893753,
                                  0.091732919216156,
                                  0.09177081286907196,
                                  0.09180489182472229,
                                  0.09170784801244736,
                                  0.09166619181632996,
                                  0.09147465229034424,
                                  0.09065674990415573,
                                  0.09165985137224197,
                                  0.0915789008140564,
                                  0.09145854413509369,
                                  0.0916421115398407,
                                  0.09175586700439453,
                                  0.0916069895029068,
                                  0.091783806681633,
                                  0.09182631224393845,
                                  0.0917629525065422,
                                  0.09174057096242905,
                                  0.09179218113422394,
                                  0.09166530519723892,
                                  0.09162689000368118,
                                  0.09167832881212234,
                                  0.09155615419149399,
                                  0.09177278727293015,
                                  0.09168699383735657,
                                  0.09166309982538223,
                                  0.09175059199333191,
                                  0.09156449139118195,
                                  0.09154761582612991,
                                  0.09163499623537064,
                                  0.09183468669652939,
                                  0.09155910462141037,
                                  0.09153896570205688,
                                  0.09173861145973206,
                                  0.09150022268295288,
                                  0.09156262874603271,
                                  0.09158863872289658,
                                  0.091577909886837,
                                  0.0914931669831276,
                                  0.09152447432279587,
                                  0.09160035848617554,
                                  0.0915502980351448,
                                  0.09158534556627274,
                                  0.09145323932170868,
                                  0.09149032831192017,
                                  0.09140226989984512,
                                  0.09157698601484299,
                                  0.09184463322162628,
                                  0.09163165837526321,
                                  0.09175490587949753,
                                  0.09173326194286346,
                                  0.09146177023649216,
                                  0.09170445799827576,
                                  0.09182441979646683,
                                  0.09142305701971054,
                                  0.09153810143470764,
                                  0.09144371002912521,
                                  0.09164082258939743,
                                  0.09147138893604279,
                                  0.09169746935367584,
                                  0.09146449714899063,
                                  0.0915309488773346,
                                  0.09153302758932114,
                                  0.09078874439001083,
                                  0.09040026366710663,
                                  0.0903313085436821,
                                  0.09044385701417923,
                                  0.09064793586730957,
                                  0.09086998552083969,
                                  0.0905841663479805,
                                  0.09128729999065399,
                                  0.09156367182731628,
                                  0.09157119691371918,
                                  0.09156181663274765,
                                  0.091734878718853,
                                  0.09174881130456924,
                                  0.09162548929452896,
                                  0.09173157066106796,
                                  0.09165001660585403,
                                  0.09184536337852478,
                                  0.09174542874097824,
                                  0.09148374944925308,
                                  0.0918145477771759,
                                  0.09172198921442032,
                                  0.09163372218608856,
                                  0.09175238013267517,
                                  0.09166068583726883,
                                  0.09152699261903763,
                                  0.09570552408695221,
                                  0.09075891226530075,
                                  0.09028211236000061,
                                  0.09057898819446564,
                                  0.09034465253353119,
                                  0.091373510658741,
                                  0.09163247048854828,
                                  0.0916837826371193,
                                  623378176.0,
                                  0.09151039272546768,
                                  0.09154417365789413,
                                  0.09182267636060715,
                                  0.0917121171951294,
                                  0.09159564971923828,
                                  0.09143423289060593,
                                  0.09164773672819138,
                                  0.09148409217596054,
                                  0.0917004719376564,
                                  0.0917796865105629,
                                  0.0917053371667862,
                                  0.09163802117109299,
                                  597140.375,
                                  0.0918307676911354,
                                  0.09166297316551208,
                                  0.09173108637332916,
                                  0.09177133440971375,
                                  0.09167598187923431,
                                  0.09167742729187012,
                                  0.09176761656999588,
                                  0.09161846339702606,
                                  0.09151538461446762,
                                  0.09162520617246628,
                                  0.09155528992414474,
                                  0.09133844822645187,
                                  0.09172411262989044,
                                  0.09168177098035812,
                                  0.09152256697416306,
                                  0.09153567254543304,
                                  0.09174582362174988,
                                  0.09146131575107574,
                                  0.09169001132249832,
                                  0.09183165431022644,
                                  0.09155264496803284,
                                  0.09179975837469101,
                                  0.09141802042722702,
                                  0.09178242087364197,
                                  0.09155882894992828,
                                  0.09155619144439697,
                                  0.09172812104225159,
                                  0.09160418808460236,
                                  0.09162043035030365,
                                  0.0916271060705185,
                                  0.0916542038321495,
                                  0.09182074666023254,
                                  0.091661237180233,
                                  0.09168948233127594,
                                  0.09161534905433655,
                                  0.09176906198263168,
                                  0.09174256771802902,
                                  0.09184432774782181,
                                  0.09175481647253036,
                                  0.09163566678762436,
                                  0.09110815823078156,
                                  0.09047967940568924,
                                  0.09134313464164734,
                                  0.09058742970228195,
                                  0.09057310223579407,
                                  0.09021198004484177,
                                  0.08950533717870712,
                                  0.08991483598947525,
                                  0.09000581502914429,
                                  0.08975820243358612,
                                  0.08989927172660828,
                                  0.08977536857128143,
                                  0.09046599268913269,
                                  0.09019088745117188,
                                  0.08985491842031479,
                                  0.09006495773792267,
                                  0.09006132930517197,
                                  0.08998249471187592,
                                  0.08991481363773346,
                                  0.09035725891590118,
                                  0.09026259183883667,
                                  0.09033244848251343,
                                  0.09021812677383423,
                                  0.08977342396974564,
                                  0.08995147794485092,
                                  0.09018651396036148,
                                  0.0902789831161499,
                                  0.09030649065971375,
                                  0.09018151462078094,
                                  0.08963830769062042,
                                  0.09024237841367722,
                                  0.08991577476263046,
                                  0.09044059365987778,
                                  0.08928724378347397,
                                  0.08972622454166412,
                                  0.08882772922515869,
                                  0.08975745737552643,
                                  0.09016703814268112,
                                  0.08963004499673843,
                                  0.0898420587182045,
                                  0.08989901095628738,
                                  0.09010357409715652,
                                  0.0901118665933609,
                                  0.09002404659986496,
                                  0.09008162468671799,
                                  0.08998696506023407,
                                  0.09019099175930023,
                                  0.09039027988910675,
                                  0.09014302492141724,
                                  0.09035284072160721,
                                  0.090223528444767,
                                  0.08909245580434799,
                                  0.08939402550458908,
                                  0.08942470699548721,
                                  0.08992207795381546,
                                  0.08974660187959671,
                                  0.09004580974578857,
                                  0.09003043174743652,
                                  0.08954887092113495,
                                  0.08958173543214798,
                                  0.0888582319021225,
                                  0.08949612826108932,
                                  0.0890330970287323,
                                  0.08879958093166351,
                                  0.08949175477027893,
                                  0.09019462019205093,
                                  0.08992094546556473,
                                  0.08964835852384567,
                                  0.08991514146327972,
                                  0.08982381969690323,
                                  0.08980073034763336,
                                  0.0896264985203743,
                                  0.08992141485214233,
                                  0.09008410573005676,
                                  0.08989385515451431,
                                  0.09021643549203873,
                                  0.09013224393129349,
                                  0.0900585800409317,
                                  0.08978967368602753,
                                  0.09012910723686218,
                                  0.08994796127080917,
                                  0.09000711888074875,
                                  0.08993218839168549,
                                  0.09006299823522568,
                                  0.08917942643165588,
                                  0.08967818319797516,
                                  0.08993396908044815,
                                  0.09023486822843552,
                                  0.0900617465376854,
                                  0.09012560546398163,
                                  0.08974074572324753,
                                  0.09009005129337311,
                                  0.08979517966508865,
                                  0.08997286111116409,
                                  0.09035619348287582,
                                  0.09010802954435349,
                                  0.09086864441633224,
                                  0.0897199809551239,
                                  0.08955328911542892,
                                  0.08973503112792969,
                                  0.0901649221777916,
                                  0.08986207097768784,
                                  0.09005796909332275,
                                  0.08977393060922623,
                                  0.09000321477651596,
                                  0.08998708426952362,
                                  0.08971834927797318,
                                  0.0897836983203888,
                                  0.09028778970241547,
                                  0.08969186991453171,
                                  0.09012392163276672,
                                  0.08985482901334763,
                                  0.09004218131303787,
                                  0.09037664532661438,
                                  0.09028197079896927,
                                  0.09011863172054291,
                                  0.0896797850728035,
                                  0.0896434634923935,
                                  0.0899115651845932,
                                  0.08928757160902023,
                                  0.08990449458360672,
                                  0.08990985155105591,
                                  0.09001431614160538,
                                  0.0895829126238823,
                                  0.09010770171880722,
                                  0.08968362212181091,
                                  0.09000500291585922,
                                  0.0898640975356102,
                                  0.08954513818025589,
                                  123260808.0,
                                  0.09009537845849991,
                                  0.08999543637037277,
                                  0.09009146690368652,
                                  0.08993417024612427,
                                  0.08979174494743347,
                                  0.09024576097726822,
                                  0.08982154726982117,
                                  0.09024430066347122,
                                  0.08962235599756241,
                                  0.08915040642023087,
                                  0.08879758417606354,
                                  0.08888117223978043,
                                  0.08869042247533798,
                                  0.08853305876255035,
                                  0.08868934214115143,
                                  0.08893421292304993,
                                  0.0886133685708046,
                                  0.08817695081233978,
                                  0.0881962701678276,
                                  0.08811213821172714,
                                  0.0891452506184578,
                                  0.08935011923313141,
                                  0.08934961259365082,
                                  0.08890455216169357,
                                  0.08886080235242844,
                                  0.0891081690788269,
                                  0.08976045250892639,
                                  0.08954772353172302,
                                  0.08939021825790405,
                                  0.08975906670093536,
                                  0.08977613598108292,
                                  0.0896851047873497,
                                  0.08977438509464264,
                                  0.09006603807210922,
                                  0.08980820327997208,
                                  0.08988718688488007,
                                  0.08971526473760605,
                                  0.09004899114370346,
                                  0.0901557132601738,
                                  0.08969920128583908,
                                  0.09032884240150452,
                                  0.08998294174671173,
                                  0.08977649360895157,
                                  0.09020925313234329,
                                  0.09029025584459305,
                                  0.08976597338914871,
                                  0.08999000489711761,
                                  0.08985752612352371,
                                  0.09009544551372528,
                                  0.08979445695877075,
                                  0.08981890231370926,
                                  0.08993826806545258,
                                  0.09029476344585419,
                                  0.0901898518204689,
                                  0.0904805064201355,
                                  0.0902039036154747,
                                  0.09018836915493011,
                                  0.09021788835525513,
                                  13.298559188842773,
                                  0.09035386145114899,
                                  0.08994783461093903,
                                  0.08987398445606232,
                                  0.0898030549287796,
                                  0.09018076211214066,
                                  0.08989060670137405,
                                  0.0900542363524437,
                                  0.08998247236013412,
                                  0.08996569365262985,
                                  0.08956349641084671,
                                  0.0899026170372963,
                                  0.08934273570775986,
                                  0.08953327685594559,
                                  0.08896757662296295,
                                  40.889766693115234,
                                  0.0892421156167984,
                                  0.09008168429136276,
                                  0.0901041328907013,
                                  0.08966265618801117,
                                  0.08987640589475632,
                                  0.09025770425796509,
                                  0.0899430364370346,
                                  0.09005750715732574,
                                  0.09002253413200378,
                                  0.08991013467311859,
                                  0.08955400437116623,
                                  0.09024599194526672,
                                  0.08989398926496506,
                                  0.09036830812692642,
                                  0.09003657847642899,
                                  0.09000864624977112,
                                  0.09005343914031982,
                                  0.09015601873397827,
                                  0.09009037166833878,
                                  0.09033283591270447,
                                  0.08988620340824127,
                                  0.09028229117393494,
                                  0.09019381552934647,
                                  0.0902322456240654,
                                  0.09032434225082397,
                                  0.09011644124984741,
                                  0.08976441621780396,
                                  0.08991662412881851,
                                  0.08978656679391861,
                                  0.08997219800949097,
                                  0.09036137908697128,
                                  0.08971432596445084,
                                  0.08993349224328995,
                                  0.08978254348039627,
                                  0.09011665731668472,
                                  0.0903627872467041,
                                  0.09019304066896439,
                                  0.08973104506731033,
                                  0.09010143578052521,
                                  0.09001858532428741,
                                  0.08998378366231918,
                                  0.09022297710180283,
                                  0.09093927592039108,
                                  0.08909867703914642,
                                  0.08914345502853394,
                                  0.08835233747959137,
                                  0.0882631167769432,
                                  0.08950014412403107,
                                  0.08979093283414841,
                                  0.09012936055660248,
                                  0.08974586427211761,
                                  0.09011997282505035,
                                  0.08974523842334747,
                                  0.09000830352306366,
                                  0.09017462283372879,
                                  0.09004103392362595,
                                  0.08963128179311752,
                                  0.08985885232686996,
                                  0.08983511477708817,
                                  0.09008574485778809,
                                  0.09016025811433792,
                                  0.08972913771867752,
                                  0.08963147550821304,
                                  0.08982467651367188,
                                  0.08994393795728683,
                                  0.08989205211400986,
                                  0.08967658132314682,
                                  0.08987260609865189,
                                  0.09007982909679413,
                                  0.09010133147239685,
                                  0.09020086377859116,
                                  0.09046005457639694,
                                  0.08994335681200027,
                                  0.09018530696630478,
                                  0.08992722630500793,
                                  0.09044454246759415,
                                  0.09027785807847977,
                                  0.08971437811851501,
                                  0.08930660039186478,
                                  0.09013891965150833,
                                  0.08999187499284744,
                                  0.08977315574884415,
                                  0.08965734392404556,
                                  0.08979973196983337,
                                  0.09008533507585526,
                                  0.08993113040924072,
                                  0.08979012817144394,
                                  0.08988290280103683,
                                  0.09004655480384827,
                                  0.09017106890678406,
                                  0.089727021753788,
                                  0.08970198780298233,
                                  0.09002815186977386,
                                  0.08984025567770004,
                                  0.09001448005437851,
                                  0.08957063406705856,
                                  0.08992192894220352,
                                  0.09023972600698471,
                                  0.09008713066577911,
                                  0.0900142714381218,
                                  0.08985602855682373,
                                  0.0900145098567009],
             'weight_decay': 0.0005}],
 'tests': {},
 'training_time': 11972.458298683167}
{'autoincrement': 4,
 'best_models': {},
 'best_models_list': [{'accuracy': 2.0,
                       'batch_size': 32,
                       'cv_score': 0.017618830126676397,
                       'cv_val_accuracy': 0.7777777777777778,
                       'cv_val_loss': 0.09876688073078792,
                       'cv_val_macroF1': 0.017618830126676397,
                       'cv_val_microF1': 0.07673131407953461,
                       'epochs': 300,
                       'final_model': GGNN1(
  (ggnn): GatedGraphConv(30, num_layers=2)
  (fc1): Linear(in_features=30, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                       'kwargs': {'aggr_type': 'mean',
                                  'd1': 30,
                                  'd2': 50,
                                  'num_classes': 24,
                                  'num_layers': 2},
                       'learning_rate': 0.01,
                       'macroF1': 0.006748704489763125,
                       'microF1': 0.08414023372287145,
                       'model': <class 'TFM_graph_classification_models.GGNN1'>,
                       'model_instance': GGNN1(
  (ggnn): GatedGraphConv(30, num_layers=2)
  (fc1): Linear(in_features=30, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                       'name': '3_GGNN1',
                       'score': 'macroF1',
                       'time': 3923.382642507553,
                       'train_loss_history': [402.3915100097656,
                                              0.627553403377533,
                                              0.1610880345106125,
                                              200590368.0,
                                              127.40691375732422,
                                              0.0838785469532013,
                                              0.08383877575397491,
                                              0.08405112475156784,
                                              0.08400005102157593,
                                              0.11704819649457932,
                                              315.0480651855469,
                                              8.739550590515137,
                                              0.26758015155792236,
                                              150.98406982421875,
                                              1.1856788396835327,
                                              1248861440.0,
                                              2.6898114681243896,
                                              0.08465886861085892,
                                              0.08455833047628403,
                                              0.08453904837369919,
                                              0.08463430404663086,
                                              0.08893854171037674,
                                              2255.366943359375,
                                              5255256.5,
                                              0.0845518633723259,
                                              0.08456744998693466,
                                              0.0846351757645607,
                                              0.08459139615297318,
                                              0.0845951959490776,
                                              0.08459458500146866,
                                              741.9649047851562,
                                              375.397216796875,
                                              0.08453738689422607,
                                              1314.5992431640625,
                                              0.08459823578596115,
                                              2760.76708984375,
                                              0.08449413627386093,
                                              0.0844498947262764,
                                              0.08453848212957382,
                                              0.08451221138238907,
                                              0.08967375755310059,
                                              0.21154144406318665,
                                              6485.376953125,
                                              123632.40625,
                                              29165.78125,
                                              38.000274658203125,
                                              0.08432421088218689,
                                              0.08440204709768295,
                                              0.08437246084213257,
                                              31.02018928527832,
                                              0.08415640890598297,
                                              4.811329364776611,
                                              0.08450804650783539,
                                              0.08431932330131531,
                                              0.0844046100974083,
                                              1.6224708557128906,
                                              0.08452630043029785,
                                              0.08453928679227829,
                                              0.0845048725605011,
                                              0.0844370573759079,
                                              0.08444765210151672,
                                              0.08450065553188324,
                                              0.08445669710636139,
                                              0.08438482880592346,
                                              0.08415114134550095,
                                              0.08441533893346786,
                                              0.08437883108854294,
                                              0.08448990434408188,
                                              0.08448030054569244,
                                              0.0844801515340805,
                                              0.08438432216644287,
                                              0.08433922380208969,
                                              0.08432400226593018,
                                              0.08451466262340546,
                                              0.08452446758747101,
                                              0.08453328907489777,
                                              0.08448510617017746,
                                              0.08447270840406418,
                                              0.08449146896600723,
                                              0.08447691053152084,
                                              0.08443857729434967,
                                              0.08449263125658035,
                                              0.08442482352256775,
                                              0.08449450135231018,
                                              0.0844353586435318,
                                              0.08452147245407104,
                                              0.0844416618347168,
                                              0.08446303755044937,
                                              0.08447965979576111,
                                              0.0843501091003418,
                                              0.08422920107841492,
                                              0.08444242179393768,
                                              0.08447030931711197,
                                              0.08448900282382965,
                                              0.08450789749622345,
                                              0.08435028791427612,
                                              0.08434262871742249,
                                              0.08449234813451767,
                                              0.08449122309684753,
                                              0.08449383825063705,
                                              0.08452076464891434,
                                              0.08449948579072952,
                                              0.08440456539392471,
                                              0.08449050039052963,
                                              0.0841154232621193,
                                              2.2428197860717773,
                                              0.08384676277637482,
                                              0.08394981920719147,
                                              0.0838002935051918,
                                              0.08447454124689102,
                                              0.08348763734102249,
                                              0.08416859805583954,
                                              0.08417699486017227,
                                              0.08438660949468613,
                                              0.4162343144416809,
                                              0.08450004458427429,
                                              0.08446227014064789,
                                              0.08444914221763611,
                                              0.0843791589140892,
                                              0.08446182310581207,
                                              0.08455074578523636,
                                              0.08446145057678223,
                                              0.0836617723107338,
                                              0.08338803797960281,
                                              0.08333099633455276,
                                              0.08318332582712173,
                                              0.08332567662000656,
                                              0.08302734792232513,
                                              0.08371946960687637,
                                              0.08449801802635193,
                                              0.08449846506118774,
                                              0.08426801860332489,
                                              0.08432383090257645,
                                              200335072.0,
                                              0.08437470346689224,
                                              0.08452057838439941,
                                              0.08447948843240738,
                                              0.08449156582355499,
                                              0.08443012088537216,
                                              0.0844721645116806,
                                              0.08447398245334625,
                                              0.08443788439035416,
                                              0.08443618565797806,
                                              0.08449234068393707,
                                              0.10008276253938675,
                                              0.08434968441724777,
                                              0.08430284261703491,
                                              0.08442266285419464,
                                              0.08438229560852051,
                                              1806.7781982421875,
                                              0.08451782912015915,
                                              0.0844816267490387,
                                              13.898186683654785,
                                              0.08445671200752258,
                                              1.4685406684875488,
                                              4.019253253936768,
                                              302.15570068359375,
                                              0.08452155441045761,
                                              0.14563511312007904,
                                              27.54737663269043,
                                              3.7263739109039307,
                                              78.2233657836914,
                                              0.08451861888170242,
                                              0.08448174595832825,
                                              407.52105712890625,
                                              19.42632484436035,
                                              0.08444830775260925,
                                              36449228.0,
                                              7.751605033874512,
                                              0.08446042984724045,
                                              0.08437091112136841,
                                              3167184.25,
                                              0.08450077474117279,
                                              0.08428910374641418,
                                              0.08419448137283325,
                                              0.08466444909572601,
                                              5.597066402435303,
                                              0.11012237519025803,
                                              0.08447238802909851,
                                              58.212467193603516,
                                              5.655828952789307,
                                              3588.997314453125,
                                              0.409007728099823,
                                              32.17847442626953,
                                              0.08447328209877014,
                                              0.0845048725605011,
                                              0.08449171483516693,
                                              0.08447052538394928,
                                              0.08445976674556732,
                                              0.08446221798658371,
                                              0.08458263427019119,
                                              0.08430005609989166,
                                              8.114312171936035,
                                              0.08432270586490631,
                                              0.08445460349321365,
                                              0.08443901687860489,
                                              900.8036499023438,
                                              0.08444087207317352,
                                              0.08441809564828873,
                                              0.4795038402080536,
                                              0.08451033383607864,
                                              0.08451204001903534,
                                              0.08450926095247269,
                                              0.08447223901748657,
                                              0.08437959104776382,
                                              0.08444824069738388,
                                              0.08453644067049026,
                                              7.690800666809082,
                                              0.08448299020528793,
                                              0.08449721336364746,
                                              0.08445368707180023,
                                              0.08438749611377716,
                                              0.08451572060585022,
                                              0.08452669531106949,
                                              89.34236907958984,
                                              0.08450479805469513,
                                              30.75047492980957,
                                              0.08451566100120544,
                                              0.08449652045965195,
                                              68.70880126953125,
                                              0.08445856720209122,
                                              12.459997177124023,
                                              0.08449093997478485,
                                              0.08446609228849411,
                                              0.08448096364736557,
                                              0.11521673947572708,
                                              0.08448649197816849,
                                              1010.2301025390625,
                                              0.08451484888792038,
                                              0.0844508707523346,
                                              0.0843883827328682,
                                              0.08444556593894958,
                                              182.40513610839844,
                                              0.08444695174694061,
                                              304.5606689453125,
                                              0.0844317376613617,
                                              0.08447675406932831,
                                              0.08446475118398666,
                                              0.08447200804948807,
                                              0.08442620933055878,
                                              0.08441336452960968,
                                              0.08443718403577805,
                                              0.08444596081972122,
                                              0.0845516175031662,
                                              0.08445774763822556,
                                              37.44003677368164,
                                              0.0844288170337677,
                                              0.08449657261371613,
                                              0.08451525121927261,
                                              0.08444896340370178,
                                              0.08451040834188461,
                                              0.08449771255254745,
                                              0.0844845324754715,
                                              0.08452331274747849,
                                              0.08448804169893265,
                                              0.08449441939592361,
                                              0.08447355777025223,
                                              217506.6875,
                                              0.08448262512683868,
                                              27566.361328125,
                                              0.084529809653759,
                                              60.93157958984375,
                                              0.0844476968050003,
                                              0.08442551642656326,
                                              81.24128723144531,
                                              0.08442741632461548,
                                              0.08438073843717575,
                                              0.08440481126308441,
                                              0.08442452549934387,
                                              0.08434814214706421,
                                              11.347413063049316,
                                              26.29210662841797,
                                              0.23023147881031036,
                                              0.08447755873203278,
                                              0.08438970148563385,
                                              0.08435468375682831,
                                              0.08439872413873672,
                                              0.08438412100076675,
                                              0.08440207690000534,
                                              0.0844302773475647,
                                              11.261951446533203,
                                              0.08435912430286407,
                                              3.4124855995178223,
                                              0.08436176180839539,
                                              0.08430063724517822,
                                              0.08941613137722015,
                                              0.08446674793958664,
                                              0.08433955907821655,
                                              0.08443570137023926,
                                              0.08435171097517014,
                                              21675608.0,
                                              0.08440378308296204,
                                              0.08433499932289124,
                                              0.40477755665779114,
                                              0.08435443788766861,
                                              0.08426611125469208,
                                              13186.3408203125,
                                              39600.96875,
                                              0.08453357219696045,
                                              0.9381150603294373,
                                              0.09587060660123825,
                                              0.514412522315979,
                                              0.09367242455482483,
                                              0.09441542625427246,
                                              0.09452634304761887,
                                              0.09454908967018127,
                                              22.32537269592285,
                                              32339.833984375,
                                              0.09451416879892349,
                                              0.09449652582406998,
                                              0.09464240074157715,
                                              0.09449930489063263,
                                              1958.1798095703125,
                                              0.09455957263708115,
                                              0.09456851333379745,
                                              1.2811931371688843,
                                              0.09453321993350983,
                                              0.09458591043949127,
                                              0.09450304508209229,
                                              5.49962043762207,
                                              72.5165023803711,
                                              0.09458471089601517,
                                              0.0945429876446724,
                                              0.09455682337284088,
                                              0.09453796595335007,
                                              0.09452159702777863,
                                              2.5312438011169434,
                                              0.0945226177573204,
                                              0.0944872573018074,
                                              5.223100185394287,
                                              2137.731201171875,
                                              0.142483189702034,
                                              0.20935583114624023,
                                              0.09459515661001205,
                                              0.09455345571041107,
                                              2812.721435546875,
                                              0.09442364424467087,
                                              0.1131102666258812,
                                              0.09442222863435745,
                                              0.09438570588827133,
                                              0.09433407336473465,
                                              0.13149750232696533,
                                              0.09370430558919907,
                                              0.09366568177938461,
                                              0.09354622662067413,
                                              0.09447423368692398,
                                              0.6216050982475281,
                                              0.09451036900281906,
                                              0.09451115876436234,
                                              1349.734619140625,
                                              0.09453874081373215,
                                              0.11682190001010895,
                                              0.10219868272542953,
                                              0.09455303847789764,
                                              0.0945776104927063,
                                              0.09455080330371857,
                                              2.9681241512298584,
                                              0.09452127665281296,
                                              0.09452573955059052,
                                              29.074121475219727,
                                              14.531797409057617,
                                              0.09461171925067902,
                                              0.09457170218229294,
                                              0.09453096240758896,
                                              0.7046006321907043,
                                              0.33546608686447144,
                                              0.09435248374938965,
                                              0.09443413466215134,
                                              0.31541207432746887,
                                              0.14433683454990387,
                                              0.09446719288825989,
                                              3715.66796875,
                                              0.0944618359208107,
                                              0.09454601258039474,
                                              0.0945052057504654,
                                              0.09456264227628708,
                                              0.09456627815961838,
                                              1.2233333587646484,
                                              0.09454774111509323,
                                              249.55780029296875,
                                              181.83404541015625,
                                              154.2233123779297,
                                              0.09459660947322845,
                                              0.09442275762557983,
                                              0.09446389973163605,
                                              0.09459325671195984,
                                              0.09457375854253769,
                                              0.0945519208908081,
                                              0.09459152817726135,
                                              0.0945110023021698,
                                              0.7131268978118896,
                                              0.09457242488861084,
                                              13.699750900268555,
                                              0.09455432742834091,
                                              0.09460796415805817,
                                              0.09450221061706543,
                                              11696.2060546875,
                                              0.09452937543392181,
                                              0.09452711045742035,
                                              0.09458191692829132,
                                              9.455473899841309,
                                              0.09451385587453842,
                                              0.09448845684528351,
                                              131.53732299804688,
                                              4293.73291015625,
                                              0.09409381449222565,
                                              0.09368418902158737,
                                              0.09371126443147659,
                                              0.09354376047849655,
                                              0.2533681094646454,
                                              0.314374715089798,
                                              0.09430520236492157,
                                              0.09440848976373672,
                                              0.094419464468956,
                                              0.09437459707260132,
                                              0.09439592063426971,
                                              1767081.0,
                                              594.7551879882812,
                                              0.09452971816062927,
                                              0.09449964016675949,
                                              0.09450231492519379,
                                              0.09459840506315231,
                                              0.09455262124538422,
                                              0.09447214007377625,
                                              0.0944393202662468,
                                              0.09440606832504272,
                                              0.09447411447763443,
                                              0.09437554329633713,
                                              0.09433301538228989,
                                              0.09440498054027557,
                                              0.10780218243598938,
                                              0.09436662495136261,
                                              0.09431613236665726,
                                              0.0945611521601677,
                                              219.7027587890625,
                                              0.4978674352169037,
                                              0.0944586768746376,
                                              0.30992811918258667,
                                              1.8426533937454224,
                                              0.5712193250656128,
                                              0.09446409344673157,
                                              0.09459622949361801,
                                              0.09451809525489807,
                                              87.38387298583984,
                                              0.09456565976142883,
                                              3497.284423828125,
                                              0.09454844892024994,
                                              0.09451253712177277,
                                              0.09457314759492874,
                                              0.09456212818622589,
                                              0.15045373141765594,
                                              0.14893552660942078,
                                              562.9702758789062,
                                              131.55374145507812,
                                              0.09452853351831436,
                                              0.09735272079706192,
                                              0.09456931799650192,
                                              0.6130989193916321,
                                              0.09453856199979782,
                                              0.09444250911474228,
                                              0.09452873468399048,
                                              0.09453690052032471,
                                              1.229264736175537,
                                              0.09449880570173264,
                                              820.4354248046875,
                                              1325.2156982421875,
                                              0.09444205462932587,
                                              0.0945630744099617,
                                              0.09459412842988968,
                                              0.0945231169462204,
                                              513502.1875,
                                              0.09456072747707367,
                                              1.1679768562316895,
                                              0.0945015400648117,
                                              0.09457283467054367,
                                              0.09455496817827225,
                                              0.19079294800758362,
                                              36.89104461669922,
                                              0.0944361463189125,
                                              0.09443370997905731,
                                              0.09447237849235535,
                                              20771922.0,
                                              0.09442625194787979,
                                              0.1135120838880539,
                                              0.09445078670978546,
                                              0.09434980154037476,
                                              0.09435220062732697,
                                              0.35446515679359436,
                                              0.09654965996742249,
                                              0.09453532844781876,
                                              0.09456611424684525,
                                              32323.8828125,
                                              978.3565063476562,
                                              0.09457312524318695,
                                              0.0945013165473938,
                                              0.09439229965209961,
                                              4412095.5,
                                              0.09448060393333435,
                                              0.09440262615680695,
                                              0.0943794921040535,
                                              0.09443134814500809,
                                              369.93389892578125,
                                              0.14076443016529083,
                                              0.09438452124595642,
                                              0.09434284269809723,
                                              0.10136646777391434,
                                              0.09443105757236481,
                                              0.09402314573526382,
                                              26166.89453125,
                                              0.09349802136421204,
                                              3.321302652359009,
                                              0.09364969283342361,
                                              0.09496324509382248,
                                              0.09372551739215851,
                                              0.09436341375112534,
                                              0.09432327002286911,
                                              0.09436825662851334,
                                              0.0944887325167656,
                                              0.09451030939817429,
                                              0.09454188495874405,
                                              0.0945427343249321,
                                              0.09454578906297684,
                                              0.09454236924648285,
                                              0.09450600296258926,
                                              0.09452961385250092,
                                              0.09452560544013977,
                                              0.09452951699495316,
                                              2834032.25,
                                              0.09452395886182785,
                                              10.720794677734375,
                                              68.72327423095703,
                                              8477249.0,
                                              0.09440808743238449,
                                              5520.7861328125,
                                              0.09362620860338211,
                                              0.09349146485328674,
                                              0.09339573979377747,
                                              149.35525512695312,
                                              278.6841735839844,
                                              0.0943543016910553,
                                              37.89771270751953,
                                              5100427.5,
                                              0.09433344006538391,
                                              6600393.5,
                                              0.09443498402833939,
                                              0.09448225051164627,
                                              0.09439776837825775,
                                              0.09439967572689056,
                                              0.09428673982620239,
                                              0.0945155993103981,
                                              0.27247631549835205,
                                              0.09454277157783508,
                                              0.09456893801689148,
                                              0.09452944248914719,
                                              0.09454522281885147,
                                              0.09451722353696823,
                                              0.09460891038179398,
                                              0.09455429017543793,
                                              10244.599609375,
                                              84.07907104492188,
                                              0.0944337323307991,
                                              0.09459497779607773,
                                              0.09454827755689621,
                                              0.09453362226486206,
                                              0.09449025988578796,
                                              0.09453912824392319,
                                              0.3738631308078766,
                                              0.09447181224822998,
                                              0.09451087564229965,
                                              0.09459631890058517,
                                              0.09454839676618576,
                                              0.09458976984024048,
                                              0.09457769244909286,
                                              0.09456919878721237,
                                              3.4914751052856445,
                                              1427.79345703125,
                                              0.4578329920768738,
                                              884.7360229492188,
                                              0.09449641406536102,
                                              0.09440287202596664,
                                              510.2096862792969,
                                              0.09450377523899078,
                                              0.09451079368591309,
                                              2.1641271114349365,
                                              0.0944771096110344,
                                              25.285322189331055,
                                              0.09460421651601791,
                                              0.09461460262537003,
                                              21697.78125,
                                              0.11157756298780441,
                                              9.951555252075195,
                                              0.09457164257764816,
                                              0.09458516538143158,
                                              770.6202392578125,
                                              0.5892700552940369,
                                              0.09393339604139328,
                                              0.09381875395774841,
                                              0.09371479600667953,
                                              0.09378113597631454,
                                              0.0938304141163826,
                                              0.09431172162294388,
                                              1034.4150390625,
                                              538.9066772460938,
                                              0.09464322030544281,
                                              0.7673370242118835,
                                              0.18193259835243225,
                                              0.09461429715156555,
                                              0.09462171792984009,
                                              0.6443932056427002,
                                              0.09465876966714859,
                                              0.09471260756254196,
                                              0.09465605765581131,
                                              0.09452185034751892,
                                              0.09450294822454453,
                                              0.12810559570789337,
                                              0.094684898853302,
                                              8.992913246154785,
                                              0.8569191694259644,
                                              582.5096435546875,
                                              0.09462642669677734,
                                              0.09460807591676712,
                                              0.09465175122022629,
                                              0.0958004742860794,
                                              0.09474059194326401,
                                              0.09462853521108627,
                                              657.1932373046875,
                                              0.19906458258628845,
                                              0.09392101317644119,
                                              0.09379742294549942,
                                              0.7946452498435974,
                                              0.1725653111934662,
                                              0.09447696059942245,
                                              0.09447885304689407,
                                              0.09469438344240189,
                                              0.09465198963880539,
                                              0.09460066258907318,
                                              560.6574096679688,
                                              0.09460125863552094,
                                              0.0946880578994751,
                                              0.09468343108892441,
                                              0.09466951340436935,
                                              7.846862316131592,
                                              0.09464780241250992,
                                              1.8931728601455688,
                                              0.0939062237739563,
                                              0.14714713394641876,
                                              0.09371401369571686,
                                              0.14943532645702362,
                                              9.442521095275879,
                                              0.09458307921886444,
                                              0.09459754824638367,
                                              0.19837377965450287,
                                              0.18834809958934784,
                                              0.09383810311555862,
                                              0.09375286102294922,
                                              0.0936644896864891,
                                              0.09361999481916428,
                                              0.0935748741030693,
                                              1.445277214050293,
                                              0.0944688618183136,
                                              12.287859916687012,
                                              0.09468797594308853,
                                              0.09469488263130188,
                                              0.09460695087909698,
                                              0.7720686197280884,
                                              0.0945553258061409,
                                              0.09460161626338959,
                                              0.09466303139925003,
                                              0.09467338770627975,
                                              0.09465459734201431,
                                              0.09461504220962524,
                                              0.0945778414607048,
                                              0.09464619308710098,
                                              0.09465964883565903,
                                              0.09464212507009506,
                                              229.66256713867188,
                                              18.826688766479492,
                                              0.09455174207687378,
                                              0.09412211924791336,
                                              17504.03515625,
                                              0.09471108019351959,
                                              0.09461493790149689,
                                              10.089583396911621,
                                              0.09461957961320877,
                                              31.376590728759766,
                                              0.09470537304878235,
                                              0.09463783353567123,
                                              5610218.0,
                                              0.09464839100837708,
                                              0.09463448077440262,
                                              0.0942724272608757,
                                              0.09408007562160492,
                                              0.48262032866477966,
                                              0.09465523064136505,
                                              2.4015886783599854,
                                              0.09462562948465347,
                                              142063.546875,
                                              0.09455953538417816,
                                              1.974007487297058,
                                              0.09464676678180695,
                                              0.09465840458869934,
                                              0.10140714049339294,
                                              0.09454220533370972,
                                              0.09462330490350723,
                                              0.09451285749673843,
                                              4.050822734832764,
                                              0.09465648978948593,
                                              0.09463375806808472,
                                              0.09465823322534561,
                                              0.09469760209321976,
                                              0.09466546028852463,
                                              932.041259765625,
                                              0.09463147073984146,
                                              3.409330129623413,
                                              54.3207893371582,
                                              0.09465781599283218,
                                              0.09465416520833969,
                                              1.7536137104034424,
                                              0.09468229115009308,
                                              84.98772430419922,
                                              0.09475217014551163,
                                              0.09401313960552216,
                                              134.52792358398438,
                                              0.09469401836395264,
                                              0.23205254971981049,
                                              0.0946466401219368,
                                              0.09470997005701065,
                                              0.09463036060333252,
                                              0.09456641972064972,
                                              0.09461867809295654,
                                              0.9174342155456543,
                                              0.09432287514209747,
                                              3.919257879257202,
                                              0.09378696233034134,
                                              0.09345461428165436,
                                              0.09351752698421478,
                                              0.09323997050523758,
                                              0.09312094748020172,
                                              0.09307726472616196,
                                              0.09321693331003189,
                                              0.09316740185022354,
                                              0.09299517422914505,
                                              0.09285470098257065,
                                              0.2572370171546936,
                                              0.09311380237340927,
                                              0.09293543547391891,
                                              0.09303213655948639,
                                              0.1493358463048935,
                                              0.09310180693864822,
                                              0.09321630746126175,
                                              10.437809944152832,
                                              0.0943022146821022,
                                              0.09423695504665375,
                                              2590.866943359375,
                                              1.4054656028747559,
                                              38.2252311706543,
                                              19227.08203125,
                                              0.09473899751901627,
                                              0.09466087073087692,
                                              0.0946071520447731,
                                              830.8789672851562,
                                              0.0945894792675972,
                                              2.7011053562164307,
                                              0.504011332988739,
                                              0.09468135982751846,
                                              0.09465712308883667,
                                              0.0945027694106102,
                                              3.204328775405884,
                                              0.09469970315694809,
                                              136.45835876464844,
                                              0.0946570485830307,
                                              276.604248046875,
                                              0.12033631652593613,
                                              0.09448137134313583,
                                              1677058.875,
                                              0.09450552612543106,
                                              6.231417179107666,
                                              0.4628032147884369,
                                              0.12920531630516052,
                                              0.5548384189605713,
                                              0.09467286616563797,
                                              0.09465939551591873,
                                              0.09464029967784882,
                                              0.09545759111642838,
                                              0.09464362263679504,
                                              1.4605083465576172,
                                              0.6265268921852112,
                                              0.13141100108623505,
                                              0.09462108463048935,
                                              0.09455641359090805,
                                              5.9682111740112305,
                                              0.09459859877824783,
                                              32.384986877441406,
                                              0.09444091469049454,
                                              0.09422245621681213,
                                              0.0940980538725853,
                                              0.09372019022703171,
                                              0.09348495304584503,
                                              0.09494473040103912,
                                              0.09471526741981506,
                                              0.09466321766376495,
                                              0.09463952481746674,
                                              0.0946340337395668,
                                              0.0946597084403038,
                                              0.10840669274330139,
                                              0.09467717260122299,
                                              0.09460677951574326,
                                              0.09988532215356827,
                                              0.09422221034765244,
                                              0.11676912754774094,
                                              0.09440311044454575,
                                              45.41225051879883,
                                              507.40032958984375,
                                              0.13748902082443237,
                                              0.09470012038946152,
                                              226.58145141601562,
                                              0.4495183825492859,
                                              0.09465526789426804,
                                              0.09466543048620224,
                                              61.091617584228516,
                                              0.09468193352222443,
                                              0.09465447813272476,
                                              0.0945592001080513,
                                              0.09467066824436188,
                                              0.09464655071496964,
                                              141.53970336914062,
                                              116328.4609375,
                                              0.09458193182945251,
                                              0.12122631818056107,
                                              4.387285232543945,
                                              2681976.5,
                                              0.09463132172822952,
                                              0.0946061760187149,
                                              0.0944783166050911,
                                              0.09446098655462265,
                                              0.09450376033782959,
                                              0.09455794841051102,
                                              0.09459281712770462,
                                              0.09462811797857285,
                                              6.4233717918396,
                                              0.09450298547744751,
                                              0.09456917643547058,
                                              0.0934096947312355,
                                              0.0934649258852005,
                                              0.09316615760326385,
                                              0.5173411965370178,
                                              1.9697647094726562,
                                              32.42613983154297,
                                              0.09472211450338364,
                                              0.09469816833734512,
                                              0.0956822857260704,
                                              0.09459832310676575,
                                              0.0946735143661499,
                                              0.09462333470582962,
                                              0.09459586441516876,
                                              1.9096038341522217,
                                              0.0946708396077156,
                                              0.0946536511182785,
                                              0.09466943144798279,
                                              0.09466858953237534,
                                              0.09464891254901886,
                                              0.09463982284069061,
                                              0.09469345957040787,
                                              0.0946459099650383,
                                              0.0945952758193016,
                                              122.37969970703125,
                                              4.63157844543457,
                                              56.718482971191406,
                                              0.18920323252677917,
                                              454.80084228515625,
                                              4036.590087890625,
                                              0.09464812278747559,
                                              127179.6875,
                                              0.09467849880456924,
                                              0.09463875740766525,
                                              0.09431662410497665,
                                              665.6954956054688,
                                              0.09449782967567444,
                                              0.09459271281957626,
                                              0.09455017000436783,
                                              2.904323101043701,
                                              0.09459733217954636,
                                              0.0946681872010231,
                                              0.09470684081315994,
                                              3.52655029296875,
                                              0.09464657306671143,
                                              0.09463555365800858,
                                              1.8135828971862793,
                                              0.09465093165636063,
                                              0.0946563184261322,
                                              0.09455125033855438,
                                              0.09466362744569778,
                                              0.0946488231420517,
                                              2.0649492740631104,
                                              1.0428603887557983,
                                              0.09464512020349503,
                                              0.09463749080896378,
                                              0.09463761001825333,
                                              204.697509765625,
                                              0.09467710554599762],
                       'val_accuracy_history': [1.7894736842105263,
                                                1.368421052631579,
                                                0.3157894736842105,
                                                1.0526315789473684,
                                                1.0,
                                                0.42105263157894735,
                                                0.6842105263157895,
                                                1.4210526315789473,
                                                0.10526315789473684,
                                                0.3157894736842105,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                3.0,
                                                0.0,
                                                2.0,
                                                1.0,
                                                2.0,
                                                1.0,
                                                0.0,
                                                2.0,
                                                1.0,
                                                2.0,
                                                1.0,
                                                2.0,
                                                3.0,
                                                1.0,
                                                2.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                2.1578947368421053,
                                                1.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                3.0,
                                                0.0,
                                                1.0,
                                                1.2105263157894737,
                                                2.0,
                                                1.0,
                                                2.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                2.0,
                                                1.0,
                                                0.5263157894736842,
                                                0.0,
                                                1.0,
                                                0.0,
                                                2.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                2.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                4.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                2.0,
                                                1.0,
                                                3.0,
                                                1.0,
                                                0.631578947368421,
                                                1.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                1.631578947368421,
                                                1.8421052631578947,
                                                0.0,
                                                2.0,
                                                0.9473684210526315,
                                                0.47368421052631576,
                                                0.7894736842105263,
                                                0.05263157894736842,
                                                3.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                1.2105263157894737,
                                                0.8421052631578947,
                                                0.21052631578947367,
                                                0.9473684210526315,
                                                1.5263157894736843,
                                                1.0526315789473684,
                                                1.0,
                                                0.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                2.0,
                                                1.0,
                                                2.0,
                                                1.0,
                                                2.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                3.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                1.6842105263157894,
                                                1.9473684210526316,
                                                0.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                3.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                2.0,
                                                2.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                3.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                3.0,
                                                1.0,
                                                1.0,
                                                2.0,
                                                1.0,
                                                2.0,
                                                3.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                2.0,
                                                3.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                3.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.7777777777777778,
                                                0.1111111111111111,
                                                0.8888888888888888,
                                                2.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                0.0,
                                                2.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                0.7777777777777778,
                                                0.7777777777777778,
                                                0.7777777777777778,
                                                1.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                3.0,
                                                1.0,
                                                2.0,
                                                2.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                2.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                2.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                2.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                0.1111111111111111,
                                                0.4444444444444444,
                                                0.8888888888888888,
                                                0.2222222222222222,
                                                0.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                2.0,
                                                2.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                2.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                0.0,
                                                3.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                2.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.8888888888888888,
                                                0.5555555555555556,
                                                0.7777777777777778,
                                                1.0,
                                                0.2222222222222222,
                                                0.3333333333333333,
                                                0.1111111111111111,
                                                0.3333333333333333,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                2.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                2.0,
                                                1.0,
                                                0.0,
                                                2.0,
                                                1.0,
                                                3.0,
                                                1.0,
                                                0.8888888888888888,
                                                1.0,
                                                0.8888888888888888,
                                                0.0,
                                                2.6666666666666665,
                                                1.0,
                                                2.0,
                                                0.0,
                                                1.7777777777777777,
                                                0.0,
                                                3.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                3.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                3.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                4.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.8888888888888888,
                                                0.4444444444444444,
                                                0.6666666666666666,
                                                0.1111111111111111,
                                                0.3333333333333333,
                                                1.0,
                                                3.0,
                                                2.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                3.0,
                                                0.0,
                                                3.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                4.0,
                                                2.0,
                                                1.0,
                                                0.0,
                                                3.0,
                                                3.0,
                                                1.0,
                                                1.0,
                                                2.0526315789473686,
                                                1.736842105263158,
                                                2.9473684210526314,
                                                1.894736842105263,
                                                4.0,
                                                2.0,
                                                1.0,
                                                2.0,
                                                2.0,
                                                1.0,
                                                2.0,
                                                3.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                2.0,
                                                2.0,
                                                0.3684210526315789,
                                                2.6315789473684212,
                                                2.526315789473684,
                                                1.894736842105263,
                                                4.0,
                                                2.0,
                                                3.0,
                                                1.0,
                                                2.0,
                                                3.0526315789473686,
                                                1.263157894736842,
                                                1.105263157894737,
                                                0.21052631578947367,
                                                1.1578947368421053,
                                                1.0,
                                                2.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                3.0,
                                                2.0,
                                                2.0,
                                                4.0,
                                                3.0,
                                                3.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                2.0,
                                                2.0,
                                                2.0,
                                                0.0,
                                                2.0,
                                                3.210526315789474,
                                                0.10526315789473684,
                                                3.0,
                                                0.0,
                                                5.0,
                                                2.0,
                                                1.0,
                                                3.0,
                                                2.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                0.2631578947368421,
                                                0.0,
                                                2.0,
                                                2.0,
                                                1.0,
                                                2.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                4.0,
                                                0.0,
                                                2.0,
                                                2.0,
                                                3.0,
                                                4.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                3.0,
                                                2.210526315789474,
                                                3.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                0.10526315789473684,
                                                4.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                2.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                1.8421052631578947,
                                                2.4210526315789473,
                                                1.4736842105263157,
                                                0.3157894736842105,
                                                0.7894736842105263,
                                                1.6842105263157894,
                                                1.631578947368421,
                                                1.9473684210526316,
                                                1.6842105263157894,
                                                1.8421052631578947,
                                                1.894736842105263,
                                                0.6842105263157895,
                                                1.0526315789473684,
                                                0.8947368421052632,
                                                2.0,
                                                2.4210526315789473,
                                                1.4210526315789473,
                                                0.9473684210526315,
                                                1.1578947368421053,
                                                0.0,
                                                0.9473684210526315,
                                                1.0,
                                                4.0,
                                                4.0,
                                                2.0,
                                                3.0,
                                                1.0,
                                                2.0,
                                                2.0,
                                                0.0,
                                                2.0,
                                                3.0,
                                                2.0,
                                                2.0,
                                                2.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                3.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                2.0,
                                                2.0,
                                                0.0,
                                                1.0,
                                                3.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                3.0,
                                                3.0,
                                                1.0,
                                                0.0,
                                                3.0,
                                                2.0,
                                                2.0,
                                                0.0,
                                                2.0,
                                                0.8947368421052632,
                                                1.5263157894736843,
                                                1.6842105263157894,
                                                0.2631578947368421,
                                                1.0,
                                                3.0,
                                                5.0,
                                                2.0,
                                                3.0,
                                                1.0,
                                                2.0,
                                                1.0,
                                                2.0,
                                                1.0,
                                                2.789473684210526,
                                                1.0,
                                                0.0,
                                                2.0,
                                                1.0,
                                                0.0,
                                                2.0,
                                                2.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                2.0,
                                                1.0,
                                                2.0,
                                                4.0,
                                                2.0,
                                                1.0,
                                                1.0,
                                                4.0,
                                                2.0,
                                                3.0,
                                                2.0,
                                                4.0,
                                                3.0,
                                                2.0,
                                                1.0,
                                                1.0,
                                                2.0,
                                                2.0,
                                                2.0,
                                                1.0,
                                                2.0,
                                                1.631578947368421,
                                                0.21052631578947367,
                                                1.736842105263158,
                                                0.9473684210526315,
                                                1.4736842105263157,
                                                1.0,
                                                2.0,
                                                2.0,
                                                3.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                2.0,
                                                2.0,
                                                1.0,
                                                4.0,
                                                2.0,
                                                3.0,
                                                3.0,
                                                4.0,
                                                2.0,
                                                3.0,
                                                0.0,
                                                2.0,
                                                3.0,
                                                1.0,
                                                2.0,
                                                3.0,
                                                2.0,
                                                2.0,
                                                1.894736842105263,
                                                2.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                2.0,
                                                1.0,
                                                5.0,
                                                3.0,
                                                3.0,
                                                2.0,
                                                2.0,
                                                1.0,
                                                1.0,
                                                2.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                2.0],
                       'val_loss': 0.0900145098567009,
                       'val_loss_history': [0.13289986550807953,
                                            30.159862518310547,
                                            0.1306895911693573,
                                            772.6478881835938,
                                            0.12578943371772766,
                                            0.1313501000404358,
                                            0.12481862306594849,
                                            0.12818337976932526,
                                            0.12867261469364166,
                                            0.13204559683799744,
                                            0.1220577135682106,
                                            0.11260901391506195,
                                            0.11409381777048111,
                                            0.11440904438495636,
                                            0.11434926837682724,
                                            0.11414065212011337,
                                            0.11439044773578644,
                                            0.11462827026844025,
                                            0.11452741175889969,
                                            0.11467263847589493,
                                            0.11460597068071365,
                                            0.11462745815515518,
                                            0.11468517035245895,
                                            0.11480430513620377,
                                            0.1147732213139534,
                                            0.11485739797353745,
                                            0.11479106545448303,
                                            0.1150507852435112,
                                            0.11502186954021454,
                                            0.11493460834026337,
                                            0.11511021852493286,
                                            0.11504624783992767,
                                            0.1150892823934555,
                                            0.11518291383981705,
                                            64640324.0,
                                            0.11498195677995682,
                                            0.11846525222063065,
                                            0.11501139402389526,
                                            0.11516815423965454,
                                            0.11503072828054428,
                                            0.11535662412643433,
                                            0.11514862626791,
                                            0.11519207060337067,
                                            0.11522386968135834,
                                            0.11516549438238144,
                                            0.11545438319444656,
                                            0.11529026180505753,
                                            0.11528454720973969,
                                            0.11551813781261444,
                                            0.11532633006572723,
                                            0.1173066645860672,
                                            0.11544819921255112,
                                            0.11565966159105301,
                                            0.1153019443154335,
                                            0.11566291004419327,
                                            0.11555980145931244,
                                            0.11566717177629471,
                                            0.11576888710260391,
                                            0.11573272943496704,
                                            0.11575499922037125,
                                            0.11589116603136063,
                                            0.11577892303466797,
                                            0.11567796766757965,
                                            0.11675383150577545,
                                            0.11572939157485962,
                                            0.11599055677652359,
                                            0.11586335301399231,
                                            0.11595223098993301,
                                            0.115937240421772,
                                            0.11586461216211319,
                                            0.11587101221084595,
                                            0.1159282699227333,
                                            0.11588104814291,
                                            0.11595000326633453,
                                            0.11601797491312027,
                                            0.11578642576932907,
                                            0.1159607544541359,
                                            0.11591330915689468,
                                            0.1158139705657959,
                                            0.1159902960062027,
                                            0.1159883365035057,
                                            0.1158863827586174,
                                            0.11616058647632599,
                                            0.116038978099823,
                                            0.11605293303728104,
                                            0.11608398705720901,
                                            0.11597417294979095,
                                            0.11598964035511017,
                                            0.11528076976537704,
                                            0.1164831668138504,
                                            0.11611490696668625,
                                            0.11596962809562683,
                                            0.11598684638738632,
                                            0.11591029167175293,
                                            0.11605054140090942,
                                            0.11600024253129959,
                                            0.1159677505493164,
                                            0.11592534184455872,
                                            0.11612578481435776,
                                            0.11618667840957642,
                                            0.11620379984378815,
                                            0.11618492752313614,
                                            0.11606466770172119,
                                            0.11596335470676422,
                                            0.11628828197717667,
                                            0.1161775216460228,
                                            0.11668728291988373,
                                            0.11626330763101578,
                                            0.11897259205579758,
                                            0.11721517890691757,
                                            0.11805533617734909,
                                            0.11683106422424316,
                                            0.11628374457359314,
                                            0.11647655814886093,
                                            0.11625979095697403,
                                            0.11615417152643204,
                                            0.11611239612102509,
                                            0.11597920954227448,
                                            0.11626138538122177,
                                            0.11619382351636887,
                                            0.11614691466093063,
                                            0.1158432811498642,
                                            0.11844415962696075,
                                            0.12012633681297302,
                                            0.12201090902090073,
                                            0.12300722301006317,
                                            0.12179263681173325,
                                            0.12022022157907486,
                                            0.1161445826292038,
                                            0.11631754785776138,
                                            0.11599871516227722,
                                            0.11595051735639572,
                                            0.11614725738763809,
                                            0.11601608991622925,
                                            0.11616994440555573,
                                            0.11606497317552567,
                                            0.11608567833900452,
                                            0.11612219363451004,
                                            0.11595229059457779,
                                            0.1160753071308136,
                                            0.11604484170675278,
                                            0.11610965430736542,
                                            0.11596120893955231,
                                            0.11606322973966599,
                                            0.1158863827586174,
                                            0.1162295863032341,
                                            0.11610893160104752,
                                            0.11617756634950638,
                                            0.11616548895835876,
                                            0.11614851653575897,
                                            0.11626169085502625,
                                            0.11611983180046082,
                                            0.116350457072258,
                                            0.1160777285695076,
                                            0.11605817079544067,
                                            0.1161554679274559,
                                            0.1161135733127594,
                                            0.11618547886610031,
                                            0.11609655618667603,
                                            0.11594346165657043,
                                            0.11609303951263428,
                                            0.11628188192844391,
                                            0.11593768745660782,
                                            0.116062231361866,
                                            0.11615301668643951,
                                            0.11612395197153091,
                                            0.11609595268964767,
                                            0.11606339365243912,
                                            0.11621040850877762,
                                            0.11611215770244598,
                                            0.11631068587303162,
                                            0.11604316532611847,
                                            0.11610415577888489,
                                            0.11617108434438705,
                                            0.11737527698278427,
                                            0.11636664718389511,
                                            0.11643633991479874,
                                            0.11641830205917358,
                                            0.11623018234968185,
                                            0.11627576500177383,
                                            0.11618880182504654,
                                            0.11590965837240219,
                                            0.11600705981254578,
                                            0.11603537946939468,
                                            0.11614096164703369,
                                            0.11609300225973129,
                                            0.11620457470417023,
                                            0.11623869836330414,
                                            0.11612760275602341,
                                            0.11619135737419128,
                                            0.11633993685245514,
                                            0.11595872789621353,
                                            0.1160760223865509,
                                            0.11613026261329651,
                                            0.11629821360111237,
                                            0.11606990545988083,
                                            0.11618290096521378,
                                            0.11590874940156937,
                                            0.116066575050354,
                                            0.1162467747926712,
                                            0.11605139076709747,
                                            0.11606994271278381,
                                            0.11615034192800522,
                                            0.11626721173524857,
                                            0.1164214015007019,
                                            0.11610067635774612,
                                            0.11607576906681061,
                                            0.11610443145036697,
                                            0.11622398346662521,
                                            0.11609799414873123,
                                            0.11592576652765274,
                                            0.11589066684246063,
                                            0.11615613847970963,
                                            0.11620646715164185,
                                            0.11605507135391235,
                                            0.11605215072631836,
                                            0.11597687005996704,
                                            0.11605720967054367,
                                            0.11622954159975052,
                                            0.11576541513204575,
                                            0.11619725078344345,
                                            0.11608881503343582,
                                            0.11614267528057098,
                                            0.11628157645463943,
                                            0.11605657637119293,
                                            0.11614761501550674,
                                            0.11601652950048447,
                                            0.11608336865901947,
                                            0.1161508783698082,
                                            0.11626416444778442,
                                            0.11630623042583466,
                                            0.1164545938372612,
                                            0.11624109745025635,
                                            0.11607788503170013,
                                            0.1161070168018341,
                                            0.11620175093412399,
                                            0.11616086959838867,
                                            0.11611688882112503,
                                            0.11600390076637268,
                                            0.11609793454408646,
                                            0.11598715931177139,
                                            0.11622306704521179,
                                            0.11612720787525177,
                                            0.11618942022323608,
                                            0.11620881408452988,
                                            0.11601428687572479,
                                            0.11616939306259155,
                                            0.11625801771879196,
                                            0.11612837016582489,
                                            0.11634919047355652,
                                            0.11619079858064651,
                                            0.11613990366458893,
                                            0.11610959470272064,
                                            0.1163247600197792,
                                            0.11617305129766464,
                                            0.11615300923585892,
                                            0.11632908880710602,
                                            0.11621703207492828,
                                            0.11634662002325058,
                                            0.11614250391721725,
                                            0.11630403250455856,
                                            0.11608672142028809,
                                            0.11608536541461945,
                                            31432.908203125,
                                            0.1162625327706337,
                                            0.11617450416088104,
                                            0.11622246354818344,
                                            0.11625657230615616,
                                            0.11630494892597198,
                                            0.1160629466176033,
                                            0.11630386859178543,
                                            0.11654241383075714,
                                            0.11633019894361496,
                                            0.11625935137271881,
                                            0.1159849464893341,
                                            0.11611940711736679,
                                            0.11628160625696182,
                                            0.11616548895835876,
                                            0.1158863976597786,
                                            0.11616910994052887,
                                            0.11640705168247223,
                                            57.9826545715332,
                                            0.11621317267417908,
                                            0.1162533313035965,
                                            0.11624367535114288,
                                            0.11624933034181595,
                                            0.11617560684680939,
                                            0.11601812392473221,
                                            0.11616231501102448,
                                            0.1162579134106636,
                                            0.1163044273853302,
                                            0.11610657721757889,
                                            0.11615923047065735,
                                            0.11604088544845581,
                                            0.11613348871469498,
                                            0.1160733699798584,
                                            0.11620483547449112,
                                            0.11624867469072342,
                                            0.11615971475839615,
                                            0.11607415229082108,
                                            75018216.0,
                                            0.09041483700275421,
                                            0.09068240970373154,
                                            0.09158043563365936,
                                            0.09161237627267838,
                                            0.09168776124715805,
                                            0.09167324006557465,
                                            0.09174589067697525,
                                            0.09159623086452484,
                                            0.091584213078022,
                                            0.09181859344244003,
                                            0.09148017317056656,
                                            0.09162292629480362,
                                            0.09164972603321075,
                                            0.09172000735998154,
                                            0.09156201779842377,
                                            0.09159772098064423,
                                            0.09161564707756042,
                                            0.09180383384227753,
                                            0.09165830910205841,
                                            0.09175550937652588,
                                            0.09164596349000931,
                                            0.09165975451469421,
                                            0.09162546694278717,
                                            0.09173918515443802,
                                            0.09151586145162582,
                                            0.09154339134693146,
                                            0.09142778068780899,
                                            0.09155946969985962,
                                            0.0915718749165535,
                                            0.09148793667554855,
                                            0.09153666347265244,
                                            0.0915401503443718,
                                            0.0918232873082161,
                                            0.09156225621700287,
                                            0.09150882065296173,
                                            0.09154771268367767,
                                            0.09136240184307098,
                                            0.09136436134576797,
                                            0.09138547629117966,
                                            0.09111592918634415,
                                            0.09049556404352188,
                                            0.09093768894672394,
                                            0.09028400480747223,
                                            0.0910341814160347,
                                            0.09143580496311188,
                                            0.09173454344272614,
                                            0.09146896004676819,
                                            0.09174791723489761,
                                            0.09165650606155396,
                                            0.09164563566446304,
                                            0.09150607138872147,
                                            0.09162599593400955,
                                            0.09160754829645157,
                                            0.0916544646024704,
                                            0.09154925495386124,
                                            0.09158218652009964,
                                            0.09168332815170288,
                                            0.09178471565246582,
                                            0.09159593284130096,
                                            0.09167377650737762,
                                            0.09158945083618164,
                                            0.09157179296016693,
                                            0.09173416346311569,
                                            0.09171082079410553,
                                            0.0915522649884224,
                                            0.09138037264347076,
                                            6550871.0,
                                            0.0917968600988388,
                                            0.09176656603813171,
                                            0.09189419448375702,
                                            0.09189261496067047,
                                            0.09148921072483063,
                                            0.0916379913687706,
                                            0.09164125472307205,
                                            0.09168343245983124,
                                            0.0915914997458458,
                                            0.0916556641459465,
                                            0.09176559001207352,
                                            0.09119798988103867,
                                            0.09149183332920074,
                                            0.09167253226041794,
                                            0.09157314896583557,
                                            0.09136173874139786,
                                            0.09172654151916504,
                                            0.09172135591506958,
                                            0.0916282907128334,
                                            0.09151092171669006,
                                            0.09152401983737946,
                                            0.09146179258823395,
                                            0.09152831137180328,
                                            0.09171596169471741,
                                            0.09146741032600403,
                                            0.09163735806941986,
                                            0.09172643721103668,
                                            0.09151304513216019,
                                            0.09167574346065521,
                                            0.09155893325805664,
                                            0.09151120483875275,
                                            0.09154047071933746,
                                            0.09145689010620117,
                                            0.09163273870944977,
                                            0.09142628312110901,
                                            0.09159869700670242,
                                            0.09154431521892548,
                                            0.09110464155673981,
                                            0.09078102558851242,
                                            0.0907980427145958,
                                            0.09076491743326187,
                                            0.09136105328798294,
                                            0.09158964455127716,
                                            0.09161243587732315,
                                            0.09146032482385635,
                                            0.09154172241687775,
                                            0.0915302038192749,
                                            0.09143901616334915,
                                            0.09158524125814438,
                                            0.09163603186607361,
                                            0.09191957861185074,
                                            0.09158330410718918,
                                            0.0916702076792717,
                                            0.09173353016376495,
                                            0.09168414771556854,
                                            0.09165389090776443,
                                            0.09153973311185837,
                                            0.09151528030633926,
                                            0.09149136394262314,
                                            0.09165231138467789,
                                            0.09150249511003494,
                                            0.09171602129936218,
                                            0.09170891344547272,
                                            0.09149736166000366,
                                            0.09154205024242401,
                                            0.09144985675811768,
                                            0.09173857420682907,
                                            0.09168737381696701,
                                            0.09159638732671738,
                                            0.09164682775735855,
                                            0.09182272851467133,
                                            0.09188602864742279,
                                            0.0916704535484314,
                                            0.09181439131498337,
                                            0.0916689857840538,
                                            0.0916949138045311,
                                            0.0918634682893753,
                                            0.091732919216156,
                                            0.09177081286907196,
                                            0.09180489182472229,
                                            0.09170784801244736,
                                            0.09166619181632996,
                                            0.09147465229034424,
                                            0.09065674990415573,
                                            0.09165985137224197,
                                            0.0915789008140564,
                                            0.09145854413509369,
                                            0.0916421115398407,
                                            0.09175586700439453,
                                            0.0916069895029068,
                                            0.091783806681633,
                                            0.09182631224393845,
                                            0.0917629525065422,
                                            0.09174057096242905,
                                            0.09179218113422394,
                                            0.09166530519723892,
                                            0.09162689000368118,
                                            0.09167832881212234,
                                            0.09155615419149399,
                                            0.09177278727293015,
                                            0.09168699383735657,
                                            0.09166309982538223,
                                            0.09175059199333191,
                                            0.09156449139118195,
                                            0.09154761582612991,
                                            0.09163499623537064,
                                            0.09183468669652939,
                                            0.09155910462141037,
                                            0.09153896570205688,
                                            0.09173861145973206,
                                            0.09150022268295288,
                                            0.09156262874603271,
                                            0.09158863872289658,
                                            0.091577909886837,
                                            0.0914931669831276,
                                            0.09152447432279587,
                                            0.09160035848617554,
                                            0.0915502980351448,
                                            0.09158534556627274,
                                            0.09145323932170868,
                                            0.09149032831192017,
                                            0.09140226989984512,
                                            0.09157698601484299,
                                            0.09184463322162628,
                                            0.09163165837526321,
                                            0.09175490587949753,
                                            0.09173326194286346,
                                            0.09146177023649216,
                                            0.09170445799827576,
                                            0.09182441979646683,
                                            0.09142305701971054,
                                            0.09153810143470764,
                                            0.09144371002912521,
                                            0.09164082258939743,
                                            0.09147138893604279,
                                            0.09169746935367584,
                                            0.09146449714899063,
                                            0.0915309488773346,
                                            0.09153302758932114,
                                            0.09078874439001083,
                                            0.09040026366710663,
                                            0.0903313085436821,
                                            0.09044385701417923,
                                            0.09064793586730957,
                                            0.09086998552083969,
                                            0.0905841663479805,
                                            0.09128729999065399,
                                            0.09156367182731628,
                                            0.09157119691371918,
                                            0.09156181663274765,
                                            0.091734878718853,
                                            0.09174881130456924,
                                            0.09162548929452896,
                                            0.09173157066106796,
                                            0.09165001660585403,
                                            0.09184536337852478,
                                            0.09174542874097824,
                                            0.09148374944925308,
                                            0.0918145477771759,
                                            0.09172198921442032,
                                            0.09163372218608856,
                                            0.09175238013267517,
                                            0.09166068583726883,
                                            0.09152699261903763,
                                            0.09570552408695221,
                                            0.09075891226530075,
                                            0.09028211236000061,
                                            0.09057898819446564,
                                            0.09034465253353119,
                                            0.091373510658741,
                                            0.09163247048854828,
                                            0.0916837826371193,
                                            623378176.0,
                                            0.09151039272546768,
                                            0.09154417365789413,
                                            0.09182267636060715,
                                            0.0917121171951294,
                                            0.09159564971923828,
                                            0.09143423289060593,
                                            0.09164773672819138,
                                            0.09148409217596054,
                                            0.0917004719376564,
                                            0.0917796865105629,
                                            0.0917053371667862,
                                            0.09163802117109299,
                                            597140.375,
                                            0.0918307676911354,
                                            0.09166297316551208,
                                            0.09173108637332916,
                                            0.09177133440971375,
                                            0.09167598187923431,
                                            0.09167742729187012,
                                            0.09176761656999588,
                                            0.09161846339702606,
                                            0.09151538461446762,
                                            0.09162520617246628,
                                            0.09155528992414474,
                                            0.09133844822645187,
                                            0.09172411262989044,
                                            0.09168177098035812,
                                            0.09152256697416306,
                                            0.09153567254543304,
                                            0.09174582362174988,
                                            0.09146131575107574,
                                            0.09169001132249832,
                                            0.09183165431022644,
                                            0.09155264496803284,
                                            0.09179975837469101,
                                            0.09141802042722702,
                                            0.09178242087364197,
                                            0.09155882894992828,
                                            0.09155619144439697,
                                            0.09172812104225159,
                                            0.09160418808460236,
                                            0.09162043035030365,
                                            0.0916271060705185,
                                            0.0916542038321495,
                                            0.09182074666023254,
                                            0.091661237180233,
                                            0.09168948233127594,
                                            0.09161534905433655,
                                            0.09176906198263168,
                                            0.09174256771802902,
                                            0.09184432774782181,
                                            0.09175481647253036,
                                            0.09163566678762436,
                                            0.09110815823078156,
                                            0.09047967940568924,
                                            0.09134313464164734,
                                            0.09058742970228195,
                                            0.09057310223579407,
                                            0.09021198004484177,
                                            0.08950533717870712,
                                            0.08991483598947525,
                                            0.09000581502914429,
                                            0.08975820243358612,
                                            0.08989927172660828,
                                            0.08977536857128143,
                                            0.09046599268913269,
                                            0.09019088745117188,
                                            0.08985491842031479,
                                            0.09006495773792267,
                                            0.09006132930517197,
                                            0.08998249471187592,
                                            0.08991481363773346,
                                            0.09035725891590118,
                                            0.09026259183883667,
                                            0.09033244848251343,
                                            0.09021812677383423,
                                            0.08977342396974564,
                                            0.08995147794485092,
                                            0.09018651396036148,
                                            0.0902789831161499,
                                            0.09030649065971375,
                                            0.09018151462078094,
                                            0.08963830769062042,
                                            0.09024237841367722,
                                            0.08991577476263046,
                                            0.09044059365987778,
                                            0.08928724378347397,
                                            0.08972622454166412,
                                            0.08882772922515869,
                                            0.08975745737552643,
                                            0.09016703814268112,
                                            0.08963004499673843,
                                            0.0898420587182045,
                                            0.08989901095628738,
                                            0.09010357409715652,
                                            0.0901118665933609,
                                            0.09002404659986496,
                                            0.09008162468671799,
                                            0.08998696506023407,
                                            0.09019099175930023,
                                            0.09039027988910675,
                                            0.09014302492141724,
                                            0.09035284072160721,
                                            0.090223528444767,
                                            0.08909245580434799,
                                            0.08939402550458908,
                                            0.08942470699548721,
                                            0.08992207795381546,
                                            0.08974660187959671,
                                            0.09004580974578857,
                                            0.09003043174743652,
                                            0.08954887092113495,
                                            0.08958173543214798,
                                            0.0888582319021225,
                                            0.08949612826108932,
                                            0.0890330970287323,
                                            0.08879958093166351,
                                            0.08949175477027893,
                                            0.09019462019205093,
                                            0.08992094546556473,
                                            0.08964835852384567,
                                            0.08991514146327972,
                                            0.08982381969690323,
                                            0.08980073034763336,
                                            0.0896264985203743,
                                            0.08992141485214233,
                                            0.09008410573005676,
                                            0.08989385515451431,
                                            0.09021643549203873,
                                            0.09013224393129349,
                                            0.0900585800409317,
                                            0.08978967368602753,
                                            0.09012910723686218,
                                            0.08994796127080917,
                                            0.09000711888074875,
                                            0.08993218839168549,
                                            0.09006299823522568,
                                            0.08917942643165588,
                                            0.08967818319797516,
                                            0.08993396908044815,
                                            0.09023486822843552,
                                            0.0900617465376854,
                                            0.09012560546398163,
                                            0.08974074572324753,
                                            0.09009005129337311,
                                            0.08979517966508865,
                                            0.08997286111116409,
                                            0.09035619348287582,
                                            0.09010802954435349,
                                            0.09086864441633224,
                                            0.0897199809551239,
                                            0.08955328911542892,
                                            0.08973503112792969,
                                            0.0901649221777916,
                                            0.08986207097768784,
                                            0.09005796909332275,
                                            0.08977393060922623,
                                            0.09000321477651596,
                                            0.08998708426952362,
                                            0.08971834927797318,
                                            0.0897836983203888,
                                            0.09028778970241547,
                                            0.08969186991453171,
                                            0.09012392163276672,
                                            0.08985482901334763,
                                            0.09004218131303787,
                                            0.09037664532661438,
                                            0.09028197079896927,
                                            0.09011863172054291,
                                            0.0896797850728035,
                                            0.0896434634923935,
                                            0.0899115651845932,
                                            0.08928757160902023,
                                            0.08990449458360672,
                                            0.08990985155105591,
                                            0.09001431614160538,
                                            0.0895829126238823,
                                            0.09010770171880722,
                                            0.08968362212181091,
                                            0.09000500291585922,
                                            0.0898640975356102,
                                            0.08954513818025589,
                                            123260808.0,
                                            0.09009537845849991,
                                            0.08999543637037277,
                                            0.09009146690368652,
                                            0.08993417024612427,
                                            0.08979174494743347,
                                            0.09024576097726822,
                                            0.08982154726982117,
                                            0.09024430066347122,
                                            0.08962235599756241,
                                            0.08915040642023087,
                                            0.08879758417606354,
                                            0.08888117223978043,
                                            0.08869042247533798,
                                            0.08853305876255035,
                                            0.08868934214115143,
                                            0.08893421292304993,
                                            0.0886133685708046,
                                            0.08817695081233978,
                                            0.0881962701678276,
                                            0.08811213821172714,
                                            0.0891452506184578,
                                            0.08935011923313141,
                                            0.08934961259365082,
                                            0.08890455216169357,
                                            0.08886080235242844,
                                            0.0891081690788269,
                                            0.08976045250892639,
                                            0.08954772353172302,
                                            0.08939021825790405,
                                            0.08975906670093536,
                                            0.08977613598108292,
                                            0.0896851047873497,
                                            0.08977438509464264,
                                            0.09006603807210922,
                                            0.08980820327997208,
                                            0.08988718688488007,
                                            0.08971526473760605,
                                            0.09004899114370346,
                                            0.0901557132601738,
                                            0.08969920128583908,
                                            0.09032884240150452,
                                            0.08998294174671173,
                                            0.08977649360895157,
                                            0.09020925313234329,
                                            0.09029025584459305,
                                            0.08976597338914871,
                                            0.08999000489711761,
                                            0.08985752612352371,
                                            0.09009544551372528,
                                            0.08979445695877075,
                                            0.08981890231370926,
                                            0.08993826806545258,
                                            0.09029476344585419,
                                            0.0901898518204689,
                                            0.0904805064201355,
                                            0.0902039036154747,
                                            0.09018836915493011,
                                            0.09021788835525513,
                                            13.298559188842773,
                                            0.09035386145114899,
                                            0.08994783461093903,
                                            0.08987398445606232,
                                            0.0898030549287796,
                                            0.09018076211214066,
                                            0.08989060670137405,
                                            0.0900542363524437,
                                            0.08998247236013412,
                                            0.08996569365262985,
                                            0.08956349641084671,
                                            0.0899026170372963,
                                            0.08934273570775986,
                                            0.08953327685594559,
                                            0.08896757662296295,
                                            40.889766693115234,
                                            0.0892421156167984,
                                            0.09008168429136276,
                                            0.0901041328907013,
                                            0.08966265618801117,
                                            0.08987640589475632,
                                            0.09025770425796509,
                                            0.0899430364370346,
                                            0.09005750715732574,
                                            0.09002253413200378,
                                            0.08991013467311859,
                                            0.08955400437116623,
                                            0.09024599194526672,
                                            0.08989398926496506,
                                            0.09036830812692642,
                                            0.09003657847642899,
                                            0.09000864624977112,
                                            0.09005343914031982,
                                            0.09015601873397827,
                                            0.09009037166833878,
                                            0.09033283591270447,
                                            0.08988620340824127,
                                            0.09028229117393494,
                                            0.09019381552934647,
                                            0.0902322456240654,
                                            0.09032434225082397,
                                            0.09011644124984741,
                                            0.08976441621780396,
                                            0.08991662412881851,
                                            0.08978656679391861,
                                            0.08997219800949097,
                                            0.09036137908697128,
                                            0.08971432596445084,
                                            0.08993349224328995,
                                            0.08978254348039627,
                                            0.09011665731668472,
                                            0.0903627872467041,
                                            0.09019304066896439,
                                            0.08973104506731033,
                                            0.09010143578052521,
                                            0.09001858532428741,
                                            0.08998378366231918,
                                            0.09022297710180283,
                                            0.09093927592039108,
                                            0.08909867703914642,
                                            0.08914345502853394,
                                            0.08835233747959137,
                                            0.0882631167769432,
                                            0.08950014412403107,
                                            0.08979093283414841,
                                            0.09012936055660248,
                                            0.08974586427211761,
                                            0.09011997282505035,
                                            0.08974523842334747,
                                            0.09000830352306366,
                                            0.09017462283372879,
                                            0.09004103392362595,
                                            0.08963128179311752,
                                            0.08985885232686996,
                                            0.08983511477708817,
                                            0.09008574485778809,
                                            0.09016025811433792,
                                            0.08972913771867752,
                                            0.08963147550821304,
                                            0.08982467651367188,
                                            0.08994393795728683,
                                            0.08989205211400986,
                                            0.08967658132314682,
                                            0.08987260609865189,
                                            0.09007982909679413,
                                            0.09010133147239685,
                                            0.09020086377859116,
                                            0.09046005457639694,
                                            0.08994335681200027,
                                            0.09018530696630478,
                                            0.08992722630500793,
                                            0.09044454246759415,
                                            0.09027785807847977,
                                            0.08971437811851501,
                                            0.08930660039186478,
                                            0.09013891965150833,
                                            0.08999187499284744,
                                            0.08977315574884415,
                                            0.08965734392404556,
                                            0.08979973196983337,
                                            0.09008533507585526,
                                            0.08993113040924072,
                                            0.08979012817144394,
                                            0.08988290280103683,
                                            0.09004655480384827,
                                            0.09017106890678406,
                                            0.089727021753788,
                                            0.08970198780298233,
                                            0.09002815186977386,
                                            0.08984025567770004,
                                            0.09001448005437851,
                                            0.08957063406705856,
                                            0.08992192894220352,
                                            0.09023972600698471,
                                            0.09008713066577911,
                                            0.0900142714381218,
                                            0.08985602855682373,
                                            0.0900145098567009],
                       'weight_decay': 0.0005}],
 'models': {}}
2019-09-14 07:07:26,943 - training_jobs - DEBUG - test_multiple_models
2019-09-14 07:07:26,943 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-14 07:07:29,472 - training_jobs - DEBUG - training time: 11975s
2019-09-14 07:07:29,472 - training_jobs - DEBUG - saving to results/20190914_034754_ggnn_.json
2019-09-14 07:07:29,474 - training_jobs - DEBUG - moved jobdict to done_trainings/task_2.yml
2019-09-14 07:07:29,474 - training_jobs - DEBUG - Finished!

2019-09-14 07:07:31,924 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 30,
 'd2': 50,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-14 07:07:31,936 - training_jobs - DEBUG - training with: 
2019-09-14 07:07:31,936 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max/
2019-09-14 07:07:31,936 - training_jobs - DEBUG - GGNN1
2019-09-14 07:07:31,936 - training_jobs - DEBUG - 
2019-09-14 07:07:31,936 - training_jobs - DEBUG - ggnn training
2019-09-14 07:07:37,150 - training_jobs - DEBUG -  saving results to results/20190914_070737_ggnn_.json
2019-09-14 07:07:37,150 - training_jobs - DEBUG -  calling modelSelection
{'best_models': {'accuracy': {'accuracy': 2.0,
                              'batch_size': 32,
                              'cv_score': 0.018014136691882596,
                              'cv_val_accuracy': 1.222222222222222,
                              'cv_val_loss': 0.09877465665340424,
                              'cv_val_macroF1': 0.018014136691882596,
                              'cv_val_microF1': 0.0756563342354067,
                              'epochs': 100,
                              'final_model': GGNN1(
  (ggnn): GatedGraphConv(30, num_layers=2)
  (fc1): Linear(in_features=30, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                              'kwargs': {'aggr_type': 'mean',
                                         'd1': 30,
                                         'd2': 50,
                                         'num_classes': 24,
                                         'num_layers': 2},
                              'learning_rate': 0.01,
                              'macroF1': 0.006748704489763125,
                              'microF1': 0.08414023372287145,
                              'model': <class 'TFM_graph_classification_models.GGNN1'>,
                              'model_instance': GGNN1(
  (ggnn): GatedGraphConv(30, num_layers=2)
  (fc1): Linear(in_features=30, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                              'score': 'f1_macro',
                              'time': 1290.9390695095062,
                              'train_loss_history': [7861777920.0,
                                                     18351.4453125,
                                                     0.08609600365161896,
                                                     0.08494096249341965,
                                                     0.0846065953373909,
                                                     0.08471822738647461,
                                                     0.08469000458717346,
                                                     25.97217559814453,
                                                     227637.984375,
                                                     0.08466333150863647,
                                                     24937190.0,
                                                     0.08460436761379242,
                                                     0.08463048934936523,
                                                     0.08467409014701843,
                                                     0.0845961719751358,
                                                     0.08458821475505829,
                                                     0.0845770314335823,
                                                     0.08458980917930603,
                                                     0.08456533402204514,
                                                     0.08460189402103424,
                                                     1.5376646518707275,
                                                     0.08462714403867722,
                                                     0.08455481380224228,
                                                     0.08421196788549423,
                                                     0.08399565517902374,
                                                     0.08464045822620392,
                                                     0.08439569920301437,
                                                     0.08447393029928207,
                                                     0.08436314016580582,
                                                     0.084059938788414,
                                                     524407552.0,
                                                     1098160512.0,
                                                     0.08424728363752365,
                                                     0.08390361070632935,
                                                     0.08380310237407684,
                                                     0.08367624133825302,
                                                     0.08372815698385239,
                                                     0.08376618474721909,
                                                     0.08380178362131119,
                                                     0.08376359194517136,
                                                     0.08374714106321335,
                                                     0.08380936831235886,
                                                     0.08483122289180756,
                                                     0.08451099693775177,
                                                     0.08451665192842484,
                                                     0.08454050868749619,
                                                     0.08456087112426758,
                                                     0.08462248742580414,
                                                     0.08458506315946579,
                                                     0.08451282978057861,
                                                     0.08458219468593597,
                                                     0.08459468930959702,
                                                     0.08456937968730927,
                                                     0.08449748158454895,
                                                     0.08456726372241974,
                                                     0.0845608189702034,
                                                     0.08448818325996399,
                                                     0.08462183177471161,
                                                     0.08448898792266846,
                                                     0.08449804782867432,
                                                     0.08443325757980347,
                                                     0.08453451097011566,
                                                     0.08442701399326324,
                                                     0.08442994952201843,
                                                     0.0843852087855339,
                                                     0.08436418324708939,
                                                     0.0844472274184227,
                                                     0.08449103683233261,
                                                     0.0844266414642334,
                                                     0.08442007750272751,
                                                     0.0844486653804779,
                                                     0.08435752987861633,
                                                     7568953.0,
                                                     0.08458959311246872,
                                                     0.08451079577207565,
                                                     0.08436400443315506,
                                                     0.08443053811788559,
                                                     0.08448266983032227,
                                                     0.08450271934270859,
                                                     0.08451101183891296,
                                                     0.08450804650783539,
                                                     0.08455737680196762,
                                                     0.08442199975252151,
                                                     0.084612637758255,
                                                     0.08451008796691895,
                                                     0.08456732332706451,
                                                     0.08454825729131699,
                                                     0.08444435149431229,
                                                     0.08454672247171402,
                                                     0.08420255780220032,
                                                     0.08453114330768585,
                                                     431.90203857421875,
                                                     0.08457901328802109,
                                                     0.08445992320775986,
                                                     0.08451400697231293,
                                                     0.08450395613908768,
                                                     0.08450470864772797,
                                                     0.08445815742015839,
                                                     0.0844666138291359,
                                                     329.9246826171875,
                                                     0.09569226950407028,
                                                     0.0945582166314125,
                                                     0.0946061983704567,
                                                     0.09453007578849792,
                                                     0.09456789493560791,
                                                     0.09460559487342834,
                                                     0.09457743912935257,
                                                     0.09454146772623062,
                                                     0.09452404081821442,
                                                     0.09458976238965988,
                                                     0.09458254277706146,
                                                     0.0945456475019455,
                                                     0.0945700854063034,
                                                     0.09454362094402313,
                                                     0.09454405307769775,
                                                     0.09458659589290619,
                                                     0.09448900073766708,
                                                     910.1299438476562,
                                                     0.0945618599653244,
                                                     0.09456372261047363,
                                                     0.09455177932977676,
                                                     0.09446676820516586,
                                                     0.09459219127893448,
                                                     3367.74267578125,
                                                     0.09448350965976715,
                                                     0.09451571851968765,
                                                     0.11896193027496338,
                                                     0.09398454427719116,
                                                     0.09457246959209442,
                                                     0.09453534334897995,
                                                     0.0946156308054924,
                                                     25620030.0,
                                                     0.09460297226905823,
                                                     0.09457835555076599,
                                                     0.09445815533399582,
                                                     0.1450498402118683,
                                                     0.13754796981811523,
                                                     0.09410639852285385,
                                                     0.09454409033060074,
                                                     0.09451742470264435,
                                                     0.09456036239862442,
                                                     0.09448345750570297,
                                                     0.09454705566167831,
                                                     0.09449545294046402,
                                                     146.2767333984375,
                                                     0.09407175332307816,
                                                     0.12538067996501923,
                                                     0.5565594434738159,
                                                     0.09349039196968079,
                                                     0.09350410848855972,
                                                     0.09352722764015198,
                                                     0.09368980675935745,
                                                     0.09434451162815094,
                                                     1.482743740081787,
                                                     0.09454360604286194,
                                                     338.6393127441406,
                                                     0.09459488093852997,
                                                     0.12255053222179413,
                                                     0.3753548860549927,
                                                     58.339622497558594,
                                                     0.09449402987957001,
                                                     0.09414922446012497,
                                                     0.09410757571458817,
                                                     0.09444070607423782,
                                                     0.09442590922117233,
                                                     1.3948286771774292,
                                                     0.0944676473736763,
                                                     0.094512939453125,
                                                     0.09456082433462143,
                                                     84.4478530883789,
                                                     0.09455475211143494,
                                                     0.0944359302520752,
                                                     0.09461703151464462,
                                                     0.09449130296707153,
                                                     0.09451261162757874,
                                                     0.09459147602319717,
                                                     0.09457153081893921,
                                                     0.09456238895654678,
                                                     0.09457340091466904,
                                                     0.09412994980812073,
                                                     0.09386325627565384,
                                                     0.09368231892585754,
                                                     0.09364684671163559,
                                                     0.09357370436191559,
                                                     1.222943663597107,
                                                     0.0935576930642128,
                                                     1.235748291015625,
                                                     0.09369612485170364,
                                                     0.09357502311468124,
                                                     0.09353245794773102,
                                                     0.09358879178762436,
                                                     0.09361980855464935,
                                                     3.59340500831604,
                                                     0.09360233694314957,
                                                     0.09356341511011124,
                                                     0.09354453533887863,
                                                     0.09342794865369797,
                                                     0.09393218904733658,
                                                     0.09447789937257767,
                                                     0.09379788488149643,
                                                     0.09371217340230942,
                                                     0.09421192109584808,
                                                     731.2669067382812,
                                                     0.09453772753477097,
                                                     1365163.25,
                                                     140.6337127685547,
                                                     623.1651611328125,
                                                     70.59658813476562,
                                                     4.27203369140625,
                                                     0.0945260226726532,
                                                     0.0945124626159668,
                                                     0.09454381465911865,
                                                     0.0944497138261795,
                                                     0.09452548623085022,
                                                     158.22738647460938,
                                                     278.7743835449219,
                                                     0.09446462988853455,
                                                     115.81387329101562,
                                                     0.09450964629650116,
                                                     0.0944800153374672,
                                                     0.09448287636041641,
                                                     87.09356689453125,
                                                     184671.34375,
                                                     0.09459525346755981,
                                                     0.09467873722314835,
                                                     0.0945337787270546,
                                                     16.996519088745117,
                                                     0.09461172670125961,
                                                     0.09443408995866776,
                                                     51.9001579284668,
                                                     0.09444574266672134,
                                                     0.38718417286872864,
                                                     25.579999923706055,
                                                     0.1037483811378479,
                                                     0.09448056668043137,
                                                     1828.41357421875,
                                                     0.09463730454444885,
                                                     0.09466267377138138,
                                                     46765636.0,
                                                     0.0946023091673851,
                                                     0.09462863951921463,
                                                     0.0946633592247963,
                                                     0.09412078559398651,
                                                     0.09366210550069809,
                                                     0.09355118870735168,
                                                     0.09358946979045868,
                                                     0.09402459114789963,
                                                     0.09453138709068298,
                                                     0.3115718960762024,
                                                     64.50550079345703,
                                                     0.09359338879585266,
                                                     0.09346934407949448,
                                                     0.09356776624917984,
                                                     0.09347664564847946,
                                                     1377079.5,
                                                     0.09446480125188828,
                                                     0.09366337954998016,
                                                     0.09351957589387894,
                                                     0.09895399957895279,
                                                     0.09445680677890778,
                                                     0.13375918567180634,
                                                     0.10281097888946533,
                                                     0.09444534033536911,
                                                     0.17989182472229004,
                                                     0.09455157071352005,
                                                     9.182299613952637,
                                                     0.10033991187810898,
                                                     0.09449930489063263,
                                                     0.09454887360334396,
                                                     0.9713547229766846,
                                                     3.086212396621704,
                                                     23652.203125,
                                                     0.0944833904504776,
                                                     0.0944383442401886,
                                                     1139.218994140625,
                                                     2.7565970420837402,
                                                     0.45776844024658203,
                                                     19.503902435302734,
                                                     0.09465982019901276,
                                                     0.0946187898516655,
                                                     0.0946582555770874,
                                                     0.7061750888824463,
                                                     154.2449951171875,
                                                     0.094508595764637,
                                                     0.09462691843509674,
                                                     0.09459841996431351,
                                                     0.09470409899950027,
                                                     0.1413435935974121,
                                                     520.33154296875,
                                                     1.0022259950637817,
                                                     0.09464074671268463,
                                                     0.0946812629699707,
                                                     0.09462562948465347,
                                                     0.09463953971862793,
                                                     44.62700271606445,
                                                     0.09461881220340729,
                                                     0.09999187290668488,
                                                     0.09446563571691513,
                                                     0.09447474777698517,
                                                     581717.375],
                              'val_accuracy_history': [0.42105263157894735,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       2.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       3.0,
                                                       3.0,
                                                       2.0,
                                                       0.0,
                                                       0.21052631578947367,
                                                       0.5789473684210527,
                                                       1.9473684210526316,
                                                       2.8421052631578947,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       0.10526315789473684,
                                                       1.4210526315789473,
                                                       0.15789473684210525,
                                                       1.263157894736842,
                                                       2.3157894736842106,
                                                       1.4210526315789473,
                                                       0.8421052631578947,
                                                       1.0,
                                                       1.6842105263157894,
                                                       0.10526315789473684,
                                                       0.5263157894736842,
                                                       0.8421052631578947,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       4.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       2.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       2.0,
                                                       2.0,
                                                       2.0,
                                                       0.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       1.0,
                                                       2.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.7777777777777778,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       1.4444444444444444,
                                                       1.0,
                                                       0.0,
                                                       0.2222222222222222,
                                                       2.3333333333333335,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       0.1111111111111111,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.8888888888888888,
                                                       1.0,
                                                       1.0,
                                                       2.0,
                                                       2.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       0.2222222222222222,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.1111111111111112,
                                                       0.6666666666666666,
                                                       0.2222222222222222,
                                                       0.3333333333333333,
                                                       0.0,
                                                       1.3333333333333333,
                                                       0.1111111111111111,
                                                       0.1111111111111111,
                                                       1.0,
                                                       0.6666666666666666,
                                                       0.0,
                                                       0.6666666666666666,
                                                       0.0,
                                                       0.0,
                                                       0.6666666666666666,
                                                       2.0,
                                                       1.0,
                                                       0.0,
                                                       0.9473684210526315,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       3.0,
                                                       1.0,
                                                       2.0,
                                                       1.0,
                                                       3.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       1.0,
                                                       0.0,
                                                       3.0,
                                                       2.0,
                                                       0.0,
                                                       4.0,
                                                       1.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       3.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       2.0,
                                                       1.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       1.105263157894737,
                                                       0.8421052631578947,
                                                       1.4736842105263157,
                                                       0.21052631578947367,
                                                       2.0,
                                                       2.4210526315789473,
                                                       1.368421052631579,
                                                       4.684210526315789,
                                                       0.10526315789473684,
                                                       3.526315789473684,
                                                       2.263157894736842,
                                                       2.1578947368421053,
                                                       0.0,
                                                       0.0,
                                                       1.6842105263157894,
                                                       1.8421052631578947,
                                                       2.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       2.0,
                                                       1.0,
                                                       3.0,
                                                       2.0,
                                                       2.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       0.05263157894736842,
                                                       4.0,
                                                       2.0,
                                                       0.0,
                                                       2.0,
                                                       1.0,
                                                       3.0,
                                                       1.0,
                                                       3.0,
                                                       1.0,
                                                       0.8421052631578947,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       2.0,
                                                       1.0,
                                                       3.0,
                                                       4.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       2.0],
                              'val_loss': 0.08984283357858658,
                              'val_loss_history': [0.12510818243026733,
                                                   0.1011715978384018,
                                                   0.10811742395162582,
                                                   0.11114459484815598,
                                                   0.11241888999938965,
                                                   0.11323241889476776,
                                                   0.11366105079650879,
                                                   247040.203125,
                                                   0.1142834946513176,
                                                   0.11444961279630661,
                                                   0.11464021354913712,
                                                   0.11465630680322647,
                                                   0.11490944027900696,
                                                   0.1149965301156044,
                                                   0.11487961560487747,
                                                   0.1148400530219078,
                                                   0.1146181970834732,
                                                   0.11503685265779495,
                                                   0.11507931351661682,
                                                   0.1152242124080658,
                                                   0.11479132622480392,
                                                   0.11491888016462326,
                                                   0.12381615489721298,
                                                   0.12579955160617828,
                                                   0.1253044605255127,
                                                   0.11393609642982483,
                                                   0.1150628849864006,
                                                   0.11508005112409592,
                                                   0.11509200185537338,
                                                   0.1261848658323288,
                                                   438586784.0,
                                                   0.12262706458568573,
                                                   0.12555918097496033,
                                                   0.12526893615722656,
                                                   0.12721216678619385,
                                                   0.12551110982894897,
                                                   0.12551085650920868,
                                                   0.12436513602733612,
                                                   0.12695075571537018,
                                                   0.12748709321022034,
                                                   0.1318335235118866,
                                                   0.12551461160182953,
                                                   0.11542754620313644,
                                                   0.11529360711574554,
                                                   0.11534396559000015,
                                                   0.11538662761449814,
                                                   0.11543147265911102,
                                                   0.11543182283639908,
                                                   0.11539394408464432,
                                                   0.11546839773654938,
                                                   0.11546298116445541,
                                                   0.11537270992994308,
                                                   0.11556874960660934,
                                                   0.11555995047092438,
                                                   0.11553175002336502,
                                                   0.11558782309293747,
                                                   0.11552610993385315,
                                                   0.1155560165643692,
                                                   0.11560266464948654,
                                                   0.11541055887937546,
                                                   0.1156027540564537,
                                                   0.11564809828996658,
                                                   0.11531238257884979,
                                                   0.11525371670722961,
                                                   0.1153678372502327,
                                                   0.11553355306386948,
                                                   0.11561305820941925,
                                                   0.11549890041351318,
                                                   0.11554332077503204,
                                                   0.11560729146003723,
                                                   0.11590125411748886,
                                                   0.11543592810630798,
                                                   0.11562886834144592,
                                                   0.11577589064836502,
                                                   0.11576344817876816,
                                                   0.11571769416332245,
                                                   0.11605023592710495,
                                                   0.11590690910816193,
                                                   0.11583130806684494,
                                                   0.11600607633590698,
                                                   0.11593754589557648,
                                                   0.11599893867969513,
                                                   0.11592314392328262,
                                                   0.11591313779354095,
                                                   0.11575457453727722,
                                                   0.11595233529806137,
                                                   0.11597788333892822,
                                                   0.11555687338113785,
                                                   0.11684586852788925,
                                                   0.11679195612668991,
                                                   0.11599123477935791,
                                                   0.11585559695959091,
                                                   0.11611767113208771,
                                                   0.11601536720991135,
                                                   0.1160048097372055,
                                                   0.1159575954079628,
                                                   0.11597613245248795,
                                                   0.11567838490009308,
                                                   0.11600735038518906,
                                                   0.11599087715148926,
                                                   0.09163205325603485,
                                                   0.0916082113981247,
                                                   0.09166108816862106,
                                                   0.09160305559635162,
                                                   0.09166563302278519,
                                                   0.09168395400047302,
                                                   0.09178026020526886,
                                                   0.09178093075752258,
                                                   0.0914832204580307,
                                                   0.09167445451021194,
                                                   0.09155388176441193,
                                                   0.09147341549396515,
                                                   0.09150148183107376,
                                                   0.09176770597696304,
                                                   0.09153672307729721,
                                                   0.09163783490657806,
                                                   0.09164226800203323,
                                                   0.09167028218507767,
                                                   0.09152018278837204,
                                                   0.0916731059551239,
                                                   0.09161270409822464,
                                                   0.0915922299027443,
                                                   0.09158787131309509,
                                                   0.09145088493824005,
                                                   0.09155438840389252,
                                                   0.091722272336483,
                                                   0.09090804308652878,
                                                   0.09138893336057663,
                                                   0.09157588332891464,
                                                   0.09179755300283432,
                                                   0.09165170043706894,
                                                   0.09155890345573425,
                                                   0.09162383526563644,
                                                   0.09175217151641846,
                                                   0.09162826836109161,
                                                   0.09139235317707062,
                                                   0.0909319669008255,
                                                   0.09162633866071701,
                                                   0.09157107770442963,
                                                   0.09177890419960022,
                                                   0.09146026521921158,
                                                   0.0915394276380539,
                                                   0.09148295223712921,
                                                   0.09171506762504578,
                                                   0.0910874754190445,
                                                   0.09055360406637192,
                                                   0.09075578302145004,
                                                   0.09061741083860397,
                                                   0.09060914814472198,
                                                   0.09105239808559418,
                                                   0.09042172133922577,
                                                   0.09135539829730988,
                                                   0.09104840457439423,
                                                   0.0914481058716774,
                                                   0.09161259233951569,
                                                   0.09155642986297607,
                                                   0.09159298986196518,
                                                   0.09178042411804199,
                                                   0.09153859317302704,
                                                   0.09158207476139069,
                                                   0.09157371520996094,
                                                   0.09075014293193817,
                                                   0.09141993522644043,
                                                   0.09157027304172516,
                                                   0.09134889394044876,
                                                   0.09141180664300919,
                                                   0.0915750116109848,
                                                   0.09154835343360901,
                                                   0.09181329607963562,
                                                   0.0916769728064537,
                                                   0.09171120822429657,
                                                   0.09161952883005142,
                                                   0.0915825217962265,
                                                   0.09158653020858765,
                                                   0.09174817055463791,
                                                   0.09173370897769928,
                                                   0.0918404832482338,
                                                   0.09175587445497513,
                                                   0.09134802222251892,
                                                   0.0908302292227745,
                                                   0.09076055139303207,
                                                   0.09075257182121277,
                                                   0.09046690165996552,
                                                   0.09077004343271255,
                                                   0.09074489027261734,
                                                   0.09072548896074295,
                                                   0.09067178517580032,
                                                   0.09074927121400833,
                                                   0.0906749963760376,
                                                   0.09057972580194473,
                                                   0.09050306677818298,
                                                   0.09064897894859314,
                                                   0.09032656997442245,
                                                   0.09060793370008469,
                                                   0.09054771065711975,
                                                   0.09069087356328964,
                                                   0.09064767509698868,
                                                   0.09145612269639969,
                                                   0.09154322743415833,
                                                   0.09049025923013687,
                                                   0.09029731899499893,
                                                   0.08968358486890793,
                                                   0.0898694321513176,
                                                   0.0896778553724289,
                                                   0.09004233777523041,
                                                   0.08995788544416428,
                                                   0.0899239107966423,
                                                   0.09008295834064484,
                                                   0.0896284356713295,
                                                   0.09004012495279312,
                                                   0.08981496840715408,
                                                   0.0895993635058403,
                                                   0.08966679126024246,
                                                   0.0899127647280693,
                                                   0.09011556953191757,
                                                   0.09024146944284439,
                                                   0.0898381695151329,
                                                   0.089955635368824,
                                                   0.08993718773126602,
                                                   0.08979099243879318,
                                                   0.0898311585187912,
                                                   0.08939468115568161,
                                                   0.09009392559528351,
                                                   0.08986905962228775,
                                                   0.08942726254463196,
                                                   0.08990700542926788,
                                                   0.0899709165096283,
                                                   0.08996371179819107,
                                                   0.0898134708404541,
                                                   0.0896100327372551,
                                                   0.08974415808916092,
                                                   0.08998530358076096,
                                                   0.0896565169095993,
                                                   0.08987461775541306,
                                                   0.0899551510810852,
                                                   0.09007469564676285,
                                                   0.09004899114370346,
                                                   0.0902426615357399,
                                                   0.09040749818086624,
                                                   0.0900820642709732,
                                                   0.0901951715350151,
                                                   0.09042607247829437,
                                                   0.08935614675283432,
                                                   0.08926407992839813,
                                                   0.0891614705324173,
                                                   0.08920969814062119,
                                                   0.08954508602619171,
                                                   0.08957324177026749,
                                                   0.08986911177635193,
                                                   0.08955825120210648,
                                                   0.09029587358236313,
                                                   0.08841820806264877,
                                                   0.08927769213914871,
                                                   0.08931326866149902,
                                                   0.08981242775917053,
                                                   0.08983969688415527,
                                                   0.08899163454771042,
                                                   0.08941895514726639,
                                                   0.08937462419271469,
                                                   0.08952030539512634,
                                                   0.08959675580263138,
                                                   0.08969853818416595,
                                                   0.08962896466255188,
                                                   0.08994295448064804,
                                                   0.08981810510158539,
                                                   0.08979881554841995,
                                                   0.09003960341215134,
                                                   0.08963511139154434,
                                                   0.08976289629936218,
                                                   0.09012604504823685,
                                                   0.0898328498005867,
                                                   0.08971914649009705,
                                                   0.08973442018032074,
                                                   0.09022703766822815,
                                                   0.08993221819400787,
                                                   0.09022209048271179,
                                                   0.0903264731168747,
                                                   0.09032102674245834,
                                                   0.09001591056585312,
                                                   0.09004697948694229,
                                                   0.0899716317653656,
                                                   0.09008210152387619,
                                                   0.08950003981590271,
                                                   0.09010134637355804,
                                                   0.08986760675907135,
                                                   0.08989552408456802,
                                                   0.08977223187685013,
                                                   0.09012626856565475,
                                                   0.09033074975013733,
                                                   0.08990726619958878,
                                                   0.09017037600278854,
                                                   0.08981708437204361,
                                                   0.08998289704322815,
                                                   0.08985056728124619,
                                                   0.09033898264169693,
                                                   0.08993609249591827,
                                                   0.08973539620637894,
                                                   0.08958485722541809,
                                                   0.09011000394821167,
                                                   0.08984283357858658],
                              'weight_decay': 0.0005},
                 'loss': {'accuracy': 2.0,
                          'batch_size': 32,
                          'cv_score': 0.018014136691882596,
                          'cv_val_accuracy': 1.222222222222222,
                          'cv_val_loss': 0.09877465665340424,
                          'cv_val_macroF1': 0.018014136691882596,
                          'cv_val_microF1': 0.0756563342354067,
                          'epochs': 100,
                          'final_model': GGNN1(
  (ggnn): GatedGraphConv(30, num_layers=2)
  (fc1): Linear(in_features=30, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                          'kwargs': {'aggr_type': 'mean',
                                     'd1': 30,
                                     'd2': 50,
                                     'num_classes': 24,
                                     'num_layers': 2},
                          'learning_rate': 0.01,
                          'macroF1': 0.006748704489763125,
                          'microF1': 0.08414023372287145,
                          'model': <class 'TFM_graph_classification_models.GGNN1'>,
                          'model_instance': GGNN1(
  (ggnn): GatedGraphConv(30, num_layers=2)
  (fc1): Linear(in_features=30, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                          'score': 'f1_macro',
                          'time': 1290.9390695095062,
                          'train_loss_history': [7861777920.0,
                                                 18351.4453125,
                                                 0.08609600365161896,
                                                 0.08494096249341965,
                                                 0.0846065953373909,
                                                 0.08471822738647461,
                                                 0.08469000458717346,
                                                 25.97217559814453,
                                                 227637.984375,
                                                 0.08466333150863647,
                                                 24937190.0,
                                                 0.08460436761379242,
                                                 0.08463048934936523,
                                                 0.08467409014701843,
                                                 0.0845961719751358,
                                                 0.08458821475505829,
                                                 0.0845770314335823,
                                                 0.08458980917930603,
                                                 0.08456533402204514,
                                                 0.08460189402103424,
                                                 1.5376646518707275,
                                                 0.08462714403867722,
                                                 0.08455481380224228,
                                                 0.08421196788549423,
                                                 0.08399565517902374,
                                                 0.08464045822620392,
                                                 0.08439569920301437,
                                                 0.08447393029928207,
                                                 0.08436314016580582,
                                                 0.084059938788414,
                                                 524407552.0,
                                                 1098160512.0,
                                                 0.08424728363752365,
                                                 0.08390361070632935,
                                                 0.08380310237407684,
                                                 0.08367624133825302,
                                                 0.08372815698385239,
                                                 0.08376618474721909,
                                                 0.08380178362131119,
                                                 0.08376359194517136,
                                                 0.08374714106321335,
                                                 0.08380936831235886,
                                                 0.08483122289180756,
                                                 0.08451099693775177,
                                                 0.08451665192842484,
                                                 0.08454050868749619,
                                                 0.08456087112426758,
                                                 0.08462248742580414,
                                                 0.08458506315946579,
                                                 0.08451282978057861,
                                                 0.08458219468593597,
                                                 0.08459468930959702,
                                                 0.08456937968730927,
                                                 0.08449748158454895,
                                                 0.08456726372241974,
                                                 0.0845608189702034,
                                                 0.08448818325996399,
                                                 0.08462183177471161,
                                                 0.08448898792266846,
                                                 0.08449804782867432,
                                                 0.08443325757980347,
                                                 0.08453451097011566,
                                                 0.08442701399326324,
                                                 0.08442994952201843,
                                                 0.0843852087855339,
                                                 0.08436418324708939,
                                                 0.0844472274184227,
                                                 0.08449103683233261,
                                                 0.0844266414642334,
                                                 0.08442007750272751,
                                                 0.0844486653804779,
                                                 0.08435752987861633,
                                                 7568953.0,
                                                 0.08458959311246872,
                                                 0.08451079577207565,
                                                 0.08436400443315506,
                                                 0.08443053811788559,
                                                 0.08448266983032227,
                                                 0.08450271934270859,
                                                 0.08451101183891296,
                                                 0.08450804650783539,
                                                 0.08455737680196762,
                                                 0.08442199975252151,
                                                 0.084612637758255,
                                                 0.08451008796691895,
                                                 0.08456732332706451,
                                                 0.08454825729131699,
                                                 0.08444435149431229,
                                                 0.08454672247171402,
                                                 0.08420255780220032,
                                                 0.08453114330768585,
                                                 431.90203857421875,
                                                 0.08457901328802109,
                                                 0.08445992320775986,
                                                 0.08451400697231293,
                                                 0.08450395613908768,
                                                 0.08450470864772797,
                                                 0.08445815742015839,
                                                 0.0844666138291359,
                                                 329.9246826171875,
                                                 0.09569226950407028,
                                                 0.0945582166314125,
                                                 0.0946061983704567,
                                                 0.09453007578849792,
                                                 0.09456789493560791,
                                                 0.09460559487342834,
                                                 0.09457743912935257,
                                                 0.09454146772623062,
                                                 0.09452404081821442,
                                                 0.09458976238965988,
                                                 0.09458254277706146,
                                                 0.0945456475019455,
                                                 0.0945700854063034,
                                                 0.09454362094402313,
                                                 0.09454405307769775,
                                                 0.09458659589290619,
                                                 0.09448900073766708,
                                                 910.1299438476562,
                                                 0.0945618599653244,
                                                 0.09456372261047363,
                                                 0.09455177932977676,
                                                 0.09446676820516586,
                                                 0.09459219127893448,
                                                 3367.74267578125,
                                                 0.09448350965976715,
                                                 0.09451571851968765,
                                                 0.11896193027496338,
                                                 0.09398454427719116,
                                                 0.09457246959209442,
                                                 0.09453534334897995,
                                                 0.0946156308054924,
                                                 25620030.0,
                                                 0.09460297226905823,
                                                 0.09457835555076599,
                                                 0.09445815533399582,
                                                 0.1450498402118683,
                                                 0.13754796981811523,
                                                 0.09410639852285385,
                                                 0.09454409033060074,
                                                 0.09451742470264435,
                                                 0.09456036239862442,
                                                 0.09448345750570297,
                                                 0.09454705566167831,
                                                 0.09449545294046402,
                                                 146.2767333984375,
                                                 0.09407175332307816,
                                                 0.12538067996501923,
                                                 0.5565594434738159,
                                                 0.09349039196968079,
                                                 0.09350410848855972,
                                                 0.09352722764015198,
                                                 0.09368980675935745,
                                                 0.09434451162815094,
                                                 1.482743740081787,
                                                 0.09454360604286194,
                                                 338.6393127441406,
                                                 0.09459488093852997,
                                                 0.12255053222179413,
                                                 0.3753548860549927,
                                                 58.339622497558594,
                                                 0.09449402987957001,
                                                 0.09414922446012497,
                                                 0.09410757571458817,
                                                 0.09444070607423782,
                                                 0.09442590922117233,
                                                 1.3948286771774292,
                                                 0.0944676473736763,
                                                 0.094512939453125,
                                                 0.09456082433462143,
                                                 84.4478530883789,
                                                 0.09455475211143494,
                                                 0.0944359302520752,
                                                 0.09461703151464462,
                                                 0.09449130296707153,
                                                 0.09451261162757874,
                                                 0.09459147602319717,
                                                 0.09457153081893921,
                                                 0.09456238895654678,
                                                 0.09457340091466904,
                                                 0.09412994980812073,
                                                 0.09386325627565384,
                                                 0.09368231892585754,
                                                 0.09364684671163559,
                                                 0.09357370436191559,
                                                 1.222943663597107,
                                                 0.0935576930642128,
                                                 1.235748291015625,
                                                 0.09369612485170364,
                                                 0.09357502311468124,
                                                 0.09353245794773102,
                                                 0.09358879178762436,
                                                 0.09361980855464935,
                                                 3.59340500831604,
                                                 0.09360233694314957,
                                                 0.09356341511011124,
                                                 0.09354453533887863,
                                                 0.09342794865369797,
                                                 0.09393218904733658,
                                                 0.09447789937257767,
                                                 0.09379788488149643,
                                                 0.09371217340230942,
                                                 0.09421192109584808,
                                                 731.2669067382812,
                                                 0.09453772753477097,
                                                 1365163.25,
                                                 140.6337127685547,
                                                 623.1651611328125,
                                                 70.59658813476562,
                                                 4.27203369140625,
                                                 0.0945260226726532,
                                                 0.0945124626159668,
                                                 0.09454381465911865,
                                                 0.0944497138261795,
                                                 0.09452548623085022,
                                                 158.22738647460938,
                                                 278.7743835449219,
                                                 0.09446462988853455,
                                                 115.81387329101562,
                                                 0.09450964629650116,
                                                 0.0944800153374672,
                                                 0.09448287636041641,
                                                 87.09356689453125,
                                                 184671.34375,
                                                 0.09459525346755981,
                                                 0.09467873722314835,
                                                 0.0945337787270546,
                                                 16.996519088745117,
                                                 0.09461172670125961,
                                                 0.09443408995866776,
                                                 51.9001579284668,
                                                 0.09444574266672134,
                                                 0.38718417286872864,
                                                 25.579999923706055,
                                                 0.1037483811378479,
                                                 0.09448056668043137,
                                                 1828.41357421875,
                                                 0.09463730454444885,
                                                 0.09466267377138138,
                                                 46765636.0,
                                                 0.0946023091673851,
                                                 0.09462863951921463,
                                                 0.0946633592247963,
                                                 0.09412078559398651,
                                                 0.09366210550069809,
                                                 0.09355118870735168,
                                                 0.09358946979045868,
                                                 0.09402459114789963,
                                                 0.09453138709068298,
                                                 0.3115718960762024,
                                                 64.50550079345703,
                                                 0.09359338879585266,
                                                 0.09346934407949448,
                                                 0.09356776624917984,
                                                 0.09347664564847946,
                                                 1377079.5,
                                                 0.09446480125188828,
                                                 0.09366337954998016,
                                                 0.09351957589387894,
                                                 0.09895399957895279,
                                                 0.09445680677890778,
                                                 0.13375918567180634,
                                                 0.10281097888946533,
                                                 0.09444534033536911,
                                                 0.17989182472229004,
                                                 0.09455157071352005,
                                                 9.182299613952637,
                                                 0.10033991187810898,
                                                 0.09449930489063263,
                                                 0.09454887360334396,
                                                 0.9713547229766846,
                                                 3.086212396621704,
                                                 23652.203125,
                                                 0.0944833904504776,
                                                 0.0944383442401886,
                                                 1139.218994140625,
                                                 2.7565970420837402,
                                                 0.45776844024658203,
                                                 19.503902435302734,
                                                 0.09465982019901276,
                                                 0.0946187898516655,
                                                 0.0946582555770874,
                                                 0.7061750888824463,
                                                 154.2449951171875,
                                                 0.094508595764637,
                                                 0.09462691843509674,
                                                 0.09459841996431351,
                                                 0.09470409899950027,
                                                 0.1413435935974121,
                                                 520.33154296875,
                                                 1.0022259950637817,
                                                 0.09464074671268463,
                                                 0.0946812629699707,
                                                 0.09462562948465347,
                                                 0.09463953971862793,
                                                 44.62700271606445,
                                                 0.09461881220340729,
                                                 0.09999187290668488,
                                                 0.09446563571691513,
                                                 0.09447474777698517,
                                                 581717.375],
                          'val_accuracy_history': [0.42105263157894735,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   2.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   3.0,
                                                   3.0,
                                                   2.0,
                                                   0.0,
                                                   0.21052631578947367,
                                                   0.5789473684210527,
                                                   1.9473684210526316,
                                                   2.8421052631578947,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   0.10526315789473684,
                                                   1.4210526315789473,
                                                   0.15789473684210525,
                                                   1.263157894736842,
                                                   2.3157894736842106,
                                                   1.4210526315789473,
                                                   0.8421052631578947,
                                                   1.0,
                                                   1.6842105263157894,
                                                   0.10526315789473684,
                                                   0.5263157894736842,
                                                   0.8421052631578947,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   4.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   2.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   2.0,
                                                   2.0,
                                                   2.0,
                                                   0.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   1.0,
                                                   2.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.7777777777777778,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   1.4444444444444444,
                                                   1.0,
                                                   0.0,
                                                   0.2222222222222222,
                                                   2.3333333333333335,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   0.1111111111111111,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.8888888888888888,
                                                   1.0,
                                                   1.0,
                                                   2.0,
                                                   2.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   0.2222222222222222,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.1111111111111112,
                                                   0.6666666666666666,
                                                   0.2222222222222222,
                                                   0.3333333333333333,
                                                   0.0,
                                                   1.3333333333333333,
                                                   0.1111111111111111,
                                                   0.1111111111111111,
                                                   1.0,
                                                   0.6666666666666666,
                                                   0.0,
                                                   0.6666666666666666,
                                                   0.0,
                                                   0.0,
                                                   0.6666666666666666,
                                                   2.0,
                                                   1.0,
                                                   0.0,
                                                   0.9473684210526315,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   3.0,
                                                   1.0,
                                                   2.0,
                                                   1.0,
                                                   3.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   1.0,
                                                   0.0,
                                                   3.0,
                                                   2.0,
                                                   0.0,
                                                   4.0,
                                                   1.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   3.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   2.0,
                                                   1.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   1.105263157894737,
                                                   0.8421052631578947,
                                                   1.4736842105263157,
                                                   0.21052631578947367,
                                                   2.0,
                                                   2.4210526315789473,
                                                   1.368421052631579,
                                                   4.684210526315789,
                                                   0.10526315789473684,
                                                   3.526315789473684,
                                                   2.263157894736842,
                                                   2.1578947368421053,
                                                   0.0,
                                                   0.0,
                                                   1.6842105263157894,
                                                   1.8421052631578947,
                                                   2.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   2.0,
                                                   1.0,
                                                   3.0,
                                                   2.0,
                                                   2.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   0.05263157894736842,
                                                   4.0,
                                                   2.0,
                                                   0.0,
                                                   2.0,
                                                   1.0,
                                                   3.0,
                                                   1.0,
                                                   3.0,
                                                   1.0,
                                                   0.8421052631578947,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   2.0,
                                                   1.0,
                                                   3.0,
                                                   4.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   2.0],
                          'val_loss': 0.08984283357858658,
                          'val_loss_history': [0.12510818243026733,
                                               0.1011715978384018,
                                               0.10811742395162582,
                                               0.11114459484815598,
                                               0.11241888999938965,
                                               0.11323241889476776,
                                               0.11366105079650879,
                                               247040.203125,
                                               0.1142834946513176,
                                               0.11444961279630661,
                                               0.11464021354913712,
                                               0.11465630680322647,
                                               0.11490944027900696,
                                               0.1149965301156044,
                                               0.11487961560487747,
                                               0.1148400530219078,
                                               0.1146181970834732,
                                               0.11503685265779495,
                                               0.11507931351661682,
                                               0.1152242124080658,
                                               0.11479132622480392,
                                               0.11491888016462326,
                                               0.12381615489721298,
                                               0.12579955160617828,
                                               0.1253044605255127,
                                               0.11393609642982483,
                                               0.1150628849864006,
                                               0.11508005112409592,
                                               0.11509200185537338,
                                               0.1261848658323288,
                                               438586784.0,
                                               0.12262706458568573,
                                               0.12555918097496033,
                                               0.12526893615722656,
                                               0.12721216678619385,
                                               0.12551110982894897,
                                               0.12551085650920868,
                                               0.12436513602733612,
                                               0.12695075571537018,
                                               0.12748709321022034,
                                               0.1318335235118866,
                                               0.12551461160182953,
                                               0.11542754620313644,
                                               0.11529360711574554,
                                               0.11534396559000015,
                                               0.11538662761449814,
                                               0.11543147265911102,
                                               0.11543182283639908,
                                               0.11539394408464432,
                                               0.11546839773654938,
                                               0.11546298116445541,
                                               0.11537270992994308,
                                               0.11556874960660934,
                                               0.11555995047092438,
                                               0.11553175002336502,
                                               0.11558782309293747,
                                               0.11552610993385315,
                                               0.1155560165643692,
                                               0.11560266464948654,
                                               0.11541055887937546,
                                               0.1156027540564537,
                                               0.11564809828996658,
                                               0.11531238257884979,
                                               0.11525371670722961,
                                               0.1153678372502327,
                                               0.11553355306386948,
                                               0.11561305820941925,
                                               0.11549890041351318,
                                               0.11554332077503204,
                                               0.11560729146003723,
                                               0.11590125411748886,
                                               0.11543592810630798,
                                               0.11562886834144592,
                                               0.11577589064836502,
                                               0.11576344817876816,
                                               0.11571769416332245,
                                               0.11605023592710495,
                                               0.11590690910816193,
                                               0.11583130806684494,
                                               0.11600607633590698,
                                               0.11593754589557648,
                                               0.11599893867969513,
                                               0.11592314392328262,
                                               0.11591313779354095,
                                               0.11575457453727722,
                                               0.11595233529806137,
                                               0.11597788333892822,
                                               0.11555687338113785,
                                               0.11684586852788925,
                                               0.11679195612668991,
                                               0.11599123477935791,
                                               0.11585559695959091,
                                               0.11611767113208771,
                                               0.11601536720991135,
                                               0.1160048097372055,
                                               0.1159575954079628,
                                               0.11597613245248795,
                                               0.11567838490009308,
                                               0.11600735038518906,
                                               0.11599087715148926,
                                               0.09163205325603485,
                                               0.0916082113981247,
                                               0.09166108816862106,
                                               0.09160305559635162,
                                               0.09166563302278519,
                                               0.09168395400047302,
                                               0.09178026020526886,
                                               0.09178093075752258,
                                               0.0914832204580307,
                                               0.09167445451021194,
                                               0.09155388176441193,
                                               0.09147341549396515,
                                               0.09150148183107376,
                                               0.09176770597696304,
                                               0.09153672307729721,
                                               0.09163783490657806,
                                               0.09164226800203323,
                                               0.09167028218507767,
                                               0.09152018278837204,
                                               0.0916731059551239,
                                               0.09161270409822464,
                                               0.0915922299027443,
                                               0.09158787131309509,
                                               0.09145088493824005,
                                               0.09155438840389252,
                                               0.091722272336483,
                                               0.09090804308652878,
                                               0.09138893336057663,
                                               0.09157588332891464,
                                               0.09179755300283432,
                                               0.09165170043706894,
                                               0.09155890345573425,
                                               0.09162383526563644,
                                               0.09175217151641846,
                                               0.09162826836109161,
                                               0.09139235317707062,
                                               0.0909319669008255,
                                               0.09162633866071701,
                                               0.09157107770442963,
                                               0.09177890419960022,
                                               0.09146026521921158,
                                               0.0915394276380539,
                                               0.09148295223712921,
                                               0.09171506762504578,
                                               0.0910874754190445,
                                               0.09055360406637192,
                                               0.09075578302145004,
                                               0.09061741083860397,
                                               0.09060914814472198,
                                               0.09105239808559418,
                                               0.09042172133922577,
                                               0.09135539829730988,
                                               0.09104840457439423,
                                               0.0914481058716774,
                                               0.09161259233951569,
                                               0.09155642986297607,
                                               0.09159298986196518,
                                               0.09178042411804199,
                                               0.09153859317302704,
                                               0.09158207476139069,
                                               0.09157371520996094,
                                               0.09075014293193817,
                                               0.09141993522644043,
                                               0.09157027304172516,
                                               0.09134889394044876,
                                               0.09141180664300919,
                                               0.0915750116109848,
                                               0.09154835343360901,
                                               0.09181329607963562,
                                               0.0916769728064537,
                                               0.09171120822429657,
                                               0.09161952883005142,
                                               0.0915825217962265,
                                               0.09158653020858765,
                                               0.09174817055463791,
                                               0.09173370897769928,
                                               0.0918404832482338,
                                               0.09175587445497513,
                                               0.09134802222251892,
                                               0.0908302292227745,
                                               0.09076055139303207,
                                               0.09075257182121277,
                                               0.09046690165996552,
                                               0.09077004343271255,
                                               0.09074489027261734,
                                               0.09072548896074295,
                                               0.09067178517580032,
                                               0.09074927121400833,
                                               0.0906749963760376,
                                               0.09057972580194473,
                                               0.09050306677818298,
                                               0.09064897894859314,
                                               0.09032656997442245,
                                               0.09060793370008469,
                                               0.09054771065711975,
                                               0.09069087356328964,
                                               0.09064767509698868,
                                               0.09145612269639969,
                                               0.09154322743415833,
                                               0.09049025923013687,
                                               0.09029731899499893,
                                               0.08968358486890793,
                                               0.0898694321513176,
                                               0.0896778553724289,
                                               0.09004233777523041,
                                               0.08995788544416428,
                                               0.0899239107966423,
                                               0.09008295834064484,
                                               0.0896284356713295,
                                               0.09004012495279312,
                                               0.08981496840715408,
                                               0.0895993635058403,
                                               0.08966679126024246,
                                               0.0899127647280693,
                                               0.09011556953191757,
                                               0.09024146944284439,
                                               0.0898381695151329,
                                               0.089955635368824,
                                               0.08993718773126602,
                                               0.08979099243879318,
                                               0.0898311585187912,
                                               0.08939468115568161,
                                               0.09009392559528351,
                                               0.08986905962228775,
                                               0.08942726254463196,
                                               0.08990700542926788,
                                               0.0899709165096283,
                                               0.08996371179819107,
                                               0.0898134708404541,
                                               0.0896100327372551,
                                               0.08974415808916092,
                                               0.08998530358076096,
                                               0.0896565169095993,
                                               0.08987461775541306,
                                               0.0899551510810852,
                                               0.09007469564676285,
                                               0.09004899114370346,
                                               0.0902426615357399,
                                               0.09040749818086624,
                                               0.0900820642709732,
                                               0.0901951715350151,
                                               0.09042607247829437,
                                               0.08935614675283432,
                                               0.08926407992839813,
                                               0.0891614705324173,
                                               0.08920969814062119,
                                               0.08954508602619171,
                                               0.08957324177026749,
                                               0.08986911177635193,
                                               0.08955825120210648,
                                               0.09029587358236313,
                                               0.08841820806264877,
                                               0.08927769213914871,
                                               0.08931326866149902,
                                               0.08981242775917053,
                                               0.08983969688415527,
                                               0.08899163454771042,
                                               0.08941895514726639,
                                               0.08937462419271469,
                                               0.08952030539512634,
                                               0.08959675580263138,
                                               0.08969853818416595,
                                               0.08962896466255188,
                                               0.08994295448064804,
                                               0.08981810510158539,
                                               0.08979881554841995,
                                               0.09003960341215134,
                                               0.08963511139154434,
                                               0.08976289629936218,
                                               0.09012604504823685,
                                               0.0898328498005867,
                                               0.08971914649009705,
                                               0.08973442018032074,
                                               0.09022703766822815,
                                               0.08993221819400787,
                                               0.09022209048271179,
                                               0.0903264731168747,
                                               0.09032102674245834,
                                               0.09001591056585312,
                                               0.09004697948694229,
                                               0.0899716317653656,
                                               0.09008210152387619,
                                               0.08950003981590271,
                                               0.09010134637355804,
                                               0.08986760675907135,
                                               0.08989552408456802,
                                               0.08977223187685013,
                                               0.09012626856565475,
                                               0.09033074975013733,
                                               0.08990726619958878,
                                               0.09017037600278854,
                                               0.08981708437204361,
                                               0.08998289704322815,
                                               0.08985056728124619,
                                               0.09033898264169693,
                                               0.08993609249591827,
                                               0.08973539620637894,
                                               0.08958485722541809,
                                               0.09011000394821167,
                                               0.08984283357858658],
                          'weight_decay': 0.0005},
                 'macroF1': {'accuracy': 2.0,
                             'batch_size': 32,
                             'cv_score': 0.018014136691882596,
                             'cv_val_accuracy': 1.222222222222222,
                             'cv_val_loss': 0.09877465665340424,
                             'cv_val_macroF1': 0.018014136691882596,
                             'cv_val_microF1': 0.0756563342354067,
                             'epochs': 100,
                             'final_model': GGNN1(
  (ggnn): GatedGraphConv(30, num_layers=2)
  (fc1): Linear(in_features=30, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                             'kwargs': {'aggr_type': 'mean',
                                        'd1': 30,
                                        'd2': 50,
                                        'num_classes': 24,
                                        'num_layers': 2},
                             'learning_rate': 0.01,
                             'macroF1': 0.006748704489763125,
                             'microF1': 0.08414023372287145,
                             'model': <class 'TFM_graph_classification_models.GGNN1'>,
                             'model_instance': GGNN1(
  (ggnn): GatedGraphConv(30, num_layers=2)
  (fc1): Linear(in_features=30, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                             'score': 'f1_macro',
                             'time': 1290.9390695095062,
                             'train_loss_history': [7861777920.0,
                                                    18351.4453125,
                                                    0.08609600365161896,
                                                    0.08494096249341965,
                                                    0.0846065953373909,
                                                    0.08471822738647461,
                                                    0.08469000458717346,
                                                    25.97217559814453,
                                                    227637.984375,
                                                    0.08466333150863647,
                                                    24937190.0,
                                                    0.08460436761379242,
                                                    0.08463048934936523,
                                                    0.08467409014701843,
                                                    0.0845961719751358,
                                                    0.08458821475505829,
                                                    0.0845770314335823,
                                                    0.08458980917930603,
                                                    0.08456533402204514,
                                                    0.08460189402103424,
                                                    1.5376646518707275,
                                                    0.08462714403867722,
                                                    0.08455481380224228,
                                                    0.08421196788549423,
                                                    0.08399565517902374,
                                                    0.08464045822620392,
                                                    0.08439569920301437,
                                                    0.08447393029928207,
                                                    0.08436314016580582,
                                                    0.084059938788414,
                                                    524407552.0,
                                                    1098160512.0,
                                                    0.08424728363752365,
                                                    0.08390361070632935,
                                                    0.08380310237407684,
                                                    0.08367624133825302,
                                                    0.08372815698385239,
                                                    0.08376618474721909,
                                                    0.08380178362131119,
                                                    0.08376359194517136,
                                                    0.08374714106321335,
                                                    0.08380936831235886,
                                                    0.08483122289180756,
                                                    0.08451099693775177,
                                                    0.08451665192842484,
                                                    0.08454050868749619,
                                                    0.08456087112426758,
                                                    0.08462248742580414,
                                                    0.08458506315946579,
                                                    0.08451282978057861,
                                                    0.08458219468593597,
                                                    0.08459468930959702,
                                                    0.08456937968730927,
                                                    0.08449748158454895,
                                                    0.08456726372241974,
                                                    0.0845608189702034,
                                                    0.08448818325996399,
                                                    0.08462183177471161,
                                                    0.08448898792266846,
                                                    0.08449804782867432,
                                                    0.08443325757980347,
                                                    0.08453451097011566,
                                                    0.08442701399326324,
                                                    0.08442994952201843,
                                                    0.0843852087855339,
                                                    0.08436418324708939,
                                                    0.0844472274184227,
                                                    0.08449103683233261,
                                                    0.0844266414642334,
                                                    0.08442007750272751,
                                                    0.0844486653804779,
                                                    0.08435752987861633,
                                                    7568953.0,
                                                    0.08458959311246872,
                                                    0.08451079577207565,
                                                    0.08436400443315506,
                                                    0.08443053811788559,
                                                    0.08448266983032227,
                                                    0.08450271934270859,
                                                    0.08451101183891296,
                                                    0.08450804650783539,
                                                    0.08455737680196762,
                                                    0.08442199975252151,
                                                    0.084612637758255,
                                                    0.08451008796691895,
                                                    0.08456732332706451,
                                                    0.08454825729131699,
                                                    0.08444435149431229,
                                                    0.08454672247171402,
                                                    0.08420255780220032,
                                                    0.08453114330768585,
                                                    431.90203857421875,
                                                    0.08457901328802109,
                                                    0.08445992320775986,
                                                    0.08451400697231293,
                                                    0.08450395613908768,
                                                    0.08450470864772797,
                                                    0.08445815742015839,
                                                    0.0844666138291359,
                                                    329.9246826171875,
                                                    0.09569226950407028,
                                                    0.0945582166314125,
                                                    0.0946061983704567,
                                                    0.09453007578849792,
                                                    0.09456789493560791,
                                                    0.09460559487342834,
                                                    0.09457743912935257,
                                                    0.09454146772623062,
                                                    0.09452404081821442,
                                                    0.09458976238965988,
                                                    0.09458254277706146,
                                                    0.0945456475019455,
                                                    0.0945700854063034,
                                                    0.09454362094402313,
                                                    0.09454405307769775,
                                                    0.09458659589290619,
                                                    0.09448900073766708,
                                                    910.1299438476562,
                                                    0.0945618599653244,
                                                    0.09456372261047363,
                                                    0.09455177932977676,
                                                    0.09446676820516586,
                                                    0.09459219127893448,
                                                    3367.74267578125,
                                                    0.09448350965976715,
                                                    0.09451571851968765,
                                                    0.11896193027496338,
                                                    0.09398454427719116,
                                                    0.09457246959209442,
                                                    0.09453534334897995,
                                                    0.0946156308054924,
                                                    25620030.0,
                                                    0.09460297226905823,
                                                    0.09457835555076599,
                                                    0.09445815533399582,
                                                    0.1450498402118683,
                                                    0.13754796981811523,
                                                    0.09410639852285385,
                                                    0.09454409033060074,
                                                    0.09451742470264435,
                                                    0.09456036239862442,
                                                    0.09448345750570297,
                                                    0.09454705566167831,
                                                    0.09449545294046402,
                                                    146.2767333984375,
                                                    0.09407175332307816,
                                                    0.12538067996501923,
                                                    0.5565594434738159,
                                                    0.09349039196968079,
                                                    0.09350410848855972,
                                                    0.09352722764015198,
                                                    0.09368980675935745,
                                                    0.09434451162815094,
                                                    1.482743740081787,
                                                    0.09454360604286194,
                                                    338.6393127441406,
                                                    0.09459488093852997,
                                                    0.12255053222179413,
                                                    0.3753548860549927,
                                                    58.339622497558594,
                                                    0.09449402987957001,
                                                    0.09414922446012497,
                                                    0.09410757571458817,
                                                    0.09444070607423782,
                                                    0.09442590922117233,
                                                    1.3948286771774292,
                                                    0.0944676473736763,
                                                    0.094512939453125,
                                                    0.09456082433462143,
                                                    84.4478530883789,
                                                    0.09455475211143494,
                                                    0.0944359302520752,
                                                    0.09461703151464462,
                                                    0.09449130296707153,
                                                    0.09451261162757874,
                                                    0.09459147602319717,
                                                    0.09457153081893921,
                                                    0.09456238895654678,
                                                    0.09457340091466904,
                                                    0.09412994980812073,
                                                    0.09386325627565384,
                                                    0.09368231892585754,
                                                    0.09364684671163559,
                                                    0.09357370436191559,
                                                    1.222943663597107,
                                                    0.0935576930642128,
                                                    1.235748291015625,
                                                    0.09369612485170364,
                                                    0.09357502311468124,
                                                    0.09353245794773102,
                                                    0.09358879178762436,
                                                    0.09361980855464935,
                                                    3.59340500831604,
                                                    0.09360233694314957,
                                                    0.09356341511011124,
                                                    0.09354453533887863,
                                                    0.09342794865369797,
                                                    0.09393218904733658,
                                                    0.09447789937257767,
                                                    0.09379788488149643,
                                                    0.09371217340230942,
                                                    0.09421192109584808,
                                                    731.2669067382812,
                                                    0.09453772753477097,
                                                    1365163.25,
                                                    140.6337127685547,
                                                    623.1651611328125,
                                                    70.59658813476562,
                                                    4.27203369140625,
                                                    0.0945260226726532,
                                                    0.0945124626159668,
                                                    0.09454381465911865,
                                                    0.0944497138261795,
                                                    0.09452548623085022,
                                                    158.22738647460938,
                                                    278.7743835449219,
                                                    0.09446462988853455,
                                                    115.81387329101562,
                                                    0.09450964629650116,
                                                    0.0944800153374672,
                                                    0.09448287636041641,
                                                    87.09356689453125,
                                                    184671.34375,
                                                    0.09459525346755981,
                                                    0.09467873722314835,
                                                    0.0945337787270546,
                                                    16.996519088745117,
                                                    0.09461172670125961,
                                                    0.09443408995866776,
                                                    51.9001579284668,
                                                    0.09444574266672134,
                                                    0.38718417286872864,
                                                    25.579999923706055,
                                                    0.1037483811378479,
                                                    0.09448056668043137,
                                                    1828.41357421875,
                                                    0.09463730454444885,
                                                    0.09466267377138138,
                                                    46765636.0,
                                                    0.0946023091673851,
                                                    0.09462863951921463,
                                                    0.0946633592247963,
                                                    0.09412078559398651,
                                                    0.09366210550069809,
                                                    0.09355118870735168,
                                                    0.09358946979045868,
                                                    0.09402459114789963,
                                                    0.09453138709068298,
                                                    0.3115718960762024,
                                                    64.50550079345703,
                                                    0.09359338879585266,
                                                    0.09346934407949448,
                                                    0.09356776624917984,
                                                    0.09347664564847946,
                                                    1377079.5,
                                                    0.09446480125188828,
                                                    0.09366337954998016,
                                                    0.09351957589387894,
                                                    0.09895399957895279,
                                                    0.09445680677890778,
                                                    0.13375918567180634,
                                                    0.10281097888946533,
                                                    0.09444534033536911,
                                                    0.17989182472229004,
                                                    0.09455157071352005,
                                                    9.182299613952637,
                                                    0.10033991187810898,
                                                    0.09449930489063263,
                                                    0.09454887360334396,
                                                    0.9713547229766846,
                                                    3.086212396621704,
                                                    23652.203125,
                                                    0.0944833904504776,
                                                    0.0944383442401886,
                                                    1139.218994140625,
                                                    2.7565970420837402,
                                                    0.45776844024658203,
                                                    19.503902435302734,
                                                    0.09465982019901276,
                                                    0.0946187898516655,
                                                    0.0946582555770874,
                                                    0.7061750888824463,
                                                    154.2449951171875,
                                                    0.094508595764637,
                                                    0.09462691843509674,
                                                    0.09459841996431351,
                                                    0.09470409899950027,
                                                    0.1413435935974121,
                                                    520.33154296875,
                                                    1.0022259950637817,
                                                    0.09464074671268463,
                                                    0.0946812629699707,
                                                    0.09462562948465347,
                                                    0.09463953971862793,
                                                    44.62700271606445,
                                                    0.09461881220340729,
                                                    0.09999187290668488,
                                                    0.09446563571691513,
                                                    0.09447474777698517,
                                                    581717.375],
                             'val_accuracy_history': [0.42105263157894735,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      3.0,
                                                      3.0,
                                                      2.0,
                                                      0.0,
                                                      0.21052631578947367,
                                                      0.5789473684210527,
                                                      1.9473684210526316,
                                                      2.8421052631578947,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.10526315789473684,
                                                      1.4210526315789473,
                                                      0.15789473684210525,
                                                      1.263157894736842,
                                                      2.3157894736842106,
                                                      1.4210526315789473,
                                                      0.8421052631578947,
                                                      1.0,
                                                      1.6842105263157894,
                                                      0.10526315789473684,
                                                      0.5263157894736842,
                                                      0.8421052631578947,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      4.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      2.0,
                                                      2.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.7777777777777778,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.4444444444444444,
                                                      1.0,
                                                      0.0,
                                                      0.2222222222222222,
                                                      2.3333333333333335,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.1111111111111111,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.8888888888888888,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.2222222222222222,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.1111111111111112,
                                                      0.6666666666666666,
                                                      0.2222222222222222,
                                                      0.3333333333333333,
                                                      0.0,
                                                      1.3333333333333333,
                                                      0.1111111111111111,
                                                      0.1111111111111111,
                                                      1.0,
                                                      0.6666666666666666,
                                                      0.0,
                                                      0.6666666666666666,
                                                      0.0,
                                                      0.0,
                                                      0.6666666666666666,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      0.9473684210526315,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      3.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      3.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      3.0,
                                                      2.0,
                                                      0.0,
                                                      4.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      3.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      1.105263157894737,
                                                      0.8421052631578947,
                                                      1.4736842105263157,
                                                      0.21052631578947367,
                                                      2.0,
                                                      2.4210526315789473,
                                                      1.368421052631579,
                                                      4.684210526315789,
                                                      0.10526315789473684,
                                                      3.526315789473684,
                                                      2.263157894736842,
                                                      2.1578947368421053,
                                                      0.0,
                                                      0.0,
                                                      1.6842105263157894,
                                                      1.8421052631578947,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      3.0,
                                                      2.0,
                                                      2.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.05263157894736842,
                                                      4.0,
                                                      2.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      3.0,
                                                      1.0,
                                                      3.0,
                                                      1.0,
                                                      0.8421052631578947,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      3.0,
                                                      4.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      2.0],
                             'val_loss': 0.08984283357858658,
                             'val_loss_history': [0.12510818243026733,
                                                  0.1011715978384018,
                                                  0.10811742395162582,
                                                  0.11114459484815598,
                                                  0.11241888999938965,
                                                  0.11323241889476776,
                                                  0.11366105079650879,
                                                  247040.203125,
                                                  0.1142834946513176,
                                                  0.11444961279630661,
                                                  0.11464021354913712,
                                                  0.11465630680322647,
                                                  0.11490944027900696,
                                                  0.1149965301156044,
                                                  0.11487961560487747,
                                                  0.1148400530219078,
                                                  0.1146181970834732,
                                                  0.11503685265779495,
                                                  0.11507931351661682,
                                                  0.1152242124080658,
                                                  0.11479132622480392,
                                                  0.11491888016462326,
                                                  0.12381615489721298,
                                                  0.12579955160617828,
                                                  0.1253044605255127,
                                                  0.11393609642982483,
                                                  0.1150628849864006,
                                                  0.11508005112409592,
                                                  0.11509200185537338,
                                                  0.1261848658323288,
                                                  438586784.0,
                                                  0.12262706458568573,
                                                  0.12555918097496033,
                                                  0.12526893615722656,
                                                  0.12721216678619385,
                                                  0.12551110982894897,
                                                  0.12551085650920868,
                                                  0.12436513602733612,
                                                  0.12695075571537018,
                                                  0.12748709321022034,
                                                  0.1318335235118866,
                                                  0.12551461160182953,
                                                  0.11542754620313644,
                                                  0.11529360711574554,
                                                  0.11534396559000015,
                                                  0.11538662761449814,
                                                  0.11543147265911102,
                                                  0.11543182283639908,
                                                  0.11539394408464432,
                                                  0.11546839773654938,
                                                  0.11546298116445541,
                                                  0.11537270992994308,
                                                  0.11556874960660934,
                                                  0.11555995047092438,
                                                  0.11553175002336502,
                                                  0.11558782309293747,
                                                  0.11552610993385315,
                                                  0.1155560165643692,
                                                  0.11560266464948654,
                                                  0.11541055887937546,
                                                  0.1156027540564537,
                                                  0.11564809828996658,
                                                  0.11531238257884979,
                                                  0.11525371670722961,
                                                  0.1153678372502327,
                                                  0.11553355306386948,
                                                  0.11561305820941925,
                                                  0.11549890041351318,
                                                  0.11554332077503204,
                                                  0.11560729146003723,
                                                  0.11590125411748886,
                                                  0.11543592810630798,
                                                  0.11562886834144592,
                                                  0.11577589064836502,
                                                  0.11576344817876816,
                                                  0.11571769416332245,
                                                  0.11605023592710495,
                                                  0.11590690910816193,
                                                  0.11583130806684494,
                                                  0.11600607633590698,
                                                  0.11593754589557648,
                                                  0.11599893867969513,
                                                  0.11592314392328262,
                                                  0.11591313779354095,
                                                  0.11575457453727722,
                                                  0.11595233529806137,
                                                  0.11597788333892822,
                                                  0.11555687338113785,
                                                  0.11684586852788925,
                                                  0.11679195612668991,
                                                  0.11599123477935791,
                                                  0.11585559695959091,
                                                  0.11611767113208771,
                                                  0.11601536720991135,
                                                  0.1160048097372055,
                                                  0.1159575954079628,
                                                  0.11597613245248795,
                                                  0.11567838490009308,
                                                  0.11600735038518906,
                                                  0.11599087715148926,
                                                  0.09163205325603485,
                                                  0.0916082113981247,
                                                  0.09166108816862106,
                                                  0.09160305559635162,
                                                  0.09166563302278519,
                                                  0.09168395400047302,
                                                  0.09178026020526886,
                                                  0.09178093075752258,
                                                  0.0914832204580307,
                                                  0.09167445451021194,
                                                  0.09155388176441193,
                                                  0.09147341549396515,
                                                  0.09150148183107376,
                                                  0.09176770597696304,
                                                  0.09153672307729721,
                                                  0.09163783490657806,
                                                  0.09164226800203323,
                                                  0.09167028218507767,
                                                  0.09152018278837204,
                                                  0.0916731059551239,
                                                  0.09161270409822464,
                                                  0.0915922299027443,
                                                  0.09158787131309509,
                                                  0.09145088493824005,
                                                  0.09155438840389252,
                                                  0.091722272336483,
                                                  0.09090804308652878,
                                                  0.09138893336057663,
                                                  0.09157588332891464,
                                                  0.09179755300283432,
                                                  0.09165170043706894,
                                                  0.09155890345573425,
                                                  0.09162383526563644,
                                                  0.09175217151641846,
                                                  0.09162826836109161,
                                                  0.09139235317707062,
                                                  0.0909319669008255,
                                                  0.09162633866071701,
                                                  0.09157107770442963,
                                                  0.09177890419960022,
                                                  0.09146026521921158,
                                                  0.0915394276380539,
                                                  0.09148295223712921,
                                                  0.09171506762504578,
                                                  0.0910874754190445,
                                                  0.09055360406637192,
                                                  0.09075578302145004,
                                                  0.09061741083860397,
                                                  0.09060914814472198,
                                                  0.09105239808559418,
                                                  0.09042172133922577,
                                                  0.09135539829730988,
                                                  0.09104840457439423,
                                                  0.0914481058716774,
                                                  0.09161259233951569,
                                                  0.09155642986297607,
                                                  0.09159298986196518,
                                                  0.09178042411804199,
                                                  0.09153859317302704,
                                                  0.09158207476139069,
                                                  0.09157371520996094,
                                                  0.09075014293193817,
                                                  0.09141993522644043,
                                                  0.09157027304172516,
                                                  0.09134889394044876,
                                                  0.09141180664300919,
                                                  0.0915750116109848,
                                                  0.09154835343360901,
                                                  0.09181329607963562,
                                                  0.0916769728064537,
                                                  0.09171120822429657,
                                                  0.09161952883005142,
                                                  0.0915825217962265,
                                                  0.09158653020858765,
                                                  0.09174817055463791,
                                                  0.09173370897769928,
                                                  0.0918404832482338,
                                                  0.09175587445497513,
                                                  0.09134802222251892,
                                                  0.0908302292227745,
                                                  0.09076055139303207,
                                                  0.09075257182121277,
                                                  0.09046690165996552,
                                                  0.09077004343271255,
                                                  0.09074489027261734,
                                                  0.09072548896074295,
                                                  0.09067178517580032,
                                                  0.09074927121400833,
                                                  0.0906749963760376,
                                                  0.09057972580194473,
                                                  0.09050306677818298,
                                                  0.09064897894859314,
                                                  0.09032656997442245,
                                                  0.09060793370008469,
                                                  0.09054771065711975,
                                                  0.09069087356328964,
                                                  0.09064767509698868,
                                                  0.09145612269639969,
                                                  0.09154322743415833,
                                                  0.09049025923013687,
                                                  0.09029731899499893,
                                                  0.08968358486890793,
                                                  0.0898694321513176,
                                                  0.0896778553724289,
                                                  0.09004233777523041,
                                                  0.08995788544416428,
                                                  0.0899239107966423,
                                                  0.09008295834064484,
                                                  0.0896284356713295,
                                                  0.09004012495279312,
                                                  0.08981496840715408,
                                                  0.0895993635058403,
                                                  0.08966679126024246,
                                                  0.0899127647280693,
                                                  0.09011556953191757,
                                                  0.09024146944284439,
                                                  0.0898381695151329,
                                                  0.089955635368824,
                                                  0.08993718773126602,
                                                  0.08979099243879318,
                                                  0.0898311585187912,
                                                  0.08939468115568161,
                                                  0.09009392559528351,
                                                  0.08986905962228775,
                                                  0.08942726254463196,
                                                  0.08990700542926788,
                                                  0.0899709165096283,
                                                  0.08996371179819107,
                                                  0.0898134708404541,
                                                  0.0896100327372551,
                                                  0.08974415808916092,
                                                  0.08998530358076096,
                                                  0.0896565169095993,
                                                  0.08987461775541306,
                                                  0.0899551510810852,
                                                  0.09007469564676285,
                                                  0.09004899114370346,
                                                  0.0902426615357399,
                                                  0.09040749818086624,
                                                  0.0900820642709732,
                                                  0.0901951715350151,
                                                  0.09042607247829437,
                                                  0.08935614675283432,
                                                  0.08926407992839813,
                                                  0.0891614705324173,
                                                  0.08920969814062119,
                                                  0.08954508602619171,
                                                  0.08957324177026749,
                                                  0.08986911177635193,
                                                  0.08955825120210648,
                                                  0.09029587358236313,
                                                  0.08841820806264877,
                                                  0.08927769213914871,
                                                  0.08931326866149902,
                                                  0.08981242775917053,
                                                  0.08983969688415527,
                                                  0.08899163454771042,
                                                  0.08941895514726639,
                                                  0.08937462419271469,
                                                  0.08952030539512634,
                                                  0.08959675580263138,
                                                  0.08969853818416595,
                                                  0.08962896466255188,
                                                  0.08994295448064804,
                                                  0.08981810510158539,
                                                  0.08979881554841995,
                                                  0.09003960341215134,
                                                  0.08963511139154434,
                                                  0.08976289629936218,
                                                  0.09012604504823685,
                                                  0.0898328498005867,
                                                  0.08971914649009705,
                                                  0.08973442018032074,
                                                  0.09022703766822815,
                                                  0.08993221819400787,
                                                  0.09022209048271179,
                                                  0.0903264731168747,
                                                  0.09032102674245834,
                                                  0.09001591056585312,
                                                  0.09004697948694229,
                                                  0.0899716317653656,
                                                  0.09008210152387619,
                                                  0.08950003981590271,
                                                  0.09010134637355804,
                                                  0.08986760675907135,
                                                  0.08989552408456802,
                                                  0.08977223187685013,
                                                  0.09012626856565475,
                                                  0.09033074975013733,
                                                  0.08990726619958878,
                                                  0.09017037600278854,
                                                  0.08981708437204361,
                                                  0.08998289704322815,
                                                  0.08985056728124619,
                                                  0.09033898264169693,
                                                  0.08993609249591827,
                                                  0.08973539620637894,
                                                  0.08958485722541809,
                                                  0.09011000394821167,
                                                  0.08984283357858658],
                             'weight_decay': 0.0005},
                 'microF1': {'accuracy': 2.0,
                             'batch_size': 32,
                             'cv_score': 0.018014136691882596,
                             'cv_val_accuracy': 1.222222222222222,
                             'cv_val_loss': 0.09877465665340424,
                             'cv_val_macroF1': 0.018014136691882596,
                             'cv_val_microF1': 0.0756563342354067,
                             'epochs': 100,
                             'final_model': GGNN1(
  (ggnn): GatedGraphConv(30, num_layers=2)
  (fc1): Linear(in_features=30, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                             'kwargs': {'aggr_type': 'mean',
                                        'd1': 30,
                                        'd2': 50,
                                        'num_classes': 24,
                                        'num_layers': 2},
                             'learning_rate': 0.01,
                             'macroF1': 0.006748704489763125,
                             'microF1': 0.08414023372287145,
                             'model': <class 'TFM_graph_classification_models.GGNN1'>,
                             'model_instance': GGNN1(
  (ggnn): GatedGraphConv(30, num_layers=2)
  (fc1): Linear(in_features=30, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                             'score': 'f1_macro',
                             'time': 1290.9390695095062,
                             'train_loss_history': [7861777920.0,
                                                    18351.4453125,
                                                    0.08609600365161896,
                                                    0.08494096249341965,
                                                    0.0846065953373909,
                                                    0.08471822738647461,
                                                    0.08469000458717346,
                                                    25.97217559814453,
                                                    227637.984375,
                                                    0.08466333150863647,
                                                    24937190.0,
                                                    0.08460436761379242,
                                                    0.08463048934936523,
                                                    0.08467409014701843,
                                                    0.0845961719751358,
                                                    0.08458821475505829,
                                                    0.0845770314335823,
                                                    0.08458980917930603,
                                                    0.08456533402204514,
                                                    0.08460189402103424,
                                                    1.5376646518707275,
                                                    0.08462714403867722,
                                                    0.08455481380224228,
                                                    0.08421196788549423,
                                                    0.08399565517902374,
                                                    0.08464045822620392,
                                                    0.08439569920301437,
                                                    0.08447393029928207,
                                                    0.08436314016580582,
                                                    0.084059938788414,
                                                    524407552.0,
                                                    1098160512.0,
                                                    0.08424728363752365,
                                                    0.08390361070632935,
                                                    0.08380310237407684,
                                                    0.08367624133825302,
                                                    0.08372815698385239,
                                                    0.08376618474721909,
                                                    0.08380178362131119,
                                                    0.08376359194517136,
                                                    0.08374714106321335,
                                                    0.08380936831235886,
                                                    0.08483122289180756,
                                                    0.08451099693775177,
                                                    0.08451665192842484,
                                                    0.08454050868749619,
                                                    0.08456087112426758,
                                                    0.08462248742580414,
                                                    0.08458506315946579,
                                                    0.08451282978057861,
                                                    0.08458219468593597,
                                                    0.08459468930959702,
                                                    0.08456937968730927,
                                                    0.08449748158454895,
                                                    0.08456726372241974,
                                                    0.0845608189702034,
                                                    0.08448818325996399,
                                                    0.08462183177471161,
                                                    0.08448898792266846,
                                                    0.08449804782867432,
                                                    0.08443325757980347,
                                                    0.08453451097011566,
                                                    0.08442701399326324,
                                                    0.08442994952201843,
                                                    0.0843852087855339,
                                                    0.08436418324708939,
                                                    0.0844472274184227,
                                                    0.08449103683233261,
                                                    0.0844266414642334,
                                                    0.08442007750272751,
                                                    0.0844486653804779,
                                                    0.08435752987861633,
                                                    7568953.0,
                                                    0.08458959311246872,
                                                    0.08451079577207565,
                                                    0.08436400443315506,
                                                    0.08443053811788559,
                                                    0.08448266983032227,
                                                    0.08450271934270859,
                                                    0.08451101183891296,
                                                    0.08450804650783539,
                                                    0.08455737680196762,
                                                    0.08442199975252151,
                                                    0.084612637758255,
                                                    0.08451008796691895,
                                                    0.08456732332706451,
                                                    0.08454825729131699,
                                                    0.08444435149431229,
                                                    0.08454672247171402,
                                                    0.08420255780220032,
                                                    0.08453114330768585,
                                                    431.90203857421875,
                                                    0.08457901328802109,
                                                    0.08445992320775986,
                                                    0.08451400697231293,
                                                    0.08450395613908768,
                                                    0.08450470864772797,
                                                    0.08445815742015839,
                                                    0.0844666138291359,
                                                    329.9246826171875,
                                                    0.09569226950407028,
                                                    0.0945582166314125,
                                                    0.0946061983704567,
                                                    0.09453007578849792,
                                                    0.09456789493560791,
                                                    0.09460559487342834,
                                                    0.09457743912935257,
                                                    0.09454146772623062,
                                                    0.09452404081821442,
                                                    0.09458976238965988,
                                                    0.09458254277706146,
                                                    0.0945456475019455,
                                                    0.0945700854063034,
                                                    0.09454362094402313,
                                                    0.09454405307769775,
                                                    0.09458659589290619,
                                                    0.09448900073766708,
                                                    910.1299438476562,
                                                    0.0945618599653244,
                                                    0.09456372261047363,
                                                    0.09455177932977676,
                                                    0.09446676820516586,
                                                    0.09459219127893448,
                                                    3367.74267578125,
                                                    0.09448350965976715,
                                                    0.09451571851968765,
                                                    0.11896193027496338,
                                                    0.09398454427719116,
                                                    0.09457246959209442,
                                                    0.09453534334897995,
                                                    0.0946156308054924,
                                                    25620030.0,
                                                    0.09460297226905823,
                                                    0.09457835555076599,
                                                    0.09445815533399582,
                                                    0.1450498402118683,
                                                    0.13754796981811523,
                                                    0.09410639852285385,
                                                    0.09454409033060074,
                                                    0.09451742470264435,
                                                    0.09456036239862442,
                                                    0.09448345750570297,
                                                    0.09454705566167831,
                                                    0.09449545294046402,
                                                    146.2767333984375,
                                                    0.09407175332307816,
                                                    0.12538067996501923,
                                                    0.5565594434738159,
                                                    0.09349039196968079,
                                                    0.09350410848855972,
                                                    0.09352722764015198,
                                                    0.09368980675935745,
                                                    0.09434451162815094,
                                                    1.482743740081787,
                                                    0.09454360604286194,
                                                    338.6393127441406,
                                                    0.09459488093852997,
                                                    0.12255053222179413,
                                                    0.3753548860549927,
                                                    58.339622497558594,
                                                    0.09449402987957001,
                                                    0.09414922446012497,
                                                    0.09410757571458817,
                                                    0.09444070607423782,
                                                    0.09442590922117233,
                                                    1.3948286771774292,
                                                    0.0944676473736763,
                                                    0.094512939453125,
                                                    0.09456082433462143,
                                                    84.4478530883789,
                                                    0.09455475211143494,
                                                    0.0944359302520752,
                                                    0.09461703151464462,
                                                    0.09449130296707153,
                                                    0.09451261162757874,
                                                    0.09459147602319717,
                                                    0.09457153081893921,
                                                    0.09456238895654678,
                                                    0.09457340091466904,
                                                    0.09412994980812073,
                                                    0.09386325627565384,
                                                    0.09368231892585754,
                                                    0.09364684671163559,
                                                    0.09357370436191559,
                                                    1.222943663597107,
                                                    0.0935576930642128,
                                                    1.235748291015625,
                                                    0.09369612485170364,
                                                    0.09357502311468124,
                                                    0.09353245794773102,
                                                    0.09358879178762436,
                                                    0.09361980855464935,
                                                    3.59340500831604,
                                                    0.09360233694314957,
                                                    0.09356341511011124,
                                                    0.09354453533887863,
                                                    0.09342794865369797,
                                                    0.09393218904733658,
                                                    0.09447789937257767,
                                                    0.09379788488149643,
                                                    0.09371217340230942,
                                                    0.09421192109584808,
                                                    731.2669067382812,
                                                    0.09453772753477097,
                                                    1365163.25,
                                                    140.6337127685547,
                                                    623.1651611328125,
                                                    70.59658813476562,
                                                    4.27203369140625,
                                                    0.0945260226726532,
                                                    0.0945124626159668,
                                                    0.09454381465911865,
                                                    0.0944497138261795,
                                                    0.09452548623085022,
                                                    158.22738647460938,
                                                    278.7743835449219,
                                                    0.09446462988853455,
                                                    115.81387329101562,
                                                    0.09450964629650116,
                                                    0.0944800153374672,
                                                    0.09448287636041641,
                                                    87.09356689453125,
                                                    184671.34375,
                                                    0.09459525346755981,
                                                    0.09467873722314835,
                                                    0.0945337787270546,
                                                    16.996519088745117,
                                                    0.09461172670125961,
                                                    0.09443408995866776,
                                                    51.9001579284668,
                                                    0.09444574266672134,
                                                    0.38718417286872864,
                                                    25.579999923706055,
                                                    0.1037483811378479,
                                                    0.09448056668043137,
                                                    1828.41357421875,
                                                    0.09463730454444885,
                                                    0.09466267377138138,
                                                    46765636.0,
                                                    0.0946023091673851,
                                                    0.09462863951921463,
                                                    0.0946633592247963,
                                                    0.09412078559398651,
                                                    0.09366210550069809,
                                                    0.09355118870735168,
                                                    0.09358946979045868,
                                                    0.09402459114789963,
                                                    0.09453138709068298,
                                                    0.3115718960762024,
                                                    64.50550079345703,
                                                    0.09359338879585266,
                                                    0.09346934407949448,
                                                    0.09356776624917984,
                                                    0.09347664564847946,
                                                    1377079.5,
                                                    0.09446480125188828,
                                                    0.09366337954998016,
                                                    0.09351957589387894,
                                                    0.09895399957895279,
                                                    0.09445680677890778,
                                                    0.13375918567180634,
                                                    0.10281097888946533,
                                                    0.09444534033536911,
                                                    0.17989182472229004,
                                                    0.09455157071352005,
                                                    9.182299613952637,
                                                    0.10033991187810898,
                                                    0.09449930489063263,
                                                    0.09454887360334396,
                                                    0.9713547229766846,
                                                    3.086212396621704,
                                                    23652.203125,
                                                    0.0944833904504776,
                                                    0.0944383442401886,
                                                    1139.218994140625,
                                                    2.7565970420837402,
                                                    0.45776844024658203,
                                                    19.503902435302734,
                                                    0.09465982019901276,
                                                    0.0946187898516655,
                                                    0.0946582555770874,
                                                    0.7061750888824463,
                                                    154.2449951171875,
                                                    0.094508595764637,
                                                    0.09462691843509674,
                                                    0.09459841996431351,
                                                    0.09470409899950027,
                                                    0.1413435935974121,
                                                    520.33154296875,
                                                    1.0022259950637817,
                                                    0.09464074671268463,
                                                    0.0946812629699707,
                                                    0.09462562948465347,
                                                    0.09463953971862793,
                                                    44.62700271606445,
                                                    0.09461881220340729,
                                                    0.09999187290668488,
                                                    0.09446563571691513,
                                                    0.09447474777698517,
                                                    581717.375],
                             'val_accuracy_history': [0.42105263157894735,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      3.0,
                                                      3.0,
                                                      2.0,
                                                      0.0,
                                                      0.21052631578947367,
                                                      0.5789473684210527,
                                                      1.9473684210526316,
                                                      2.8421052631578947,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.10526315789473684,
                                                      1.4210526315789473,
                                                      0.15789473684210525,
                                                      1.263157894736842,
                                                      2.3157894736842106,
                                                      1.4210526315789473,
                                                      0.8421052631578947,
                                                      1.0,
                                                      1.6842105263157894,
                                                      0.10526315789473684,
                                                      0.5263157894736842,
                                                      0.8421052631578947,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      4.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      2.0,
                                                      2.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.7777777777777778,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.4444444444444444,
                                                      1.0,
                                                      0.0,
                                                      0.2222222222222222,
                                                      2.3333333333333335,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.1111111111111111,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.8888888888888888,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.2222222222222222,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.1111111111111112,
                                                      0.6666666666666666,
                                                      0.2222222222222222,
                                                      0.3333333333333333,
                                                      0.0,
                                                      1.3333333333333333,
                                                      0.1111111111111111,
                                                      0.1111111111111111,
                                                      1.0,
                                                      0.6666666666666666,
                                                      0.0,
                                                      0.6666666666666666,
                                                      0.0,
                                                      0.0,
                                                      0.6666666666666666,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      0.9473684210526315,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      3.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      3.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      3.0,
                                                      2.0,
                                                      0.0,
                                                      4.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      3.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      1.105263157894737,
                                                      0.8421052631578947,
                                                      1.4736842105263157,
                                                      0.21052631578947367,
                                                      2.0,
                                                      2.4210526315789473,
                                                      1.368421052631579,
                                                      4.684210526315789,
                                                      0.10526315789473684,
                                                      3.526315789473684,
                                                      2.263157894736842,
                                                      2.1578947368421053,
                                                      0.0,
                                                      0.0,
                                                      1.6842105263157894,
                                                      1.8421052631578947,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      3.0,
                                                      2.0,
                                                      2.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.05263157894736842,
                                                      4.0,
                                                      2.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      3.0,
                                                      1.0,
                                                      3.0,
                                                      1.0,
                                                      0.8421052631578947,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      3.0,
                                                      4.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      2.0],
                             'val_loss': 0.08984283357858658,
                             'val_loss_history': [0.12510818243026733,
                                                  0.1011715978384018,
                                                  0.10811742395162582,
                                                  0.11114459484815598,
                                                  0.11241888999938965,
                                                  0.11323241889476776,
                                                  0.11366105079650879,
                                                  247040.203125,
                                                  0.1142834946513176,
                                                  0.11444961279630661,
                                                  0.11464021354913712,
                                                  0.11465630680322647,
                                                  0.11490944027900696,
                                                  0.1149965301156044,
                                                  0.11487961560487747,
                                                  0.1148400530219078,
                                                  0.1146181970834732,
                                                  0.11503685265779495,
                                                  0.11507931351661682,
                                                  0.1152242124080658,
                                                  0.11479132622480392,
                                                  0.11491888016462326,
                                                  0.12381615489721298,
                                                  0.12579955160617828,
                                                  0.1253044605255127,
                                                  0.11393609642982483,
                                                  0.1150628849864006,
                                                  0.11508005112409592,
                                                  0.11509200185537338,
                                                  0.1261848658323288,
                                                  438586784.0,
                                                  0.12262706458568573,
                                                  0.12555918097496033,
                                                  0.12526893615722656,
                                                  0.12721216678619385,
                                                  0.12551110982894897,
                                                  0.12551085650920868,
                                                  0.12436513602733612,
                                                  0.12695075571537018,
                                                  0.12748709321022034,
                                                  0.1318335235118866,
                                                  0.12551461160182953,
                                                  0.11542754620313644,
                                                  0.11529360711574554,
                                                  0.11534396559000015,
                                                  0.11538662761449814,
                                                  0.11543147265911102,
                                                  0.11543182283639908,
                                                  0.11539394408464432,
                                                  0.11546839773654938,
                                                  0.11546298116445541,
                                                  0.11537270992994308,
                                                  0.11556874960660934,
                                                  0.11555995047092438,
                                                  0.11553175002336502,
                                                  0.11558782309293747,
                                                  0.11552610993385315,
                                                  0.1155560165643692,
                                                  0.11560266464948654,
                                                  0.11541055887937546,
                                                  0.1156027540564537,
                                                  0.11564809828996658,
                                                  0.11531238257884979,
                                                  0.11525371670722961,
                                                  0.1153678372502327,
                                                  0.11553355306386948,
                                                  0.11561305820941925,
                                                  0.11549890041351318,
                                                  0.11554332077503204,
                                                  0.11560729146003723,
                                                  0.11590125411748886,
                                                  0.11543592810630798,
                                                  0.11562886834144592,
                                                  0.11577589064836502,
                                                  0.11576344817876816,
                                                  0.11571769416332245,
                                                  0.11605023592710495,
                                                  0.11590690910816193,
                                                  0.11583130806684494,
                                                  0.11600607633590698,
                                                  0.11593754589557648,
                                                  0.11599893867969513,
                                                  0.11592314392328262,
                                                  0.11591313779354095,
                                                  0.11575457453727722,
                                                  0.11595233529806137,
                                                  0.11597788333892822,
                                                  0.11555687338113785,
                                                  0.11684586852788925,
                                                  0.11679195612668991,
                                                  0.11599123477935791,
                                                  0.11585559695959091,
                                                  0.11611767113208771,
                                                  0.11601536720991135,
                                                  0.1160048097372055,
                                                  0.1159575954079628,
                                                  0.11597613245248795,
                                                  0.11567838490009308,
                                                  0.11600735038518906,
                                                  0.11599087715148926,
                                                  0.09163205325603485,
                                                  0.0916082113981247,
                                                  0.09166108816862106,
                                                  0.09160305559635162,
                                                  0.09166563302278519,
                                                  0.09168395400047302,
                                                  0.09178026020526886,
                                                  0.09178093075752258,
                                                  0.0914832204580307,
                                                  0.09167445451021194,
                                                  0.09155388176441193,
                                                  0.09147341549396515,
                                                  0.09150148183107376,
                                                  0.09176770597696304,
                                                  0.09153672307729721,
                                                  0.09163783490657806,
                                                  0.09164226800203323,
                                                  0.09167028218507767,
                                                  0.09152018278837204,
                                                  0.0916731059551239,
                                                  0.09161270409822464,
                                                  0.0915922299027443,
                                                  0.09158787131309509,
                                                  0.09145088493824005,
                                                  0.09155438840389252,
                                                  0.091722272336483,
                                                  0.09090804308652878,
                                                  0.09138893336057663,
                                                  0.09157588332891464,
                                                  0.09179755300283432,
                                                  0.09165170043706894,
                                                  0.09155890345573425,
                                                  0.09162383526563644,
                                                  0.09175217151641846,
                                                  0.09162826836109161,
                                                  0.09139235317707062,
                                                  0.0909319669008255,
                                                  0.09162633866071701,
                                                  0.09157107770442963,
                                                  0.09177890419960022,
                                                  0.09146026521921158,
                                                  0.0915394276380539,
                                                  0.09148295223712921,
                                                  0.09171506762504578,
                                                  0.0910874754190445,
                                                  0.09055360406637192,
                                                  0.09075578302145004,
                                                  0.09061741083860397,
                                                  0.09060914814472198,
                                                  0.09105239808559418,
                                                  0.09042172133922577,
                                                  0.09135539829730988,
                                                  0.09104840457439423,
                                                  0.0914481058716774,
                                                  0.09161259233951569,
                                                  0.09155642986297607,
                                                  0.09159298986196518,
                                                  0.09178042411804199,
                                                  0.09153859317302704,
                                                  0.09158207476139069,
                                                  0.09157371520996094,
                                                  0.09075014293193817,
                                                  0.09141993522644043,
                                                  0.09157027304172516,
                                                  0.09134889394044876,
                                                  0.09141180664300919,
                                                  0.0915750116109848,
                                                  0.09154835343360901,
                                                  0.09181329607963562,
                                                  0.0916769728064537,
                                                  0.09171120822429657,
                                                  0.09161952883005142,
                                                  0.0915825217962265,
                                                  0.09158653020858765,
                                                  0.09174817055463791,
                                                  0.09173370897769928,
                                                  0.0918404832482338,
                                                  0.09175587445497513,
                                                  0.09134802222251892,
                                                  0.0908302292227745,
                                                  0.09076055139303207,
                                                  0.09075257182121277,
                                                  0.09046690165996552,
                                                  0.09077004343271255,
                                                  0.09074489027261734,
                                                  0.09072548896074295,
                                                  0.09067178517580032,
                                                  0.09074927121400833,
                                                  0.0906749963760376,
                                                  0.09057972580194473,
                                                  0.09050306677818298,
                                                  0.09064897894859314,
                                                  0.09032656997442245,
                                                  0.09060793370008469,
                                                  0.09054771065711975,
                                                  0.09069087356328964,
                                                  0.09064767509698868,
                                                  0.09145612269639969,
                                                  0.09154322743415833,
                                                  0.09049025923013687,
                                                  0.09029731899499893,
                                                  0.08968358486890793,
                                                  0.0898694321513176,
                                                  0.0896778553724289,
                                                  0.09004233777523041,
                                                  0.08995788544416428,
                                                  0.0899239107966423,
                                                  0.09008295834064484,
                                                  0.0896284356713295,
                                                  0.09004012495279312,
                                                  0.08981496840715408,
                                                  0.0895993635058403,
                                                  0.08966679126024246,
                                                  0.0899127647280693,
                                                  0.09011556953191757,
                                                  0.09024146944284439,
                                                  0.0898381695151329,
                                                  0.089955635368824,
                                                  0.08993718773126602,
                                                  0.08979099243879318,
                                                  0.0898311585187912,
                                                  0.08939468115568161,
                                                  0.09009392559528351,
                                                  0.08986905962228775,
                                                  0.08942726254463196,
                                                  0.08990700542926788,
                                                  0.0899709165096283,
                                                  0.08996371179819107,
                                                  0.0898134708404541,
                                                  0.0896100327372551,
                                                  0.08974415808916092,
                                                  0.08998530358076096,
                                                  0.0896565169095993,
                                                  0.08987461775541306,
                                                  0.0899551510810852,
                                                  0.09007469564676285,
                                                  0.09004899114370346,
                                                  0.0902426615357399,
                                                  0.09040749818086624,
                                                  0.0900820642709732,
                                                  0.0901951715350151,
                                                  0.09042607247829437,
                                                  0.08935614675283432,
                                                  0.08926407992839813,
                                                  0.0891614705324173,
                                                  0.08920969814062119,
                                                  0.08954508602619171,
                                                  0.08957324177026749,
                                                  0.08986911177635193,
                                                  0.08955825120210648,
                                                  0.09029587358236313,
                                                  0.08841820806264877,
                                                  0.08927769213914871,
                                                  0.08931326866149902,
                                                  0.08981242775917053,
                                                  0.08983969688415527,
                                                  0.08899163454771042,
                                                  0.08941895514726639,
                                                  0.08937462419271469,
                                                  0.08952030539512634,
                                                  0.08959675580263138,
                                                  0.08969853818416595,
                                                  0.08962896466255188,
                                                  0.08994295448064804,
                                                  0.08981810510158539,
                                                  0.08979881554841995,
                                                  0.09003960341215134,
                                                  0.08963511139154434,
                                                  0.08976289629936218,
                                                  0.09012604504823685,
                                                  0.0898328498005867,
                                                  0.08971914649009705,
                                                  0.08973442018032074,
                                                  0.09022703766822815,
                                                  0.08993221819400787,
                                                  0.09022209048271179,
                                                  0.0903264731168747,
                                                  0.09032102674245834,
                                                  0.09001591056585312,
                                                  0.09004697948694229,
                                                  0.0899716317653656,
                                                  0.09008210152387619,
                                                  0.08950003981590271,
                                                  0.09010134637355804,
                                                  0.08986760675907135,
                                                  0.08989552408456802,
                                                  0.08977223187685013,
                                                  0.09012626856565475,
                                                  0.09033074975013733,
                                                  0.08990726619958878,
                                                  0.09017037600278854,
                                                  0.08981708437204361,
                                                  0.08998289704322815,
                                                  0.08985056728124619,
                                                  0.09033898264169693,
                                                  0.08993609249591827,
                                                  0.08973539620637894,
                                                  0.08958485722541809,
                                                  0.09011000394821167,
                                                  0.08984283357858658],
                             'weight_decay': 0.0005}},
 'models': [{'accuracy': 2.0,
             'batch_size': 32,
             'cv_score': 0.018014136691882596,
             'cv_val_accuracy': 1.222222222222222,
             'cv_val_loss': 0.09877465665340424,
             'cv_val_macroF1': 0.018014136691882596,
             'cv_val_microF1': 0.0756563342354067,
             'epochs': 100,
             'final_model': GGNN1(
  (ggnn): GatedGraphConv(30, num_layers=2)
  (fc1): Linear(in_features=30, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
             'kwargs': {'aggr_type': 'mean',
                        'd1': 30,
                        'd2': 50,
                        'num_classes': 24,
                        'num_layers': 2},
             'learning_rate': 0.01,
             'macroF1': 0.006748704489763125,
             'microF1': 0.08414023372287145,
             'model': <class 'TFM_graph_classification_models.GGNN1'>,
             'model_instance': GGNN1(
  (ggnn): GatedGraphConv(30, num_layers=2)
  (fc1): Linear(in_features=30, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
             'score': 'f1_macro',
             'time': 1290.9390695095062,
             'train_loss_history': [7861777920.0,
                                    18351.4453125,
                                    0.08609600365161896,
                                    0.08494096249341965,
                                    0.0846065953373909,
                                    0.08471822738647461,
                                    0.08469000458717346,
                                    25.97217559814453,
                                    227637.984375,
                                    0.08466333150863647,
                                    24937190.0,
                                    0.08460436761379242,
                                    0.08463048934936523,
                                    0.08467409014701843,
                                    0.0845961719751358,
                                    0.08458821475505829,
                                    0.0845770314335823,
                                    0.08458980917930603,
                                    0.08456533402204514,
                                    0.08460189402103424,
                                    1.5376646518707275,
                                    0.08462714403867722,
                                    0.08455481380224228,
                                    0.08421196788549423,
                                    0.08399565517902374,
                                    0.08464045822620392,
                                    0.08439569920301437,
                                    0.08447393029928207,
                                    0.08436314016580582,
                                    0.084059938788414,
                                    524407552.0,
                                    1098160512.0,
                                    0.08424728363752365,
                                    0.08390361070632935,
                                    0.08380310237407684,
                                    0.08367624133825302,
                                    0.08372815698385239,
                                    0.08376618474721909,
                                    0.08380178362131119,
                                    0.08376359194517136,
                                    0.08374714106321335,
                                    0.08380936831235886,
                                    0.08483122289180756,
                                    0.08451099693775177,
                                    0.08451665192842484,
                                    0.08454050868749619,
                                    0.08456087112426758,
                                    0.08462248742580414,
                                    0.08458506315946579,
                                    0.08451282978057861,
                                    0.08458219468593597,
                                    0.08459468930959702,
                                    0.08456937968730927,
                                    0.08449748158454895,
                                    0.08456726372241974,
                                    0.0845608189702034,
                                    0.08448818325996399,
                                    0.08462183177471161,
                                    0.08448898792266846,
                                    0.08449804782867432,
                                    0.08443325757980347,
                                    0.08453451097011566,
                                    0.08442701399326324,
                                    0.08442994952201843,
                                    0.0843852087855339,
                                    0.08436418324708939,
                                    0.0844472274184227,
                                    0.08449103683233261,
                                    0.0844266414642334,
                                    0.08442007750272751,
                                    0.0844486653804779,
                                    0.08435752987861633,
                                    7568953.0,
                                    0.08458959311246872,
                                    0.08451079577207565,
                                    0.08436400443315506,
                                    0.08443053811788559,
                                    0.08448266983032227,
                                    0.08450271934270859,
                                    0.08451101183891296,
                                    0.08450804650783539,
                                    0.08455737680196762,
                                    0.08442199975252151,
                                    0.084612637758255,
                                    0.08451008796691895,
                                    0.08456732332706451,
                                    0.08454825729131699,
                                    0.08444435149431229,
                                    0.08454672247171402,
                                    0.08420255780220032,
                                    0.08453114330768585,
                                    431.90203857421875,
                                    0.08457901328802109,
                                    0.08445992320775986,
                                    0.08451400697231293,
                                    0.08450395613908768,
                                    0.08450470864772797,
                                    0.08445815742015839,
                                    0.0844666138291359,
                                    329.9246826171875,
                                    0.09569226950407028,
                                    0.0945582166314125,
                                    0.0946061983704567,
                                    0.09453007578849792,
                                    0.09456789493560791,
                                    0.09460559487342834,
                                    0.09457743912935257,
                                    0.09454146772623062,
                                    0.09452404081821442,
                                    0.09458976238965988,
                                    0.09458254277706146,
                                    0.0945456475019455,
                                    0.0945700854063034,
                                    0.09454362094402313,
                                    0.09454405307769775,
                                    0.09458659589290619,
                                    0.09448900073766708,
                                    910.1299438476562,
                                    0.0945618599653244,
                                    0.09456372261047363,
                                    0.09455177932977676,
                                    0.09446676820516586,
                                    0.09459219127893448,
                                    3367.74267578125,
                                    0.09448350965976715,
                                    0.09451571851968765,
                                    0.11896193027496338,
                                    0.09398454427719116,
                                    0.09457246959209442,
                                    0.09453534334897995,
                                    0.0946156308054924,
                                    25620030.0,
                                    0.09460297226905823,
                                    0.09457835555076599,
                                    0.09445815533399582,
                                    0.1450498402118683,
                                    0.13754796981811523,
                                    0.09410639852285385,
                                    0.09454409033060074,
                                    0.09451742470264435,
                                    0.09456036239862442,
                                    0.09448345750570297,
                                    0.09454705566167831,
                                    0.09449545294046402,
                                    146.2767333984375,
                                    0.09407175332307816,
                                    0.12538067996501923,
                                    0.5565594434738159,
                                    0.09349039196968079,
                                    0.09350410848855972,
                                    0.09352722764015198,
                                    0.09368980675935745,
                                    0.09434451162815094,
                                    1.482743740081787,
                                    0.09454360604286194,
                                    338.6393127441406,
                                    0.09459488093852997,
                                    0.12255053222179413,
                                    0.3753548860549927,
                                    58.339622497558594,
                                    0.09449402987957001,
                                    0.09414922446012497,
                                    0.09410757571458817,
                                    0.09444070607423782,
                                    0.09442590922117233,
                                    1.3948286771774292,
                                    0.0944676473736763,
                                    0.094512939453125,
                                    0.09456082433462143,
                                    84.4478530883789,
                                    0.09455475211143494,
                                    0.0944359302520752,
                                    0.09461703151464462,
                                    0.09449130296707153,
                                    0.09451261162757874,
                                    0.09459147602319717,
                                    0.09457153081893921,
                                    0.09456238895654678,
                                    0.09457340091466904,
                                    0.09412994980812073,
                                    0.09386325627565384,
                                    0.09368231892585754,
                                    0.09364684671163559,
                                    0.09357370436191559,
                                    1.222943663597107,
                                    0.0935576930642128,
                                    1.235748291015625,
                                    0.09369612485170364,
                                    0.09357502311468124,
                                    0.09353245794773102,
                                    0.09358879178762436,
                                    0.09361980855464935,
                                    3.59340500831604,
                                    0.09360233694314957,
                                    0.09356341511011124,
                                    0.09354453533887863,
                                    0.09342794865369797,
                                    0.09393218904733658,
                                    0.09447789937257767,
                                    0.09379788488149643,
                                    0.09371217340230942,
                                    0.09421192109584808,
                                    731.2669067382812,
                                    0.09453772753477097,
                                    1365163.25,
                                    140.6337127685547,
                                    623.1651611328125,
                                    70.59658813476562,
                                    4.27203369140625,
                                    0.0945260226726532,
                                    0.0945124626159668,
                                    0.09454381465911865,
                                    0.0944497138261795,
                                    0.09452548623085022,
                                    158.22738647460938,
                                    278.7743835449219,
                                    0.09446462988853455,
                                    115.81387329101562,
                                    0.09450964629650116,
                                    0.0944800153374672,
                                    0.09448287636041641,
                                    87.09356689453125,
                                    184671.34375,
                                    0.09459525346755981,
                                    0.09467873722314835,
                                    0.0945337787270546,
                                    16.996519088745117,
                                    0.09461172670125961,
                                    0.09443408995866776,
                                    51.9001579284668,
                                    0.09444574266672134,
                                    0.38718417286872864,
                                    25.579999923706055,
                                    0.1037483811378479,
                                    0.09448056668043137,
                                    1828.41357421875,
                                    0.09463730454444885,
                                    0.09466267377138138,
                                    46765636.0,
                                    0.0946023091673851,
                                    0.09462863951921463,
                                    0.0946633592247963,
                                    0.09412078559398651,
                                    0.09366210550069809,
                                    0.09355118870735168,
                                    0.09358946979045868,
                                    0.09402459114789963,
                                    0.09453138709068298,
                                    0.3115718960762024,
                                    64.50550079345703,
                                    0.09359338879585266,
                                    0.09346934407949448,
                                    0.09356776624917984,
                                    0.09347664564847946,
                                    1377079.5,
                                    0.09446480125188828,
                                    0.09366337954998016,
                                    0.09351957589387894,
                                    0.09895399957895279,
                                    0.09445680677890778,
                                    0.13375918567180634,
                                    0.10281097888946533,
                                    0.09444534033536911,
                                    0.17989182472229004,
                                    0.09455157071352005,
                                    9.182299613952637,
                                    0.10033991187810898,
                                    0.09449930489063263,
                                    0.09454887360334396,
                                    0.9713547229766846,
                                    3.086212396621704,
                                    23652.203125,
                                    0.0944833904504776,
                                    0.0944383442401886,
                                    1139.218994140625,
                                    2.7565970420837402,
                                    0.45776844024658203,
                                    19.503902435302734,
                                    0.09465982019901276,
                                    0.0946187898516655,
                                    0.0946582555770874,
                                    0.7061750888824463,
                                    154.2449951171875,
                                    0.094508595764637,
                                    0.09462691843509674,
                                    0.09459841996431351,
                                    0.09470409899950027,
                                    0.1413435935974121,
                                    520.33154296875,
                                    1.0022259950637817,
                                    0.09464074671268463,
                                    0.0946812629699707,
                                    0.09462562948465347,
                                    0.09463953971862793,
                                    44.62700271606445,
                                    0.09461881220340729,
                                    0.09999187290668488,
                                    0.09446563571691513,
                                    0.09447474777698517,
                                    581717.375],
             'val_accuracy_history': [0.42105263157894735,
                                      1.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      2.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      3.0,
                                      3.0,
                                      2.0,
                                      0.0,
                                      0.21052631578947367,
                                      0.5789473684210527,
                                      1.9473684210526316,
                                      2.8421052631578947,
                                      1.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      0.10526315789473684,
                                      1.4210526315789473,
                                      0.15789473684210525,
                                      1.263157894736842,
                                      2.3157894736842106,
                                      1.4210526315789473,
                                      0.8421052631578947,
                                      1.0,
                                      1.6842105263157894,
                                      0.10526315789473684,
                                      0.5263157894736842,
                                      0.8421052631578947,
                                      1.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      4.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      2.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      2.0,
                                      2.0,
                                      2.0,
                                      0.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      1.0,
                                      2.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.7777777777777778,
                                      0.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      1.4444444444444444,
                                      1.0,
                                      0.0,
                                      0.2222222222222222,
                                      2.3333333333333335,
                                      0.0,
                                      1.0,
                                      0.0,
                                      0.1111111111111111,
                                      0.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.8888888888888888,
                                      1.0,
                                      1.0,
                                      2.0,
                                      2.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      0.2222222222222222,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.1111111111111112,
                                      0.6666666666666666,
                                      0.2222222222222222,
                                      0.3333333333333333,
                                      0.0,
                                      1.3333333333333333,
                                      0.1111111111111111,
                                      0.1111111111111111,
                                      1.0,
                                      0.6666666666666666,
                                      0.0,
                                      0.6666666666666666,
                                      0.0,
                                      0.0,
                                      0.6666666666666666,
                                      2.0,
                                      1.0,
                                      0.0,
                                      0.9473684210526315,
                                      1.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      3.0,
                                      1.0,
                                      2.0,
                                      1.0,
                                      3.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      1.0,
                                      0.0,
                                      3.0,
                                      2.0,
                                      0.0,
                                      4.0,
                                      1.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      3.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      2.0,
                                      1.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      1.105263157894737,
                                      0.8421052631578947,
                                      1.4736842105263157,
                                      0.21052631578947367,
                                      2.0,
                                      2.4210526315789473,
                                      1.368421052631579,
                                      4.684210526315789,
                                      0.10526315789473684,
                                      3.526315789473684,
                                      2.263157894736842,
                                      2.1578947368421053,
                                      0.0,
                                      0.0,
                                      1.6842105263157894,
                                      1.8421052631578947,
                                      2.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      2.0,
                                      1.0,
                                      3.0,
                                      2.0,
                                      2.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      0.05263157894736842,
                                      4.0,
                                      2.0,
                                      0.0,
                                      2.0,
                                      1.0,
                                      3.0,
                                      1.0,
                                      3.0,
                                      1.0,
                                      0.8421052631578947,
                                      1.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      2.0,
                                      1.0,
                                      3.0,
                                      4.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      2.0],
             'val_loss': 0.08984283357858658,
             'val_loss_history': [0.12510818243026733,
                                  0.1011715978384018,
                                  0.10811742395162582,
                                  0.11114459484815598,
                                  0.11241888999938965,
                                  0.11323241889476776,
                                  0.11366105079650879,
                                  247040.203125,
                                  0.1142834946513176,
                                  0.11444961279630661,
                                  0.11464021354913712,
                                  0.11465630680322647,
                                  0.11490944027900696,
                                  0.1149965301156044,
                                  0.11487961560487747,
                                  0.1148400530219078,
                                  0.1146181970834732,
                                  0.11503685265779495,
                                  0.11507931351661682,
                                  0.1152242124080658,
                                  0.11479132622480392,
                                  0.11491888016462326,
                                  0.12381615489721298,
                                  0.12579955160617828,
                                  0.1253044605255127,
                                  0.11393609642982483,
                                  0.1150628849864006,
                                  0.11508005112409592,
                                  0.11509200185537338,
                                  0.1261848658323288,
                                  438586784.0,
                                  0.12262706458568573,
                                  0.12555918097496033,
                                  0.12526893615722656,
                                  0.12721216678619385,
                                  0.12551110982894897,
                                  0.12551085650920868,
                                  0.12436513602733612,
                                  0.12695075571537018,
                                  0.12748709321022034,
                                  0.1318335235118866,
                                  0.12551461160182953,
                                  0.11542754620313644,
                                  0.11529360711574554,
                                  0.11534396559000015,
                                  0.11538662761449814,
                                  0.11543147265911102,
                                  0.11543182283639908,
                                  0.11539394408464432,
                                  0.11546839773654938,
                                  0.11546298116445541,
                                  0.11537270992994308,
                                  0.11556874960660934,
                                  0.11555995047092438,
                                  0.11553175002336502,
                                  0.11558782309293747,
                                  0.11552610993385315,
                                  0.1155560165643692,
                                  0.11560266464948654,
                                  0.11541055887937546,
                                  0.1156027540564537,
                                  0.11564809828996658,
                                  0.11531238257884979,
                                  0.11525371670722961,
                                  0.1153678372502327,
                                  0.11553355306386948,
                                  0.11561305820941925,
                                  0.11549890041351318,
                                  0.11554332077503204,
                                  0.11560729146003723,
                                  0.11590125411748886,
                                  0.11543592810630798,
                                  0.11562886834144592,
                                  0.11577589064836502,
                                  0.11576344817876816,
                                  0.11571769416332245,
                                  0.11605023592710495,
                                  0.11590690910816193,
                                  0.11583130806684494,
                                  0.11600607633590698,
                                  0.11593754589557648,
                                  0.11599893867969513,
                                  0.11592314392328262,
                                  0.11591313779354095,
                                  0.11575457453727722,
                                  0.11595233529806137,
                                  0.11597788333892822,
                                  0.11555687338113785,
                                  0.11684586852788925,
                                  0.11679195612668991,
                                  0.11599123477935791,
                                  0.11585559695959091,
                                  0.11611767113208771,
                                  0.11601536720991135,
                                  0.1160048097372055,
                                  0.1159575954079628,
                                  0.11597613245248795,
                                  0.11567838490009308,
                                  0.11600735038518906,
                                  0.11599087715148926,
                                  0.09163205325603485,
                                  0.0916082113981247,
                                  0.09166108816862106,
                                  0.09160305559635162,
                                  0.09166563302278519,
                                  0.09168395400047302,
                                  0.09178026020526886,
                                  0.09178093075752258,
                                  0.0914832204580307,
                                  0.09167445451021194,
                                  0.09155388176441193,
                                  0.09147341549396515,
                                  0.09150148183107376,
                                  0.09176770597696304,
                                  0.09153672307729721,
                                  0.09163783490657806,
                                  0.09164226800203323,
                                  0.09167028218507767,
                                  0.09152018278837204,
                                  0.0916731059551239,
                                  0.09161270409822464,
                                  0.0915922299027443,
                                  0.09158787131309509,
                                  0.09145088493824005,
                                  0.09155438840389252,
                                  0.091722272336483,
                                  0.09090804308652878,
                                  0.09138893336057663,
                                  0.09157588332891464,
                                  0.09179755300283432,
                                  0.09165170043706894,
                                  0.09155890345573425,
                                  0.09162383526563644,
                                  0.09175217151641846,
                                  0.09162826836109161,
                                  0.09139235317707062,
                                  0.0909319669008255,
                                  0.09162633866071701,
                                  0.09157107770442963,
                                  0.09177890419960022,
                                  0.09146026521921158,
                                  0.0915394276380539,
                                  0.09148295223712921,
                                  0.09171506762504578,
                                  0.0910874754190445,
                                  0.09055360406637192,
                                  0.09075578302145004,
                                  0.09061741083860397,
                                  0.09060914814472198,
                                  0.09105239808559418,
                                  0.09042172133922577,
                                  0.09135539829730988,
                                  0.09104840457439423,
                                  0.0914481058716774,
                                  0.09161259233951569,
                                  0.09155642986297607,
                                  0.09159298986196518,
                                  0.09178042411804199,
                                  0.09153859317302704,
                                  0.09158207476139069,
                                  0.09157371520996094,
                                  0.09075014293193817,
                                  0.09141993522644043,
                                  0.09157027304172516,
                                  0.09134889394044876,
                                  0.09141180664300919,
                                  0.0915750116109848,
                                  0.09154835343360901,
                                  0.09181329607963562,
                                  0.0916769728064537,
                                  0.09171120822429657,
                                  0.09161952883005142,
                                  0.0915825217962265,
                                  0.09158653020858765,
                                  0.09174817055463791,
                                  0.09173370897769928,
                                  0.0918404832482338,
                                  0.09175587445497513,
                                  0.09134802222251892,
                                  0.0908302292227745,
                                  0.09076055139303207,
                                  0.09075257182121277,
                                  0.09046690165996552,
                                  0.09077004343271255,
                                  0.09074489027261734,
                                  0.09072548896074295,
                                  0.09067178517580032,
                                  0.09074927121400833,
                                  0.0906749963760376,
                                  0.09057972580194473,
                                  0.09050306677818298,
                                  0.09064897894859314,
                                  0.09032656997442245,
                                  0.09060793370008469,
                                  0.09054771065711975,
                                  0.09069087356328964,
                                  0.09064767509698868,
                                  0.09145612269639969,
                                  0.09154322743415833,
                                  0.09049025923013687,
                                  0.09029731899499893,
                                  0.08968358486890793,
                                  0.0898694321513176,
                                  0.0896778553724289,
                                  0.09004233777523041,
                                  0.08995788544416428,
                                  0.0899239107966423,
                                  0.09008295834064484,
                                  0.0896284356713295,
                                  0.09004012495279312,
                                  0.08981496840715408,
                                  0.0895993635058403,
                                  0.08966679126024246,
                                  0.0899127647280693,
                                  0.09011556953191757,
                                  0.09024146944284439,
                                  0.0898381695151329,
                                  0.089955635368824,
                                  0.08993718773126602,
                                  0.08979099243879318,
                                  0.0898311585187912,
                                  0.08939468115568161,
                                  0.09009392559528351,
                                  0.08986905962228775,
                                  0.08942726254463196,
                                  0.08990700542926788,
                                  0.0899709165096283,
                                  0.08996371179819107,
                                  0.0898134708404541,
                                  0.0896100327372551,
                                  0.08974415808916092,
                                  0.08998530358076096,
                                  0.0896565169095993,
                                  0.08987461775541306,
                                  0.0899551510810852,
                                  0.09007469564676285,
                                  0.09004899114370346,
                                  0.0902426615357399,
                                  0.09040749818086624,
                                  0.0900820642709732,
                                  0.0901951715350151,
                                  0.09042607247829437,
                                  0.08935614675283432,
                                  0.08926407992839813,
                                  0.0891614705324173,
                                  0.08920969814062119,
                                  0.08954508602619171,
                                  0.08957324177026749,
                                  0.08986911177635193,
                                  0.08955825120210648,
                                  0.09029587358236313,
                                  0.08841820806264877,
                                  0.08927769213914871,
                                  0.08931326866149902,
                                  0.08981242775917053,
                                  0.08983969688415527,
                                  0.08899163454771042,
                                  0.08941895514726639,
                                  0.08937462419271469,
                                  0.08952030539512634,
                                  0.08959675580263138,
                                  0.08969853818416595,
                                  0.08962896466255188,
                                  0.08994295448064804,
                                  0.08981810510158539,
                                  0.08979881554841995,
                                  0.09003960341215134,
                                  0.08963511139154434,
                                  0.08976289629936218,
                                  0.09012604504823685,
                                  0.0898328498005867,
                                  0.08971914649009705,
                                  0.08973442018032074,
                                  0.09022703766822815,
                                  0.08993221819400787,
                                  0.09022209048271179,
                                  0.0903264731168747,
                                  0.09032102674245834,
                                  0.09001591056585312,
                                  0.09004697948694229,
                                  0.0899716317653656,
                                  0.09008210152387619,
                                  0.08950003981590271,
                                  0.09010134637355804,
                                  0.08986760675907135,
                                  0.08989552408456802,
                                  0.08977223187685013,
                                  0.09012626856565475,
                                  0.09033074975013733,
                                  0.08990726619958878,
                                  0.09017037600278854,
                                  0.08981708437204361,
                                  0.08998289704322815,
                                  0.08985056728124619,
                                  0.09033898264169693,
                                  0.08993609249591827,
                                  0.08973539620637894,
                                  0.08958485722541809,
                                  0.09011000394821167,
                                  0.08984283357858658],
             'weight_decay': 0.0005}],
 'tests': {},
 'training_time': 3934.418103456497}
{'autoincrement': 4,
 'best_models': {},
 'best_models_list': [{'accuracy': 2.0,
                       'batch_size': 32,
                       'cv_score': 0.018014136691882596,
                       'cv_val_accuracy': 1.222222222222222,
                       'cv_val_loss': 0.09877465665340424,
                       'cv_val_macroF1': 0.018014136691882596,
                       'cv_val_microF1': 0.0756563342354067,
                       'epochs': 100,
                       'final_model': GGNN1(
  (ggnn): GatedGraphConv(30, num_layers=2)
  (fc1): Linear(in_features=30, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                       'kwargs': {'aggr_type': 'mean',
                                  'd1': 30,
                                  'd2': 50,
                                  'num_classes': 24,
                                  'num_layers': 2},
                       'learning_rate': 0.01,
                       'macroF1': 0.006748704489763125,
                       'microF1': 0.08414023372287145,
                       'model': <class 'TFM_graph_classification_models.GGNN1'>,
                       'model_instance': GGNN1(
  (ggnn): GatedGraphConv(30, num_layers=2)
  (fc1): Linear(in_features=30, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                       'name': '3_GGNN1',
                       'score': 'macroF1',
                       'time': 1290.9390695095062,
                       'train_loss_history': [7861777920.0,
                                              18351.4453125,
                                              0.08609600365161896,
                                              0.08494096249341965,
                                              0.0846065953373909,
                                              0.08471822738647461,
                                              0.08469000458717346,
                                              25.97217559814453,
                                              227637.984375,
                                              0.08466333150863647,
                                              24937190.0,
                                              0.08460436761379242,
                                              0.08463048934936523,
                                              0.08467409014701843,
                                              0.0845961719751358,
                                              0.08458821475505829,
                                              0.0845770314335823,
                                              0.08458980917930603,
                                              0.08456533402204514,
                                              0.08460189402103424,
                                              1.5376646518707275,
                                              0.08462714403867722,
                                              0.08455481380224228,
                                              0.08421196788549423,
                                              0.08399565517902374,
                                              0.08464045822620392,
                                              0.08439569920301437,
                                              0.08447393029928207,
                                              0.08436314016580582,
                                              0.084059938788414,
                                              524407552.0,
                                              1098160512.0,
                                              0.08424728363752365,
                                              0.08390361070632935,
                                              0.08380310237407684,
                                              0.08367624133825302,
                                              0.08372815698385239,
                                              0.08376618474721909,
                                              0.08380178362131119,
                                              0.08376359194517136,
                                              0.08374714106321335,
                                              0.08380936831235886,
                                              0.08483122289180756,
                                              0.08451099693775177,
                                              0.08451665192842484,
                                              0.08454050868749619,
                                              0.08456087112426758,
                                              0.08462248742580414,
                                              0.08458506315946579,
                                              0.08451282978057861,
                                              0.08458219468593597,
                                              0.08459468930959702,
                                              0.08456937968730927,
                                              0.08449748158454895,
                                              0.08456726372241974,
                                              0.0845608189702034,
                                              0.08448818325996399,
                                              0.08462183177471161,
                                              0.08448898792266846,
                                              0.08449804782867432,
                                              0.08443325757980347,
                                              0.08453451097011566,
                                              0.08442701399326324,
                                              0.08442994952201843,
                                              0.0843852087855339,
                                              0.08436418324708939,
                                              0.0844472274184227,
                                              0.08449103683233261,
                                              0.0844266414642334,
                                              0.08442007750272751,
                                              0.0844486653804779,
                                              0.08435752987861633,
                                              7568953.0,
                                              0.08458959311246872,
                                              0.08451079577207565,
                                              0.08436400443315506,
                                              0.08443053811788559,
                                              0.08448266983032227,
                                              0.08450271934270859,
                                              0.08451101183891296,
                                              0.08450804650783539,
                                              0.08455737680196762,
                                              0.08442199975252151,
                                              0.084612637758255,
                                              0.08451008796691895,
                                              0.08456732332706451,
                                              0.08454825729131699,
                                              0.08444435149431229,
                                              0.08454672247171402,
                                              0.08420255780220032,
                                              0.08453114330768585,
                                              431.90203857421875,
                                              0.08457901328802109,
                                              0.08445992320775986,
                                              0.08451400697231293,
                                              0.08450395613908768,
                                              0.08450470864772797,
                                              0.08445815742015839,
                                              0.0844666138291359,
                                              329.9246826171875,
                                              0.09569226950407028,
                                              0.0945582166314125,
                                              0.0946061983704567,
                                              0.09453007578849792,
                                              0.09456789493560791,
                                              0.09460559487342834,
                                              0.09457743912935257,
                                              0.09454146772623062,
                                              0.09452404081821442,
                                              0.09458976238965988,
                                              0.09458254277706146,
                                              0.0945456475019455,
                                              0.0945700854063034,
                                              0.09454362094402313,
                                              0.09454405307769775,
                                              0.09458659589290619,
                                              0.09448900073766708,
                                              910.1299438476562,
                                              0.0945618599653244,
                                              0.09456372261047363,
                                              0.09455177932977676,
                                              0.09446676820516586,
                                              0.09459219127893448,
                                              3367.74267578125,
                                              0.09448350965976715,
                                              0.09451571851968765,
                                              0.11896193027496338,
                                              0.09398454427719116,
                                              0.09457246959209442,
                                              0.09453534334897995,
                                              0.0946156308054924,
                                              25620030.0,
                                              0.09460297226905823,
                                              0.09457835555076599,
                                              0.09445815533399582,
                                              0.1450498402118683,
                                              0.13754796981811523,
                                              0.09410639852285385,
                                              0.09454409033060074,
                                              0.09451742470264435,
                                              0.09456036239862442,
                                              0.09448345750570297,
                                              0.09454705566167831,
                                              0.09449545294046402,
                                              146.2767333984375,
                                              0.09407175332307816,
                                              0.12538067996501923,
                                              0.5565594434738159,
                                              0.09349039196968079,
                                              0.09350410848855972,
                                              0.09352722764015198,
                                              0.09368980675935745,
                                              0.09434451162815094,
                                              1.482743740081787,
                                              0.09454360604286194,
                                              338.6393127441406,
                                              0.09459488093852997,
                                              0.12255053222179413,
                                              0.3753548860549927,
                                              58.339622497558594,
                                              0.09449402987957001,
                                              0.09414922446012497,
                                              0.09410757571458817,
                                              0.09444070607423782,
                                              0.09442590922117233,
                                              1.3948286771774292,
                                              0.0944676473736763,
                                              0.094512939453125,
                                              0.09456082433462143,
                                              84.4478530883789,
                                              0.09455475211143494,
                                              0.0944359302520752,
                                              0.09461703151464462,
                                              0.09449130296707153,
                                              0.09451261162757874,
                                              0.09459147602319717,
                                              0.09457153081893921,
                                              0.09456238895654678,
                                              0.09457340091466904,
                                              0.09412994980812073,
                                              0.09386325627565384,
                                              0.09368231892585754,
                                              0.09364684671163559,
                                              0.09357370436191559,
                                              1.222943663597107,
                                              0.0935576930642128,
                                              1.235748291015625,
                                              0.09369612485170364,
                                              0.09357502311468124,
                                              0.09353245794773102,
                                              0.09358879178762436,
                                              0.09361980855464935,
                                              3.59340500831604,
                                              0.09360233694314957,
                                              0.09356341511011124,
                                              0.09354453533887863,
                                              0.09342794865369797,
                                              0.09393218904733658,
                                              0.09447789937257767,
                                              0.09379788488149643,
                                              0.09371217340230942,
                                              0.09421192109584808,
                                              731.2669067382812,
                                              0.09453772753477097,
                                              1365163.25,
                                              140.6337127685547,
                                              623.1651611328125,
                                              70.59658813476562,
                                              4.27203369140625,
                                              0.0945260226726532,
                                              0.0945124626159668,
                                              0.09454381465911865,
                                              0.0944497138261795,
                                              0.09452548623085022,
                                              158.22738647460938,
                                              278.7743835449219,
                                              0.09446462988853455,
                                              115.81387329101562,
                                              0.09450964629650116,
                                              0.0944800153374672,
                                              0.09448287636041641,
                                              87.09356689453125,
                                              184671.34375,
                                              0.09459525346755981,
                                              0.09467873722314835,
                                              0.0945337787270546,
                                              16.996519088745117,
                                              0.09461172670125961,
                                              0.09443408995866776,
                                              51.9001579284668,
                                              0.09444574266672134,
                                              0.38718417286872864,
                                              25.579999923706055,
                                              0.1037483811378479,
                                              0.09448056668043137,
                                              1828.41357421875,
                                              0.09463730454444885,
                                              0.09466267377138138,
                                              46765636.0,
                                              0.0946023091673851,
                                              0.09462863951921463,
                                              0.0946633592247963,
                                              0.09412078559398651,
                                              0.09366210550069809,
                                              0.09355118870735168,
                                              0.09358946979045868,
                                              0.09402459114789963,
                                              0.09453138709068298,
                                              0.3115718960762024,
                                              64.50550079345703,
                                              0.09359338879585266,
                                              0.09346934407949448,
                                              0.09356776624917984,
                                              0.09347664564847946,
                                              1377079.5,
                                              0.09446480125188828,
                                              0.09366337954998016,
                                              0.09351957589387894,
                                              0.09895399957895279,
                                              0.09445680677890778,
                                              0.13375918567180634,
                                              0.10281097888946533,
                                              0.09444534033536911,
                                              0.17989182472229004,
                                              0.09455157071352005,
                                              9.182299613952637,
                                              0.10033991187810898,
                                              0.09449930489063263,
                                              0.09454887360334396,
                                              0.9713547229766846,
                                              3.086212396621704,
                                              23652.203125,
                                              0.0944833904504776,
                                              0.0944383442401886,
                                              1139.218994140625,
                                              2.7565970420837402,
                                              0.45776844024658203,
                                              19.503902435302734,
                                              0.09465982019901276,
                                              0.0946187898516655,
                                              0.0946582555770874,
                                              0.7061750888824463,
                                              154.2449951171875,
                                              0.094508595764637,
                                              0.09462691843509674,
                                              0.09459841996431351,
                                              0.09470409899950027,
                                              0.1413435935974121,
                                              520.33154296875,
                                              1.0022259950637817,
                                              0.09464074671268463,
                                              0.0946812629699707,
                                              0.09462562948465347,
                                              0.09463953971862793,
                                              44.62700271606445,
                                              0.09461881220340729,
                                              0.09999187290668488,
                                              0.09446563571691513,
                                              0.09447474777698517,
                                              581717.375],
                       'val_accuracy_history': [0.42105263157894735,
                                                1.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                2.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                3.0,
                                                3.0,
                                                2.0,
                                                0.0,
                                                0.21052631578947367,
                                                0.5789473684210527,
                                                1.9473684210526316,
                                                2.8421052631578947,
                                                1.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                0.10526315789473684,
                                                1.4210526315789473,
                                                0.15789473684210525,
                                                1.263157894736842,
                                                2.3157894736842106,
                                                1.4210526315789473,
                                                0.8421052631578947,
                                                1.0,
                                                1.6842105263157894,
                                                0.10526315789473684,
                                                0.5263157894736842,
                                                0.8421052631578947,
                                                1.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                4.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                2.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                2.0,
                                                2.0,
                                                2.0,
                                                0.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                1.0,
                                                2.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.7777777777777778,
                                                0.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                1.4444444444444444,
                                                1.0,
                                                0.0,
                                                0.2222222222222222,
                                                2.3333333333333335,
                                                0.0,
                                                1.0,
                                                0.0,
                                                0.1111111111111111,
                                                0.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.8888888888888888,
                                                1.0,
                                                1.0,
                                                2.0,
                                                2.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                0.2222222222222222,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.1111111111111112,
                                                0.6666666666666666,
                                                0.2222222222222222,
                                                0.3333333333333333,
                                                0.0,
                                                1.3333333333333333,
                                                0.1111111111111111,
                                                0.1111111111111111,
                                                1.0,
                                                0.6666666666666666,
                                                0.0,
                                                0.6666666666666666,
                                                0.0,
                                                0.0,
                                                0.6666666666666666,
                                                2.0,
                                                1.0,
                                                0.0,
                                                0.9473684210526315,
                                                1.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                3.0,
                                                1.0,
                                                2.0,
                                                1.0,
                                                3.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                1.0,
                                                0.0,
                                                3.0,
                                                2.0,
                                                0.0,
                                                4.0,
                                                1.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                3.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                2.0,
                                                1.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                1.105263157894737,
                                                0.8421052631578947,
                                                1.4736842105263157,
                                                0.21052631578947367,
                                                2.0,
                                                2.4210526315789473,
                                                1.368421052631579,
                                                4.684210526315789,
                                                0.10526315789473684,
                                                3.526315789473684,
                                                2.263157894736842,
                                                2.1578947368421053,
                                                0.0,
                                                0.0,
                                                1.6842105263157894,
                                                1.8421052631578947,
                                                2.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                2.0,
                                                1.0,
                                                3.0,
                                                2.0,
                                                2.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                0.05263157894736842,
                                                4.0,
                                                2.0,
                                                0.0,
                                                2.0,
                                                1.0,
                                                3.0,
                                                1.0,
                                                3.0,
                                                1.0,
                                                0.8421052631578947,
                                                1.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                2.0,
                                                1.0,
                                                3.0,
                                                4.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                2.0],
                       'val_loss': 0.08984283357858658,
                       'val_loss_history': [0.12510818243026733,
                                            0.1011715978384018,
                                            0.10811742395162582,
                                            0.11114459484815598,
                                            0.11241888999938965,
                                            0.11323241889476776,
                                            0.11366105079650879,
                                            247040.203125,
                                            0.1142834946513176,
                                            0.11444961279630661,
                                            0.11464021354913712,
                                            0.11465630680322647,
                                            0.11490944027900696,
                                            0.1149965301156044,
                                            0.11487961560487747,
                                            0.1148400530219078,
                                            0.1146181970834732,
                                            0.11503685265779495,
                                            0.11507931351661682,
                                            0.1152242124080658,
                                            0.11479132622480392,
                                            0.11491888016462326,
                                            0.12381615489721298,
                                            0.12579955160617828,
                                            0.1253044605255127,
                                            0.11393609642982483,
                                            0.1150628849864006,
                                            0.11508005112409592,
                                            0.11509200185537338,
                                            0.1261848658323288,
                                            438586784.0,
                                            0.12262706458568573,
                                            0.12555918097496033,
                                            0.12526893615722656,
                                            0.12721216678619385,
                                            0.12551110982894897,
                                            0.12551085650920868,
                                            0.12436513602733612,
                                            0.12695075571537018,
                                            0.12748709321022034,
                                            0.1318335235118866,
                                            0.12551461160182953,
                                            0.11542754620313644,
                                            0.11529360711574554,
                                            0.11534396559000015,
                                            0.11538662761449814,
                                            0.11543147265911102,
                                            0.11543182283639908,
                                            0.11539394408464432,
                                            0.11546839773654938,
                                            0.11546298116445541,
                                            0.11537270992994308,
                                            0.11556874960660934,
                                            0.11555995047092438,
                                            0.11553175002336502,
                                            0.11558782309293747,
                                            0.11552610993385315,
                                            0.1155560165643692,
                                            0.11560266464948654,
                                            0.11541055887937546,
                                            0.1156027540564537,
                                            0.11564809828996658,
                                            0.11531238257884979,
                                            0.11525371670722961,
                                            0.1153678372502327,
                                            0.11553355306386948,
                                            0.11561305820941925,
                                            0.11549890041351318,
                                            0.11554332077503204,
                                            0.11560729146003723,
                                            0.11590125411748886,
                                            0.11543592810630798,
                                            0.11562886834144592,
                                            0.11577589064836502,
                                            0.11576344817876816,
                                            0.11571769416332245,
                                            0.11605023592710495,
                                            0.11590690910816193,
                                            0.11583130806684494,
                                            0.11600607633590698,
                                            0.11593754589557648,
                                            0.11599893867969513,
                                            0.11592314392328262,
                                            0.11591313779354095,
                                            0.11575457453727722,
                                            0.11595233529806137,
                                            0.11597788333892822,
                                            0.11555687338113785,
                                            0.11684586852788925,
                                            0.11679195612668991,
                                            0.11599123477935791,
                                            0.11585559695959091,
                                            0.11611767113208771,
                                            0.11601536720991135,
                                            0.1160048097372055,
                                            0.1159575954079628,
                                            0.11597613245248795,
                                            0.11567838490009308,
                                            0.11600735038518906,
                                            0.11599087715148926,
                                            0.09163205325603485,
                                            0.0916082113981247,
                                            0.09166108816862106,
                                            0.09160305559635162,
                                            0.09166563302278519,
                                            0.09168395400047302,
                                            0.09178026020526886,
                                            0.09178093075752258,
                                            0.0914832204580307,
                                            0.09167445451021194,
                                            0.09155388176441193,
                                            0.09147341549396515,
                                            0.09150148183107376,
                                            0.09176770597696304,
                                            0.09153672307729721,
                                            0.09163783490657806,
                                            0.09164226800203323,
                                            0.09167028218507767,
                                            0.09152018278837204,
                                            0.0916731059551239,
                                            0.09161270409822464,
                                            0.0915922299027443,
                                            0.09158787131309509,
                                            0.09145088493824005,
                                            0.09155438840389252,
                                            0.091722272336483,
                                            0.09090804308652878,
                                            0.09138893336057663,
                                            0.09157588332891464,
                                            0.09179755300283432,
                                            0.09165170043706894,
                                            0.09155890345573425,
                                            0.09162383526563644,
                                            0.09175217151641846,
                                            0.09162826836109161,
                                            0.09139235317707062,
                                            0.0909319669008255,
                                            0.09162633866071701,
                                            0.09157107770442963,
                                            0.09177890419960022,
                                            0.09146026521921158,
                                            0.0915394276380539,
                                            0.09148295223712921,
                                            0.09171506762504578,
                                            0.0910874754190445,
                                            0.09055360406637192,
                                            0.09075578302145004,
                                            0.09061741083860397,
                                            0.09060914814472198,
                                            0.09105239808559418,
                                            0.09042172133922577,
                                            0.09135539829730988,
                                            0.09104840457439423,
                                            0.0914481058716774,
                                            0.09161259233951569,
                                            0.09155642986297607,
                                            0.09159298986196518,
                                            0.09178042411804199,
                                            0.09153859317302704,
                                            0.09158207476139069,
                                            0.09157371520996094,
                                            0.09075014293193817,
                                            0.09141993522644043,
                                            0.09157027304172516,
                                            0.09134889394044876,
                                            0.09141180664300919,
                                            0.0915750116109848,
                                            0.09154835343360901,
                                            0.09181329607963562,
                                            0.0916769728064537,
                                            0.09171120822429657,
                                            0.09161952883005142,
                                            0.0915825217962265,
                                            0.09158653020858765,
                                            0.09174817055463791,
                                            0.09173370897769928,
                                            0.0918404832482338,
                                            0.09175587445497513,
                                            0.09134802222251892,
                                            0.0908302292227745,
                                            0.09076055139303207,
                                            0.09075257182121277,
                                            0.09046690165996552,
                                            0.09077004343271255,
                                            0.09074489027261734,
                                            0.09072548896074295,
                                            0.09067178517580032,
                                            0.09074927121400833,
                                            0.0906749963760376,
                                            0.09057972580194473,
                                            0.09050306677818298,
                                            0.09064897894859314,
                                            0.09032656997442245,
                                            0.09060793370008469,
                                            0.09054771065711975,
                                            0.09069087356328964,
                                            0.09064767509698868,
                                            0.09145612269639969,
                                            0.09154322743415833,
                                            0.09049025923013687,
                                            0.09029731899499893,
                                            0.08968358486890793,
                                            0.0898694321513176,
                                            0.0896778553724289,
                                            0.09004233777523041,
                                            0.08995788544416428,
                                            0.0899239107966423,
                                            0.09008295834064484,
                                            0.0896284356713295,
                                            0.09004012495279312,
                                            0.08981496840715408,
                                            0.0895993635058403,
                                            0.08966679126024246,
                                            0.0899127647280693,
                                            0.09011556953191757,
                                            0.09024146944284439,
                                            0.0898381695151329,
                                            0.089955635368824,
                                            0.08993718773126602,
                                            0.08979099243879318,
                                            0.0898311585187912,
                                            0.08939468115568161,
                                            0.09009392559528351,
                                            0.08986905962228775,
                                            0.08942726254463196,
                                            0.08990700542926788,
                                            0.0899709165096283,
                                            0.08996371179819107,
                                            0.0898134708404541,
                                            0.0896100327372551,
                                            0.08974415808916092,
                                            0.08998530358076096,
                                            0.0896565169095993,
                                            0.08987461775541306,
                                            0.0899551510810852,
                                            0.09007469564676285,
                                            0.09004899114370346,
                                            0.0902426615357399,
                                            0.09040749818086624,
                                            0.0900820642709732,
                                            0.0901951715350151,
                                            0.09042607247829437,
                                            0.08935614675283432,
                                            0.08926407992839813,
                                            0.0891614705324173,
                                            0.08920969814062119,
                                            0.08954508602619171,
                                            0.08957324177026749,
                                            0.08986911177635193,
                                            0.08955825120210648,
                                            0.09029587358236313,
                                            0.08841820806264877,
                                            0.08927769213914871,
                                            0.08931326866149902,
                                            0.08981242775917053,
                                            0.08983969688415527,
                                            0.08899163454771042,
                                            0.08941895514726639,
                                            0.08937462419271469,
                                            0.08952030539512634,
                                            0.08959675580263138,
                                            0.08969853818416595,
                                            0.08962896466255188,
                                            0.08994295448064804,
                                            0.08981810510158539,
                                            0.08979881554841995,
                                            0.09003960341215134,
                                            0.08963511139154434,
                                            0.08976289629936218,
                                            0.09012604504823685,
                                            0.0898328498005867,
                                            0.08971914649009705,
                                            0.08973442018032074,
                                            0.09022703766822815,
                                            0.08993221819400787,
                                            0.09022209048271179,
                                            0.0903264731168747,
                                            0.09032102674245834,
                                            0.09001591056585312,
                                            0.09004697948694229,
                                            0.0899716317653656,
                                            0.09008210152387619,
                                            0.08950003981590271,
                                            0.09010134637355804,
                                            0.08986760675907135,
                                            0.08989552408456802,
                                            0.08977223187685013,
                                            0.09012626856565475,
                                            0.09033074975013733,
                                            0.08990726619958878,
                                            0.09017037600278854,
                                            0.08981708437204361,
                                            0.08998289704322815,
                                            0.08985056728124619,
                                            0.09033898264169693,
                                            0.08993609249591827,
                                            0.08973539620637894,
                                            0.08958485722541809,
                                            0.09011000394821167,
                                            0.08984283357858658],
                       'weight_decay': 0.0005}],
 'models': {}}
2019-09-14 08:13:11,606 - training_jobs - DEBUG - test_multiple_models
2019-09-14 08:13:11,606 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-14 08:13:14,027 - training_jobs - DEBUG - training time: 3937s
2019-09-14 08:13:14,027 - training_jobs - DEBUG - saving to results/20190914_070737_ggnn_.json
2019-09-14 08:13:14,029 - training_jobs - DEBUG - moved jobdict to done_trainings/task_0.yml
2019-09-14 08:13:14,029 - training_jobs - DEBUG - Finished!

2019-09-14 08:13:16,475 - training_jobs - DEBUG - training trains/task_3.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 50,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-14 08:13:16,486 - training_jobs - DEBUG - training with: 
2019-09-14 08:13:16,486 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max/
2019-09-14 08:13:16,486 - training_jobs - DEBUG - GGNN1
2019-09-14 08:13:16,486 - training_jobs - DEBUG - 
2019-09-14 08:13:16,486 - training_jobs - DEBUG - ggnn training
2019-09-14 08:13:21,768 - training_jobs - DEBUG -  saving results to results/20190914_081321_ggnn_.json
2019-09-14 08:13:21,768 - training_jobs - DEBUG -  calling modelSelection
{'best_models': {'accuracy': {'accuracy': 3.0,
                              'batch_size': 32,
                              'cv_score': 0.005336667319455597,
                              'cv_val_accuracy': 2.0,
                              'cv_val_loss': 0.0991933469971021,
                              'cv_val_macroF1': 0.005336667319455597,
                              'cv_val_microF1': 0.066966913828706,
                              'epochs': 100,
                              'final_model': GGNN1(
  (ggnn): GatedGraphConv(60, num_layers=2)
  (fc1): Linear(in_features=60, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                              'kwargs': {'aggr_type': 'mean',
                                         'd1': 60,
                                         'd2': 50,
                                         'num_classes': 24,
                                         'num_layers': 2},
                              'learning_rate': 0.01,
                              'macroF1': 0.006754945585160564,
                              'microF1': 0.08414023372287145,
                              'model': <class 'TFM_graph_classification_models.GGNN1'>,
                              'model_instance': GGNN1(
  (ggnn): GatedGraphConv(60, num_layers=2)
  (fc1): Linear(in_features=60, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                              'score': 'f1_macro',
                              'time': 1386.8977355957031,
                              'train_loss_history': [1930582528.0,
                                                     5816673280.0,
                                                     143432992.0,
                                                     401414400.0,
                                                     0.08453511446714401,
                                                     0.09732901304960251,
                                                     0.08421100676059723,
                                                     179.138427734375,
                                                     0.08405710756778717,
                                                     0.09024409204721451,
                                                     0.08389213681221008,
                                                     0.08385269343852997,
                                                     0.08379144221544266,
                                                     1.6190478801727295,
                                                     0.0840674564242363,
                                                     0.0839853435754776,
                                                     0.08404428511857986,
                                                     0.08388674259185791,
                                                     0.08402787148952484,
                                                     0.08394353091716766,
                                                     10159.8349609375,
                                                     0.08386019617319107,
                                                     0.08378172665834427,
                                                     0.08385205268859863,
                                                     46.16153335571289,
                                                     0.08443432301282883,
                                                     0.08436936885118484,
                                                     0.0838686004281044,
                                                     0.08384396880865097,
                                                     0.0838937908411026,
                                                     0.08472973853349686,
                                                     0.08419333398342133,
                                                     0.08412699401378632,
                                                     0.08394647389650345,
                                                     0.08405932784080505,
                                                     0.08477608859539032,
                                                     0.08448567986488342,
                                                     0.08438331633806229,
                                                     0.08440301567316055,
                                                     0.08459638804197311,
                                                     0.08455643802881241,
                                                     0.08457781374454498,
                                                     0.0844746083021164,
                                                     1.218392252922058,
                                                     92.7166976928711,
                                                     0.08451459556818008,
                                                     0.0845295637845993,
                                                     0.08447882533073425,
                                                     0.08454097807407379,
                                                     0.08445699512958527,
                                                     686.81591796875,
                                                     0.08460459113121033,
                                                     143.66787719726562,
                                                     0.08449937403202057,
                                                     0.08453886210918427,
                                                     0.08456281572580338,
                                                     0.08450835943222046,
                                                     0.0844619944691658,
                                                     0.08456537127494812,
                                                     0.08452136069536209,
                                                     0.08452289551496506,
                                                     0.08451851457357407,
                                                     0.08440401405096054,
                                                     0.08451830595731735,
                                                     0.08450745046138763,
                                                     0.08453594148159027,
                                                     0.08453307300806046,
                                                     0.08445210754871368,
                                                     0.08442220091819763,
                                                     0.08446472138166428,
                                                     0.08453156054019928,
                                                     0.08442430943250656,
                                                     0.08437874913215637,
                                                     0.0843782126903534,
                                                     0.08437833935022354,
                                                     0.08440952003002167,
                                                     0.08439227938652039,
                                                     0.08432715386152267,
                                                     0.08439390361309052,
                                                     4549.99755859375,
                                                     0.0845685750246048,
                                                     0.0844576433300972,
                                                     0.08457537740468979,
                                                     0.08446572721004486,
                                                     0.08438173681497574,
                                                     0.08444757014513016,
                                                     0.08448383957147598,
                                                     0.08457212895154953,
                                                     0.08460312336683273,
                                                     0.0844893679022789,
                                                     0.08443479239940643,
                                                     0.08454051613807678,
                                                     0.0845152959227562,
                                                     0.0845574289560318,
                                                     0.08444813638925552,
                                                     0.08446840941905975,
                                                     0.08447453379631042,
                                                     0.08441149443387985,
                                                     0.0844496637582779,
                                                     0.08445731550455093,
                                                     0.09589837491512299,
                                                     76.78620910644531,
                                                     0.09452478587627411,
                                                     0.09451538324356079,
                                                     3594.9453125,
                                                     0.09456611424684525,
                                                     0.09456440061330795,
                                                     0.09461269527673721,
                                                     0.09654979407787323,
                                                     0.09458601474761963,
                                                     0.09456944465637207,
                                                     0.09449096769094467,
                                                     0.09453420341014862,
                                                     0.09456656128168106,
                                                     0.0945281833410263,
                                                     0.09457744657993317,
                                                     0.09452137351036072,
                                                     3293.878662109375,
                                                     0.0944976657629013,
                                                     0.09452024847269058,
                                                     0.09451670944690704,
                                                     0.09454703330993652,
                                                     2545.70849609375,
                                                     0.09458226710557938,
                                                     0.09455703943967819,
                                                     0.09453553706407547,
                                                     0.3361894488334656,
                                                     0.09449301660060883,
                                                     0.19670675694942474,
                                                     0.1361423134803772,
                                                     0.09369964897632599,
                                                     0.09379451721906662,
                                                     0.09369710832834244,
                                                     0.09350265562534332,
                                                     0.09347912669181824,
                                                     0.09346462041139603,
                                                     0.0935664027929306,
                                                     0.09353747218847275,
                                                     0.09344299137592316,
                                                     0.09350886195898056,
                                                     0.46246740221977234,
                                                     0.0934191569685936,
                                                     0.09330388158559799,
                                                     0.09347691386938095,
                                                     0.09354433417320251,
                                                     0.09353521466255188,
                                                     0.09354806691408157,
                                                     4.875348091125488,
                                                     0.09350661933422089,
                                                     0.09341655671596527,
                                                     0.09375137090682983,
                                                     0.09359747171401978,
                                                     0.09344623237848282,
                                                     0.09351751208305359,
                                                     0.09441114217042923,
                                                     0.09437780827283859,
                                                     0.09366247802972794,
                                                     0.09349711984395981,
                                                     0.09343573451042175,
                                                     0.09347439557313919,
                                                     0.09343110024929047,
                                                     0.09334275126457214,
                                                     0.09329673647880554,
                                                     0.09324946254491806,
                                                     0.09332489967346191,
                                                     0.09335567057132721,
                                                     0.09336070716381073,
                                                     0.09327112883329391,
                                                     0.09470103681087494,
                                                     0.0943952202796936,
                                                     223.92398071289062,
                                                     0.2761259078979492,
                                                     568.1837158203125,
                                                     0.09453856199979782,
                                                     3.640625476837158,
                                                     8.571806907653809,
                                                     0.09439113736152649,
                                                     0.09438219666481018,
                                                     0.09455494582653046,
                                                     0.09456533193588257,
                                                     0.09450738877058029,
                                                     0.09454187005758286,
                                                     0.09451670944690704,
                                                     0.28767964243888855,
                                                     0.09446129947900772,
                                                     0.09447275102138519,
                                                     261.5072937011719,
                                                     0.5570076704025269,
                                                     351.6172790527344,
                                                     223.679931640625,
                                                     0.09456315636634827,
                                                     1275.6224365234375,
                                                     996.783203125,
                                                     468.9762878417969,
                                                     0.09383884072303772,
                                                     0.09372755140066147,
                                                     0.09372410923242569,
                                                     0.4396379292011261,
                                                     1.5778316259384155,
                                                     0.09457020461559296,
                                                     0.4057060182094574,
                                                     0.09465852379798889,
                                                     0.09467104822397232,
                                                     804.968017578125,
                                                     0.09458814561367035,
                                                     0.09467726200819016,
                                                     0.16680659353733063,
                                                     0.09450245648622513,
                                                     0.09450561553239822,
                                                     0.09438183158636093,
                                                     0.09459953755140305,
                                                     0.09469465166330338,
                                                     0.09475258737802505,
                                                     0.09472335129976273,
                                                     0.09469188004732132,
                                                     0.09467725455760956,
                                                     0.09465447068214417,
                                                     107.36640167236328,
                                                     0.09468750655651093,
                                                     0.09464620053768158,
                                                     0.09461190551519394,
                                                     8.436896324157715,
                                                     0.8304502367973328,
                                                     0.09465745091438293,
                                                     0.09457870572805405,
                                                     0.09460476040840149,
                                                     0.5350560545921326,
                                                     0.09462569653987885,
                                                     12.481863975524902,
                                                     0.09459514170885086,
                                                     0.09474245458841324,
                                                     0.09460768103599548,
                                                     1.460088849067688,
                                                     0.09470023959875107,
                                                     0.0946817472577095,
                                                     39852.96484375,
                                                     0.14213499426841736,
                                                     2483.219970703125,
                                                     0.09459136426448822,
                                                     10.684162139892578,
                                                     0.09467890858650208,
                                                     0.09464482218027115,
                                                     0.09463319927453995,
                                                     305.5508728027344,
                                                     0.09451852738857269,
                                                     266.2228088378906,
                                                     0.09464354813098907,
                                                     0.09453378617763519,
                                                     0.09454832971096039,
                                                     47629.37109375,
                                                     0.09382311999797821,
                                                     0.0934765487909317,
                                                     0.09329262375831604,
                                                     0.09430080652236938,
                                                     0.09367848187685013,
                                                     2579.875732421875,
                                                     0.5042579770088196,
                                                     0.09466729313135147,
                                                     0.094603031873703,
                                                     0.09465199708938599,
                                                     0.09458494931459427,
                                                     0.09463470429182053,
                                                     10456.1650390625,
                                                     0.09460638463497162,
                                                     0.09465428441762924,
                                                     0.09468037635087967,
                                                     0.09464714676141739,
                                                     0.9984434247016907,
                                                     718732.0625,
                                                     0.09467263519763947,
                                                     786.500244140625,
                                                     0.0946587324142456,
                                                     25.26593589782715,
                                                     0.0946255475282669,
                                                     0.8189891576766968,
                                                     0.09464100748300552,
                                                     0.20862728357315063,
                                                     212.48797607421875,
                                                     0.11569085717201233,
                                                     0.09459014981985092,
                                                     0.10856836289167404,
                                                     10806.7138671875,
                                                     0.09449947625398636,
                                                     0.27257049083709717,
                                                     0.09450982511043549,
                                                     0.09409075230360031,
                                                     3391.76318359375,
                                                     0.09542278200387955,
                                                     3508036.0,
                                                     0.0946173369884491,
                                                     0.09459066390991211,
                                                     0.0945025086402893,
                                                     0.3716455399990082,
                                                     0.09453834593296051,
                                                     0.09462703764438629,
                                                     0.09462379664182663,
                                                     5380.2314453125,
                                                     0.09367762506008148,
                                                     0.09457611292600632,
                                                     0.0945659652352333],
                              'val_accuracy_history': [1.105263157894737,
                                                       1.0526315789473684,
                                                       0.15789473684210525,
                                                       0.9473684210526315,
                                                       0.8947368421052632,
                                                       0.3684210526315789,
                                                       0.631578947368421,
                                                       1.0526315789473684,
                                                       1.0526315789473684,
                                                       0.631578947368421,
                                                       1.0,
                                                       1.4736842105263157,
                                                       0.3157894736842105,
                                                       1.4210526315789473,
                                                       0.7894736842105263,
                                                       1.3157894736842106,
                                                       0.5789473684210527,
                                                       1.263157894736842,
                                                       0.631578947368421,
                                                       1.631578947368421,
                                                       1.105263157894737,
                                                       0.42105263157894735,
                                                       1.2105263157894737,
                                                       1.1578947368421053,
                                                       1.0,
                                                       2.0,
                                                       1.0526315789473684,
                                                       0.05263157894736842,
                                                       1.368421052631579,
                                                       0.21052631578947367,
                                                       0.0,
                                                       1.1578947368421053,
                                                       1.263157894736842,
                                                       1.5263157894736843,
                                                       0.0,
                                                       2.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       1.0,
                                                       4.0,
                                                       2.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       3.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       3.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       2.0,
                                                       2.0,
                                                       0.0,
                                                       1.0,
                                                       2.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.8421052631578947,
                                                       1.0,
                                                       0.0,
                                                       2.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       3.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       3.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       3.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.1111111111111111,
                                                       0.4444444444444444,
                                                       0.0,
                                                       1.7777777777777777,
                                                       0.0,
                                                       0.7777777777777778,
                                                       0.0,
                                                       0.0,
                                                       0.8888888888888888,
                                                       0.0,
                                                       1.1111111111111112,
                                                       0.4444444444444444,
                                                       0.1111111111111111,
                                                       0.0,
                                                       0.1111111111111111,
                                                       0.2222222222222222,
                                                       0.1111111111111111,
                                                       0.8888888888888888,
                                                       0.7777777777777778,
                                                       0.8888888888888888,
                                                       0.1111111111111111,
                                                       0.2222222222222222,
                                                       1.3333333333333333,
                                                       0.7777777777777778,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       1.3333333333333333,
                                                       0.0,
                                                       0.3333333333333333,
                                                       0.0,
                                                       1.2222222222222223,
                                                       0.1111111111111111,
                                                       0.0,
                                                       1.3333333333333333,
                                                       0.6666666666666666,
                                                       0.2222222222222222,
                                                       2.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       3.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       2.0,
                                                       0.0,
                                                       0.0,
                                                       0.4444444444444444,
                                                       1.5555555555555556,
                                                       0.4444444444444444,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       0.0,
                                                       4.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       0.7368421052631579,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       3.0,
                                                       0.0,
                                                       0.0,
                                                       3.0,
                                                       0.0,
                                                       2.0,
                                                       1.0,
                                                       2.0,
                                                       2.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       1.0,
                                                       2.0,
                                                       3.0,
                                                       1.0,
                                                       1.0,
                                                       2.0,
                                                       0.0,
                                                       3.0,
                                                       0.0,
                                                       1.0,
                                                       0.0,
                                                       1.0,
                                                       1.0,
                                                       2.0,
                                                       1.0,
                                                       1.0,
                                                       3.0,
                                                       2.0,
                                                       2.0,
                                                       3.0,
                                                       4.0,
                                                       3.0,
                                                       3.0,
                                                       1.0,
                                                       1.0,
                                                       1.368421052631579,
                                                       2.736842105263158,
                                                       1.1578947368421053,
                                                       1.6842105263157894,
                                                       1.9473684210526316,
                                                       1.8421052631578947,
                                                       1.0,
                                                       2.0,
                                                       1.0,
                                                       1.0,
                                                       1.0,
                                                       2.0,
                                                       1.0,
                                                       3.0,
                                                       1.0,
                                                       0.0,
                                                       5.0,
                                                       1.0,
                                                       4.0,
                                                       2.0,
                                                       2.0,
                                                       1.0,
                                                       1.0,
                                                       0.0,
                                                       4.0,
                                                       1.0,
                                                       1.0,
                                                       3.0,
                                                       1.0,
                                                       2.0,
                                                       0.3684210526315789,
                                                       5.0,
                                                       1.0,
                                                       3.0,
                                                       2.0,
                                                       0.0,
                                                       2.0,
                                                       3.0,
                                                       1.0,
                                                       1.0,
                                                       2.0,
                                                       1.0,
                                                       0.0,
                                                       0.0,
                                                       2.0,
                                                       1.0,
                                                       2.0,
                                                       2.6315789473684212,
                                                       1.3157894736842106,
                                                       2.0,
                                                       3.0],
                              'val_loss': 0.08996066451072693,
                              'val_loss_history': [1034503488.0,
                                                   847939968.0,
                                                   237004064.0,
                                                   0.1263040006160736,
                                                   0.1310095340013504,
                                                   0.1337033212184906,
                                                   0.12615786492824554,
                                                   0.12673823535442352,
                                                   0.12818212807178497,
                                                   0.12805397808551788,
                                                   0.12788242101669312,
                                                   0.12742243707180023,
                                                   0.12486802786588669,
                                                   0.12135627865791321,
                                                   0.1297132968902588,
                                                   0.12731346487998962,
                                                   0.12576653063297272,
                                                   0.11891531199216843,
                                                   0.12929166853427887,
                                                   0.12054215371608734,
                                                   0.1322227418422699,
                                                   0.11912821978330612,
                                                   0.12460339814424515,
                                                   0.1240234524011612,
                                                   0.11438535153865814,
                                                   0.11396811902523041,
                                                   0.11890871822834015,
                                                   0.12567488849163055,
                                                   0.12431469559669495,
                                                   0.12262961268424988,
                                                   0.114720918238163,
                                                   0.12223060429096222,
                                                   0.12178576737642288,
                                                   0.12323581427335739,
                                                   0.11141052842140198,
                                                   0.11490409821271896,
                                                   0.11491867899894714,
                                                   0.11480910331010818,
                                                   0.11481461673974991,
                                                   0.11511991918087006,
                                                   0.11507649719715118,
                                                   0.11513196676969528,
                                                   0.11517633497714996,
                                                   0.11511987447738647,
                                                   0.11503130942583084,
                                                   0.11513296514749527,
                                                   0.11526677012443542,
                                                   0.11538837105035782,
                                                   0.11536581069231033,
                                                   0.11482106149196625,
                                                   0.11543918401002884,
                                                   0.1155194565653801,
                                                   0.11556079238653183,
                                                   0.11545369774103165,
                                                   0.11537531018257141,
                                                   0.11555165797472,
                                                   0.11537749320268631,
                                                   0.11569669842720032,
                                                   0.11556733399629593,
                                                   0.11561307311058044,
                                                   0.11569474637508392,
                                                   0.11569544672966003,
                                                   0.11562191694974899,
                                                   0.11569502204656601,
                                                   0.11571890860795975,
                                                   0.11563991010189056,
                                                   0.11592783033847809,
                                                   0.11566539108753204,
                                                   0.11559756100177765,
                                                   0.11567602306604385,
                                                   0.11575625836849213,
                                                   0.1158311516046524,
                                                   0.11566069722175598,
                                                   0.11581201106309891,
                                                   0.11579643934965134,
                                                   0.11583234369754791,
                                                   0.11580903083086014,
                                                   0.11596063524484634,
                                                   0.11589496582746506,
                                                   0.11581369489431381,
                                                   0.11581697314977646,
                                                   0.11586833000183105,
                                                   0.11823365092277527,
                                                   0.11571573466062546,
                                                   0.1159466952085495,
                                                   0.11612705141305923,
                                                   0.11606323719024658,
                                                   0.11564084887504578,
                                                   0.11597100645303726,
                                                   0.1160222664475441,
                                                   0.11600326001644135,
                                                   0.11590395122766495,
                                                   0.11599953472614288,
                                                   0.11587393283843994,
                                                   0.1161334291100502,
                                                   0.11603137850761414,
                                                   0.11602283269166946,
                                                   0.11603374034166336,
                                                   0.11604278534650803,
                                                   0.11608204990625381,
                                                   0.09147002547979355,
                                                   0.09143483638763428,
                                                   0.09163686633110046,
                                                   0.09171552211046219,
                                                   0.09144914895296097,
                                                   0.09174562990665436,
                                                   0.091587595641613,
                                                   0.09154273569583893,
                                                   0.09181218594312668,
                                                   0.09172553569078445,
                                                   0.09155931323766708,
                                                   0.09146300703287125,
                                                   0.09142749756574631,
                                                   0.09154758602380753,
                                                   0.09157166630029678,
                                                   0.09156911820173264,
                                                   0.09155336022377014,
                                                   0.09153666347265244,
                                                   0.09141728281974792,
                                                   0.0918840542435646,
                                                   0.09160088747739792,
                                                   0.0916093960404396,
                                                   0.09157142043113708,
                                                   0.09171904623508453,
                                                   0.0915781706571579,
                                                   0.09155244380235672,
                                                   0.09143797308206558,
                                                   0.09141530841588974,
                                                   0.09086361527442932,
                                                   0.0905296728014946,
                                                   0.09057653695344925,
                                                   0.09012796729803085,
                                                   0.09028708189725876,
                                                   0.09044957906007767,
                                                   0.09055684506893158,
                                                   0.0902361124753952,
                                                   0.09055016934871674,
                                                   0.09031613171100616,
                                                   0.0902179628610611,
                                                   0.09052954614162445,
                                                   0.0900455117225647,
                                                   0.0902295783162117,
                                                   0.09020984917879105,
                                                   0.09043499827384949,
                                                   0.09051758050918579,
                                                   0.0904468297958374,
                                                   0.09017684310674667,
                                                   0.0902678519487381,
                                                   0.09037574380636215,
                                                   0.0904354378581047,
                                                   0.09113160520792007,
                                                   0.09062957018613815,
                                                   0.09017972648143768,
                                                   0.09062566608190536,
                                                   0.09158965945243835,
                                                   0.091673843562603,
                                                   0.0899815633893013,
                                                   0.09016478061676025,
                                                   0.0909065455198288,
                                                   0.09008615463972092,
                                                   0.09036406874656677,
                                                   0.09002280980348587,
                                                   0.09014122933149338,
                                                   0.09028293937444687,
                                                   0.0899188220500946,
                                                   0.08988888561725616,
                                                   0.09009919315576553,
                                                   0.09167507290840149,
                                                   0.09156660735607147,
                                                   0.09164106100797653,
                                                   0.09177406877279282,
                                                   0.09153330326080322,
                                                   0.09173539280891418,
                                                   0.09170960634946823,
                                                   0.09177560359239578,
                                                   0.09162052720785141,
                                                   0.09143483638763428,
                                                   0.09150806069374084,
                                                   0.09180814772844315,
                                                   0.09179021418094635,
                                                   0.09148918092250824,
                                                   0.09165149182081223,
                                                   0.09149252623319626,
                                                   0.09190665930509567,
                                                   0.0916096493601799,
                                                   0.09160439670085907,
                                                   0.09168170392513275,
                                                   0.09163866192102432,
                                                   0.09190444648265839,
                                                   0.091668039560318,
                                                   0.09162873029708862,
                                                   0.09173496067523956,
                                                   0.09242351353168488,
                                                   0.09048132598400116,
                                                   0.09090138971805573,
                                                   0.0904548242688179,
                                                   0.09145405888557434,
                                                   0.09131670743227005,
                                                   0.09178461879491806,
                                                   0.09153732657432556,
                                                   0.08973786979913712,
                                                   0.08988428860902786,
                                                   0.09021115303039551,
                                                   0.09000113606452942,
                                                   0.08976569026708603,
                                                   0.08987715095281601,
                                                   0.089874766767025,
                                                   0.08953355252742767,
                                                   0.08985365927219391,
                                                   0.08929459005594254,
                                                   0.09014719724655151,
                                                   0.0901370495557785,
                                                   0.08988743275403976,
                                                   0.09002864360809326,
                                                   0.08994632959365845,
                                                   0.09004314243793488,
                                                   0.08995210379362106,
                                                   0.09007275104522705,
                                                   0.09029345214366913,
                                                   0.090416319668293,
                                                   0.0899377390742302,
                                                   0.08996004611253738,
                                                   0.08988483250141144,
                                                   0.08986671268939972,
                                                   0.0900193452835083,
                                                   0.0899295061826706,
                                                   0.08994332700967789,
                                                   0.08973976969718933,
                                                   0.08997202664613724,
                                                   0.09010780602693558,
                                                   0.08996426314115524,
                                                   0.08962488174438477,
                                                   0.0902571827173233,
                                                   0.08992582559585571,
                                                   0.0900769904255867,
                                                   0.0898761972784996,
                                                   0.09017913788557053,
                                                   0.08997853100299835,
                                                   0.08989366888999939,
                                                   0.09010732173919678,
                                                   0.09005895256996155,
                                                   0.09000539779663086,
                                                   0.08995525538921356,
                                                   0.08972669392824173,
                                                   0.09011795371770859,
                                                   0.09001778066158295,
                                                   0.09011020511388779,
                                                   0.08981508761644363,
                                                   0.09017821401357651,
                                                   0.08959657698869705,
                                                   0.08875498175621033,
                                                   0.08870062232017517,
                                                   0.08880362659692764,
                                                   0.08918870985507965,
                                                   0.08892866224050522,
                                                   0.08985244482755661,
                                                   0.08982453495264053,
                                                   0.09029696136713028,
                                                   0.09009458869695663,
                                                   0.09013500809669495,
                                                   0.09010349214076996,
                                                   0.08994513750076294,
                                                   0.08971379697322845,
                                                   0.09011200815439224,
                                                   0.0903470441699028,
                                                   0.09037244319915771,
                                                   0.09014169126749039,
                                                   0.08994632959365845,
                                                   0.08982060849666595,
                                                   0.08958274871110916,
                                                   0.08996281772851944,
                                                   0.08989058434963226,
                                                   0.08994904905557632,
                                                   0.0900254026055336,
                                                   0.08989007025957108,
                                                   0.08997174352407455,
                                                   0.08993948996067047,
                                                   0.08963050693273544,
                                                   0.08996085822582245,
                                                   0.09006138890981674,
                                                   0.09014187753200531,
                                                   0.09013525396585464,
                                                   0.08976943790912628,
                                                   0.0901322066783905,
                                                   0.08987969905138016,
                                                   0.08936253190040588,
                                                   0.08984756469726562,
                                                   0.08993438631296158,
                                                   0.0899040624499321,
                                                   0.09000980108976364,
                                                   0.08996778726577759,
                                                   0.08943293988704681,
                                                   0.09004877507686615,
                                                   0.08976338803768158,
                                                   0.08978376537561417,
                                                   0.09000024944543839,
                                                   0.08958359062671661,
                                                   0.08903425931930542,
                                                   0.09001847356557846,
                                                   0.08996066451072693],
                              'weight_decay': 0.0005},
                 'loss': {'accuracy': 3.0,
                          'batch_size': 32,
                          'cv_score': 0.005336667319455597,
                          'cv_val_accuracy': 2.0,
                          'cv_val_loss': 0.0991933469971021,
                          'cv_val_macroF1': 0.005336667319455597,
                          'cv_val_microF1': 0.066966913828706,
                          'epochs': 100,
                          'final_model': GGNN1(
  (ggnn): GatedGraphConv(60, num_layers=2)
  (fc1): Linear(in_features=60, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                          'kwargs': {'aggr_type': 'mean',
                                     'd1': 60,
                                     'd2': 50,
                                     'num_classes': 24,
                                     'num_layers': 2},
                          'learning_rate': 0.01,
                          'macroF1': 0.006754945585160564,
                          'microF1': 0.08414023372287145,
                          'model': <class 'TFM_graph_classification_models.GGNN1'>,
                          'model_instance': GGNN1(
  (ggnn): GatedGraphConv(60, num_layers=2)
  (fc1): Linear(in_features=60, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                          'score': 'f1_macro',
                          'time': 1386.8977355957031,
                          'train_loss_history': [1930582528.0,
                                                 5816673280.0,
                                                 143432992.0,
                                                 401414400.0,
                                                 0.08453511446714401,
                                                 0.09732901304960251,
                                                 0.08421100676059723,
                                                 179.138427734375,
                                                 0.08405710756778717,
                                                 0.09024409204721451,
                                                 0.08389213681221008,
                                                 0.08385269343852997,
                                                 0.08379144221544266,
                                                 1.6190478801727295,
                                                 0.0840674564242363,
                                                 0.0839853435754776,
                                                 0.08404428511857986,
                                                 0.08388674259185791,
                                                 0.08402787148952484,
                                                 0.08394353091716766,
                                                 10159.8349609375,
                                                 0.08386019617319107,
                                                 0.08378172665834427,
                                                 0.08385205268859863,
                                                 46.16153335571289,
                                                 0.08443432301282883,
                                                 0.08436936885118484,
                                                 0.0838686004281044,
                                                 0.08384396880865097,
                                                 0.0838937908411026,
                                                 0.08472973853349686,
                                                 0.08419333398342133,
                                                 0.08412699401378632,
                                                 0.08394647389650345,
                                                 0.08405932784080505,
                                                 0.08477608859539032,
                                                 0.08448567986488342,
                                                 0.08438331633806229,
                                                 0.08440301567316055,
                                                 0.08459638804197311,
                                                 0.08455643802881241,
                                                 0.08457781374454498,
                                                 0.0844746083021164,
                                                 1.218392252922058,
                                                 92.7166976928711,
                                                 0.08451459556818008,
                                                 0.0845295637845993,
                                                 0.08447882533073425,
                                                 0.08454097807407379,
                                                 0.08445699512958527,
                                                 686.81591796875,
                                                 0.08460459113121033,
                                                 143.66787719726562,
                                                 0.08449937403202057,
                                                 0.08453886210918427,
                                                 0.08456281572580338,
                                                 0.08450835943222046,
                                                 0.0844619944691658,
                                                 0.08456537127494812,
                                                 0.08452136069536209,
                                                 0.08452289551496506,
                                                 0.08451851457357407,
                                                 0.08440401405096054,
                                                 0.08451830595731735,
                                                 0.08450745046138763,
                                                 0.08453594148159027,
                                                 0.08453307300806046,
                                                 0.08445210754871368,
                                                 0.08442220091819763,
                                                 0.08446472138166428,
                                                 0.08453156054019928,
                                                 0.08442430943250656,
                                                 0.08437874913215637,
                                                 0.0843782126903534,
                                                 0.08437833935022354,
                                                 0.08440952003002167,
                                                 0.08439227938652039,
                                                 0.08432715386152267,
                                                 0.08439390361309052,
                                                 4549.99755859375,
                                                 0.0845685750246048,
                                                 0.0844576433300972,
                                                 0.08457537740468979,
                                                 0.08446572721004486,
                                                 0.08438173681497574,
                                                 0.08444757014513016,
                                                 0.08448383957147598,
                                                 0.08457212895154953,
                                                 0.08460312336683273,
                                                 0.0844893679022789,
                                                 0.08443479239940643,
                                                 0.08454051613807678,
                                                 0.0845152959227562,
                                                 0.0845574289560318,
                                                 0.08444813638925552,
                                                 0.08446840941905975,
                                                 0.08447453379631042,
                                                 0.08441149443387985,
                                                 0.0844496637582779,
                                                 0.08445731550455093,
                                                 0.09589837491512299,
                                                 76.78620910644531,
                                                 0.09452478587627411,
                                                 0.09451538324356079,
                                                 3594.9453125,
                                                 0.09456611424684525,
                                                 0.09456440061330795,
                                                 0.09461269527673721,
                                                 0.09654979407787323,
                                                 0.09458601474761963,
                                                 0.09456944465637207,
                                                 0.09449096769094467,
                                                 0.09453420341014862,
                                                 0.09456656128168106,
                                                 0.0945281833410263,
                                                 0.09457744657993317,
                                                 0.09452137351036072,
                                                 3293.878662109375,
                                                 0.0944976657629013,
                                                 0.09452024847269058,
                                                 0.09451670944690704,
                                                 0.09454703330993652,
                                                 2545.70849609375,
                                                 0.09458226710557938,
                                                 0.09455703943967819,
                                                 0.09453553706407547,
                                                 0.3361894488334656,
                                                 0.09449301660060883,
                                                 0.19670675694942474,
                                                 0.1361423134803772,
                                                 0.09369964897632599,
                                                 0.09379451721906662,
                                                 0.09369710832834244,
                                                 0.09350265562534332,
                                                 0.09347912669181824,
                                                 0.09346462041139603,
                                                 0.0935664027929306,
                                                 0.09353747218847275,
                                                 0.09344299137592316,
                                                 0.09350886195898056,
                                                 0.46246740221977234,
                                                 0.0934191569685936,
                                                 0.09330388158559799,
                                                 0.09347691386938095,
                                                 0.09354433417320251,
                                                 0.09353521466255188,
                                                 0.09354806691408157,
                                                 4.875348091125488,
                                                 0.09350661933422089,
                                                 0.09341655671596527,
                                                 0.09375137090682983,
                                                 0.09359747171401978,
                                                 0.09344623237848282,
                                                 0.09351751208305359,
                                                 0.09441114217042923,
                                                 0.09437780827283859,
                                                 0.09366247802972794,
                                                 0.09349711984395981,
                                                 0.09343573451042175,
                                                 0.09347439557313919,
                                                 0.09343110024929047,
                                                 0.09334275126457214,
                                                 0.09329673647880554,
                                                 0.09324946254491806,
                                                 0.09332489967346191,
                                                 0.09335567057132721,
                                                 0.09336070716381073,
                                                 0.09327112883329391,
                                                 0.09470103681087494,
                                                 0.0943952202796936,
                                                 223.92398071289062,
                                                 0.2761259078979492,
                                                 568.1837158203125,
                                                 0.09453856199979782,
                                                 3.640625476837158,
                                                 8.571806907653809,
                                                 0.09439113736152649,
                                                 0.09438219666481018,
                                                 0.09455494582653046,
                                                 0.09456533193588257,
                                                 0.09450738877058029,
                                                 0.09454187005758286,
                                                 0.09451670944690704,
                                                 0.28767964243888855,
                                                 0.09446129947900772,
                                                 0.09447275102138519,
                                                 261.5072937011719,
                                                 0.5570076704025269,
                                                 351.6172790527344,
                                                 223.679931640625,
                                                 0.09456315636634827,
                                                 1275.6224365234375,
                                                 996.783203125,
                                                 468.9762878417969,
                                                 0.09383884072303772,
                                                 0.09372755140066147,
                                                 0.09372410923242569,
                                                 0.4396379292011261,
                                                 1.5778316259384155,
                                                 0.09457020461559296,
                                                 0.4057060182094574,
                                                 0.09465852379798889,
                                                 0.09467104822397232,
                                                 804.968017578125,
                                                 0.09458814561367035,
                                                 0.09467726200819016,
                                                 0.16680659353733063,
                                                 0.09450245648622513,
                                                 0.09450561553239822,
                                                 0.09438183158636093,
                                                 0.09459953755140305,
                                                 0.09469465166330338,
                                                 0.09475258737802505,
                                                 0.09472335129976273,
                                                 0.09469188004732132,
                                                 0.09467725455760956,
                                                 0.09465447068214417,
                                                 107.36640167236328,
                                                 0.09468750655651093,
                                                 0.09464620053768158,
                                                 0.09461190551519394,
                                                 8.436896324157715,
                                                 0.8304502367973328,
                                                 0.09465745091438293,
                                                 0.09457870572805405,
                                                 0.09460476040840149,
                                                 0.5350560545921326,
                                                 0.09462569653987885,
                                                 12.481863975524902,
                                                 0.09459514170885086,
                                                 0.09474245458841324,
                                                 0.09460768103599548,
                                                 1.460088849067688,
                                                 0.09470023959875107,
                                                 0.0946817472577095,
                                                 39852.96484375,
                                                 0.14213499426841736,
                                                 2483.219970703125,
                                                 0.09459136426448822,
                                                 10.684162139892578,
                                                 0.09467890858650208,
                                                 0.09464482218027115,
                                                 0.09463319927453995,
                                                 305.5508728027344,
                                                 0.09451852738857269,
                                                 266.2228088378906,
                                                 0.09464354813098907,
                                                 0.09453378617763519,
                                                 0.09454832971096039,
                                                 47629.37109375,
                                                 0.09382311999797821,
                                                 0.0934765487909317,
                                                 0.09329262375831604,
                                                 0.09430080652236938,
                                                 0.09367848187685013,
                                                 2579.875732421875,
                                                 0.5042579770088196,
                                                 0.09466729313135147,
                                                 0.094603031873703,
                                                 0.09465199708938599,
                                                 0.09458494931459427,
                                                 0.09463470429182053,
                                                 10456.1650390625,
                                                 0.09460638463497162,
                                                 0.09465428441762924,
                                                 0.09468037635087967,
                                                 0.09464714676141739,
                                                 0.9984434247016907,
                                                 718732.0625,
                                                 0.09467263519763947,
                                                 786.500244140625,
                                                 0.0946587324142456,
                                                 25.26593589782715,
                                                 0.0946255475282669,
                                                 0.8189891576766968,
                                                 0.09464100748300552,
                                                 0.20862728357315063,
                                                 212.48797607421875,
                                                 0.11569085717201233,
                                                 0.09459014981985092,
                                                 0.10856836289167404,
                                                 10806.7138671875,
                                                 0.09449947625398636,
                                                 0.27257049083709717,
                                                 0.09450982511043549,
                                                 0.09409075230360031,
                                                 3391.76318359375,
                                                 0.09542278200387955,
                                                 3508036.0,
                                                 0.0946173369884491,
                                                 0.09459066390991211,
                                                 0.0945025086402893,
                                                 0.3716455399990082,
                                                 0.09453834593296051,
                                                 0.09462703764438629,
                                                 0.09462379664182663,
                                                 5380.2314453125,
                                                 0.09367762506008148,
                                                 0.09457611292600632,
                                                 0.0945659652352333],
                          'val_accuracy_history': [1.105263157894737,
                                                   1.0526315789473684,
                                                   0.15789473684210525,
                                                   0.9473684210526315,
                                                   0.8947368421052632,
                                                   0.3684210526315789,
                                                   0.631578947368421,
                                                   1.0526315789473684,
                                                   1.0526315789473684,
                                                   0.631578947368421,
                                                   1.0,
                                                   1.4736842105263157,
                                                   0.3157894736842105,
                                                   1.4210526315789473,
                                                   0.7894736842105263,
                                                   1.3157894736842106,
                                                   0.5789473684210527,
                                                   1.263157894736842,
                                                   0.631578947368421,
                                                   1.631578947368421,
                                                   1.105263157894737,
                                                   0.42105263157894735,
                                                   1.2105263157894737,
                                                   1.1578947368421053,
                                                   1.0,
                                                   2.0,
                                                   1.0526315789473684,
                                                   0.05263157894736842,
                                                   1.368421052631579,
                                                   0.21052631578947367,
                                                   0.0,
                                                   1.1578947368421053,
                                                   1.263157894736842,
                                                   1.5263157894736843,
                                                   0.0,
                                                   2.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   1.0,
                                                   4.0,
                                                   2.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   3.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   3.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   2.0,
                                                   2.0,
                                                   0.0,
                                                   1.0,
                                                   2.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.8421052631578947,
                                                   1.0,
                                                   0.0,
                                                   2.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   3.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   3.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   3.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.1111111111111111,
                                                   0.4444444444444444,
                                                   0.0,
                                                   1.7777777777777777,
                                                   0.0,
                                                   0.7777777777777778,
                                                   0.0,
                                                   0.0,
                                                   0.8888888888888888,
                                                   0.0,
                                                   1.1111111111111112,
                                                   0.4444444444444444,
                                                   0.1111111111111111,
                                                   0.0,
                                                   0.1111111111111111,
                                                   0.2222222222222222,
                                                   0.1111111111111111,
                                                   0.8888888888888888,
                                                   0.7777777777777778,
                                                   0.8888888888888888,
                                                   0.1111111111111111,
                                                   0.2222222222222222,
                                                   1.3333333333333333,
                                                   0.7777777777777778,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   1.3333333333333333,
                                                   0.0,
                                                   0.3333333333333333,
                                                   0.0,
                                                   1.2222222222222223,
                                                   0.1111111111111111,
                                                   0.0,
                                                   1.3333333333333333,
                                                   0.6666666666666666,
                                                   0.2222222222222222,
                                                   2.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   3.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   2.0,
                                                   0.0,
                                                   0.0,
                                                   0.4444444444444444,
                                                   1.5555555555555556,
                                                   0.4444444444444444,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   0.0,
                                                   4.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   0.7368421052631579,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   3.0,
                                                   0.0,
                                                   0.0,
                                                   3.0,
                                                   0.0,
                                                   2.0,
                                                   1.0,
                                                   2.0,
                                                   2.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   1.0,
                                                   2.0,
                                                   3.0,
                                                   1.0,
                                                   1.0,
                                                   2.0,
                                                   0.0,
                                                   3.0,
                                                   0.0,
                                                   1.0,
                                                   0.0,
                                                   1.0,
                                                   1.0,
                                                   2.0,
                                                   1.0,
                                                   1.0,
                                                   3.0,
                                                   2.0,
                                                   2.0,
                                                   3.0,
                                                   4.0,
                                                   3.0,
                                                   3.0,
                                                   1.0,
                                                   1.0,
                                                   1.368421052631579,
                                                   2.736842105263158,
                                                   1.1578947368421053,
                                                   1.6842105263157894,
                                                   1.9473684210526316,
                                                   1.8421052631578947,
                                                   1.0,
                                                   2.0,
                                                   1.0,
                                                   1.0,
                                                   1.0,
                                                   2.0,
                                                   1.0,
                                                   3.0,
                                                   1.0,
                                                   0.0,
                                                   5.0,
                                                   1.0,
                                                   4.0,
                                                   2.0,
                                                   2.0,
                                                   1.0,
                                                   1.0,
                                                   0.0,
                                                   4.0,
                                                   1.0,
                                                   1.0,
                                                   3.0,
                                                   1.0,
                                                   2.0,
                                                   0.3684210526315789,
                                                   5.0,
                                                   1.0,
                                                   3.0,
                                                   2.0,
                                                   0.0,
                                                   2.0,
                                                   3.0,
                                                   1.0,
                                                   1.0,
                                                   2.0,
                                                   1.0,
                                                   0.0,
                                                   0.0,
                                                   2.0,
                                                   1.0,
                                                   2.0,
                                                   2.6315789473684212,
                                                   1.3157894736842106,
                                                   2.0,
                                                   3.0],
                          'val_loss': 0.08996066451072693,
                          'val_loss_history': [1034503488.0,
                                               847939968.0,
                                               237004064.0,
                                               0.1263040006160736,
                                               0.1310095340013504,
                                               0.1337033212184906,
                                               0.12615786492824554,
                                               0.12673823535442352,
                                               0.12818212807178497,
                                               0.12805397808551788,
                                               0.12788242101669312,
                                               0.12742243707180023,
                                               0.12486802786588669,
                                               0.12135627865791321,
                                               0.1297132968902588,
                                               0.12731346487998962,
                                               0.12576653063297272,
                                               0.11891531199216843,
                                               0.12929166853427887,
                                               0.12054215371608734,
                                               0.1322227418422699,
                                               0.11912821978330612,
                                               0.12460339814424515,
                                               0.1240234524011612,
                                               0.11438535153865814,
                                               0.11396811902523041,
                                               0.11890871822834015,
                                               0.12567488849163055,
                                               0.12431469559669495,
                                               0.12262961268424988,
                                               0.114720918238163,
                                               0.12223060429096222,
                                               0.12178576737642288,
                                               0.12323581427335739,
                                               0.11141052842140198,
                                               0.11490409821271896,
                                               0.11491867899894714,
                                               0.11480910331010818,
                                               0.11481461673974991,
                                               0.11511991918087006,
                                               0.11507649719715118,
                                               0.11513196676969528,
                                               0.11517633497714996,
                                               0.11511987447738647,
                                               0.11503130942583084,
                                               0.11513296514749527,
                                               0.11526677012443542,
                                               0.11538837105035782,
                                               0.11536581069231033,
                                               0.11482106149196625,
                                               0.11543918401002884,
                                               0.1155194565653801,
                                               0.11556079238653183,
                                               0.11545369774103165,
                                               0.11537531018257141,
                                               0.11555165797472,
                                               0.11537749320268631,
                                               0.11569669842720032,
                                               0.11556733399629593,
                                               0.11561307311058044,
                                               0.11569474637508392,
                                               0.11569544672966003,
                                               0.11562191694974899,
                                               0.11569502204656601,
                                               0.11571890860795975,
                                               0.11563991010189056,
                                               0.11592783033847809,
                                               0.11566539108753204,
                                               0.11559756100177765,
                                               0.11567602306604385,
                                               0.11575625836849213,
                                               0.1158311516046524,
                                               0.11566069722175598,
                                               0.11581201106309891,
                                               0.11579643934965134,
                                               0.11583234369754791,
                                               0.11580903083086014,
                                               0.11596063524484634,
                                               0.11589496582746506,
                                               0.11581369489431381,
                                               0.11581697314977646,
                                               0.11586833000183105,
                                               0.11823365092277527,
                                               0.11571573466062546,
                                               0.1159466952085495,
                                               0.11612705141305923,
                                               0.11606323719024658,
                                               0.11564084887504578,
                                               0.11597100645303726,
                                               0.1160222664475441,
                                               0.11600326001644135,
                                               0.11590395122766495,
                                               0.11599953472614288,
                                               0.11587393283843994,
                                               0.1161334291100502,
                                               0.11603137850761414,
                                               0.11602283269166946,
                                               0.11603374034166336,
                                               0.11604278534650803,
                                               0.11608204990625381,
                                               0.09147002547979355,
                                               0.09143483638763428,
                                               0.09163686633110046,
                                               0.09171552211046219,
                                               0.09144914895296097,
                                               0.09174562990665436,
                                               0.091587595641613,
                                               0.09154273569583893,
                                               0.09181218594312668,
                                               0.09172553569078445,
                                               0.09155931323766708,
                                               0.09146300703287125,
                                               0.09142749756574631,
                                               0.09154758602380753,
                                               0.09157166630029678,
                                               0.09156911820173264,
                                               0.09155336022377014,
                                               0.09153666347265244,
                                               0.09141728281974792,
                                               0.0918840542435646,
                                               0.09160088747739792,
                                               0.0916093960404396,
                                               0.09157142043113708,
                                               0.09171904623508453,
                                               0.0915781706571579,
                                               0.09155244380235672,
                                               0.09143797308206558,
                                               0.09141530841588974,
                                               0.09086361527442932,
                                               0.0905296728014946,
                                               0.09057653695344925,
                                               0.09012796729803085,
                                               0.09028708189725876,
                                               0.09044957906007767,
                                               0.09055684506893158,
                                               0.0902361124753952,
                                               0.09055016934871674,
                                               0.09031613171100616,
                                               0.0902179628610611,
                                               0.09052954614162445,
                                               0.0900455117225647,
                                               0.0902295783162117,
                                               0.09020984917879105,
                                               0.09043499827384949,
                                               0.09051758050918579,
                                               0.0904468297958374,
                                               0.09017684310674667,
                                               0.0902678519487381,
                                               0.09037574380636215,
                                               0.0904354378581047,
                                               0.09113160520792007,
                                               0.09062957018613815,
                                               0.09017972648143768,
                                               0.09062566608190536,
                                               0.09158965945243835,
                                               0.091673843562603,
                                               0.0899815633893013,
                                               0.09016478061676025,
                                               0.0909065455198288,
                                               0.09008615463972092,
                                               0.09036406874656677,
                                               0.09002280980348587,
                                               0.09014122933149338,
                                               0.09028293937444687,
                                               0.0899188220500946,
                                               0.08988888561725616,
                                               0.09009919315576553,
                                               0.09167507290840149,
                                               0.09156660735607147,
                                               0.09164106100797653,
                                               0.09177406877279282,
                                               0.09153330326080322,
                                               0.09173539280891418,
                                               0.09170960634946823,
                                               0.09177560359239578,
                                               0.09162052720785141,
                                               0.09143483638763428,
                                               0.09150806069374084,
                                               0.09180814772844315,
                                               0.09179021418094635,
                                               0.09148918092250824,
                                               0.09165149182081223,
                                               0.09149252623319626,
                                               0.09190665930509567,
                                               0.0916096493601799,
                                               0.09160439670085907,
                                               0.09168170392513275,
                                               0.09163866192102432,
                                               0.09190444648265839,
                                               0.091668039560318,
                                               0.09162873029708862,
                                               0.09173496067523956,
                                               0.09242351353168488,
                                               0.09048132598400116,
                                               0.09090138971805573,
                                               0.0904548242688179,
                                               0.09145405888557434,
                                               0.09131670743227005,
                                               0.09178461879491806,
                                               0.09153732657432556,
                                               0.08973786979913712,
                                               0.08988428860902786,
                                               0.09021115303039551,
                                               0.09000113606452942,
                                               0.08976569026708603,
                                               0.08987715095281601,
                                               0.089874766767025,
                                               0.08953355252742767,
                                               0.08985365927219391,
                                               0.08929459005594254,
                                               0.09014719724655151,
                                               0.0901370495557785,
                                               0.08988743275403976,
                                               0.09002864360809326,
                                               0.08994632959365845,
                                               0.09004314243793488,
                                               0.08995210379362106,
                                               0.09007275104522705,
                                               0.09029345214366913,
                                               0.090416319668293,
                                               0.0899377390742302,
                                               0.08996004611253738,
                                               0.08988483250141144,
                                               0.08986671268939972,
                                               0.0900193452835083,
                                               0.0899295061826706,
                                               0.08994332700967789,
                                               0.08973976969718933,
                                               0.08997202664613724,
                                               0.09010780602693558,
                                               0.08996426314115524,
                                               0.08962488174438477,
                                               0.0902571827173233,
                                               0.08992582559585571,
                                               0.0900769904255867,
                                               0.0898761972784996,
                                               0.09017913788557053,
                                               0.08997853100299835,
                                               0.08989366888999939,
                                               0.09010732173919678,
                                               0.09005895256996155,
                                               0.09000539779663086,
                                               0.08995525538921356,
                                               0.08972669392824173,
                                               0.09011795371770859,
                                               0.09001778066158295,
                                               0.09011020511388779,
                                               0.08981508761644363,
                                               0.09017821401357651,
                                               0.08959657698869705,
                                               0.08875498175621033,
                                               0.08870062232017517,
                                               0.08880362659692764,
                                               0.08918870985507965,
                                               0.08892866224050522,
                                               0.08985244482755661,
                                               0.08982453495264053,
                                               0.09029696136713028,
                                               0.09009458869695663,
                                               0.09013500809669495,
                                               0.09010349214076996,
                                               0.08994513750076294,
                                               0.08971379697322845,
                                               0.09011200815439224,
                                               0.0903470441699028,
                                               0.09037244319915771,
                                               0.09014169126749039,
                                               0.08994632959365845,
                                               0.08982060849666595,
                                               0.08958274871110916,
                                               0.08996281772851944,
                                               0.08989058434963226,
                                               0.08994904905557632,
                                               0.0900254026055336,
                                               0.08989007025957108,
                                               0.08997174352407455,
                                               0.08993948996067047,
                                               0.08963050693273544,
                                               0.08996085822582245,
                                               0.09006138890981674,
                                               0.09014187753200531,
                                               0.09013525396585464,
                                               0.08976943790912628,
                                               0.0901322066783905,
                                               0.08987969905138016,
                                               0.08936253190040588,
                                               0.08984756469726562,
                                               0.08993438631296158,
                                               0.0899040624499321,
                                               0.09000980108976364,
                                               0.08996778726577759,
                                               0.08943293988704681,
                                               0.09004877507686615,
                                               0.08976338803768158,
                                               0.08978376537561417,
                                               0.09000024944543839,
                                               0.08958359062671661,
                                               0.08903425931930542,
                                               0.09001847356557846,
                                               0.08996066451072693],
                          'weight_decay': 0.0005},
                 'macroF1': {'accuracy': 3.0,
                             'batch_size': 32,
                             'cv_score': 0.005336667319455597,
                             'cv_val_accuracy': 2.0,
                             'cv_val_loss': 0.0991933469971021,
                             'cv_val_macroF1': 0.005336667319455597,
                             'cv_val_microF1': 0.066966913828706,
                             'epochs': 100,
                             'final_model': GGNN1(
  (ggnn): GatedGraphConv(60, num_layers=2)
  (fc1): Linear(in_features=60, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                             'kwargs': {'aggr_type': 'mean',
                                        'd1': 60,
                                        'd2': 50,
                                        'num_classes': 24,
                                        'num_layers': 2},
                             'learning_rate': 0.01,
                             'macroF1': 0.006754945585160564,
                             'microF1': 0.08414023372287145,
                             'model': <class 'TFM_graph_classification_models.GGNN1'>,
                             'model_instance': GGNN1(
  (ggnn): GatedGraphConv(60, num_layers=2)
  (fc1): Linear(in_features=60, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                             'score': 'f1_macro',
                             'time': 1386.8977355957031,
                             'train_loss_history': [1930582528.0,
                                                    5816673280.0,
                                                    143432992.0,
                                                    401414400.0,
                                                    0.08453511446714401,
                                                    0.09732901304960251,
                                                    0.08421100676059723,
                                                    179.138427734375,
                                                    0.08405710756778717,
                                                    0.09024409204721451,
                                                    0.08389213681221008,
                                                    0.08385269343852997,
                                                    0.08379144221544266,
                                                    1.6190478801727295,
                                                    0.0840674564242363,
                                                    0.0839853435754776,
                                                    0.08404428511857986,
                                                    0.08388674259185791,
                                                    0.08402787148952484,
                                                    0.08394353091716766,
                                                    10159.8349609375,
                                                    0.08386019617319107,
                                                    0.08378172665834427,
                                                    0.08385205268859863,
                                                    46.16153335571289,
                                                    0.08443432301282883,
                                                    0.08436936885118484,
                                                    0.0838686004281044,
                                                    0.08384396880865097,
                                                    0.0838937908411026,
                                                    0.08472973853349686,
                                                    0.08419333398342133,
                                                    0.08412699401378632,
                                                    0.08394647389650345,
                                                    0.08405932784080505,
                                                    0.08477608859539032,
                                                    0.08448567986488342,
                                                    0.08438331633806229,
                                                    0.08440301567316055,
                                                    0.08459638804197311,
                                                    0.08455643802881241,
                                                    0.08457781374454498,
                                                    0.0844746083021164,
                                                    1.218392252922058,
                                                    92.7166976928711,
                                                    0.08451459556818008,
                                                    0.0845295637845993,
                                                    0.08447882533073425,
                                                    0.08454097807407379,
                                                    0.08445699512958527,
                                                    686.81591796875,
                                                    0.08460459113121033,
                                                    143.66787719726562,
                                                    0.08449937403202057,
                                                    0.08453886210918427,
                                                    0.08456281572580338,
                                                    0.08450835943222046,
                                                    0.0844619944691658,
                                                    0.08456537127494812,
                                                    0.08452136069536209,
                                                    0.08452289551496506,
                                                    0.08451851457357407,
                                                    0.08440401405096054,
                                                    0.08451830595731735,
                                                    0.08450745046138763,
                                                    0.08453594148159027,
                                                    0.08453307300806046,
                                                    0.08445210754871368,
                                                    0.08442220091819763,
                                                    0.08446472138166428,
                                                    0.08453156054019928,
                                                    0.08442430943250656,
                                                    0.08437874913215637,
                                                    0.0843782126903534,
                                                    0.08437833935022354,
                                                    0.08440952003002167,
                                                    0.08439227938652039,
                                                    0.08432715386152267,
                                                    0.08439390361309052,
                                                    4549.99755859375,
                                                    0.0845685750246048,
                                                    0.0844576433300972,
                                                    0.08457537740468979,
                                                    0.08446572721004486,
                                                    0.08438173681497574,
                                                    0.08444757014513016,
                                                    0.08448383957147598,
                                                    0.08457212895154953,
                                                    0.08460312336683273,
                                                    0.0844893679022789,
                                                    0.08443479239940643,
                                                    0.08454051613807678,
                                                    0.0845152959227562,
                                                    0.0845574289560318,
                                                    0.08444813638925552,
                                                    0.08446840941905975,
                                                    0.08447453379631042,
                                                    0.08441149443387985,
                                                    0.0844496637582779,
                                                    0.08445731550455093,
                                                    0.09589837491512299,
                                                    76.78620910644531,
                                                    0.09452478587627411,
                                                    0.09451538324356079,
                                                    3594.9453125,
                                                    0.09456611424684525,
                                                    0.09456440061330795,
                                                    0.09461269527673721,
                                                    0.09654979407787323,
                                                    0.09458601474761963,
                                                    0.09456944465637207,
                                                    0.09449096769094467,
                                                    0.09453420341014862,
                                                    0.09456656128168106,
                                                    0.0945281833410263,
                                                    0.09457744657993317,
                                                    0.09452137351036072,
                                                    3293.878662109375,
                                                    0.0944976657629013,
                                                    0.09452024847269058,
                                                    0.09451670944690704,
                                                    0.09454703330993652,
                                                    2545.70849609375,
                                                    0.09458226710557938,
                                                    0.09455703943967819,
                                                    0.09453553706407547,
                                                    0.3361894488334656,
                                                    0.09449301660060883,
                                                    0.19670675694942474,
                                                    0.1361423134803772,
                                                    0.09369964897632599,
                                                    0.09379451721906662,
                                                    0.09369710832834244,
                                                    0.09350265562534332,
                                                    0.09347912669181824,
                                                    0.09346462041139603,
                                                    0.0935664027929306,
                                                    0.09353747218847275,
                                                    0.09344299137592316,
                                                    0.09350886195898056,
                                                    0.46246740221977234,
                                                    0.0934191569685936,
                                                    0.09330388158559799,
                                                    0.09347691386938095,
                                                    0.09354433417320251,
                                                    0.09353521466255188,
                                                    0.09354806691408157,
                                                    4.875348091125488,
                                                    0.09350661933422089,
                                                    0.09341655671596527,
                                                    0.09375137090682983,
                                                    0.09359747171401978,
                                                    0.09344623237848282,
                                                    0.09351751208305359,
                                                    0.09441114217042923,
                                                    0.09437780827283859,
                                                    0.09366247802972794,
                                                    0.09349711984395981,
                                                    0.09343573451042175,
                                                    0.09347439557313919,
                                                    0.09343110024929047,
                                                    0.09334275126457214,
                                                    0.09329673647880554,
                                                    0.09324946254491806,
                                                    0.09332489967346191,
                                                    0.09335567057132721,
                                                    0.09336070716381073,
                                                    0.09327112883329391,
                                                    0.09470103681087494,
                                                    0.0943952202796936,
                                                    223.92398071289062,
                                                    0.2761259078979492,
                                                    568.1837158203125,
                                                    0.09453856199979782,
                                                    3.640625476837158,
                                                    8.571806907653809,
                                                    0.09439113736152649,
                                                    0.09438219666481018,
                                                    0.09455494582653046,
                                                    0.09456533193588257,
                                                    0.09450738877058029,
                                                    0.09454187005758286,
                                                    0.09451670944690704,
                                                    0.28767964243888855,
                                                    0.09446129947900772,
                                                    0.09447275102138519,
                                                    261.5072937011719,
                                                    0.5570076704025269,
                                                    351.6172790527344,
                                                    223.679931640625,
                                                    0.09456315636634827,
                                                    1275.6224365234375,
                                                    996.783203125,
                                                    468.9762878417969,
                                                    0.09383884072303772,
                                                    0.09372755140066147,
                                                    0.09372410923242569,
                                                    0.4396379292011261,
                                                    1.5778316259384155,
                                                    0.09457020461559296,
                                                    0.4057060182094574,
                                                    0.09465852379798889,
                                                    0.09467104822397232,
                                                    804.968017578125,
                                                    0.09458814561367035,
                                                    0.09467726200819016,
                                                    0.16680659353733063,
                                                    0.09450245648622513,
                                                    0.09450561553239822,
                                                    0.09438183158636093,
                                                    0.09459953755140305,
                                                    0.09469465166330338,
                                                    0.09475258737802505,
                                                    0.09472335129976273,
                                                    0.09469188004732132,
                                                    0.09467725455760956,
                                                    0.09465447068214417,
                                                    107.36640167236328,
                                                    0.09468750655651093,
                                                    0.09464620053768158,
                                                    0.09461190551519394,
                                                    8.436896324157715,
                                                    0.8304502367973328,
                                                    0.09465745091438293,
                                                    0.09457870572805405,
                                                    0.09460476040840149,
                                                    0.5350560545921326,
                                                    0.09462569653987885,
                                                    12.481863975524902,
                                                    0.09459514170885086,
                                                    0.09474245458841324,
                                                    0.09460768103599548,
                                                    1.460088849067688,
                                                    0.09470023959875107,
                                                    0.0946817472577095,
                                                    39852.96484375,
                                                    0.14213499426841736,
                                                    2483.219970703125,
                                                    0.09459136426448822,
                                                    10.684162139892578,
                                                    0.09467890858650208,
                                                    0.09464482218027115,
                                                    0.09463319927453995,
                                                    305.5508728027344,
                                                    0.09451852738857269,
                                                    266.2228088378906,
                                                    0.09464354813098907,
                                                    0.09453378617763519,
                                                    0.09454832971096039,
                                                    47629.37109375,
                                                    0.09382311999797821,
                                                    0.0934765487909317,
                                                    0.09329262375831604,
                                                    0.09430080652236938,
                                                    0.09367848187685013,
                                                    2579.875732421875,
                                                    0.5042579770088196,
                                                    0.09466729313135147,
                                                    0.094603031873703,
                                                    0.09465199708938599,
                                                    0.09458494931459427,
                                                    0.09463470429182053,
                                                    10456.1650390625,
                                                    0.09460638463497162,
                                                    0.09465428441762924,
                                                    0.09468037635087967,
                                                    0.09464714676141739,
                                                    0.9984434247016907,
                                                    718732.0625,
                                                    0.09467263519763947,
                                                    786.500244140625,
                                                    0.0946587324142456,
                                                    25.26593589782715,
                                                    0.0946255475282669,
                                                    0.8189891576766968,
                                                    0.09464100748300552,
                                                    0.20862728357315063,
                                                    212.48797607421875,
                                                    0.11569085717201233,
                                                    0.09459014981985092,
                                                    0.10856836289167404,
                                                    10806.7138671875,
                                                    0.09449947625398636,
                                                    0.27257049083709717,
                                                    0.09450982511043549,
                                                    0.09409075230360031,
                                                    3391.76318359375,
                                                    0.09542278200387955,
                                                    3508036.0,
                                                    0.0946173369884491,
                                                    0.09459066390991211,
                                                    0.0945025086402893,
                                                    0.3716455399990082,
                                                    0.09453834593296051,
                                                    0.09462703764438629,
                                                    0.09462379664182663,
                                                    5380.2314453125,
                                                    0.09367762506008148,
                                                    0.09457611292600632,
                                                    0.0945659652352333],
                             'val_accuracy_history': [1.105263157894737,
                                                      1.0526315789473684,
                                                      0.15789473684210525,
                                                      0.9473684210526315,
                                                      0.8947368421052632,
                                                      0.3684210526315789,
                                                      0.631578947368421,
                                                      1.0526315789473684,
                                                      1.0526315789473684,
                                                      0.631578947368421,
                                                      1.0,
                                                      1.4736842105263157,
                                                      0.3157894736842105,
                                                      1.4210526315789473,
                                                      0.7894736842105263,
                                                      1.3157894736842106,
                                                      0.5789473684210527,
                                                      1.263157894736842,
                                                      0.631578947368421,
                                                      1.631578947368421,
                                                      1.105263157894737,
                                                      0.42105263157894735,
                                                      1.2105263157894737,
                                                      1.1578947368421053,
                                                      1.0,
                                                      2.0,
                                                      1.0526315789473684,
                                                      0.05263157894736842,
                                                      1.368421052631579,
                                                      0.21052631578947367,
                                                      0.0,
                                                      1.1578947368421053,
                                                      1.263157894736842,
                                                      1.5263157894736843,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      4.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      3.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      3.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      2.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.8421052631578947,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      3.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      3.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      3.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.1111111111111111,
                                                      0.4444444444444444,
                                                      0.0,
                                                      1.7777777777777777,
                                                      0.0,
                                                      0.7777777777777778,
                                                      0.0,
                                                      0.0,
                                                      0.8888888888888888,
                                                      0.0,
                                                      1.1111111111111112,
                                                      0.4444444444444444,
                                                      0.1111111111111111,
                                                      0.0,
                                                      0.1111111111111111,
                                                      0.2222222222222222,
                                                      0.1111111111111111,
                                                      0.8888888888888888,
                                                      0.7777777777777778,
                                                      0.8888888888888888,
                                                      0.1111111111111111,
                                                      0.2222222222222222,
                                                      1.3333333333333333,
                                                      0.7777777777777778,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.3333333333333333,
                                                      0.0,
                                                      0.3333333333333333,
                                                      0.0,
                                                      1.2222222222222223,
                                                      0.1111111111111111,
                                                      0.0,
                                                      1.3333333333333333,
                                                      0.6666666666666666,
                                                      0.2222222222222222,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      3.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.4444444444444444,
                                                      1.5555555555555556,
                                                      0.4444444444444444,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      4.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      0.7368421052631579,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      3.0,
                                                      0.0,
                                                      0.0,
                                                      3.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      3.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      3.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      3.0,
                                                      2.0,
                                                      2.0,
                                                      3.0,
                                                      4.0,
                                                      3.0,
                                                      3.0,
                                                      1.0,
                                                      1.0,
                                                      1.368421052631579,
                                                      2.736842105263158,
                                                      1.1578947368421053,
                                                      1.6842105263157894,
                                                      1.9473684210526316,
                                                      1.8421052631578947,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      3.0,
                                                      1.0,
                                                      0.0,
                                                      5.0,
                                                      1.0,
                                                      4.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      4.0,
                                                      1.0,
                                                      1.0,
                                                      3.0,
                                                      1.0,
                                                      2.0,
                                                      0.3684210526315789,
                                                      5.0,
                                                      1.0,
                                                      3.0,
                                                      2.0,
                                                      0.0,
                                                      2.0,
                                                      3.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      2.6315789473684212,
                                                      1.3157894736842106,
                                                      2.0,
                                                      3.0],
                             'val_loss': 0.08996066451072693,
                             'val_loss_history': [1034503488.0,
                                                  847939968.0,
                                                  237004064.0,
                                                  0.1263040006160736,
                                                  0.1310095340013504,
                                                  0.1337033212184906,
                                                  0.12615786492824554,
                                                  0.12673823535442352,
                                                  0.12818212807178497,
                                                  0.12805397808551788,
                                                  0.12788242101669312,
                                                  0.12742243707180023,
                                                  0.12486802786588669,
                                                  0.12135627865791321,
                                                  0.1297132968902588,
                                                  0.12731346487998962,
                                                  0.12576653063297272,
                                                  0.11891531199216843,
                                                  0.12929166853427887,
                                                  0.12054215371608734,
                                                  0.1322227418422699,
                                                  0.11912821978330612,
                                                  0.12460339814424515,
                                                  0.1240234524011612,
                                                  0.11438535153865814,
                                                  0.11396811902523041,
                                                  0.11890871822834015,
                                                  0.12567488849163055,
                                                  0.12431469559669495,
                                                  0.12262961268424988,
                                                  0.114720918238163,
                                                  0.12223060429096222,
                                                  0.12178576737642288,
                                                  0.12323581427335739,
                                                  0.11141052842140198,
                                                  0.11490409821271896,
                                                  0.11491867899894714,
                                                  0.11480910331010818,
                                                  0.11481461673974991,
                                                  0.11511991918087006,
                                                  0.11507649719715118,
                                                  0.11513196676969528,
                                                  0.11517633497714996,
                                                  0.11511987447738647,
                                                  0.11503130942583084,
                                                  0.11513296514749527,
                                                  0.11526677012443542,
                                                  0.11538837105035782,
                                                  0.11536581069231033,
                                                  0.11482106149196625,
                                                  0.11543918401002884,
                                                  0.1155194565653801,
                                                  0.11556079238653183,
                                                  0.11545369774103165,
                                                  0.11537531018257141,
                                                  0.11555165797472,
                                                  0.11537749320268631,
                                                  0.11569669842720032,
                                                  0.11556733399629593,
                                                  0.11561307311058044,
                                                  0.11569474637508392,
                                                  0.11569544672966003,
                                                  0.11562191694974899,
                                                  0.11569502204656601,
                                                  0.11571890860795975,
                                                  0.11563991010189056,
                                                  0.11592783033847809,
                                                  0.11566539108753204,
                                                  0.11559756100177765,
                                                  0.11567602306604385,
                                                  0.11575625836849213,
                                                  0.1158311516046524,
                                                  0.11566069722175598,
                                                  0.11581201106309891,
                                                  0.11579643934965134,
                                                  0.11583234369754791,
                                                  0.11580903083086014,
                                                  0.11596063524484634,
                                                  0.11589496582746506,
                                                  0.11581369489431381,
                                                  0.11581697314977646,
                                                  0.11586833000183105,
                                                  0.11823365092277527,
                                                  0.11571573466062546,
                                                  0.1159466952085495,
                                                  0.11612705141305923,
                                                  0.11606323719024658,
                                                  0.11564084887504578,
                                                  0.11597100645303726,
                                                  0.1160222664475441,
                                                  0.11600326001644135,
                                                  0.11590395122766495,
                                                  0.11599953472614288,
                                                  0.11587393283843994,
                                                  0.1161334291100502,
                                                  0.11603137850761414,
                                                  0.11602283269166946,
                                                  0.11603374034166336,
                                                  0.11604278534650803,
                                                  0.11608204990625381,
                                                  0.09147002547979355,
                                                  0.09143483638763428,
                                                  0.09163686633110046,
                                                  0.09171552211046219,
                                                  0.09144914895296097,
                                                  0.09174562990665436,
                                                  0.091587595641613,
                                                  0.09154273569583893,
                                                  0.09181218594312668,
                                                  0.09172553569078445,
                                                  0.09155931323766708,
                                                  0.09146300703287125,
                                                  0.09142749756574631,
                                                  0.09154758602380753,
                                                  0.09157166630029678,
                                                  0.09156911820173264,
                                                  0.09155336022377014,
                                                  0.09153666347265244,
                                                  0.09141728281974792,
                                                  0.0918840542435646,
                                                  0.09160088747739792,
                                                  0.0916093960404396,
                                                  0.09157142043113708,
                                                  0.09171904623508453,
                                                  0.0915781706571579,
                                                  0.09155244380235672,
                                                  0.09143797308206558,
                                                  0.09141530841588974,
                                                  0.09086361527442932,
                                                  0.0905296728014946,
                                                  0.09057653695344925,
                                                  0.09012796729803085,
                                                  0.09028708189725876,
                                                  0.09044957906007767,
                                                  0.09055684506893158,
                                                  0.0902361124753952,
                                                  0.09055016934871674,
                                                  0.09031613171100616,
                                                  0.0902179628610611,
                                                  0.09052954614162445,
                                                  0.0900455117225647,
                                                  0.0902295783162117,
                                                  0.09020984917879105,
                                                  0.09043499827384949,
                                                  0.09051758050918579,
                                                  0.0904468297958374,
                                                  0.09017684310674667,
                                                  0.0902678519487381,
                                                  0.09037574380636215,
                                                  0.0904354378581047,
                                                  0.09113160520792007,
                                                  0.09062957018613815,
                                                  0.09017972648143768,
                                                  0.09062566608190536,
                                                  0.09158965945243835,
                                                  0.091673843562603,
                                                  0.0899815633893013,
                                                  0.09016478061676025,
                                                  0.0909065455198288,
                                                  0.09008615463972092,
                                                  0.09036406874656677,
                                                  0.09002280980348587,
                                                  0.09014122933149338,
                                                  0.09028293937444687,
                                                  0.0899188220500946,
                                                  0.08988888561725616,
                                                  0.09009919315576553,
                                                  0.09167507290840149,
                                                  0.09156660735607147,
                                                  0.09164106100797653,
                                                  0.09177406877279282,
                                                  0.09153330326080322,
                                                  0.09173539280891418,
                                                  0.09170960634946823,
                                                  0.09177560359239578,
                                                  0.09162052720785141,
                                                  0.09143483638763428,
                                                  0.09150806069374084,
                                                  0.09180814772844315,
                                                  0.09179021418094635,
                                                  0.09148918092250824,
                                                  0.09165149182081223,
                                                  0.09149252623319626,
                                                  0.09190665930509567,
                                                  0.0916096493601799,
                                                  0.09160439670085907,
                                                  0.09168170392513275,
                                                  0.09163866192102432,
                                                  0.09190444648265839,
                                                  0.091668039560318,
                                                  0.09162873029708862,
                                                  0.09173496067523956,
                                                  0.09242351353168488,
                                                  0.09048132598400116,
                                                  0.09090138971805573,
                                                  0.0904548242688179,
                                                  0.09145405888557434,
                                                  0.09131670743227005,
                                                  0.09178461879491806,
                                                  0.09153732657432556,
                                                  0.08973786979913712,
                                                  0.08988428860902786,
                                                  0.09021115303039551,
                                                  0.09000113606452942,
                                                  0.08976569026708603,
                                                  0.08987715095281601,
                                                  0.089874766767025,
                                                  0.08953355252742767,
                                                  0.08985365927219391,
                                                  0.08929459005594254,
                                                  0.09014719724655151,
                                                  0.0901370495557785,
                                                  0.08988743275403976,
                                                  0.09002864360809326,
                                                  0.08994632959365845,
                                                  0.09004314243793488,
                                                  0.08995210379362106,
                                                  0.09007275104522705,
                                                  0.09029345214366913,
                                                  0.090416319668293,
                                                  0.0899377390742302,
                                                  0.08996004611253738,
                                                  0.08988483250141144,
                                                  0.08986671268939972,
                                                  0.0900193452835083,
                                                  0.0899295061826706,
                                                  0.08994332700967789,
                                                  0.08973976969718933,
                                                  0.08997202664613724,
                                                  0.09010780602693558,
                                                  0.08996426314115524,
                                                  0.08962488174438477,
                                                  0.0902571827173233,
                                                  0.08992582559585571,
                                                  0.0900769904255867,
                                                  0.0898761972784996,
                                                  0.09017913788557053,
                                                  0.08997853100299835,
                                                  0.08989366888999939,
                                                  0.09010732173919678,
                                                  0.09005895256996155,
                                                  0.09000539779663086,
                                                  0.08995525538921356,
                                                  0.08972669392824173,
                                                  0.09011795371770859,
                                                  0.09001778066158295,
                                                  0.09011020511388779,
                                                  0.08981508761644363,
                                                  0.09017821401357651,
                                                  0.08959657698869705,
                                                  0.08875498175621033,
                                                  0.08870062232017517,
                                                  0.08880362659692764,
                                                  0.08918870985507965,
                                                  0.08892866224050522,
                                                  0.08985244482755661,
                                                  0.08982453495264053,
                                                  0.09029696136713028,
                                                  0.09009458869695663,
                                                  0.09013500809669495,
                                                  0.09010349214076996,
                                                  0.08994513750076294,
                                                  0.08971379697322845,
                                                  0.09011200815439224,
                                                  0.0903470441699028,
                                                  0.09037244319915771,
                                                  0.09014169126749039,
                                                  0.08994632959365845,
                                                  0.08982060849666595,
                                                  0.08958274871110916,
                                                  0.08996281772851944,
                                                  0.08989058434963226,
                                                  0.08994904905557632,
                                                  0.0900254026055336,
                                                  0.08989007025957108,
                                                  0.08997174352407455,
                                                  0.08993948996067047,
                                                  0.08963050693273544,
                                                  0.08996085822582245,
                                                  0.09006138890981674,
                                                  0.09014187753200531,
                                                  0.09013525396585464,
                                                  0.08976943790912628,
                                                  0.0901322066783905,
                                                  0.08987969905138016,
                                                  0.08936253190040588,
                                                  0.08984756469726562,
                                                  0.08993438631296158,
                                                  0.0899040624499321,
                                                  0.09000980108976364,
                                                  0.08996778726577759,
                                                  0.08943293988704681,
                                                  0.09004877507686615,
                                                  0.08976338803768158,
                                                  0.08978376537561417,
                                                  0.09000024944543839,
                                                  0.08958359062671661,
                                                  0.08903425931930542,
                                                  0.09001847356557846,
                                                  0.08996066451072693],
                             'weight_decay': 0.0005},
                 'microF1': {'accuracy': 3.0,
                             'batch_size': 32,
                             'cv_score': 0.005336667319455597,
                             'cv_val_accuracy': 2.0,
                             'cv_val_loss': 0.0991933469971021,
                             'cv_val_macroF1': 0.005336667319455597,
                             'cv_val_microF1': 0.066966913828706,
                             'epochs': 100,
                             'final_model': GGNN1(
  (ggnn): GatedGraphConv(60, num_layers=2)
  (fc1): Linear(in_features=60, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                             'kwargs': {'aggr_type': 'mean',
                                        'd1': 60,
                                        'd2': 50,
                                        'num_classes': 24,
                                        'num_layers': 2},
                             'learning_rate': 0.01,
                             'macroF1': 0.006754945585160564,
                             'microF1': 0.08414023372287145,
                             'model': <class 'TFM_graph_classification_models.GGNN1'>,
                             'model_instance': GGNN1(
  (ggnn): GatedGraphConv(60, num_layers=2)
  (fc1): Linear(in_features=60, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                             'score': 'f1_macro',
                             'time': 1386.8977355957031,
                             'train_loss_history': [1930582528.0,
                                                    5816673280.0,
                                                    143432992.0,
                                                    401414400.0,
                                                    0.08453511446714401,
                                                    0.09732901304960251,
                                                    0.08421100676059723,
                                                    179.138427734375,
                                                    0.08405710756778717,
                                                    0.09024409204721451,
                                                    0.08389213681221008,
                                                    0.08385269343852997,
                                                    0.08379144221544266,
                                                    1.6190478801727295,
                                                    0.0840674564242363,
                                                    0.0839853435754776,
                                                    0.08404428511857986,
                                                    0.08388674259185791,
                                                    0.08402787148952484,
                                                    0.08394353091716766,
                                                    10159.8349609375,
                                                    0.08386019617319107,
                                                    0.08378172665834427,
                                                    0.08385205268859863,
                                                    46.16153335571289,
                                                    0.08443432301282883,
                                                    0.08436936885118484,
                                                    0.0838686004281044,
                                                    0.08384396880865097,
                                                    0.0838937908411026,
                                                    0.08472973853349686,
                                                    0.08419333398342133,
                                                    0.08412699401378632,
                                                    0.08394647389650345,
                                                    0.08405932784080505,
                                                    0.08477608859539032,
                                                    0.08448567986488342,
                                                    0.08438331633806229,
                                                    0.08440301567316055,
                                                    0.08459638804197311,
                                                    0.08455643802881241,
                                                    0.08457781374454498,
                                                    0.0844746083021164,
                                                    1.218392252922058,
                                                    92.7166976928711,
                                                    0.08451459556818008,
                                                    0.0845295637845993,
                                                    0.08447882533073425,
                                                    0.08454097807407379,
                                                    0.08445699512958527,
                                                    686.81591796875,
                                                    0.08460459113121033,
                                                    143.66787719726562,
                                                    0.08449937403202057,
                                                    0.08453886210918427,
                                                    0.08456281572580338,
                                                    0.08450835943222046,
                                                    0.0844619944691658,
                                                    0.08456537127494812,
                                                    0.08452136069536209,
                                                    0.08452289551496506,
                                                    0.08451851457357407,
                                                    0.08440401405096054,
                                                    0.08451830595731735,
                                                    0.08450745046138763,
                                                    0.08453594148159027,
                                                    0.08453307300806046,
                                                    0.08445210754871368,
                                                    0.08442220091819763,
                                                    0.08446472138166428,
                                                    0.08453156054019928,
                                                    0.08442430943250656,
                                                    0.08437874913215637,
                                                    0.0843782126903534,
                                                    0.08437833935022354,
                                                    0.08440952003002167,
                                                    0.08439227938652039,
                                                    0.08432715386152267,
                                                    0.08439390361309052,
                                                    4549.99755859375,
                                                    0.0845685750246048,
                                                    0.0844576433300972,
                                                    0.08457537740468979,
                                                    0.08446572721004486,
                                                    0.08438173681497574,
                                                    0.08444757014513016,
                                                    0.08448383957147598,
                                                    0.08457212895154953,
                                                    0.08460312336683273,
                                                    0.0844893679022789,
                                                    0.08443479239940643,
                                                    0.08454051613807678,
                                                    0.0845152959227562,
                                                    0.0845574289560318,
                                                    0.08444813638925552,
                                                    0.08446840941905975,
                                                    0.08447453379631042,
                                                    0.08441149443387985,
                                                    0.0844496637582779,
                                                    0.08445731550455093,
                                                    0.09589837491512299,
                                                    76.78620910644531,
                                                    0.09452478587627411,
                                                    0.09451538324356079,
                                                    3594.9453125,
                                                    0.09456611424684525,
                                                    0.09456440061330795,
                                                    0.09461269527673721,
                                                    0.09654979407787323,
                                                    0.09458601474761963,
                                                    0.09456944465637207,
                                                    0.09449096769094467,
                                                    0.09453420341014862,
                                                    0.09456656128168106,
                                                    0.0945281833410263,
                                                    0.09457744657993317,
                                                    0.09452137351036072,
                                                    3293.878662109375,
                                                    0.0944976657629013,
                                                    0.09452024847269058,
                                                    0.09451670944690704,
                                                    0.09454703330993652,
                                                    2545.70849609375,
                                                    0.09458226710557938,
                                                    0.09455703943967819,
                                                    0.09453553706407547,
                                                    0.3361894488334656,
                                                    0.09449301660060883,
                                                    0.19670675694942474,
                                                    0.1361423134803772,
                                                    0.09369964897632599,
                                                    0.09379451721906662,
                                                    0.09369710832834244,
                                                    0.09350265562534332,
                                                    0.09347912669181824,
                                                    0.09346462041139603,
                                                    0.0935664027929306,
                                                    0.09353747218847275,
                                                    0.09344299137592316,
                                                    0.09350886195898056,
                                                    0.46246740221977234,
                                                    0.0934191569685936,
                                                    0.09330388158559799,
                                                    0.09347691386938095,
                                                    0.09354433417320251,
                                                    0.09353521466255188,
                                                    0.09354806691408157,
                                                    4.875348091125488,
                                                    0.09350661933422089,
                                                    0.09341655671596527,
                                                    0.09375137090682983,
                                                    0.09359747171401978,
                                                    0.09344623237848282,
                                                    0.09351751208305359,
                                                    0.09441114217042923,
                                                    0.09437780827283859,
                                                    0.09366247802972794,
                                                    0.09349711984395981,
                                                    0.09343573451042175,
                                                    0.09347439557313919,
                                                    0.09343110024929047,
                                                    0.09334275126457214,
                                                    0.09329673647880554,
                                                    0.09324946254491806,
                                                    0.09332489967346191,
                                                    0.09335567057132721,
                                                    0.09336070716381073,
                                                    0.09327112883329391,
                                                    0.09470103681087494,
                                                    0.0943952202796936,
                                                    223.92398071289062,
                                                    0.2761259078979492,
                                                    568.1837158203125,
                                                    0.09453856199979782,
                                                    3.640625476837158,
                                                    8.571806907653809,
                                                    0.09439113736152649,
                                                    0.09438219666481018,
                                                    0.09455494582653046,
                                                    0.09456533193588257,
                                                    0.09450738877058029,
                                                    0.09454187005758286,
                                                    0.09451670944690704,
                                                    0.28767964243888855,
                                                    0.09446129947900772,
                                                    0.09447275102138519,
                                                    261.5072937011719,
                                                    0.5570076704025269,
                                                    351.6172790527344,
                                                    223.679931640625,
                                                    0.09456315636634827,
                                                    1275.6224365234375,
                                                    996.783203125,
                                                    468.9762878417969,
                                                    0.09383884072303772,
                                                    0.09372755140066147,
                                                    0.09372410923242569,
                                                    0.4396379292011261,
                                                    1.5778316259384155,
                                                    0.09457020461559296,
                                                    0.4057060182094574,
                                                    0.09465852379798889,
                                                    0.09467104822397232,
                                                    804.968017578125,
                                                    0.09458814561367035,
                                                    0.09467726200819016,
                                                    0.16680659353733063,
                                                    0.09450245648622513,
                                                    0.09450561553239822,
                                                    0.09438183158636093,
                                                    0.09459953755140305,
                                                    0.09469465166330338,
                                                    0.09475258737802505,
                                                    0.09472335129976273,
                                                    0.09469188004732132,
                                                    0.09467725455760956,
                                                    0.09465447068214417,
                                                    107.36640167236328,
                                                    0.09468750655651093,
                                                    0.09464620053768158,
                                                    0.09461190551519394,
                                                    8.436896324157715,
                                                    0.8304502367973328,
                                                    0.09465745091438293,
                                                    0.09457870572805405,
                                                    0.09460476040840149,
                                                    0.5350560545921326,
                                                    0.09462569653987885,
                                                    12.481863975524902,
                                                    0.09459514170885086,
                                                    0.09474245458841324,
                                                    0.09460768103599548,
                                                    1.460088849067688,
                                                    0.09470023959875107,
                                                    0.0946817472577095,
                                                    39852.96484375,
                                                    0.14213499426841736,
                                                    2483.219970703125,
                                                    0.09459136426448822,
                                                    10.684162139892578,
                                                    0.09467890858650208,
                                                    0.09464482218027115,
                                                    0.09463319927453995,
                                                    305.5508728027344,
                                                    0.09451852738857269,
                                                    266.2228088378906,
                                                    0.09464354813098907,
                                                    0.09453378617763519,
                                                    0.09454832971096039,
                                                    47629.37109375,
                                                    0.09382311999797821,
                                                    0.0934765487909317,
                                                    0.09329262375831604,
                                                    0.09430080652236938,
                                                    0.09367848187685013,
                                                    2579.875732421875,
                                                    0.5042579770088196,
                                                    0.09466729313135147,
                                                    0.094603031873703,
                                                    0.09465199708938599,
                                                    0.09458494931459427,
                                                    0.09463470429182053,
                                                    10456.1650390625,
                                                    0.09460638463497162,
                                                    0.09465428441762924,
                                                    0.09468037635087967,
                                                    0.09464714676141739,
                                                    0.9984434247016907,
                                                    718732.0625,
                                                    0.09467263519763947,
                                                    786.500244140625,
                                                    0.0946587324142456,
                                                    25.26593589782715,
                                                    0.0946255475282669,
                                                    0.8189891576766968,
                                                    0.09464100748300552,
                                                    0.20862728357315063,
                                                    212.48797607421875,
                                                    0.11569085717201233,
                                                    0.09459014981985092,
                                                    0.10856836289167404,
                                                    10806.7138671875,
                                                    0.09449947625398636,
                                                    0.27257049083709717,
                                                    0.09450982511043549,
                                                    0.09409075230360031,
                                                    3391.76318359375,
                                                    0.09542278200387955,
                                                    3508036.0,
                                                    0.0946173369884491,
                                                    0.09459066390991211,
                                                    0.0945025086402893,
                                                    0.3716455399990082,
                                                    0.09453834593296051,
                                                    0.09462703764438629,
                                                    0.09462379664182663,
                                                    5380.2314453125,
                                                    0.09367762506008148,
                                                    0.09457611292600632,
                                                    0.0945659652352333],
                             'val_accuracy_history': [1.105263157894737,
                                                      1.0526315789473684,
                                                      0.15789473684210525,
                                                      0.9473684210526315,
                                                      0.8947368421052632,
                                                      0.3684210526315789,
                                                      0.631578947368421,
                                                      1.0526315789473684,
                                                      1.0526315789473684,
                                                      0.631578947368421,
                                                      1.0,
                                                      1.4736842105263157,
                                                      0.3157894736842105,
                                                      1.4210526315789473,
                                                      0.7894736842105263,
                                                      1.3157894736842106,
                                                      0.5789473684210527,
                                                      1.263157894736842,
                                                      0.631578947368421,
                                                      1.631578947368421,
                                                      1.105263157894737,
                                                      0.42105263157894735,
                                                      1.2105263157894737,
                                                      1.1578947368421053,
                                                      1.0,
                                                      2.0,
                                                      1.0526315789473684,
                                                      0.05263157894736842,
                                                      1.368421052631579,
                                                      0.21052631578947367,
                                                      0.0,
                                                      1.1578947368421053,
                                                      1.263157894736842,
                                                      1.5263157894736843,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      4.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      3.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      3.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      2.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.8421052631578947,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      3.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      3.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      3.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.1111111111111111,
                                                      0.4444444444444444,
                                                      0.0,
                                                      1.7777777777777777,
                                                      0.0,
                                                      0.7777777777777778,
                                                      0.0,
                                                      0.0,
                                                      0.8888888888888888,
                                                      0.0,
                                                      1.1111111111111112,
                                                      0.4444444444444444,
                                                      0.1111111111111111,
                                                      0.0,
                                                      0.1111111111111111,
                                                      0.2222222222222222,
                                                      0.1111111111111111,
                                                      0.8888888888888888,
                                                      0.7777777777777778,
                                                      0.8888888888888888,
                                                      0.1111111111111111,
                                                      0.2222222222222222,
                                                      1.3333333333333333,
                                                      0.7777777777777778,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.3333333333333333,
                                                      0.0,
                                                      0.3333333333333333,
                                                      0.0,
                                                      1.2222222222222223,
                                                      0.1111111111111111,
                                                      0.0,
                                                      1.3333333333333333,
                                                      0.6666666666666666,
                                                      0.2222222222222222,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      3.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      0.0,
                                                      0.4444444444444444,
                                                      1.5555555555555556,
                                                      0.4444444444444444,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      0.0,
                                                      4.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      0.7368421052631579,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      3.0,
                                                      0.0,
                                                      0.0,
                                                      3.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      2.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      3.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      0.0,
                                                      3.0,
                                                      0.0,
                                                      1.0,
                                                      0.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      3.0,
                                                      2.0,
                                                      2.0,
                                                      3.0,
                                                      4.0,
                                                      3.0,
                                                      3.0,
                                                      1.0,
                                                      1.0,
                                                      1.368421052631579,
                                                      2.736842105263158,
                                                      1.1578947368421053,
                                                      1.6842105263157894,
                                                      1.9473684210526316,
                                                      1.8421052631578947,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      3.0,
                                                      1.0,
                                                      0.0,
                                                      5.0,
                                                      1.0,
                                                      4.0,
                                                      2.0,
                                                      2.0,
                                                      1.0,
                                                      1.0,
                                                      0.0,
                                                      4.0,
                                                      1.0,
                                                      1.0,
                                                      3.0,
                                                      1.0,
                                                      2.0,
                                                      0.3684210526315789,
                                                      5.0,
                                                      1.0,
                                                      3.0,
                                                      2.0,
                                                      0.0,
                                                      2.0,
                                                      3.0,
                                                      1.0,
                                                      1.0,
                                                      2.0,
                                                      1.0,
                                                      0.0,
                                                      0.0,
                                                      2.0,
                                                      1.0,
                                                      2.0,
                                                      2.6315789473684212,
                                                      1.3157894736842106,
                                                      2.0,
                                                      3.0],
                             'val_loss': 0.08996066451072693,
                             'val_loss_history': [1034503488.0,
                                                  847939968.0,
                                                  237004064.0,
                                                  0.1263040006160736,
                                                  0.1310095340013504,
                                                  0.1337033212184906,
                                                  0.12615786492824554,
                                                  0.12673823535442352,
                                                  0.12818212807178497,
                                                  0.12805397808551788,
                                                  0.12788242101669312,
                                                  0.12742243707180023,
                                                  0.12486802786588669,
                                                  0.12135627865791321,
                                                  0.1297132968902588,
                                                  0.12731346487998962,
                                                  0.12576653063297272,
                                                  0.11891531199216843,
                                                  0.12929166853427887,
                                                  0.12054215371608734,
                                                  0.1322227418422699,
                                                  0.11912821978330612,
                                                  0.12460339814424515,
                                                  0.1240234524011612,
                                                  0.11438535153865814,
                                                  0.11396811902523041,
                                                  0.11890871822834015,
                                                  0.12567488849163055,
                                                  0.12431469559669495,
                                                  0.12262961268424988,
                                                  0.114720918238163,
                                                  0.12223060429096222,
                                                  0.12178576737642288,
                                                  0.12323581427335739,
                                                  0.11141052842140198,
                                                  0.11490409821271896,
                                                  0.11491867899894714,
                                                  0.11480910331010818,
                                                  0.11481461673974991,
                                                  0.11511991918087006,
                                                  0.11507649719715118,
                                                  0.11513196676969528,
                                                  0.11517633497714996,
                                                  0.11511987447738647,
                                                  0.11503130942583084,
                                                  0.11513296514749527,
                                                  0.11526677012443542,
                                                  0.11538837105035782,
                                                  0.11536581069231033,
                                                  0.11482106149196625,
                                                  0.11543918401002884,
                                                  0.1155194565653801,
                                                  0.11556079238653183,
                                                  0.11545369774103165,
                                                  0.11537531018257141,
                                                  0.11555165797472,
                                                  0.11537749320268631,
                                                  0.11569669842720032,
                                                  0.11556733399629593,
                                                  0.11561307311058044,
                                                  0.11569474637508392,
                                                  0.11569544672966003,
                                                  0.11562191694974899,
                                                  0.11569502204656601,
                                                  0.11571890860795975,
                                                  0.11563991010189056,
                                                  0.11592783033847809,
                                                  0.11566539108753204,
                                                  0.11559756100177765,
                                                  0.11567602306604385,
                                                  0.11575625836849213,
                                                  0.1158311516046524,
                                                  0.11566069722175598,
                                                  0.11581201106309891,
                                                  0.11579643934965134,
                                                  0.11583234369754791,
                                                  0.11580903083086014,
                                                  0.11596063524484634,
                                                  0.11589496582746506,
                                                  0.11581369489431381,
                                                  0.11581697314977646,
                                                  0.11586833000183105,
                                                  0.11823365092277527,
                                                  0.11571573466062546,
                                                  0.1159466952085495,
                                                  0.11612705141305923,
                                                  0.11606323719024658,
                                                  0.11564084887504578,
                                                  0.11597100645303726,
                                                  0.1160222664475441,
                                                  0.11600326001644135,
                                                  0.11590395122766495,
                                                  0.11599953472614288,
                                                  0.11587393283843994,
                                                  0.1161334291100502,
                                                  0.11603137850761414,
                                                  0.11602283269166946,
                                                  0.11603374034166336,
                                                  0.11604278534650803,
                                                  0.11608204990625381,
                                                  0.09147002547979355,
                                                  0.09143483638763428,
                                                  0.09163686633110046,
                                                  0.09171552211046219,
                                                  0.09144914895296097,
                                                  0.09174562990665436,
                                                  0.091587595641613,
                                                  0.09154273569583893,
                                                  0.09181218594312668,
                                                  0.09172553569078445,
                                                  0.09155931323766708,
                                                  0.09146300703287125,
                                                  0.09142749756574631,
                                                  0.09154758602380753,
                                                  0.09157166630029678,
                                                  0.09156911820173264,
                                                  0.09155336022377014,
                                                  0.09153666347265244,
                                                  0.09141728281974792,
                                                  0.0918840542435646,
                                                  0.09160088747739792,
                                                  0.0916093960404396,
                                                  0.09157142043113708,
                                                  0.09171904623508453,
                                                  0.0915781706571579,
                                                  0.09155244380235672,
                                                  0.09143797308206558,
                                                  0.09141530841588974,
                                                  0.09086361527442932,
                                                  0.0905296728014946,
                                                  0.09057653695344925,
                                                  0.09012796729803085,
                                                  0.09028708189725876,
                                                  0.09044957906007767,
                                                  0.09055684506893158,
                                                  0.0902361124753952,
                                                  0.09055016934871674,
                                                  0.09031613171100616,
                                                  0.0902179628610611,
                                                  0.09052954614162445,
                                                  0.0900455117225647,
                                                  0.0902295783162117,
                                                  0.09020984917879105,
                                                  0.09043499827384949,
                                                  0.09051758050918579,
                                                  0.0904468297958374,
                                                  0.09017684310674667,
                                                  0.0902678519487381,
                                                  0.09037574380636215,
                                                  0.0904354378581047,
                                                  0.09113160520792007,
                                                  0.09062957018613815,
                                                  0.09017972648143768,
                                                  0.09062566608190536,
                                                  0.09158965945243835,
                                                  0.091673843562603,
                                                  0.0899815633893013,
                                                  0.09016478061676025,
                                                  0.0909065455198288,
                                                  0.09008615463972092,
                                                  0.09036406874656677,
                                                  0.09002280980348587,
                                                  0.09014122933149338,
                                                  0.09028293937444687,
                                                  0.0899188220500946,
                                                  0.08988888561725616,
                                                  0.09009919315576553,
                                                  0.09167507290840149,
                                                  0.09156660735607147,
                                                  0.09164106100797653,
                                                  0.09177406877279282,
                                                  0.09153330326080322,
                                                  0.09173539280891418,
                                                  0.09170960634946823,
                                                  0.09177560359239578,
                                                  0.09162052720785141,
                                                  0.09143483638763428,
                                                  0.09150806069374084,
                                                  0.09180814772844315,
                                                  0.09179021418094635,
                                                  0.09148918092250824,
                                                  0.09165149182081223,
                                                  0.09149252623319626,
                                                  0.09190665930509567,
                                                  0.0916096493601799,
                                                  0.09160439670085907,
                                                  0.09168170392513275,
                                                  0.09163866192102432,
                                                  0.09190444648265839,
                                                  0.091668039560318,
                                                  0.09162873029708862,
                                                  0.09173496067523956,
                                                  0.09242351353168488,
                                                  0.09048132598400116,
                                                  0.09090138971805573,
                                                  0.0904548242688179,
                                                  0.09145405888557434,
                                                  0.09131670743227005,
                                                  0.09178461879491806,
                                                  0.09153732657432556,
                                                  0.08973786979913712,
                                                  0.08988428860902786,
                                                  0.09021115303039551,
                                                  0.09000113606452942,
                                                  0.08976569026708603,
                                                  0.08987715095281601,
                                                  0.089874766767025,
                                                  0.08953355252742767,
                                                  0.08985365927219391,
                                                  0.08929459005594254,
                                                  0.09014719724655151,
                                                  0.0901370495557785,
                                                  0.08988743275403976,
                                                  0.09002864360809326,
                                                  0.08994632959365845,
                                                  0.09004314243793488,
                                                  0.08995210379362106,
                                                  0.09007275104522705,
                                                  0.09029345214366913,
                                                  0.090416319668293,
                                                  0.0899377390742302,
                                                  0.08996004611253738,
                                                  0.08988483250141144,
                                                  0.08986671268939972,
                                                  0.0900193452835083,
                                                  0.0899295061826706,
                                                  0.08994332700967789,
                                                  0.08973976969718933,
                                                  0.08997202664613724,
                                                  0.09010780602693558,
                                                  0.08996426314115524,
                                                  0.08962488174438477,
                                                  0.0902571827173233,
                                                  0.08992582559585571,
                                                  0.0900769904255867,
                                                  0.0898761972784996,
                                                  0.09017913788557053,
                                                  0.08997853100299835,
                                                  0.08989366888999939,
                                                  0.09010732173919678,
                                                  0.09005895256996155,
                                                  0.09000539779663086,
                                                  0.08995525538921356,
                                                  0.08972669392824173,
                                                  0.09011795371770859,
                                                  0.09001778066158295,
                                                  0.09011020511388779,
                                                  0.08981508761644363,
                                                  0.09017821401357651,
                                                  0.08959657698869705,
                                                  0.08875498175621033,
                                                  0.08870062232017517,
                                                  0.08880362659692764,
                                                  0.08918870985507965,
                                                  0.08892866224050522,
                                                  0.08985244482755661,
                                                  0.08982453495264053,
                                                  0.09029696136713028,
                                                  0.09009458869695663,
                                                  0.09013500809669495,
                                                  0.09010349214076996,
                                                  0.08994513750076294,
                                                  0.08971379697322845,
                                                  0.09011200815439224,
                                                  0.0903470441699028,
                                                  0.09037244319915771,
                                                  0.09014169126749039,
                                                  0.08994632959365845,
                                                  0.08982060849666595,
                                                  0.08958274871110916,
                                                  0.08996281772851944,
                                                  0.08989058434963226,
                                                  0.08994904905557632,
                                                  0.0900254026055336,
                                                  0.08989007025957108,
                                                  0.08997174352407455,
                                                  0.08993948996067047,
                                                  0.08963050693273544,
                                                  0.08996085822582245,
                                                  0.09006138890981674,
                                                  0.09014187753200531,
                                                  0.09013525396585464,
                                                  0.08976943790912628,
                                                  0.0901322066783905,
                                                  0.08987969905138016,
                                                  0.08936253190040588,
                                                  0.08984756469726562,
                                                  0.08993438631296158,
                                                  0.0899040624499321,
                                                  0.09000980108976364,
                                                  0.08996778726577759,
                                                  0.08943293988704681,
                                                  0.09004877507686615,
                                                  0.08976338803768158,
                                                  0.08978376537561417,
                                                  0.09000024944543839,
                                                  0.08958359062671661,
                                                  0.08903425931930542,
                                                  0.09001847356557846,
                                                  0.08996066451072693],
                             'weight_decay': 0.0005}},
 'models': [{'accuracy': 3.0,
             'batch_size': 32,
             'cv_score': 0.005336667319455597,
             'cv_val_accuracy': 2.0,
             'cv_val_loss': 0.0991933469971021,
             'cv_val_macroF1': 0.005336667319455597,
             'cv_val_microF1': 0.066966913828706,
             'epochs': 100,
             'final_model': GGNN1(
  (ggnn): GatedGraphConv(60, num_layers=2)
  (fc1): Linear(in_features=60, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
             'kwargs': {'aggr_type': 'mean',
                        'd1': 60,
                        'd2': 50,
                        'num_classes': 24,
                        'num_layers': 2},
             'learning_rate': 0.01,
             'macroF1': 0.006754945585160564,
             'microF1': 0.08414023372287145,
             'model': <class 'TFM_graph_classification_models.GGNN1'>,
             'model_instance': GGNN1(
  (ggnn): GatedGraphConv(60, num_layers=2)
  (fc1): Linear(in_features=60, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
             'score': 'f1_macro',
             'time': 1386.8977355957031,
             'train_loss_history': [1930582528.0,
                                    5816673280.0,
                                    143432992.0,
                                    401414400.0,
                                    0.08453511446714401,
                                    0.09732901304960251,
                                    0.08421100676059723,
                                    179.138427734375,
                                    0.08405710756778717,
                                    0.09024409204721451,
                                    0.08389213681221008,
                                    0.08385269343852997,
                                    0.08379144221544266,
                                    1.6190478801727295,
                                    0.0840674564242363,
                                    0.0839853435754776,
                                    0.08404428511857986,
                                    0.08388674259185791,
                                    0.08402787148952484,
                                    0.08394353091716766,
                                    10159.8349609375,
                                    0.08386019617319107,
                                    0.08378172665834427,
                                    0.08385205268859863,
                                    46.16153335571289,
                                    0.08443432301282883,
                                    0.08436936885118484,
                                    0.0838686004281044,
                                    0.08384396880865097,
                                    0.0838937908411026,
                                    0.08472973853349686,
                                    0.08419333398342133,
                                    0.08412699401378632,
                                    0.08394647389650345,
                                    0.08405932784080505,
                                    0.08477608859539032,
                                    0.08448567986488342,
                                    0.08438331633806229,
                                    0.08440301567316055,
                                    0.08459638804197311,
                                    0.08455643802881241,
                                    0.08457781374454498,
                                    0.0844746083021164,
                                    1.218392252922058,
                                    92.7166976928711,
                                    0.08451459556818008,
                                    0.0845295637845993,
                                    0.08447882533073425,
                                    0.08454097807407379,
                                    0.08445699512958527,
                                    686.81591796875,
                                    0.08460459113121033,
                                    143.66787719726562,
                                    0.08449937403202057,
                                    0.08453886210918427,
                                    0.08456281572580338,
                                    0.08450835943222046,
                                    0.0844619944691658,
                                    0.08456537127494812,
                                    0.08452136069536209,
                                    0.08452289551496506,
                                    0.08451851457357407,
                                    0.08440401405096054,
                                    0.08451830595731735,
                                    0.08450745046138763,
                                    0.08453594148159027,
                                    0.08453307300806046,
                                    0.08445210754871368,
                                    0.08442220091819763,
                                    0.08446472138166428,
                                    0.08453156054019928,
                                    0.08442430943250656,
                                    0.08437874913215637,
                                    0.0843782126903534,
                                    0.08437833935022354,
                                    0.08440952003002167,
                                    0.08439227938652039,
                                    0.08432715386152267,
                                    0.08439390361309052,
                                    4549.99755859375,
                                    0.0845685750246048,
                                    0.0844576433300972,
                                    0.08457537740468979,
                                    0.08446572721004486,
                                    0.08438173681497574,
                                    0.08444757014513016,
                                    0.08448383957147598,
                                    0.08457212895154953,
                                    0.08460312336683273,
                                    0.0844893679022789,
                                    0.08443479239940643,
                                    0.08454051613807678,
                                    0.0845152959227562,
                                    0.0845574289560318,
                                    0.08444813638925552,
                                    0.08446840941905975,
                                    0.08447453379631042,
                                    0.08441149443387985,
                                    0.0844496637582779,
                                    0.08445731550455093,
                                    0.09589837491512299,
                                    76.78620910644531,
                                    0.09452478587627411,
                                    0.09451538324356079,
                                    3594.9453125,
                                    0.09456611424684525,
                                    0.09456440061330795,
                                    0.09461269527673721,
                                    0.09654979407787323,
                                    0.09458601474761963,
                                    0.09456944465637207,
                                    0.09449096769094467,
                                    0.09453420341014862,
                                    0.09456656128168106,
                                    0.0945281833410263,
                                    0.09457744657993317,
                                    0.09452137351036072,
                                    3293.878662109375,
                                    0.0944976657629013,
                                    0.09452024847269058,
                                    0.09451670944690704,
                                    0.09454703330993652,
                                    2545.70849609375,
                                    0.09458226710557938,
                                    0.09455703943967819,
                                    0.09453553706407547,
                                    0.3361894488334656,
                                    0.09449301660060883,
                                    0.19670675694942474,
                                    0.1361423134803772,
                                    0.09369964897632599,
                                    0.09379451721906662,
                                    0.09369710832834244,
                                    0.09350265562534332,
                                    0.09347912669181824,
                                    0.09346462041139603,
                                    0.0935664027929306,
                                    0.09353747218847275,
                                    0.09344299137592316,
                                    0.09350886195898056,
                                    0.46246740221977234,
                                    0.0934191569685936,
                                    0.09330388158559799,
                                    0.09347691386938095,
                                    0.09354433417320251,
                                    0.09353521466255188,
                                    0.09354806691408157,
                                    4.875348091125488,
                                    0.09350661933422089,
                                    0.09341655671596527,
                                    0.09375137090682983,
                                    0.09359747171401978,
                                    0.09344623237848282,
                                    0.09351751208305359,
                                    0.09441114217042923,
                                    0.09437780827283859,
                                    0.09366247802972794,
                                    0.09349711984395981,
                                    0.09343573451042175,
                                    0.09347439557313919,
                                    0.09343110024929047,
                                    0.09334275126457214,
                                    0.09329673647880554,
                                    0.09324946254491806,
                                    0.09332489967346191,
                                    0.09335567057132721,
                                    0.09336070716381073,
                                    0.09327112883329391,
                                    0.09470103681087494,
                                    0.0943952202796936,
                                    223.92398071289062,
                                    0.2761259078979492,
                                    568.1837158203125,
                                    0.09453856199979782,
                                    3.640625476837158,
                                    8.571806907653809,
                                    0.09439113736152649,
                                    0.09438219666481018,
                                    0.09455494582653046,
                                    0.09456533193588257,
                                    0.09450738877058029,
                                    0.09454187005758286,
                                    0.09451670944690704,
                                    0.28767964243888855,
                                    0.09446129947900772,
                                    0.09447275102138519,
                                    261.5072937011719,
                                    0.5570076704025269,
                                    351.6172790527344,
                                    223.679931640625,
                                    0.09456315636634827,
                                    1275.6224365234375,
                                    996.783203125,
                                    468.9762878417969,
                                    0.09383884072303772,
                                    0.09372755140066147,
                                    0.09372410923242569,
                                    0.4396379292011261,
                                    1.5778316259384155,
                                    0.09457020461559296,
                                    0.4057060182094574,
                                    0.09465852379798889,
                                    0.09467104822397232,
                                    804.968017578125,
                                    0.09458814561367035,
                                    0.09467726200819016,
                                    0.16680659353733063,
                                    0.09450245648622513,
                                    0.09450561553239822,
                                    0.09438183158636093,
                                    0.09459953755140305,
                                    0.09469465166330338,
                                    0.09475258737802505,
                                    0.09472335129976273,
                                    0.09469188004732132,
                                    0.09467725455760956,
                                    0.09465447068214417,
                                    107.36640167236328,
                                    0.09468750655651093,
                                    0.09464620053768158,
                                    0.09461190551519394,
                                    8.436896324157715,
                                    0.8304502367973328,
                                    0.09465745091438293,
                                    0.09457870572805405,
                                    0.09460476040840149,
                                    0.5350560545921326,
                                    0.09462569653987885,
                                    12.481863975524902,
                                    0.09459514170885086,
                                    0.09474245458841324,
                                    0.09460768103599548,
                                    1.460088849067688,
                                    0.09470023959875107,
                                    0.0946817472577095,
                                    39852.96484375,
                                    0.14213499426841736,
                                    2483.219970703125,
                                    0.09459136426448822,
                                    10.684162139892578,
                                    0.09467890858650208,
                                    0.09464482218027115,
                                    0.09463319927453995,
                                    305.5508728027344,
                                    0.09451852738857269,
                                    266.2228088378906,
                                    0.09464354813098907,
                                    0.09453378617763519,
                                    0.09454832971096039,
                                    47629.37109375,
                                    0.09382311999797821,
                                    0.0934765487909317,
                                    0.09329262375831604,
                                    0.09430080652236938,
                                    0.09367848187685013,
                                    2579.875732421875,
                                    0.5042579770088196,
                                    0.09466729313135147,
                                    0.094603031873703,
                                    0.09465199708938599,
                                    0.09458494931459427,
                                    0.09463470429182053,
                                    10456.1650390625,
                                    0.09460638463497162,
                                    0.09465428441762924,
                                    0.09468037635087967,
                                    0.09464714676141739,
                                    0.9984434247016907,
                                    718732.0625,
                                    0.09467263519763947,
                                    786.500244140625,
                                    0.0946587324142456,
                                    25.26593589782715,
                                    0.0946255475282669,
                                    0.8189891576766968,
                                    0.09464100748300552,
                                    0.20862728357315063,
                                    212.48797607421875,
                                    0.11569085717201233,
                                    0.09459014981985092,
                                    0.10856836289167404,
                                    10806.7138671875,
                                    0.09449947625398636,
                                    0.27257049083709717,
                                    0.09450982511043549,
                                    0.09409075230360031,
                                    3391.76318359375,
                                    0.09542278200387955,
                                    3508036.0,
                                    0.0946173369884491,
                                    0.09459066390991211,
                                    0.0945025086402893,
                                    0.3716455399990082,
                                    0.09453834593296051,
                                    0.09462703764438629,
                                    0.09462379664182663,
                                    5380.2314453125,
                                    0.09367762506008148,
                                    0.09457611292600632,
                                    0.0945659652352333],
             'val_accuracy_history': [1.105263157894737,
                                      1.0526315789473684,
                                      0.15789473684210525,
                                      0.9473684210526315,
                                      0.8947368421052632,
                                      0.3684210526315789,
                                      0.631578947368421,
                                      1.0526315789473684,
                                      1.0526315789473684,
                                      0.631578947368421,
                                      1.0,
                                      1.4736842105263157,
                                      0.3157894736842105,
                                      1.4210526315789473,
                                      0.7894736842105263,
                                      1.3157894736842106,
                                      0.5789473684210527,
                                      1.263157894736842,
                                      0.631578947368421,
                                      1.631578947368421,
                                      1.105263157894737,
                                      0.42105263157894735,
                                      1.2105263157894737,
                                      1.1578947368421053,
                                      1.0,
                                      2.0,
                                      1.0526315789473684,
                                      0.05263157894736842,
                                      1.368421052631579,
                                      0.21052631578947367,
                                      0.0,
                                      1.1578947368421053,
                                      1.263157894736842,
                                      1.5263157894736843,
                                      0.0,
                                      2.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      1.0,
                                      4.0,
                                      2.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      3.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      3.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      2.0,
                                      2.0,
                                      0.0,
                                      1.0,
                                      2.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.8421052631578947,
                                      1.0,
                                      0.0,
                                      2.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      3.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      3.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      3.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.1111111111111111,
                                      0.4444444444444444,
                                      0.0,
                                      1.7777777777777777,
                                      0.0,
                                      0.7777777777777778,
                                      0.0,
                                      0.0,
                                      0.8888888888888888,
                                      0.0,
                                      1.1111111111111112,
                                      0.4444444444444444,
                                      0.1111111111111111,
                                      0.0,
                                      0.1111111111111111,
                                      0.2222222222222222,
                                      0.1111111111111111,
                                      0.8888888888888888,
                                      0.7777777777777778,
                                      0.8888888888888888,
                                      0.1111111111111111,
                                      0.2222222222222222,
                                      1.3333333333333333,
                                      0.7777777777777778,
                                      0.0,
                                      0.0,
                                      1.0,
                                      1.3333333333333333,
                                      0.0,
                                      0.3333333333333333,
                                      0.0,
                                      1.2222222222222223,
                                      0.1111111111111111,
                                      0.0,
                                      1.3333333333333333,
                                      0.6666666666666666,
                                      0.2222222222222222,
                                      2.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      3.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      2.0,
                                      0.0,
                                      0.0,
                                      0.4444444444444444,
                                      1.5555555555555556,
                                      0.4444444444444444,
                                      1.0,
                                      0.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      0.0,
                                      4.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      0.7368421052631579,
                                      1.0,
                                      0.0,
                                      1.0,
                                      3.0,
                                      0.0,
                                      0.0,
                                      3.0,
                                      0.0,
                                      2.0,
                                      1.0,
                                      2.0,
                                      2.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      1.0,
                                      2.0,
                                      3.0,
                                      1.0,
                                      1.0,
                                      2.0,
                                      0.0,
                                      3.0,
                                      0.0,
                                      1.0,
                                      0.0,
                                      1.0,
                                      1.0,
                                      2.0,
                                      1.0,
                                      1.0,
                                      3.0,
                                      2.0,
                                      2.0,
                                      3.0,
                                      4.0,
                                      3.0,
                                      3.0,
                                      1.0,
                                      1.0,
                                      1.368421052631579,
                                      2.736842105263158,
                                      1.1578947368421053,
                                      1.6842105263157894,
                                      1.9473684210526316,
                                      1.8421052631578947,
                                      1.0,
                                      2.0,
                                      1.0,
                                      1.0,
                                      1.0,
                                      2.0,
                                      1.0,
                                      3.0,
                                      1.0,
                                      0.0,
                                      5.0,
                                      1.0,
                                      4.0,
                                      2.0,
                                      2.0,
                                      1.0,
                                      1.0,
                                      0.0,
                                      4.0,
                                      1.0,
                                      1.0,
                                      3.0,
                                      1.0,
                                      2.0,
                                      0.3684210526315789,
                                      5.0,
                                      1.0,
                                      3.0,
                                      2.0,
                                      0.0,
                                      2.0,
                                      3.0,
                                      1.0,
                                      1.0,
                                      2.0,
                                      1.0,
                                      0.0,
                                      0.0,
                                      2.0,
                                      1.0,
                                      2.0,
                                      2.6315789473684212,
                                      1.3157894736842106,
                                      2.0,
                                      3.0],
             'val_loss': 0.08996066451072693,
             'val_loss_history': [1034503488.0,
                                  847939968.0,
                                  237004064.0,
                                  0.1263040006160736,
                                  0.1310095340013504,
                                  0.1337033212184906,
                                  0.12615786492824554,
                                  0.12673823535442352,
                                  0.12818212807178497,
                                  0.12805397808551788,
                                  0.12788242101669312,
                                  0.12742243707180023,
                                  0.12486802786588669,
                                  0.12135627865791321,
                                  0.1297132968902588,
                                  0.12731346487998962,
                                  0.12576653063297272,
                                  0.11891531199216843,
                                  0.12929166853427887,
                                  0.12054215371608734,
                                  0.1322227418422699,
                                  0.11912821978330612,
                                  0.12460339814424515,
                                  0.1240234524011612,
                                  0.11438535153865814,
                                  0.11396811902523041,
                                  0.11890871822834015,
                                  0.12567488849163055,
                                  0.12431469559669495,
                                  0.12262961268424988,
                                  0.114720918238163,
                                  0.12223060429096222,
                                  0.12178576737642288,
                                  0.12323581427335739,
                                  0.11141052842140198,
                                  0.11490409821271896,
                                  0.11491867899894714,
                                  0.11480910331010818,
                                  0.11481461673974991,
                                  0.11511991918087006,
                                  0.11507649719715118,
                                  0.11513196676969528,
                                  0.11517633497714996,
                                  0.11511987447738647,
                                  0.11503130942583084,
                                  0.11513296514749527,
                                  0.11526677012443542,
                                  0.11538837105035782,
                                  0.11536581069231033,
                                  0.11482106149196625,
                                  0.11543918401002884,
                                  0.1155194565653801,
                                  0.11556079238653183,
                                  0.11545369774103165,
                                  0.11537531018257141,
                                  0.11555165797472,
                                  0.11537749320268631,
                                  0.11569669842720032,
                                  0.11556733399629593,
                                  0.11561307311058044,
                                  0.11569474637508392,
                                  0.11569544672966003,
                                  0.11562191694974899,
                                  0.11569502204656601,
                                  0.11571890860795975,
                                  0.11563991010189056,
                                  0.11592783033847809,
                                  0.11566539108753204,
                                  0.11559756100177765,
                                  0.11567602306604385,
                                  0.11575625836849213,
                                  0.1158311516046524,
                                  0.11566069722175598,
                                  0.11581201106309891,
                                  0.11579643934965134,
                                  0.11583234369754791,
                                  0.11580903083086014,
                                  0.11596063524484634,
                                  0.11589496582746506,
                                  0.11581369489431381,
                                  0.11581697314977646,
                                  0.11586833000183105,
                                  0.11823365092277527,
                                  0.11571573466062546,
                                  0.1159466952085495,
                                  0.11612705141305923,
                                  0.11606323719024658,
                                  0.11564084887504578,
                                  0.11597100645303726,
                                  0.1160222664475441,
                                  0.11600326001644135,
                                  0.11590395122766495,
                                  0.11599953472614288,
                                  0.11587393283843994,
                                  0.1161334291100502,
                                  0.11603137850761414,
                                  0.11602283269166946,
                                  0.11603374034166336,
                                  0.11604278534650803,
                                  0.11608204990625381,
                                  0.09147002547979355,
                                  0.09143483638763428,
                                  0.09163686633110046,
                                  0.09171552211046219,
                                  0.09144914895296097,
                                  0.09174562990665436,
                                  0.091587595641613,
                                  0.09154273569583893,
                                  0.09181218594312668,
                                  0.09172553569078445,
                                  0.09155931323766708,
                                  0.09146300703287125,
                                  0.09142749756574631,
                                  0.09154758602380753,
                                  0.09157166630029678,
                                  0.09156911820173264,
                                  0.09155336022377014,
                                  0.09153666347265244,
                                  0.09141728281974792,
                                  0.0918840542435646,
                                  0.09160088747739792,
                                  0.0916093960404396,
                                  0.09157142043113708,
                                  0.09171904623508453,
                                  0.0915781706571579,
                                  0.09155244380235672,
                                  0.09143797308206558,
                                  0.09141530841588974,
                                  0.09086361527442932,
                                  0.0905296728014946,
                                  0.09057653695344925,
                                  0.09012796729803085,
                                  0.09028708189725876,
                                  0.09044957906007767,
                                  0.09055684506893158,
                                  0.0902361124753952,
                                  0.09055016934871674,
                                  0.09031613171100616,
                                  0.0902179628610611,
                                  0.09052954614162445,
                                  0.0900455117225647,
                                  0.0902295783162117,
                                  0.09020984917879105,
                                  0.09043499827384949,
                                  0.09051758050918579,
                                  0.0904468297958374,
                                  0.09017684310674667,
                                  0.0902678519487381,
                                  0.09037574380636215,
                                  0.0904354378581047,
                                  0.09113160520792007,
                                  0.09062957018613815,
                                  0.09017972648143768,
                                  0.09062566608190536,
                                  0.09158965945243835,
                                  0.091673843562603,
                                  0.0899815633893013,
                                  0.09016478061676025,
                                  0.0909065455198288,
                                  0.09008615463972092,
                                  0.09036406874656677,
                                  0.09002280980348587,
                                  0.09014122933149338,
                                  0.09028293937444687,
                                  0.0899188220500946,
                                  0.08988888561725616,
                                  0.09009919315576553,
                                  0.09167507290840149,
                                  0.09156660735607147,
                                  0.09164106100797653,
                                  0.09177406877279282,
                                  0.09153330326080322,
                                  0.09173539280891418,
                                  0.09170960634946823,
                                  0.09177560359239578,
                                  0.09162052720785141,
                                  0.09143483638763428,
                                  0.09150806069374084,
                                  0.09180814772844315,
                                  0.09179021418094635,
                                  0.09148918092250824,
                                  0.09165149182081223,
                                  0.09149252623319626,
                                  0.09190665930509567,
                                  0.0916096493601799,
                                  0.09160439670085907,
                                  0.09168170392513275,
                                  0.09163866192102432,
                                  0.09190444648265839,
                                  0.091668039560318,
                                  0.09162873029708862,
                                  0.09173496067523956,
                                  0.09242351353168488,
                                  0.09048132598400116,
                                  0.09090138971805573,
                                  0.0904548242688179,
                                  0.09145405888557434,
                                  0.09131670743227005,
                                  0.09178461879491806,
                                  0.09153732657432556,
                                  0.08973786979913712,
                                  0.08988428860902786,
                                  0.09021115303039551,
                                  0.09000113606452942,
                                  0.08976569026708603,
                                  0.08987715095281601,
                                  0.089874766767025,
                                  0.08953355252742767,
                                  0.08985365927219391,
                                  0.08929459005594254,
                                  0.09014719724655151,
                                  0.0901370495557785,
                                  0.08988743275403976,
                                  0.09002864360809326,
                                  0.08994632959365845,
                                  0.09004314243793488,
                                  0.08995210379362106,
                                  0.09007275104522705,
                                  0.09029345214366913,
                                  0.090416319668293,
                                  0.0899377390742302,
                                  0.08996004611253738,
                                  0.08988483250141144,
                                  0.08986671268939972,
                                  0.0900193452835083,
                                  0.0899295061826706,
                                  0.08994332700967789,
                                  0.08973976969718933,
                                  0.08997202664613724,
                                  0.09010780602693558,
                                  0.08996426314115524,
                                  0.08962488174438477,
                                  0.0902571827173233,
                                  0.08992582559585571,
                                  0.0900769904255867,
                                  0.0898761972784996,
                                  0.09017913788557053,
                                  0.08997853100299835,
                                  0.08989366888999939,
                                  0.09010732173919678,
                                  0.09005895256996155,
                                  0.09000539779663086,
                                  0.08995525538921356,
                                  0.08972669392824173,
                                  0.09011795371770859,
                                  0.09001778066158295,
                                  0.09011020511388779,
                                  0.08981508761644363,
                                  0.09017821401357651,
                                  0.08959657698869705,
                                  0.08875498175621033,
                                  0.08870062232017517,
                                  0.08880362659692764,
                                  0.08918870985507965,
                                  0.08892866224050522,
                                  0.08985244482755661,
                                  0.08982453495264053,
                                  0.09029696136713028,
                                  0.09009458869695663,
                                  0.09013500809669495,
                                  0.09010349214076996,
                                  0.08994513750076294,
                                  0.08971379697322845,
                                  0.09011200815439224,
                                  0.0903470441699028,
                                  0.09037244319915771,
                                  0.09014169126749039,
                                  0.08994632959365845,
                                  0.08982060849666595,
                                  0.08958274871110916,
                                  0.08996281772851944,
                                  0.08989058434963226,
                                  0.08994904905557632,
                                  0.0900254026055336,
                                  0.08989007025957108,
                                  0.08997174352407455,
                                  0.08993948996067047,
                                  0.08963050693273544,
                                  0.08996085822582245,
                                  0.09006138890981674,
                                  0.09014187753200531,
                                  0.09013525396585464,
                                  0.08976943790912628,
                                  0.0901322066783905,
                                  0.08987969905138016,
                                  0.08936253190040588,
                                  0.08984756469726562,
                                  0.08993438631296158,
                                  0.0899040624499321,
                                  0.09000980108976364,
                                  0.08996778726577759,
                                  0.08943293988704681,
                                  0.09004877507686615,
                                  0.08976338803768158,
                                  0.08978376537561417,
                                  0.09000024944543839,
                                  0.08958359062671661,
                                  0.08903425931930542,
                                  0.09001847356557846,
                                  0.08996066451072693],
             'weight_decay': 0.0005}],
 'tests': {},
 'training_time': 4262.922671318054}
{'autoincrement': 4,
 'best_models': {},
 'best_models_list': [{'accuracy': 3.0,
                       'batch_size': 32,
                       'cv_score': 0.005336667319455597,
                       'cv_val_accuracy': 2.0,
                       'cv_val_loss': 0.0991933469971021,
                       'cv_val_macroF1': 0.005336667319455597,
                       'cv_val_microF1': 0.066966913828706,
                       'epochs': 100,
                       'final_model': GGNN1(
  (ggnn): GatedGraphConv(60, num_layers=2)
  (fc1): Linear(in_features=60, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                       'kwargs': {'aggr_type': 'mean',
                                  'd1': 60,
                                  'd2': 50,
                                  'num_classes': 24,
                                  'num_layers': 2},
                       'learning_rate': 0.01,
                       'macroF1': 0.006754945585160564,
                       'microF1': 0.08414023372287145,
                       'model': <class 'TFM_graph_classification_models.GGNN1'>,
                       'model_instance': GGNN1(
  (ggnn): GatedGraphConv(60, num_layers=2)
  (fc1): Linear(in_features=60, out_features=50, bias=True)
  (fc2): Linear(in_features=50, out_features=24, bias=True)
),
                       'name': '3_GGNN1',
                       'score': 'macroF1',
                       'time': 1386.8977355957031,
                       'train_loss_history': [1930582528.0,
                                              5816673280.0,
                                              143432992.0,
                                              401414400.0,
                                              0.08453511446714401,
                                              0.09732901304960251,
                                              0.08421100676059723,
                                              179.138427734375,
                                              0.08405710756778717,
                                              0.09024409204721451,
                                              0.08389213681221008,
                                              0.08385269343852997,
                                              0.08379144221544266,
                                              1.6190478801727295,
                                              0.0840674564242363,
                                              0.0839853435754776,
                                              0.08404428511857986,
                                              0.08388674259185791,
                                              0.08402787148952484,
                                              0.08394353091716766,
                                              10159.8349609375,
                                              0.08386019617319107,
                                              0.08378172665834427,
                                              0.08385205268859863,
                                              46.16153335571289,
                                              0.08443432301282883,
                                              0.08436936885118484,
                                              0.0838686004281044,
                                              0.08384396880865097,
                                              0.0838937908411026,
                                              0.08472973853349686,
                                              0.08419333398342133,
                                              0.08412699401378632,
                                              0.08394647389650345,
                                              0.08405932784080505,
                                              0.08477608859539032,
                                              0.08448567986488342,
                                              0.08438331633806229,
                                              0.08440301567316055,
                                              0.08459638804197311,
                                              0.08455643802881241,
                                              0.08457781374454498,
                                              0.0844746083021164,
                                              1.218392252922058,
                                              92.7166976928711,
                                              0.08451459556818008,
                                              0.0845295637845993,
                                              0.08447882533073425,
                                              0.08454097807407379,
                                              0.08445699512958527,
                                              686.81591796875,
                                              0.08460459113121033,
                                              143.66787719726562,
                                              0.08449937403202057,
                                              0.08453886210918427,
                                              0.08456281572580338,
                                              0.08450835943222046,
                                              0.0844619944691658,
                                              0.08456537127494812,
                                              0.08452136069536209,
                                              0.08452289551496506,
                                              0.08451851457357407,
                                              0.08440401405096054,
                                              0.08451830595731735,
                                              0.08450745046138763,
                                              0.08453594148159027,
                                              0.08453307300806046,
                                              0.08445210754871368,
                                              0.08442220091819763,
                                              0.08446472138166428,
                                              0.08453156054019928,
                                              0.08442430943250656,
                                              0.08437874913215637,
                                              0.0843782126903534,
                                              0.08437833935022354,
                                              0.08440952003002167,
                                              0.08439227938652039,
                                              0.08432715386152267,
                                              0.08439390361309052,
                                              4549.99755859375,
                                              0.0845685750246048,
                                              0.0844576433300972,
                                              0.08457537740468979,
                                              0.08446572721004486,
                                              0.08438173681497574,
                                              0.08444757014513016,
                                              0.08448383957147598,
                                              0.08457212895154953,
                                              0.08460312336683273,
                                              0.0844893679022789,
                                              0.08443479239940643,
                                              0.08454051613807678,
                                              0.0845152959227562,
                                              0.0845574289560318,
                                              0.08444813638925552,
                                              0.08446840941905975,
                                              0.08447453379631042,
                                              0.08441149443387985,
                                              0.0844496637582779,
                                              0.08445731550455093,
                                              0.09589837491512299,
                                              76.78620910644531,
                                              0.09452478587627411,
                                              0.09451538324356079,
                                              3594.9453125,
                                              0.09456611424684525,
                                              0.09456440061330795,
                                              0.09461269527673721,
                                              0.09654979407787323,
                                              0.09458601474761963,
                                              0.09456944465637207,
                                              0.09449096769094467,
                                              0.09453420341014862,
                                              0.09456656128168106,
                                              0.0945281833410263,
                                              0.09457744657993317,
                                              0.09452137351036072,
                                              3293.878662109375,
                                              0.0944976657629013,
                                              0.09452024847269058,
                                              0.09451670944690704,
                                              0.09454703330993652,
                                              2545.70849609375,
                                              0.09458226710557938,
                                              0.09455703943967819,
                                              0.09453553706407547,
                                              0.3361894488334656,
                                              0.09449301660060883,
                                              0.19670675694942474,
                                              0.1361423134803772,
                                              0.09369964897632599,
                                              0.09379451721906662,
                                              0.09369710832834244,
                                              0.09350265562534332,
                                              0.09347912669181824,
                                              0.09346462041139603,
                                              0.0935664027929306,
                                              0.09353747218847275,
                                              0.09344299137592316,
                                              0.09350886195898056,
                                              0.46246740221977234,
                                              0.0934191569685936,
                                              0.09330388158559799,
                                              0.09347691386938095,
                                              0.09354433417320251,
                                              0.09353521466255188,
                                              0.09354806691408157,
                                              4.875348091125488,
                                              0.09350661933422089,
                                              0.09341655671596527,
                                              0.09375137090682983,
                                              0.09359747171401978,
                                              0.09344623237848282,
                                              0.09351751208305359,
                                              0.09441114217042923,
                                              0.09437780827283859,
                                              0.09366247802972794,
                                              0.09349711984395981,
                                              0.09343573451042175,
                                              0.09347439557313919,
                                              0.09343110024929047,
                                              0.09334275126457214,
                                              0.09329673647880554,
                                              0.09324946254491806,
                                              0.09332489967346191,
                                              0.09335567057132721,
                                              0.09336070716381073,
                                              0.09327112883329391,
                                              0.09470103681087494,
                                              0.0943952202796936,
                                              223.92398071289062,
                                              0.2761259078979492,
                                              568.1837158203125,
                                              0.09453856199979782,
                                              3.640625476837158,
                                              8.571806907653809,
                                              0.09439113736152649,
                                              0.09438219666481018,
                                              0.09455494582653046,
                                              0.09456533193588257,
                                              0.09450738877058029,
                                              0.09454187005758286,
                                              0.09451670944690704,
                                              0.28767964243888855,
                                              0.09446129947900772,
                                              0.09447275102138519,
                                              261.5072937011719,
                                              0.5570076704025269,
                                              351.6172790527344,
                                              223.679931640625,
                                              0.09456315636634827,
                                              1275.6224365234375,
                                              996.783203125,
                                              468.9762878417969,
                                              0.09383884072303772,
                                              0.09372755140066147,
                                              0.09372410923242569,
                                              0.4396379292011261,
                                              1.5778316259384155,
                                              0.09457020461559296,
                                              0.4057060182094574,
                                              0.09465852379798889,
                                              0.09467104822397232,
                                              804.968017578125,
                                              0.09458814561367035,
                                              0.09467726200819016,
                                              0.16680659353733063,
                                              0.09450245648622513,
                                              0.09450561553239822,
                                              0.09438183158636093,
                                              0.09459953755140305,
                                              0.09469465166330338,
                                              0.09475258737802505,
                                              0.09472335129976273,
                                              0.09469188004732132,
                                              0.09467725455760956,
                                              0.09465447068214417,
                                              107.36640167236328,
                                              0.09468750655651093,
                                              0.09464620053768158,
                                              0.09461190551519394,
                                              8.436896324157715,
                                              0.8304502367973328,
                                              0.09465745091438293,
                                              0.09457870572805405,
                                              0.09460476040840149,
                                              0.5350560545921326,
                                              0.09462569653987885,
                                              12.481863975524902,
                                              0.09459514170885086,
                                              0.09474245458841324,
                                              0.09460768103599548,
                                              1.460088849067688,
                                              0.09470023959875107,
                                              0.0946817472577095,
                                              39852.96484375,
                                              0.14213499426841736,
                                              2483.219970703125,
                                              0.09459136426448822,
                                              10.684162139892578,
                                              0.09467890858650208,
                                              0.09464482218027115,
                                              0.09463319927453995,
                                              305.5508728027344,
                                              0.09451852738857269,
                                              266.2228088378906,
                                              0.09464354813098907,
                                              0.09453378617763519,
                                              0.09454832971096039,
                                              47629.37109375,
                                              0.09382311999797821,
                                              0.0934765487909317,
                                              0.09329262375831604,
                                              0.09430080652236938,
                                              0.09367848187685013,
                                              2579.875732421875,
                                              0.5042579770088196,
                                              0.09466729313135147,
                                              0.094603031873703,
                                              0.09465199708938599,
                                              0.09458494931459427,
                                              0.09463470429182053,
                                              10456.1650390625,
                                              0.09460638463497162,
                                              0.09465428441762924,
                                              0.09468037635087967,
                                              0.09464714676141739,
                                              0.9984434247016907,
                                              718732.0625,
                                              0.09467263519763947,
                                              786.500244140625,
                                              0.0946587324142456,
                                              25.26593589782715,
                                              0.0946255475282669,
                                              0.8189891576766968,
                                              0.09464100748300552,
                                              0.20862728357315063,
                                              212.48797607421875,
                                              0.11569085717201233,
                                              0.09459014981985092,
                                              0.10856836289167404,
                                              10806.7138671875,
                                              0.09449947625398636,
                                              0.27257049083709717,
                                              0.09450982511043549,
                                              0.09409075230360031,
                                              3391.76318359375,
                                              0.09542278200387955,
                                              3508036.0,
                                              0.0946173369884491,
                                              0.09459066390991211,
                                              0.0945025086402893,
                                              0.3716455399990082,
                                              0.09453834593296051,
                                              0.09462703764438629,
                                              0.09462379664182663,
                                              5380.2314453125,
                                              0.09367762506008148,
                                              0.09457611292600632,
                                              0.0945659652352333],
                       'val_accuracy_history': [1.105263157894737,
                                                1.0526315789473684,
                                                0.15789473684210525,
                                                0.9473684210526315,
                                                0.8947368421052632,
                                                0.3684210526315789,
                                                0.631578947368421,
                                                1.0526315789473684,
                                                1.0526315789473684,
                                                0.631578947368421,
                                                1.0,
                                                1.4736842105263157,
                                                0.3157894736842105,
                                                1.4210526315789473,
                                                0.7894736842105263,
                                                1.3157894736842106,
                                                0.5789473684210527,
                                                1.263157894736842,
                                                0.631578947368421,
                                                1.631578947368421,
                                                1.105263157894737,
                                                0.42105263157894735,
                                                1.2105263157894737,
                                                1.1578947368421053,
                                                1.0,
                                                2.0,
                                                1.0526315789473684,
                                                0.05263157894736842,
                                                1.368421052631579,
                                                0.21052631578947367,
                                                0.0,
                                                1.1578947368421053,
                                                1.263157894736842,
                                                1.5263157894736843,
                                                0.0,
                                                2.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                1.0,
                                                4.0,
                                                2.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                3.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                3.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                2.0,
                                                2.0,
                                                0.0,
                                                1.0,
                                                2.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.8421052631578947,
                                                1.0,
                                                0.0,
                                                2.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                3.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                3.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                3.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.1111111111111111,
                                                0.4444444444444444,
                                                0.0,
                                                1.7777777777777777,
                                                0.0,
                                                0.7777777777777778,
                                                0.0,
                                                0.0,
                                                0.8888888888888888,
                                                0.0,
                                                1.1111111111111112,
                                                0.4444444444444444,
                                                0.1111111111111111,
                                                0.0,
                                                0.1111111111111111,
                                                0.2222222222222222,
                                                0.1111111111111111,
                                                0.8888888888888888,
                                                0.7777777777777778,
                                                0.8888888888888888,
                                                0.1111111111111111,
                                                0.2222222222222222,
                                                1.3333333333333333,
                                                0.7777777777777778,
                                                0.0,
                                                0.0,
                                                1.0,
                                                1.3333333333333333,
                                                0.0,
                                                0.3333333333333333,
                                                0.0,
                                                1.2222222222222223,
                                                0.1111111111111111,
                                                0.0,
                                                1.3333333333333333,
                                                0.6666666666666666,
                                                0.2222222222222222,
                                                2.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                3.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                2.0,
                                                0.0,
                                                0.0,
                                                0.4444444444444444,
                                                1.5555555555555556,
                                                0.4444444444444444,
                                                1.0,
                                                0.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                0.0,
                                                4.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                0.7368421052631579,
                                                1.0,
                                                0.0,
                                                1.0,
                                                3.0,
                                                0.0,
                                                0.0,
                                                3.0,
                                                0.0,
                                                2.0,
                                                1.0,
                                                2.0,
                                                2.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                1.0,
                                                2.0,
                                                3.0,
                                                1.0,
                                                1.0,
                                                2.0,
                                                0.0,
                                                3.0,
                                                0.0,
                                                1.0,
                                                0.0,
                                                1.0,
                                                1.0,
                                                2.0,
                                                1.0,
                                                1.0,
                                                3.0,
                                                2.0,
                                                2.0,
                                                3.0,
                                                4.0,
                                                3.0,
                                                3.0,
                                                1.0,
                                                1.0,
                                                1.368421052631579,
                                                2.736842105263158,
                                                1.1578947368421053,
                                                1.6842105263157894,
                                                1.9473684210526316,
                                                1.8421052631578947,
                                                1.0,
                                                2.0,
                                                1.0,
                                                1.0,
                                                1.0,
                                                2.0,
                                                1.0,
                                                3.0,
                                                1.0,
                                                0.0,
                                                5.0,
                                                1.0,
                                                4.0,
                                                2.0,
                                                2.0,
                                                1.0,
                                                1.0,
                                                0.0,
                                                4.0,
                                                1.0,
                                                1.0,
                                                3.0,
                                                1.0,
                                                2.0,
                                                0.3684210526315789,
                                                5.0,
                                                1.0,
                                                3.0,
                                                2.0,
                                                0.0,
                                                2.0,
                                                3.0,
                                                1.0,
                                                1.0,
                                                2.0,
                                                1.0,
                                                0.0,
                                                0.0,
                                                2.0,
                                                1.0,
                                                2.0,
                                                2.6315789473684212,
                                                1.3157894736842106,
                                                2.0,
                                                3.0],
                       'val_loss': 0.08996066451072693,
                       'val_loss_history': [1034503488.0,
                                            847939968.0,
                                            237004064.0,
                                            0.1263040006160736,
                                            0.1310095340013504,
                                            0.1337033212184906,
                                            0.12615786492824554,
                                            0.12673823535442352,
                                            0.12818212807178497,
                                            0.12805397808551788,
                                            0.12788242101669312,
                                            0.12742243707180023,
                                            0.12486802786588669,
                                            0.12135627865791321,
                                            0.1297132968902588,
                                            0.12731346487998962,
                                            0.12576653063297272,
                                            0.11891531199216843,
                                            0.12929166853427887,
                                            0.12054215371608734,
                                            0.1322227418422699,
                                            0.11912821978330612,
                                            0.12460339814424515,
                                            0.1240234524011612,
                                            0.11438535153865814,
                                            0.11396811902523041,
                                            0.11890871822834015,
                                            0.12567488849163055,
                                            0.12431469559669495,
                                            0.12262961268424988,
                                            0.114720918238163,
                                            0.12223060429096222,
                                            0.12178576737642288,
                                            0.12323581427335739,
                                            0.11141052842140198,
                                            0.11490409821271896,
                                            0.11491867899894714,
                                            0.11480910331010818,
                                            0.11481461673974991,
                                            0.11511991918087006,
                                            0.11507649719715118,
                                            0.11513196676969528,
                                            0.11517633497714996,
                                            0.11511987447738647,
                                            0.11503130942583084,
                                            0.11513296514749527,
                                            0.11526677012443542,
                                            0.11538837105035782,
                                            0.11536581069231033,
                                            0.11482106149196625,
                                            0.11543918401002884,
                                            0.1155194565653801,
                                            0.11556079238653183,
                                            0.11545369774103165,
                                            0.11537531018257141,
                                            0.11555165797472,
                                            0.11537749320268631,
                                            0.11569669842720032,
                                            0.11556733399629593,
                                            0.11561307311058044,
                                            0.11569474637508392,
                                            0.11569544672966003,
                                            0.11562191694974899,
                                            0.11569502204656601,
                                            0.11571890860795975,
                                            0.11563991010189056,
                                            0.11592783033847809,
                                            0.11566539108753204,
                                            0.11559756100177765,
                                            0.11567602306604385,
                                            0.11575625836849213,
                                            0.1158311516046524,
                                            0.11566069722175598,
                                            0.11581201106309891,
                                            0.11579643934965134,
                                            0.11583234369754791,
                                            0.11580903083086014,
                                            0.11596063524484634,
                                            0.11589496582746506,
                                            0.11581369489431381,
                                            0.11581697314977646,
                                            0.11586833000183105,
                                            0.11823365092277527,
                                            0.11571573466062546,
                                            0.1159466952085495,
                                            0.11612705141305923,
                                            0.11606323719024658,
                                            0.11564084887504578,
                                            0.11597100645303726,
                                            0.1160222664475441,
                                            0.11600326001644135,
                                            0.11590395122766495,
                                            0.11599953472614288,
                                            0.11587393283843994,
                                            0.1161334291100502,
                                            0.11603137850761414,
                                            0.11602283269166946,
                                            0.11603374034166336,
                                            0.11604278534650803,
                                            0.11608204990625381,
                                            0.09147002547979355,
                                            0.09143483638763428,
                                            0.09163686633110046,
                                            0.09171552211046219,
                                            0.09144914895296097,
                                            0.09174562990665436,
                                            0.091587595641613,
                                            0.09154273569583893,
                                            0.09181218594312668,
                                            0.09172553569078445,
                                            0.09155931323766708,
                                            0.09146300703287125,
                                            0.09142749756574631,
                                            0.09154758602380753,
                                            0.09157166630029678,
                                            0.09156911820173264,
                                            0.09155336022377014,
                                            0.09153666347265244,
                                            0.09141728281974792,
                                            0.0918840542435646,
                                            0.09160088747739792,
                                            0.0916093960404396,
                                            0.09157142043113708,
                                            0.09171904623508453,
                                            0.0915781706571579,
                                            0.09155244380235672,
                                            0.09143797308206558,
                                            0.09141530841588974,
                                            0.09086361527442932,
                                            0.0905296728014946,
                                            0.09057653695344925,
                                            0.09012796729803085,
                                            0.09028708189725876,
                                            0.09044957906007767,
                                            0.09055684506893158,
                                            0.0902361124753952,
                                            0.09055016934871674,
                                            0.09031613171100616,
                                            0.0902179628610611,
                                            0.09052954614162445,
                                            0.0900455117225647,
                                            0.0902295783162117,
                                            0.09020984917879105,
                                            0.09043499827384949,
                                            0.09051758050918579,
                                            0.0904468297958374,
                                            0.09017684310674667,
                                            0.0902678519487381,
                                            0.09037574380636215,
                                            0.0904354378581047,
                                            0.09113160520792007,
                                            0.09062957018613815,
                                            0.09017972648143768,
                                            0.09062566608190536,
                                            0.09158965945243835,
                                            0.091673843562603,
                                            0.0899815633893013,
                                            0.09016478061676025,
                                            0.0909065455198288,
                                            0.09008615463972092,
                                            0.09036406874656677,
                                            0.09002280980348587,
                                            0.09014122933149338,
                                            0.09028293937444687,
                                            0.0899188220500946,
                                            0.08988888561725616,
                                            0.09009919315576553,
                                            0.09167507290840149,
                                            0.09156660735607147,
                                            0.09164106100797653,
                                            0.09177406877279282,
                                            0.09153330326080322,
                                            0.09173539280891418,
                                            0.09170960634946823,
                                            0.09177560359239578,
                                            0.09162052720785141,
                                            0.09143483638763428,
                                            0.09150806069374084,
                                            0.09180814772844315,
                                            0.09179021418094635,
                                            0.09148918092250824,
                                            0.09165149182081223,
                                            0.09149252623319626,
                                            0.09190665930509567,
                                            0.0916096493601799,
                                            0.09160439670085907,
                                            0.09168170392513275,
                                            0.09163866192102432,
                                            0.09190444648265839,
                                            0.091668039560318,
                                            0.09162873029708862,
                                            0.09173496067523956,
                                            0.09242351353168488,
                                            0.09048132598400116,
                                            0.09090138971805573,
                                            0.0904548242688179,
                                            0.09145405888557434,
                                            0.09131670743227005,
                                            0.09178461879491806,
                                            0.09153732657432556,
                                            0.08973786979913712,
                                            0.08988428860902786,
                                            0.09021115303039551,
                                            0.09000113606452942,
                                            0.08976569026708603,
                                            0.08987715095281601,
                                            0.089874766767025,
                                            0.08953355252742767,
                                            0.08985365927219391,
                                            0.08929459005594254,
                                            0.09014719724655151,
                                            0.0901370495557785,
                                            0.08988743275403976,
                                            0.09002864360809326,
                                            0.08994632959365845,
                                            0.09004314243793488,
                                            0.08995210379362106,
                                            0.09007275104522705,
                                            0.09029345214366913,
                                            0.090416319668293,
                                            0.0899377390742302,
                                            0.08996004611253738,
                                            0.08988483250141144,
                                            0.08986671268939972,
                                            0.0900193452835083,
                                            0.0899295061826706,
                                            0.08994332700967789,
                                            0.08973976969718933,
                                            0.08997202664613724,
                                            0.09010780602693558,
                                            0.08996426314115524,
                                            0.08962488174438477,
                                            0.0902571827173233,
                                            0.08992582559585571,
                                            0.0900769904255867,
                                            0.0898761972784996,
                                            0.09017913788557053,
                                            0.08997853100299835,
                                            0.08989366888999939,
                                            0.09010732173919678,
                                            0.09005895256996155,
                                            0.09000539779663086,
                                            0.08995525538921356,
                                            0.08972669392824173,
                                            0.09011795371770859,
                                            0.09001778066158295,
                                            0.09011020511388779,
                                            0.08981508761644363,
                                            0.09017821401357651,
                                            0.08959657698869705,
                                            0.08875498175621033,
                                            0.08870062232017517,
                                            0.08880362659692764,
                                            0.08918870985507965,
                                            0.08892866224050522,
                                            0.08985244482755661,
                                            0.08982453495264053,
                                            0.09029696136713028,
                                            0.09009458869695663,
                                            0.09013500809669495,
                                            0.09010349214076996,
                                            0.08994513750076294,
                                            0.08971379697322845,
                                            0.09011200815439224,
                                            0.0903470441699028,
                                            0.09037244319915771,
                                            0.09014169126749039,
                                            0.08994632959365845,
                                            0.08982060849666595,
                                            0.08958274871110916,
                                            0.08996281772851944,
                                            0.08989058434963226,
                                            0.08994904905557632,
                                            0.0900254026055336,
                                            0.08989007025957108,
                                            0.08997174352407455,
                                            0.08993948996067047,
                                            0.08963050693273544,
                                            0.08996085822582245,
                                            0.09006138890981674,
                                            0.09014187753200531,
                                            0.09013525396585464,
                                            0.08976943790912628,
                                            0.0901322066783905,
                                            0.08987969905138016,
                                            0.08936253190040588,
                                            0.08984756469726562,
                                            0.08993438631296158,
                                            0.0899040624499321,
                                            0.09000980108976364,
                                            0.08996778726577759,
                                            0.08943293988704681,
                                            0.09004877507686615,
                                            0.08976338803768158,
                                            0.08978376537561417,
                                            0.09000024944543839,
                                            0.08958359062671661,
                                            0.08903425931930542,
                                            0.09001847356557846,
                                            0.08996066451072693],
                       'weight_decay': 0.0005}],
 'models': {}}
2019-09-14 09:24:24,729 - training_jobs - DEBUG - test_multiple_models
2019-09-14 09:24:24,729 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-14 09:24:26,731 - training_jobs - ERROR - Error with trains/task_3.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 641, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 523, in training_dispatcher
    models_folder='models/gnn/')
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1793, in test_multiple_models
    testresult = testModel(bmodel, test_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1669, in testModel
    _, pred = model(data).max(dim=1)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 30, in forward
    x = self.ggnn(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 103, in forward
    h = self.rnn(m, h)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/rnn.py", line 800, in forward
    self.bias_ih, self.bias_hh,
RuntimeError: CUDA out of memory. Tried to allocate 716.38 MiB (GPU 0; 3.95 GiB total capacity; 2.32 GiB already allocated; 361.38 MiB free; 396.84 MiB cached)
2019-09-14 09:24:29,485 - training_jobs - DEBUG - training trains/task_5.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 50,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max/',
 'epochs': 300,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-14 09:24:29,497 - training_jobs - DEBUG - training with: 
2019-09-14 09:24:29,497 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max/
2019-09-14 09:24:29,497 - training_jobs - DEBUG - GGNN1
2019-09-14 09:24:29,497 - training_jobs - DEBUG - 
2019-09-14 09:24:29,497 - training_jobs - DEBUG - ggnn training
2019-09-14 09:24:35,784 - training_jobs - DEBUG -  saving results to results/20190914_092435_ggnn_.json
2019-09-14 09:24:35,785 - training_jobs - DEBUG -  calling modelSelection
2019-09-14 10:01:02,730 - training_jobs - ERROR - Error with trains/task_5.yml
Traceback (most recent call last):
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 422, in get
    data = torch.load(filename)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/serialization.py", line 368, in load
    return _load(f, map_location, pickle_module)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/serialization.py", line 524, in _load
    return legacy_load(f)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/serialization.py", line 448, in legacy_load
    with closing(tarfile.open(fileobj=f, mode='r:', format=tarfile.PAX_FORMAT)) as tar, \
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/tarfile.py", line 1589, in open
    return func(name, filemode, fileobj, **kwargs)
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/tarfile.py", line 1619, in taropen
    return cls(name, mode, fileobj, **kwargs)
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/tarfile.py", line 1474, in __init__
    self.offset = self.fileobj.tell()
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "pipeline_train_model.py", line 641, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 511, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1541, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1365, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1123, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 941, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 828, in train_model_GGNN
    for batch in loader:
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 618, in __next__
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 618, in <listcomp>
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 126, in __getitem__
    data = self.get(idx)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 438, in get
    raise IndexError("error getting: ", idx)
IndexError: ('error getting: ', 9250)
2019-09-14 19:27:24,277 - training_jobs - DEBUG - ('tasks/baseline_smaller_datasets', '.yaml')
2019-09-14 19:29:04,333 - training_jobs - DEBUG - ('tasks/baseline_smaller_datasets', '.yaml')
2019-09-14 19:29:07,451 - training_jobs - DEBUG - training trains/task_4.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 50,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-14 19:29:07,498 - training_jobs - DEBUG - training with: 
2019-09-14 19:29:07,498 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max/
2019-09-14 19:29:07,498 - training_jobs - DEBUG - GGNN1
2019-09-14 19:29:07,498 - training_jobs - DEBUG - 
2019-09-14 19:29:07,498 - training_jobs - DEBUG - ggnn training
2019-09-14 19:29:28,356 - training_jobs - DEBUG -  saving results to results/20190914_192928_ggnn_.json
2019-09-14 19:29:28,356 - training_jobs - DEBUG -  calling modelSelection
2019-09-14 21:52:25,334 - training_jobs - DEBUG - test_multiple_models
2019-09-14 21:52:25,461 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-14 21:54:12,063 - training_jobs - ERROR - Error with trains/task_4.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 641, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 523, in training_dispatcher
    models_folder='models/gnn/')
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1793, in test_multiple_models
    testresult = testModel(bmodel, test_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1669, in testModel
    _, pred = model(data).max(dim=1)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 30, in forward
    x = self.ggnn(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 103, in forward
    h = self.rnn(m, h)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/rnn.py", line 800, in forward
    self.bias_ih, self.bias_hh,
RuntimeError: CUDA out of memory. Tried to allocate 716.38 MiB (GPU 0; 3.95 GiB total capacity; 2.32 GiB already allocated; 180.81 MiB free; 395.09 MiB cached)
2019-09-14 21:54:18,812 - training_jobs - DEBUG - training trains/task_7.yml
{'C': 1,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'x_topo_feats',
 'max_iter': 100,
 'model': 'LogisticRegression',
 'multi_class': 'ovr',
 'penalty': 'l2',
 'solver': 'newton-cg'}
2019-09-14 21:54:18,881 - training_jobs - DEBUG - training with: 
2019-09-14 21:54:18,882 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-14 21:54:18,882 - training_jobs - DEBUG - LogisticRegression
2019-09-14 21:54:18,882 - training_jobs - DEBUG - x_topo_feats
2019-09-14 21:54:18,882 - training_jobs - DEBUG - baseline training
2019-09-14 21:54:19,194 - training_jobs - DEBUG -  calling cv_train_models
2019-09-14 21:54:41,246 - training_jobs - DEBUG - training time: 22s
2019-09-14 21:54:41,246 - training_jobs - DEBUG - saving to results/20190914_215419_baseline_x_topo_feats.json
2019-09-14 21:54:41,270 - training_jobs - DEBUG - moved jobdict to done_trainings/task_7.yml
2019-09-14 21:54:41,270 - training_jobs - DEBUG - Finished!

2019-09-14 21:54:43,680 - training_jobs - DEBUG - training trains/task_3.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 50,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-14 21:54:43,692 - training_jobs - DEBUG - training with: 
2019-09-14 21:54:43,692 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max/
2019-09-14 21:54:43,692 - training_jobs - DEBUG - GGNN1
2019-09-14 21:54:43,692 - training_jobs - DEBUG - 
2019-09-14 21:54:43,692 - training_jobs - DEBUG - ggnn training
2019-09-14 21:54:51,900 - training_jobs - DEBUG -  saving results to results/20190914_215451_ggnn_.json
2019-09-14 21:54:51,900 - training_jobs - DEBUG -  calling modelSelection
2019-09-14 23:06:15,263 - training_jobs - DEBUG - test_multiple_models
2019-09-14 23:06:15,263 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-14 23:06:17,479 - training_jobs - ERROR - Error with trains/task_3.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 641, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 523, in training_dispatcher
    models_folder='models/gnn/')
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1793, in test_multiple_models
    testresult = testModel(bmodel, test_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1669, in testModel
    _, pred = model(data).max(dim=1)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 30, in forward
    x = self.ggnn(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 103, in forward
    h = self.rnn(m, h)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/rnn.py", line 800, in forward
    self.bias_ih, self.bias_hh,
RuntimeError: CUDA out of memory. Tried to allocate 716.38 MiB (GPU 0; 3.95 GiB total capacity; 2.32 GiB already allocated; 164.81 MiB free; 394.09 MiB cached)
2019-09-14 23:06:20,617 - training_jobs - DEBUG - training trains/task_10.yml
{'C': 1,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'x_topo_feats',
 'max_iter': 100,
 'model': 'LogisticRegression',
 'multi_class': 'ovr',
 'penalty': 'l2',
 'solver': 'newton-cg'}
2019-09-14 23:06:21,477 - training_jobs - DEBUG - training with: 
2019-09-14 23:06:21,478 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-14 23:06:21,478 - training_jobs - DEBUG - LogisticRegression
2019-09-14 23:06:21,478 - training_jobs - DEBUG - x_topo_feats
2019-09-14 23:06:21,478 - training_jobs - DEBUG - baseline training
2019-09-14 23:06:22,029 - training_jobs - DEBUG -  calling cv_train_models
2019-09-14 23:06:26,062 - training_jobs - DEBUG - training time: 4s
2019-09-14 23:06:26,062 - training_jobs - DEBUG - saving to results/20190914_230622_baseline_x_topo_feats.json
2019-09-14 23:06:26,090 - training_jobs - DEBUG - moved jobdict to done_trainings/task_10.yml
2019-09-14 23:06:26,090 - training_jobs - DEBUG - Finished!

2019-09-14 23:06:29,174 - training_jobs - DEBUG - training trains/task_9.yml
{'C': 1,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'x_topo_feats',
 'max_iter': 100,
 'model': 'LogisticRegression',
 'multi_class': 'ovr',
 'penalty': 'l2',
 'solver': 'newton-cg'}
2019-09-14 23:06:29,187 - training_jobs - DEBUG - training with: 
2019-09-14 23:06:29,188 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-14 23:06:29,188 - training_jobs - DEBUG - LogisticRegression
2019-09-14 23:06:29,188 - training_jobs - DEBUG - x_topo_feats
2019-09-14 23:06:29,188 - training_jobs - DEBUG - baseline training
2019-09-14 23:06:29,888 - training_jobs - DEBUG -  calling cv_train_models
2019-09-14 23:06:33,573 - training_jobs - DEBUG - training time: 4s
2019-09-14 23:06:33,585 - training_jobs - DEBUG - saving to results/20190914_230629_baseline_x_topo_feats.json
2019-09-14 23:06:33,606 - training_jobs - DEBUG - moved jobdict to done_trainings/task_9.yml
2019-09-14 23:06:33,606 - training_jobs - DEBUG - Finished!

2019-09-14 23:06:36,345 - training_jobs - DEBUG - training trains/task_8.yml
{'C': 1,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'x_topo_feats',
 'max_iter': 100,
 'model': 'LogisticRegression',
 'multi_class': 'ovr',
 'penalty': 'l2',
 'solver': 'newton-cg'}
2019-09-14 23:06:36,358 - training_jobs - DEBUG - training with: 
2019-09-14 23:06:36,358 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-14 23:06:36,358 - training_jobs - DEBUG - LogisticRegression
2019-09-14 23:06:36,358 - training_jobs - DEBUG - x_topo_feats
2019-09-14 23:06:36,358 - training_jobs - DEBUG - baseline training
2019-09-14 23:06:37,274 - training_jobs - DEBUG -  calling cv_train_models
2019-09-14 23:07:23,678 - training_jobs - DEBUG - training time: 46s
2019-09-14 23:07:23,678 - training_jobs - DEBUG - saving to results/20190914_230637_baseline_x_topo_feats.json
2019-09-14 23:07:23,700 - training_jobs - DEBUG - moved jobdict to done_trainings/task_8.yml
2019-09-14 23:07:23,701 - training_jobs - DEBUG - Finished!

2019-09-14 23:07:26,211 - training_jobs - DEBUG - training trains/task_6.yml
{'C': 1,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'x_topo_feats',
 'max_iter': 100,
 'model': 'LogisticRegression',
 'multi_class': 'ovr',
 'penalty': 'l2',
 'solver': 'newton-cg'}
2019-09-14 23:07:26,247 - training_jobs - DEBUG - training with: 
2019-09-14 23:07:26,247 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-14 23:07:26,247 - training_jobs - DEBUG - LogisticRegression
2019-09-14 23:07:26,247 - training_jobs - DEBUG - x_topo_feats
2019-09-14 23:07:26,248 - training_jobs - DEBUG - baseline training
2019-09-14 23:07:26,743 - training_jobs - DEBUG -  calling cv_train_models
2019-09-14 23:07:37,950 - training_jobs - DEBUG - training time: 11s
2019-09-14 23:07:37,950 - training_jobs - DEBUG - saving to results/20190914_230726_baseline_x_topo_feats.json
2019-09-14 23:07:37,974 - training_jobs - DEBUG - moved jobdict to done_trainings/task_6.yml
2019-09-14 23:07:37,974 - training_jobs - DEBUG - Finished!

2019-09-14 23:07:40,449 - training_jobs - DEBUG - training trains/task_5.yml
{'C': 1,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'x_topo_feats',
 'max_iter': 100,
 'model': 'LogisticRegression',
 'multi_class': 'ovr',
 'penalty': 'l2',
 'solver': 'newton-cg'}
2019-09-14 23:07:40,500 - training_jobs - DEBUG - training with: 
2019-09-14 23:07:40,500 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-14 23:07:40,500 - training_jobs - DEBUG - LogisticRegression
2019-09-14 23:07:40,500 - training_jobs - DEBUG - x_topo_feats
2019-09-14 23:07:40,500 - training_jobs - DEBUG - baseline training
2019-09-14 23:07:40,853 - training_jobs - DEBUG -  calling cv_train_models
2019-09-14 23:07:49,396 - training_jobs - DEBUG - training time: 9s
2019-09-14 23:07:49,397 - training_jobs - DEBUG - saving to results/20190914_230740_baseline_x_topo_feats.json
2019-09-14 23:07:49,410 - training_jobs - DEBUG - moved jobdict to done_trainings/task_5.yml
2019-09-14 23:07:49,410 - training_jobs - DEBUG - Finished!

2019-09-14 23:07:52,066 - training_jobs - DEBUG - training trains/task_1.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 30,
 'd2': 50,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-14 23:07:52,078 - training_jobs - DEBUG - training with: 
2019-09-14 23:07:52,078 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max/
2019-09-14 23:07:52,078 - training_jobs - DEBUG - GGNN1
2019-09-14 23:07:52,079 - training_jobs - DEBUG - 
2019-09-14 23:07:52,079 - training_jobs - DEBUG - ggnn training
2019-09-14 23:07:59,568 - training_jobs - DEBUG -  saving results to results/20190914_230759_ggnn_.json
2019-09-14 23:07:59,568 - training_jobs - DEBUG -  calling modelSelection
2019-09-14 23:58:17,429 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 422, in get
    data = torch.load(filename)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/serialization.py", line 368, in load
    return _load(f, map_location, pickle_module)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/serialization.py", line 375, in _load
    deserialized_objects = {}
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "pipeline_train_model.py", line 641, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 511, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1541, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1365, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1123, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 941, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 828, in train_model_GGNN
    for batch in loader:
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 618, in __next__
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 618, in <listcomp>
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 126, in __getitem__
    data = self.get(idx)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 438, in get
    raise IndexError("error getting: ", idx)
IndexError: ('error getting: ', 5371)
2019-09-15 01:35:09,859 - training_jobs - DEBUG - ('tasks/nlp_mlp3_all_datasets', '.yaml')
2019-09-15 01:35:09,955 - training_jobs - DEBUG - ('tasks/baseline_xgboost_all_dataset', '.yaml')
2019-09-15 01:35:10,057 - training_jobs - DEBUG - ('tasks/baseline_nn_mlp3_all_datasets', '.yaml')
2019-09-15 01:35:10,149 - training_jobs - DEBUG - ('tasks/nlp_xgboost_all_datasets', '.yaml')
2019-09-15 01:35:10,211 - training_jobs - DEBUG - ('tasks/gnn_ggnn5_all_datasets', '.yaml')
2019-09-15 01:35:10,260 - training_jobs - DEBUG - ('tasks/gnn_ggnn6_all_datasets', '.yaml')
2019-09-15 01:35:10,313 - training_jobs - DEBUG - ('tasks/nlp_randomforest_all_datasets', '.yaml')
2019-09-15 01:35:10,344 - training_jobs - DEBUG - ('tasks/nlp_mlp2_all_datasets', '.yaml')
2019-09-15 01:35:10,354 - training_jobs - DEBUG - ('tasks/gnn_ggnn1_all_datasets', '.yaml')
2019-09-15 01:35:10,416 - training_jobs - DEBUG - ('tasks/baseline_nn_mlp2_all_datasets', '.yaml')
2019-09-15 01:35:10,427 - training_jobs - DEBUG - ('tasks/baseline_nn_mlp4_all_datasets', '.yaml')
2019-09-15 01:35:10,463 - training_jobs - DEBUG - ('tasks/nlp_mlp4_all_datasets', '.yaml')
2019-09-15 01:35:10,475 - training_jobs - DEBUG - ('tasks/baseline_randomforest_all_datasets', '.yaml')
2019-09-15 01:35:10,531 - training_jobs - DEBUG - ('tasks/baseline_logistic_all_datasets', '.yaml')
2019-09-15 01:35:10,541 - training_jobs - DEBUG - ('tasks/nlp_logisticregression_all_datasets', '.yaml')
2019-09-15 01:35:13,014 - training_jobs - DEBUG - training trains/task_115.yml
{'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-15 01:35:13,027 - training_jobs - DEBUG - training with: 
2019-09-15 01:35:13,027 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-15 01:35:13,027 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 01:35:13,027 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 01:35:13,027 - training_jobs - DEBUG - nlp training
2019-09-15 01:35:14,344 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-15 01:35:15,928 - training_jobs - DEBUG - training time: 2s
2019-09-15 01:35:15,928 - training_jobs - DEBUG - saving to results/20190915_013514_nlp_tfidf_and_topo_feats.json
2019-09-15 01:35:15,931 - training_jobs - DEBUG - moved jobdict to done_trainings/task_115.yml
2019-09-15 01:35:15,931 - training_jobs - DEBUG - Finished!

2019-09-15 01:35:18,254 - training_jobs - DEBUG - training trains/task_20.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'x_topo_feats',
 'max_depth': 8,
 'model': 'XGBClassifier'}
2019-09-15 01:35:18,266 - training_jobs - DEBUG - training with: 
2019-09-15 01:35:18,266 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 01:35:18,266 - training_jobs - DEBUG - XGBClassifier
2019-09-15 01:35:18,266 - training_jobs - DEBUG - x_topo_feats
2019-09-15 01:35:18,266 - training_jobs - DEBUG - baseline training
2019-09-15 01:35:19,045 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 01:37:19,494 - training_jobs - DEBUG - training time: 120s
2019-09-15 01:37:19,494 - training_jobs - DEBUG - saving to results/20190915_013519_baseline_x_topo_feats.json
2019-09-15 01:37:19,500 - training_jobs - DEBUG - moved jobdict to done_trainings/task_20.yml
2019-09-15 01:37:19,500 - training_jobs - DEBUG - Finished!

2019-09-15 01:37:21,982 - training_jobs - DEBUG - training trains/task_104.yml
{'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-15 01:37:21,993 - training_jobs - DEBUG - training with: 
2019-09-15 01:37:21,993 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 01:37:21,993 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 01:37:21,993 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 01:37:21,993 - training_jobs - DEBUG - nlp training
2019-09-15 01:37:22,812 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-15 01:37:23,791 - training_jobs - DEBUG - training time: 1s
2019-09-15 01:37:23,791 - training_jobs - DEBUG - saving to results/20190915_013722_nlp_tfidf_and_topo_feats.json
2019-09-15 01:37:23,794 - training_jobs - DEBUG - moved jobdict to done_trainings/task_104.yml
2019-09-15 01:37:23,794 - training_jobs - DEBUG - Finished!

2019-09-15 01:37:26,177 - training_jobs - DEBUG - training trains/task_130.yml
{'d1': 85,
 'd2': 20,
 'd3': 15,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'model': 'mlp2',
 'num_epochs': 30}
2019-09-15 01:37:26,189 - training_jobs - DEBUG - training with: 
2019-09-15 01:37:26,189 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 01:37:26,189 - training_jobs - DEBUG - mlp2
2019-09-15 01:37:26,189 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 01:37:26,189 - training_jobs - DEBUG - nlp_nn training
2019-09-15 01:37:27,462 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-15 01:39:24,809 - training_jobs - DEBUG - training time: 117s
2019-09-15 01:39:24,809 - training_jobs - DEBUG - saving to results/20190915_013727_nlp_nn_tfidf_and_topo_feats.json
2019-09-15 01:39:24,813 - training_jobs - DEBUG - moved jobdict to done_trainings/task_130.yml
2019-09-15 01:39:24,813 - training_jobs - DEBUG - Finished!

2019-09-15 01:39:27,393 - training_jobs - DEBUG - training trains/task_106.yml
{'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-15 01:39:27,404 - training_jobs - DEBUG - training with: 
2019-09-15 01:39:27,405 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 01:39:27,405 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 01:39:27,405 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 01:39:27,405 - training_jobs - DEBUG - nlp training
2019-09-15 01:39:27,798 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-15 01:39:29,263 - training_jobs - DEBUG - training time: 1s
2019-09-15 01:39:29,263 - training_jobs - DEBUG - saving to results/20190915_013927_nlp_tfidf_and_topo_feats.json
2019-09-15 01:39:29,266 - training_jobs - DEBUG - moved jobdict to done_trainings/task_106.yml
2019-09-15 01:39:29,266 - training_jobs - DEBUG - Finished!

2019-09-15 01:39:31,850 - training_jobs - DEBUG - training trains/task_207.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'model': 'mlp4',
 'num_epochs': 100}
2019-09-15 01:39:31,862 - training_jobs - DEBUG - training with: 
2019-09-15 01:39:31,862 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-15 01:39:31,862 - training_jobs - DEBUG - mlp4
2019-09-15 01:39:31,862 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 01:39:31,863 - training_jobs - DEBUG - moved jobdict to done_trainings/task_207.yml
2019-09-15 01:39:31,863 - training_jobs - DEBUG - Finished!

2019-09-15 01:39:34,377 - training_jobs - DEBUG - training trains/task_4.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'model': 'mlp3',
 'num_epochs': 30}
2019-09-15 01:39:34,388 - training_jobs - DEBUG - training with: 
2019-09-15 01:39:34,388 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 01:39:34,388 - training_jobs - DEBUG - mlp3
2019-09-15 01:39:34,388 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 01:39:34,389 - training_jobs - DEBUG - moved jobdict to done_trainings/task_4.yml
2019-09-15 01:39:34,389 - training_jobs - DEBUG - Finished!

2019-09-15 01:39:36,769 - training_jobs - DEBUG - training trains/task_255.yml
{'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-15 01:39:36,779 - training_jobs - DEBUG - training with: 
2019-09-15 01:39:36,779 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-15 01:39:36,779 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 01:39:36,779 - training_jobs - DEBUG - topo and code feats
2019-09-15 01:39:36,779 - training_jobs - DEBUG - baseline training
2019-09-15 01:39:36,849 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 01:39:37,841 - training_jobs - DEBUG - training time: 1s
2019-09-15 01:39:37,841 - training_jobs - DEBUG - saving to results/20190915_013936_baseline_topo_and_code_feats.json
2019-09-15 01:39:37,847 - training_jobs - DEBUG - moved jobdict to done_trainings/task_255.yml
2019-09-15 01:39:37,847 - training_jobs - DEBUG - Finished!

2019-09-15 01:39:40,338 - training_jobs - DEBUG - training trains/task_264.yml
{'C': 1,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'x_topo_feats',
 'max_iter': 100,
 'model': 'LogisticRegression',
 'multi_class': 'ovr',
 'penalty': 'l2',
 'solver': 'newton-cg'}
2019-09-15 01:39:40,349 - training_jobs - DEBUG - training with: 
2019-09-15 01:39:40,349 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-15 01:39:40,349 - training_jobs - DEBUG - LogisticRegression
2019-09-15 01:39:40,349 - training_jobs - DEBUG - x_topo_feats
2019-09-15 01:39:40,349 - training_jobs - DEBUG - baseline training
2019-09-15 01:39:40,685 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 01:39:46,339 - training_jobs - DEBUG - training time: 6s
2019-09-15 01:39:46,356 - training_jobs - DEBUG - saving to results/20190915_013940_baseline_x_topo_feats.json
2019-09-15 01:39:46,369 - training_jobs - DEBUG - moved jobdict to done_trainings/task_264.yml
2019-09-15 01:39:46,369 - training_jobs - DEBUG - Finished!

2019-09-15 01:39:48,932 - training_jobs - DEBUG - training trains/task_18.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'max_depth': 8,
 'model': 'XGBClassifier'}
2019-09-15 01:39:48,944 - training_jobs - DEBUG - training with: 
2019-09-15 01:39:48,944 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-15 01:39:48,944 - training_jobs - DEBUG - XGBClassifier
2019-09-15 01:39:48,944 - training_jobs - DEBUG - topo and code feats
2019-09-15 01:39:48,944 - training_jobs - DEBUG - baseline training
2019-09-15 01:39:49,538 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 01:41:15,429 - training_jobs - DEBUG - training time: 86s
2019-09-15 01:41:15,429 - training_jobs - DEBUG - saving to results/20190915_013949_baseline_topo_and_code_feats.json
2019-09-15 01:41:15,440 - training_jobs - DEBUG - moved jobdict to done_trainings/task_18.yml
2019-09-15 01:41:15,440 - training_jobs - DEBUG - Finished!

2019-09-15 01:41:18,308 - training_jobs - DEBUG - training trains/task_67.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'max_depth': 150,
 'model': 'XGBClassifier'}
2019-09-15 01:41:18,321 - training_jobs - DEBUG - training with: 
2019-09-15 01:41:18,321 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 01:41:18,321 - training_jobs - DEBUG - XGBClassifier
2019-09-15 01:41:18,321 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 01:41:18,321 - training_jobs - DEBUG - nlp training
2019-09-15 01:41:19,921 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-15 02:00:16,195 - training_jobs - DEBUG - training trains/task_67.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'max_depth': 150,
 'model': 'XGBClassifier'}
2019-09-15 02:00:16,208 - training_jobs - DEBUG - training with: 
2019-09-15 02:00:16,208 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 02:00:16,208 - training_jobs - DEBUG - XGBClassifier
2019-09-15 02:00:16,208 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 02:00:16,208 - training_jobs - DEBUG - nlp training
2019-09-15 02:00:16,870 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-15 02:47:01,916 - training_jobs - DEBUG - training time: 2805s
2019-09-15 02:47:01,916 - training_jobs - DEBUG - saving to results/20190915_020016_nlp_tfidf_and_topo_feats.json
2019-09-15 02:47:01,917 - training_jobs - ERROR - Error with trains/task_67.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 352, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
  File "pipeline_train_model.py", line 46, in add_dataset_file
    for k2,v2 in v:
ValueError: too many values to unpack (expected 2)
2019-09-15 02:47:04,411 - training_jobs - DEBUG - training trains/task_109.yml
{'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-15 02:47:04,424 - training_jobs - DEBUG - training with: 
2019-09-15 02:47:04,424 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 02:47:04,424 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 02:47:04,424 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 02:47:04,424 - training_jobs - DEBUG - nlp training
2019-09-15 02:47:05,127 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-15 02:47:10,018 - training_jobs - DEBUG - training time: 5s
2019-09-15 02:47:10,018 - training_jobs - DEBUG - saving to results/20190915_024705_nlp_tfidf_and_topo_feats.json
2019-09-15 02:47:10,018 - training_jobs - ERROR - Error with trains/task_109.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 352, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
  File "pipeline_train_model.py", line 46, in add_dataset_file
    for k2,v2 in v:
ValueError: too many values to unpack (expected 2)
2019-09-15 02:47:12,685 - training_jobs - DEBUG - training trains/task_194.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'model': 'mlp4',
 'num_epochs': 30}
2019-09-15 02:47:12,697 - training_jobs - DEBUG - training with: 
2019-09-15 02:47:12,697 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-15 02:47:12,697 - training_jobs - DEBUG - mlp4
2019-09-15 02:47:12,697 - training_jobs - DEBUG - topo and code feats
2019-09-15 02:47:12,698 - training_jobs - DEBUG - moved jobdict to done_trainings/task_194.yml
2019-09-15 02:47:12,698 - training_jobs - DEBUG - Finished!

2019-09-15 02:47:15,077 - training_jobs - DEBUG - training trains/task_139.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 02:47:15,090 - training_jobs - DEBUG - training with: 
2019-09-15 02:47:15,090 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 02:47:15,090 - training_jobs - DEBUG - GGNN1
2019-09-15 02:47:15,090 - training_jobs - DEBUG - 
2019-09-15 02:47:15,090 - training_jobs - DEBUG - ggnn training
2019-09-15 02:47:15,132 - training_jobs - ERROR - Error with trains/task_139.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 470, in training_dispatcher
    train_dataset = FunctionsDataset(root=os.path.join(dataset_folder,'training_set'))
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 223, in __init__
    super(FunctionsDataset, self).__init__(root, transform, pre_transform)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 85, in __init__
    self._process()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 113, in _process
    if files_exist(self.processed_paths):  # pragma: no cover
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 102, in processed_paths
    files = to_list(self.processed_file_names)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 255, in processed_file_names
    for item in os.listdir(processed_path):
FileNotFoundError: [Errno 2] No such file or directory: 'tmp/symbols_dataset_2_precomp_split_undersample_max_half/training_set/processed'
2019-09-15 02:47:17,613 - training_jobs - DEBUG - training trains/task_119.yml
{'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-15 02:47:17,623 - training_jobs - DEBUG - training with: 
2019-09-15 02:47:17,624 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 02:47:17,624 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 02:47:17,624 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 02:47:17,624 - training_jobs - DEBUG - nlp training
2019-09-15 02:47:18,185 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-15 02:47:20,625 - training_jobs - DEBUG - training time: 2s
2019-09-15 02:47:20,625 - training_jobs - DEBUG - saving to results/20190915_024718_nlp_tfidf_and_topo_feats.json
2019-09-15 02:47:20,625 - training_jobs - ERROR - Error with trains/task_119.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 352, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
  File "pipeline_train_model.py", line 46, in add_dataset_file
    for k2,v2 in v:
ValueError: too many values to unpack (expected 2)
2019-09-15 02:47:23,011 - training_jobs - DEBUG - training trains/task_2.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'model': 'mlp3',
 'num_epochs': 30}
2019-09-15 02:47:23,022 - training_jobs - DEBUG - training with: 
2019-09-15 02:47:23,022 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-15 02:47:23,022 - training_jobs - DEBUG - mlp3
2019-09-15 02:47:23,022 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 02:47:23,023 - training_jobs - DEBUG - moved jobdict to done_trainings/task_2.yml
2019-09-15 02:47:23,023 - training_jobs - DEBUG - Finished!

2019-09-15 02:47:25,439 - training_jobs - DEBUG - training trains/task_213.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'model': 'mlp4',
 'num_epochs': 100}
2019-09-15 02:47:25,451 - training_jobs - DEBUG - training with: 
2019-09-15 02:47:25,452 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-15 02:47:25,452 - training_jobs - DEBUG - mlp4
2019-09-15 02:47:25,452 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 02:47:25,453 - training_jobs - DEBUG - moved jobdict to done_trainings/task_213.yml
2019-09-15 02:47:25,453 - training_jobs - DEBUG - Finished!

2019-09-15 02:47:27,827 - training_jobs - DEBUG - training trains/task_197.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'model': 'mlp4',
 'num_epochs': 100}
2019-09-15 02:47:27,838 - training_jobs - DEBUG - training with: 
2019-09-15 02:47:27,838 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 02:47:27,839 - training_jobs - DEBUG - mlp4
2019-09-15 02:47:27,839 - training_jobs - DEBUG - topo and code feats
2019-09-15 02:47:27,840 - training_jobs - DEBUG - moved jobdict to done_trainings/task_197.yml
2019-09-15 02:47:27,840 - training_jobs - DEBUG - Finished!

2019-09-15 02:47:30,224 - training_jobs - DEBUG - training trains/task_79.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 15,
 'd4': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 02:47:30,236 - training_jobs - DEBUG - training with: 
2019-09-15 02:47:30,236 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 02:47:30,236 - training_jobs - DEBUG - GGNN5
2019-09-15 02:47:30,236 - training_jobs - DEBUG - 
2019-09-15 02:47:30,237 - training_jobs - DEBUG - moved jobdict to done_trainings/task_79.yml
2019-09-15 02:47:30,238 - training_jobs - DEBUG - Finished!

2019-09-15 02:47:32,783 - training_jobs - DEBUG - training trains/task_155.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 02:47:32,795 - training_jobs - DEBUG - training with: 
2019-09-15 02:47:32,795 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 02:47:32,796 - training_jobs - DEBUG - GGNN1
2019-09-15 02:47:32,796 - training_jobs - DEBUG - 
2019-09-15 02:47:32,796 - training_jobs - DEBUG - ggnn training
2019-09-15 02:47:32,796 - training_jobs - ERROR - Error with trains/task_155.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 470, in training_dispatcher
    train_dataset = FunctionsDataset(root=os.path.join(dataset_folder,'training_set'))
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 223, in __init__
    super(FunctionsDataset, self).__init__(root, transform, pre_transform)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 85, in __init__
    self._process()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 113, in _process
    if files_exist(self.processed_paths):  # pragma: no cover
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 102, in processed_paths
    files = to_list(self.processed_file_names)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 255, in processed_file_names
    for item in os.listdir(processed_path):
FileNotFoundError: [Errno 2] No such file or directory: 'tmp/symbols_dataset_1_precomp_split_undersample_max_half/training_set/processed'
2019-09-15 02:47:35,089 - training_jobs - DEBUG - training trains/task_127.yml
{'d1': 85,
 'd2': 20,
 'd3': 15,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'model': 'mlp2',
 'num_epochs': 100}
2019-09-15 02:47:35,100 - training_jobs - DEBUG - training with: 
2019-09-15 02:47:35,100 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 02:47:35,101 - training_jobs - DEBUG - mlp2
2019-09-15 02:47:35,101 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 02:47:35,101 - training_jobs - DEBUG - nlp_nn training
2019-09-15 02:47:35,742 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-15 02:54:37,112 - training_jobs - DEBUG - training time: 421s
2019-09-15 02:54:37,112 - training_jobs - DEBUG - saving to results/20190915_024735_nlp_nn_tfidf_and_topo_feats.json
2019-09-15 02:54:37,113 - training_jobs - ERROR - Error with trains/task_127.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 449, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
  File "pipeline_train_model.py", line 46, in add_dataset_file
    for k2,v2 in v:
ValueError: too many values to unpack (expected 2)
2019-09-15 02:54:39,536 - training_jobs - DEBUG - training trains/task_7.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'model': 'mlp3',
 'num_epochs': 100}
2019-09-15 02:54:39,547 - training_jobs - DEBUG - training with: 
2019-09-15 02:54:39,547 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 02:54:39,547 - training_jobs - DEBUG - mlp3
2019-09-15 02:54:39,547 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 02:54:39,548 - training_jobs - DEBUG - moved jobdict to done_trainings/task_7.yml
2019-09-15 02:54:39,548 - training_jobs - DEBUG - Finished!

2019-09-15 02:54:41,910 - training_jobs - DEBUG - training trains/task_162.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 02:54:42,169 - training_jobs - DEBUG - training with: 
2019-09-15 02:54:42,170 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 02:54:42,170 - training_jobs - DEBUG - GGNN1
2019-09-15 02:54:42,170 - training_jobs - DEBUG - 
2019-09-15 02:54:42,170 - training_jobs - DEBUG - ggnn training
2019-09-15 02:54:42,170 - training_jobs - ERROR - Error with trains/task_162.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 470, in training_dispatcher
    train_dataset = FunctionsDataset(root=os.path.join(dataset_folder,'training_set'))
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 223, in __init__
    super(FunctionsDataset, self).__init__(root, transform, pre_transform)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 85, in __init__
    self._process()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 113, in _process
    if files_exist(self.processed_paths):  # pragma: no cover
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 102, in processed_paths
    files = to_list(self.processed_file_names)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 255, in processed_file_names
    for item in os.listdir(processed_path):
FileNotFoundError: [Errno 2] No such file or directory: 'tmp/symbols_dataset_2_precomp_split_undersample_max_half/training_set/processed'
2019-09-15 02:54:44,572 - training_jobs - DEBUG - training trains/task_120.yml
{'d1': 85,
 'd2': 20,
 'd3': 15,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'model': 'mlp2',
 'num_epochs': 30}
2019-09-15 02:54:44,596 - training_jobs - DEBUG - training with: 
2019-09-15 02:54:44,596 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-15 02:54:44,596 - training_jobs - DEBUG - mlp2
2019-09-15 02:54:44,596 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 02:54:44,596 - training_jobs - DEBUG - nlp_nn training
2019-09-15 02:54:45,085 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-15 02:56:09,396 - training_jobs - DEBUG - training time: 84s
2019-09-15 02:56:09,396 - training_jobs - DEBUG - saving to results/20190915_025445_nlp_nn_tfidf_and_topo_feats.json
2019-09-15 02:56:09,396 - training_jobs - ERROR - Error with trains/task_120.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 449, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
  File "pipeline_train_model.py", line 46, in add_dataset_file
    for k2,v2 in v:
ValueError: too many values to unpack (expected 2)
2019-09-15 02:56:11,855 - training_jobs - DEBUG - training trains/task_243.yml
{'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'x_topo_feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-15 02:56:11,866 - training_jobs - DEBUG - training with: 
2019-09-15 02:56:11,867 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 02:56:11,867 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 02:56:11,867 - training_jobs - DEBUG - x_topo_feats
2019-09-15 02:56:11,867 - training_jobs - DEBUG - baseline training
2019-09-15 02:56:11,984 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 02:56:15,922 - training_jobs - DEBUG - training time: 4s
2019-09-15 02:56:15,923 - training_jobs - DEBUG - saving to results/20190915_025611_baseline_x_topo_feats.json
2019-09-15 02:56:15,923 - training_jobs - ERROR - Error with trains/task_243.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 176, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
  File "pipeline_train_model.py", line 46, in add_dataset_file
    for k2,v2 in v:
ValueError: too many values to unpack (expected 2)
2019-09-15 02:56:18,406 - training_jobs - DEBUG - training trains/task_152.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 02:56:18,435 - training_jobs - DEBUG - training with: 
2019-09-15 02:56:18,435 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-15 02:56:18,435 - training_jobs - DEBUG - GGNN1
2019-09-15 02:56:18,435 - training_jobs - DEBUG - 
2019-09-15 02:56:18,435 - training_jobs - DEBUG - ggnn training
2019-09-15 02:56:18,436 - training_jobs - ERROR - Error with trains/task_152.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 470, in training_dispatcher
    train_dataset = FunctionsDataset(root=os.path.join(dataset_folder,'training_set'))
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 223, in __init__
    super(FunctionsDataset, self).__init__(root, transform, pre_transform)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 85, in __init__
    self._process()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 113, in _process
    if files_exist(self.processed_paths):  # pragma: no cover
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 102, in processed_paths
    files = to_list(self.processed_file_names)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 255, in processed_file_names
    for item in os.listdir(processed_path):
FileNotFoundError: [Errno 2] No such file or directory: 'tmp/symbols_dataset_1_precomp_split_undersample_max_third/training_set/processed'
2019-09-15 02:56:20,829 - training_jobs - DEBUG - training trains/task_19.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'max_depth': 150,
 'model': 'XGBClassifier'}
2019-09-15 02:56:20,850 - training_jobs - DEBUG - training with: 
2019-09-15 02:56:20,850 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-15 02:56:20,850 - training_jobs - DEBUG - XGBClassifier
2019-09-15 02:56:20,850 - training_jobs - DEBUG - topo and code feats
2019-09-15 02:56:20,850 - training_jobs - DEBUG - baseline training
2019-09-15 02:56:21,029 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 03:00:03,857 - training_jobs - DEBUG - training time: 223s
2019-09-15 03:00:03,858 - training_jobs - DEBUG - saving to results/20190915_025621_baseline_topo_and_code_feats.json
2019-09-15 03:00:03,858 - training_jobs - ERROR - Error with trains/task_19.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 176, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
  File "pipeline_train_model.py", line 46, in add_dataset_file
    for k2,v2 in v:
ValueError: too many values to unpack (expected 2)
2019-09-15 03:00:06,804 - training_jobs - DEBUG - training trains/task_202.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'model': 'mlp4',
 'num_epochs': 30}
2019-09-15 03:00:06,909 - training_jobs - DEBUG - training with: 
2019-09-15 03:00:06,909 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 03:00:06,909 - training_jobs - DEBUG - mlp4
2019-09-15 03:00:06,909 - training_jobs - DEBUG - topo and code feats
2019-09-15 03:00:06,911 - training_jobs - DEBUG - moved jobdict to done_trainings/task_202.yml
2019-09-15 03:00:06,911 - training_jobs - DEBUG - Finished!

2019-09-15 03:00:09,809 - training_jobs - DEBUG - training trains/task_280.yml
{'C': 1,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'max_iter': 100,
 'model': 'LogisticRegression',
 'multi_class': 'ovr',
 'penalty': 'l2',
 'solver': 'newton-cg'}
2019-09-15 03:00:09,832 - training_jobs - DEBUG - training with: 
2019-09-15 03:00:09,832 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-15 03:00:09,832 - training_jobs - DEBUG - LogisticRegression
2019-09-15 03:00:09,833 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 03:00:09,833 - training_jobs - DEBUG - nlp training
2019-09-15 03:00:10,268 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-15 03:00:21,432 - training_jobs - DEBUG - training time: 11s
2019-09-15 03:00:21,432 - training_jobs - DEBUG - saving to results/20190915_030010_nlp_tfidf_and_topo_feats.json
2019-09-15 03:00:21,433 - training_jobs - ERROR - Error with trains/task_280.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 352, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
  File "pipeline_train_model.py", line 46, in add_dataset_file
    for k2,v2 in v:
ValueError: too many values to unpack (expected 2)
2019-09-15 03:00:23,864 - training_jobs - DEBUG - training trains/task_132.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 03:00:23,878 - training_jobs - DEBUG - training with: 
2019-09-15 03:00:23,878 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-15 03:00:23,878 - training_jobs - DEBUG - GGNN1
2019-09-15 03:00:23,878 - training_jobs - DEBUG - 
2019-09-15 03:00:23,878 - training_jobs - DEBUG - ggnn training
2019-09-15 03:00:23,879 - training_jobs - ERROR - Error with trains/task_132.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 470, in training_dispatcher
    train_dataset = FunctionsDataset(root=os.path.join(dataset_folder,'training_set'))
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 223, in __init__
    super(FunctionsDataset, self).__init__(root, transform, pre_transform)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 85, in __init__
    self._process()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 113, in _process
    if files_exist(self.processed_paths):  # pragma: no cover
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 102, in processed_paths
    files = to_list(self.processed_file_names)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 255, in processed_file_names
    for item in os.listdir(processed_path):
FileNotFoundError: [Errno 2] No such file or directory: 'tmp/symbols_dataset_3_precomp_split_undersample_max_third/training_set/processed'
2019-09-15 03:00:26,415 - training_jobs - DEBUG - training trains/task_180.yml
{'d1': 85,
 'd2': 20,
 'd3': 15,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'model': 'mlp2',
 'num_epochs': 30}
2019-09-15 03:00:26,427 - training_jobs - DEBUG - training with: 
2019-09-15 03:00:26,427 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-15 03:00:26,427 - training_jobs - DEBUG - mlp2
2019-09-15 03:00:26,427 - training_jobs - DEBUG - topo and code feats
2019-09-15 03:00:26,427 - training_jobs - DEBUG - baseline_nn training
2019-09-15 03:00:26,515 - training_jobs - DEBUG -  calling nn_train_models
2019-09-15 03:01:51,889 - training_jobs - DEBUG - training time: 85s
2019-09-15 03:01:51,889 - training_jobs - DEBUG - saving to results/20190915_030026_baseline_nn_topo_and_code_feats.json
2019-09-15 03:01:51,889 - training_jobs - ERROR - Error with trains/task_180.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 260, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
  File "pipeline_train_model.py", line 46, in add_dataset_file
    for k2,v2 in v:
ValueError: too many values to unpack (expected 2)
2019-09-15 03:01:54,305 - training_jobs - DEBUG - training trains/task_241.yml
{'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'x_topo_feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-15 03:01:54,317 - training_jobs - DEBUG - training with: 
2019-09-15 03:01:54,317 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 03:01:54,317 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 03:01:54,317 - training_jobs - DEBUG - x_topo_feats
2019-09-15 03:01:54,317 - training_jobs - DEBUG - baseline training
2019-09-15 03:01:54,435 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 03:01:56,317 - training_jobs - DEBUG - training time: 2s
2019-09-15 03:01:56,317 - training_jobs - DEBUG - saving to results/20190915_030154_baseline_x_topo_feats.json
2019-09-15 03:01:56,318 - training_jobs - ERROR - Error with trains/task_241.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 176, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
  File "pipeline_train_model.py", line 46, in add_dataset_file
    for k2,v2 in v:
ValueError: too many values to unpack (expected 2)
2019-09-15 03:01:58,923 - training_jobs - DEBUG - training trains/task_250.yml
{'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'x_topo_feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-15 03:01:58,935 - training_jobs - DEBUG - training with: 
2019-09-15 03:01:58,935 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-15 03:01:58,935 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 03:01:58,935 - training_jobs - DEBUG - x_topo_feats
2019-09-15 03:01:58,935 - training_jobs - DEBUG - baseline training
2019-09-15 03:01:58,981 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 03:01:59,722 - training_jobs - DEBUG - training time: 1s
2019-09-15 03:01:59,723 - training_jobs - DEBUG - saving to results/20190915_030158_baseline_x_topo_feats.json
2019-09-15 03:01:59,723 - training_jobs - ERROR - Error with trains/task_250.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 176, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
  File "pipeline_train_model.py", line 46, in add_dataset_file
    for k2,v2 in v:
ValueError: too many values to unpack (expected 2)
2019-09-15 03:02:02,321 - training_jobs - DEBUG - training trains/task_233.yml
{'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'x_topo_feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-15 03:02:02,333 - training_jobs - DEBUG - training with: 
2019-09-15 03:02:02,333 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 03:02:02,333 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 03:02:02,333 - training_jobs - DEBUG - x_topo_feats
2019-09-15 03:02:02,333 - training_jobs - DEBUG - baseline training
2019-09-15 03:02:02,382 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 03:02:03,737 - training_jobs - DEBUG - training time: 1s
2019-09-15 03:02:03,737 - training_jobs - DEBUG - saving to results/20190915_030202_baseline_x_topo_feats.json
2019-09-15 03:02:03,737 - training_jobs - ERROR - Error with trains/task_233.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 176, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
  File "pipeline_train_model.py", line 46, in add_dataset_file
    for k2,v2 in v:
ValueError: too many values to unpack (expected 2)
2019-09-15 03:02:06,195 - training_jobs - DEBUG - training trains/task_261.yml
{'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-15 03:02:06,213 - training_jobs - DEBUG - training with: 
2019-09-15 03:02:06,213 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 03:02:06,213 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 03:02:06,213 - training_jobs - DEBUG - topo and code feats
2019-09-15 03:02:06,214 - training_jobs - DEBUG - baseline training
2019-09-15 03:02:06,390 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 03:02:07,431 - training_jobs - DEBUG - training time: 1s
2019-09-15 03:02:07,431 - training_jobs - DEBUG - saving to results/20190915_030206_baseline_topo_and_code_feats.json
2019-09-15 03:02:07,431 - training_jobs - ERROR - Error with trains/task_261.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 176, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
  File "pipeline_train_model.py", line 46, in add_dataset_file
    for k2,v2 in v:
ValueError: too many values to unpack (expected 2)
2019-09-15 03:02:09,955 - training_jobs - DEBUG - training trains/task_125.yml
{'d1': 85,
 'd2': 20,
 'd3': 15,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'model': 'mlp2',
 'num_epochs': 100}
2019-09-15 03:02:09,975 - training_jobs - DEBUG - training with: 
2019-09-15 03:02:09,975 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 03:02:09,975 - training_jobs - DEBUG - mlp2
2019-09-15 03:02:09,975 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 03:02:09,975 - training_jobs - DEBUG - nlp_nn training
2019-09-15 03:02:10,405 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-15 03:06:54,333 - training_jobs - DEBUG - training time: 284s
2019-09-15 03:06:54,333 - training_jobs - DEBUG - saving to results/20190915_030210_nlp_nn_tfidf_and_topo_feats.json
2019-09-15 03:06:54,333 - training_jobs - ERROR - Error with trains/task_125.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 449, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
  File "pipeline_train_model.py", line 46, in add_dataset_file
    for k2,v2 in v:
ValueError: too many values to unpack (expected 2)
2019-09-15 03:06:56,841 - training_jobs - DEBUG - training trains/task_47.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'max_depth': 150,
 'model': 'XGBClassifier'}
2019-09-15 03:06:57,075 - training_jobs - DEBUG - training with: 
2019-09-15 03:06:57,075 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-15 03:06:57,075 - training_jobs - DEBUG - XGBClassifier
2019-09-15 03:06:57,075 - training_jobs - DEBUG - topo and code feats
2019-09-15 03:06:57,075 - training_jobs - DEBUG - baseline training
2019-09-15 03:06:57,144 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 03:07:46,622 - training_jobs - DEBUG - training time: 49s
2019-09-15 03:07:46,623 - training_jobs - DEBUG - saving to results/20190915_030657_baseline_topo_and_code_feats.json
2019-09-15 03:07:46,623 - training_jobs - ERROR - Error with trains/task_47.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 176, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
  File "pipeline_train_model.py", line 46, in add_dataset_file
    for k2,v2 in v:
ValueError: too many values to unpack (expected 2)
2019-09-15 03:07:49,923 - training_jobs - DEBUG - training trains/task_159.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 03:07:50,047 - training_jobs - DEBUG - training with: 
2019-09-15 03:07:50,047 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-15 03:07:50,047 - training_jobs - DEBUG - GGNN1
2019-09-15 03:07:50,047 - training_jobs - DEBUG - 
2019-09-15 03:07:50,047 - training_jobs - DEBUG - ggnn training
2019-09-15 03:07:50,047 - training_jobs - ERROR - Error with trains/task_159.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 470, in training_dispatcher
    train_dataset = FunctionsDataset(root=os.path.join(dataset_folder,'training_set'))
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 223, in __init__
    super(FunctionsDataset, self).__init__(root, transform, pre_transform)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 85, in __init__
    self._process()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 113, in _process
    if files_exist(self.processed_paths):  # pragma: no cover
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 102, in processed_paths
    files = to_list(self.processed_file_names)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 255, in processed_file_names
    for item in os.listdir(processed_path):
FileNotFoundError: [Errno 2] No such file or directory: 'tmp/symbols_dataset_3_precomp_split_undersample_max_half/training_set/processed'
2019-09-15 03:07:53,713 - training_jobs - DEBUG - training trains/task_236.yml
{'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-15 03:07:53,749 - training_jobs - DEBUG - training with: 
2019-09-15 03:07:53,750 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 03:07:53,750 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 03:07:53,750 - training_jobs - DEBUG - topo and code feats
2019-09-15 03:07:53,750 - training_jobs - DEBUG - baseline training
2019-09-15 03:07:53,851 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 03:07:54,788 - training_jobs - DEBUG - training time: 1s
2019-09-15 03:07:54,788 - training_jobs - DEBUG - saving to results/20190915_030753_baseline_topo_and_code_feats.json
2019-09-15 03:07:54,788 - training_jobs - ERROR - Error with trains/task_236.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 176, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
  File "pipeline_train_model.py", line 46, in add_dataset_file
    for k2,v2 in v:
ValueError: too many values to unpack (expected 2)
2019-09-15 03:07:57,641 - training_jobs - DEBUG - training trains/task_268.yml
{'C': 1,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'x_topo_feats',
 'max_iter': 100,
 'model': 'LogisticRegression',
 'multi_class': 'ovr',
 'penalty': 'l2',
 'solver': 'newton-cg'}
2019-09-15 03:07:57,671 - training_jobs - DEBUG - training with: 
2019-09-15 03:07:57,671 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 03:07:57,671 - training_jobs - DEBUG - LogisticRegression
2019-09-15 03:07:57,671 - training_jobs - DEBUG - x_topo_feats
2019-09-15 03:07:57,671 - training_jobs - DEBUG - baseline training
2019-09-15 03:07:57,728 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 03:08:27,277 - training_jobs - DEBUG - training time: 30s
2019-09-15 03:08:27,287 - training_jobs - DEBUG - saving to results/20190915_030757_baseline_x_topo_feats.json
2019-09-15 03:08:27,288 - training_jobs - ERROR - Error with trains/task_268.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 176, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
  File "pipeline_train_model.py", line 46, in add_dataset_file
    for k2,v2 in v:
ValueError: too many values to unpack (expected 2)
2019-09-15 03:08:29,927 - training_jobs - DEBUG - training trains/task_187.yml
{'d1': 85,
 'd2': 20,
 'd3': 15,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'model': 'mlp2',
 'num_epochs': 100}
2019-09-15 03:08:29,961 - training_jobs - DEBUG - training with: 
2019-09-15 03:08:29,961 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 03:08:29,961 - training_jobs - DEBUG - mlp2
2019-09-15 03:08:29,961 - training_jobs - DEBUG - topo and code feats
2019-09-15 03:08:29,961 - training_jobs - DEBUG - baseline_nn training
2019-09-15 03:08:30,136 - training_jobs - DEBUG -  calling nn_train_models
2019-09-15 03:16:03,359 - training_jobs - DEBUG - training time: 453s
2019-09-15 03:16:03,359 - training_jobs - DEBUG - saving to results/20190915_030830_baseline_nn_topo_and_code_feats.json
2019-09-15 03:16:03,359 - training_jobs - ERROR - Error with trains/task_187.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 260, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
  File "pipeline_train_model.py", line 46, in add_dataset_file
    for k2,v2 in v:
ValueError: too many values to unpack (expected 2)
2019-09-15 03:16:07,652 - training_jobs - DEBUG - training trains/task_198.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'model': 'mlp4',
 'num_epochs': 30}
2019-09-15 03:16:07,707 - training_jobs - DEBUG - training with: 
2019-09-15 03:16:07,707 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 03:16:07,707 - training_jobs - DEBUG - mlp4
2019-09-15 03:16:07,707 - training_jobs - DEBUG - topo and code feats
2019-09-15 03:16:07,708 - training_jobs - DEBUG - moved jobdict to done_trainings/task_198.yml
2019-09-15 03:16:07,708 - training_jobs - DEBUG - Finished!

2019-09-15 03:16:10,427 - training_jobs - DEBUG - training trains/task_90.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 03:16:10,465 - training_jobs - DEBUG - training with: 
2019-09-15 03:16:10,465 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 03:16:10,465 - training_jobs - DEBUG - GGNN6
2019-09-15 03:16:10,465 - training_jobs - DEBUG - 
2019-09-15 03:16:10,467 - training_jobs - DEBUG - moved jobdict to done_trainings/task_90.yml
2019-09-15 03:16:10,467 - training_jobs - DEBUG - Finished!

2019-09-15 03:16:13,249 - training_jobs - DEBUG - training trains/task_281.yml
{'C': 1,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'max_iter': 100,
 'model': 'LogisticRegression',
 'multi_class': 'ovr',
 'penalty': 'l2',
 'solver': 'newton-cg'}
2019-09-15 03:16:13,262 - training_jobs - DEBUG - training with: 
2019-09-15 03:16:13,262 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 03:16:13,262 - training_jobs - DEBUG - LogisticRegression
2019-09-15 03:16:13,262 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 03:16:13,262 - training_jobs - DEBUG - nlp training
2019-09-15 03:16:13,876 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-15 03:16:29,790 - training_jobs - DEBUG - training time: 16s
2019-09-15 03:16:29,790 - training_jobs - DEBUG - saving to results/20190915_031613_nlp_tfidf_and_topo_feats.json
2019-09-15 03:16:29,791 - training_jobs - ERROR - Error with trains/task_281.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 352, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
  File "pipeline_train_model.py", line 46, in add_dataset_file
    for k2,v2 in v:
ValueError: too many values to unpack (expected 2)
2019-09-15 03:16:32,114 - training_jobs - DEBUG - training trains/task_161.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 03:16:32,211 - training_jobs - DEBUG - training with: 
2019-09-15 03:16:32,211 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 03:16:32,212 - training_jobs - DEBUG - GGNN1
2019-09-15 03:16:32,212 - training_jobs - DEBUG - 
2019-09-15 03:16:32,212 - training_jobs - DEBUG - ggnn training
2019-09-15 03:16:32,212 - training_jobs - ERROR - Error with trains/task_161.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 470, in training_dispatcher
    train_dataset = FunctionsDataset(root=os.path.join(dataset_folder,'training_set'))
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 223, in __init__
    super(FunctionsDataset, self).__init__(root, transform, pre_transform)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 85, in __init__
    self._process()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 113, in _process
    if files_exist(self.processed_paths):  # pragma: no cover
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 102, in processed_paths
    files = to_list(self.processed_file_names)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 255, in processed_file_names
    for item in os.listdir(processed_path):
FileNotFoundError: [Errno 2] No such file or directory: 'tmp/symbols_dataset_2_precomp_split_undersample_max_third/training_set/processed'
2019-09-15 03:16:34,733 - training_jobs - DEBUG - training trains/task_17.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'x_topo_feats',
 'max_depth': 150,
 'model': 'XGBClassifier'}
2019-09-15 03:16:34,744 - training_jobs - DEBUG - training with: 
2019-09-15 03:16:34,744 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-15 03:16:34,744 - training_jobs - DEBUG - XGBClassifier
2019-09-15 03:16:34,745 - training_jobs - DEBUG - x_topo_feats
2019-09-15 03:16:34,745 - training_jobs - DEBUG - baseline training
2019-09-15 03:16:34,863 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 03:20:48,632 - training_jobs - DEBUG - training time: 254s
2019-09-15 03:20:48,632 - training_jobs - DEBUG - saving to results/20190915_031634_baseline_x_topo_feats.json
2019-09-15 03:20:48,633 - training_jobs - ERROR - Error with trains/task_17.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 176, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
  File "pipeline_train_model.py", line 46, in add_dataset_file
    for k2,v2 in v:
ValueError: too many values to unpack (expected 2)
2019-09-15 03:20:51,415 - training_jobs - DEBUG - training trains/task_0.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'model': 'mlp3',
 'num_epochs': 30}
2019-09-15 03:20:51,449 - training_jobs - DEBUG - training with: 
2019-09-15 03:20:51,449 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-15 03:20:51,449 - training_jobs - DEBUG - mlp3
2019-09-15 03:20:51,449 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 03:20:51,450 - training_jobs - DEBUG - moved jobdict to done_trainings/task_0.yml
2019-09-15 03:20:51,450 - training_jobs - DEBUG - Finished!

2019-09-15 03:20:54,211 - training_jobs - DEBUG - training trains/task_11.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'model': 'mlp3',
 'num_epochs': 100}
2019-09-15 03:20:54,224 - training_jobs - DEBUG - training with: 
2019-09-15 03:20:54,224 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 03:20:54,224 - training_jobs - DEBUG - mlp3
2019-09-15 03:20:54,224 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 03:20:54,225 - training_jobs - DEBUG - moved jobdict to done_trainings/task_11.yml
2019-09-15 03:20:54,225 - training_jobs - DEBUG - Finished!

2019-09-15 03:20:57,107 - training_jobs - DEBUG - training trains/task_22.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'max_depth': 8,
 'model': 'XGBClassifier'}
2019-09-15 03:20:57,130 - training_jobs - DEBUG - training with: 
2019-09-15 03:20:57,130 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 03:20:57,130 - training_jobs - DEBUG - XGBClassifier
2019-09-15 03:20:57,130 - training_jobs - DEBUG - topo and code feats
2019-09-15 03:20:57,130 - training_jobs - DEBUG - baseline training
2019-09-15 03:20:57,225 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 03:23:33,330 - training_jobs - DEBUG - training time: 156s
2019-09-15 03:23:33,330 - training_jobs - DEBUG - saving to results/20190915_032057_baseline_topo_and_code_feats.json
2019-09-15 03:23:33,331 - training_jobs - ERROR - Error with trains/task_22.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 176, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
  File "pipeline_train_model.py", line 46, in add_dataset_file
    for k2,v2 in v:
ValueError: too many values to unpack (expected 2)
2019-09-15 03:23:36,176 - training_jobs - DEBUG - training trains/task_136.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 03:23:36,213 - training_jobs - DEBUG - training with: 
2019-09-15 03:23:36,213 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 03:23:36,213 - training_jobs - DEBUG - GGNN1
2019-09-15 03:23:36,213 - training_jobs - DEBUG - 
2019-09-15 03:23:36,213 - training_jobs - DEBUG - ggnn training
2019-09-15 03:23:36,213 - training_jobs - ERROR - Error with trains/task_136.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 470, in training_dispatcher
    train_dataset = FunctionsDataset(root=os.path.join(dataset_folder,'training_set'))
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 223, in __init__
    super(FunctionsDataset, self).__init__(root, transform, pre_transform)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 85, in __init__
    self._process()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 113, in _process
    if files_exist(self.processed_paths):  # pragma: no cover
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 102, in processed_paths
    files = to_list(self.processed_file_names)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 255, in processed_file_names
    for item in os.listdir(processed_path):
FileNotFoundError: [Errno 2] No such file or directory: 'tmp/symbols_dataset_2_precomp_split_undersample_max_third/training_set/processed'
2019-09-15 03:23:39,125 - training_jobs - DEBUG - training trains/task_142.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 03:23:39,172 - training_jobs - DEBUG - training with: 
2019-09-15 03:23:39,172 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 03:23:39,172 - training_jobs - DEBUG - GGNN1
2019-09-15 03:23:39,172 - training_jobs - DEBUG - 
2019-09-15 03:23:39,172 - training_jobs - DEBUG - ggnn training
2019-09-15 03:23:39,172 - training_jobs - ERROR - Error with trains/task_142.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 470, in training_dispatcher
    train_dataset = FunctionsDataset(root=os.path.join(dataset_folder,'training_set'))
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 223, in __init__
    super(FunctionsDataset, self).__init__(root, transform, pre_transform)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 85, in __init__
    self._process()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 113, in _process
    if files_exist(self.processed_paths):  # pragma: no cover
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 102, in processed_paths
    files = to_list(self.processed_file_names)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 255, in processed_file_names
    for item in os.listdir(processed_path):
FileNotFoundError: [Errno 2] No such file or directory: 'tmp/symbols_dataset_1_precomp_split_undersample_max_half/training_set/processed'
2019-09-15 03:23:41,953 - training_jobs - DEBUG - training trains/task_133.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 03:23:41,968 - training_jobs - DEBUG - training with: 
2019-09-15 03:23:41,968 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-15 03:23:41,968 - training_jobs - DEBUG - GGNN1
2019-09-15 03:23:41,968 - training_jobs - DEBUG - 
2019-09-15 03:23:41,968 - training_jobs - DEBUG - ggnn training
2019-09-15 03:23:41,968 - training_jobs - ERROR - Error with trains/task_133.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 470, in training_dispatcher
    train_dataset = FunctionsDataset(root=os.path.join(dataset_folder,'training_set'))
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 223, in __init__
    super(FunctionsDataset, self).__init__(root, transform, pre_transform)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 85, in __init__
    self._process()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 113, in _process
    if files_exist(self.processed_paths):  # pragma: no cover
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 102, in processed_paths
    files = to_list(self.processed_file_names)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 255, in processed_file_names
    for item in os.listdir(processed_path):
FileNotFoundError: [Errno 2] No such file or directory: 'tmp/symbols_dataset_3_precomp_split_undersample_max_third/training_set/processed'
2019-09-15 03:23:44,799 - training_jobs - DEBUG - training trains/task_57.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'model': 'mlp3',
 'num_epochs': 100}
2019-09-15 03:23:44,875 - training_jobs - DEBUG - training with: 
2019-09-15 03:23:44,875 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-15 03:23:44,875 - training_jobs - DEBUG - mlp3
2019-09-15 03:23:44,875 - training_jobs - DEBUG - topo and code feats
2019-09-15 03:23:44,876 - training_jobs - DEBUG - moved jobdict to done_trainings/task_57.yml
2019-09-15 03:23:44,876 - training_jobs - DEBUG - Finished!

2019-09-15 03:23:47,472 - training_jobs - DEBUG - training trains/task_24.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'x_topo_feats',
 'max_depth': 8,
 'model': 'XGBClassifier'}
2019-09-15 03:23:47,493 - training_jobs - DEBUG - training with: 
2019-09-15 03:23:47,493 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 03:23:47,493 - training_jobs - DEBUG - XGBClassifier
2019-09-15 03:23:47,493 - training_jobs - DEBUG - x_topo_feats
2019-09-15 03:23:47,493 - training_jobs - DEBUG - baseline training
2019-09-15 03:23:47,612 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 03:28:05,403 - training_jobs - DEBUG - training time: 258s
2019-09-15 03:28:05,403 - training_jobs - DEBUG - saving to results/20190915_032347_baseline_x_topo_feats.json
2019-09-15 03:28:05,404 - training_jobs - ERROR - Error with trains/task_24.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 176, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
  File "pipeline_train_model.py", line 46, in add_dataset_file
    for k2,v2 in v:
ValueError: too many values to unpack (expected 2)
2019-09-15 03:28:08,107 - training_jobs - DEBUG - training trains/task_95.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 03:28:08,181 - training_jobs - DEBUG - training with: 
2019-09-15 03:28:08,181 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 03:28:08,181 - training_jobs - DEBUG - GGNN6
2019-09-15 03:28:08,182 - training_jobs - DEBUG - 
2019-09-15 03:28:08,183 - training_jobs - DEBUG - moved jobdict to done_trainings/task_95.yml
2019-09-15 03:28:08,183 - training_jobs - DEBUG - Finished!

2019-09-15 03:28:10,888 - training_jobs - DEBUG - training trains/task_108.yml
{'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-15 03:28:10,902 - training_jobs - DEBUG - training with: 
2019-09-15 03:28:10,902 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 03:28:10,902 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 03:28:10,902 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 03:28:10,902 - training_jobs - DEBUG - nlp training
2019-09-15 03:28:11,623 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-15 03:28:12,924 - training_jobs - DEBUG - training time: 1s
2019-09-15 03:28:12,924 - training_jobs - DEBUG - saving to results/20190915_032811_nlp_tfidf_and_topo_feats.json
2019-09-15 03:28:12,924 - training_jobs - ERROR - Error with trains/task_108.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 352, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
  File "pipeline_train_model.py", line 46, in add_dataset_file
    for k2,v2 in v:
ValueError: too many values to unpack (expected 2)
2019-09-15 03:28:15,620 - training_jobs - DEBUG - training trains/task_259.yml
{'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'x_topo_feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-15 03:28:15,741 - training_jobs - DEBUG - training with: 
2019-09-15 03:28:15,742 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 03:28:15,742 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 03:28:15,742 - training_jobs - DEBUG - x_topo_feats
2019-09-15 03:28:15,742 - training_jobs - DEBUG - baseline training
2019-09-15 03:28:15,868 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 03:28:17,515 - training_jobs - DEBUG - training time: 2s
2019-09-15 03:28:17,515 - training_jobs - DEBUG - saving to results/20190915_032815_baseline_x_topo_feats.json
2019-09-15 03:28:17,515 - training_jobs - ERROR - Error with trains/task_259.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 176, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
  File "pipeline_train_model.py", line 46, in add_dataset_file
    for k2,v2 in v:
ValueError: too many values to unpack (expected 2)
2019-09-15 03:28:20,207 - training_jobs - DEBUG - training trains/task_219.yml
{'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'x_topo_feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-15 03:28:20,429 - training_jobs - DEBUG - training with: 
2019-09-15 03:28:20,429 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-15 03:28:20,429 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 03:28:20,429 - training_jobs - DEBUG - x_topo_feats
2019-09-15 03:28:20,429 - training_jobs - DEBUG - baseline training
2019-09-15 03:28:20,480 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 03:28:21,919 - training_jobs - DEBUG - training time: 1s
2019-09-15 03:28:21,919 - training_jobs - DEBUG - saving to results/20190915_032820_baseline_x_topo_feats.json
2019-09-15 03:28:21,919 - training_jobs - ERROR - Error with trains/task_219.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 176, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
  File "pipeline_train_model.py", line 46, in add_dataset_file
    for k2,v2 in v:
ValueError: too many values to unpack (expected 2)
2019-09-15 03:28:24,589 - training_jobs - DEBUG - training trains/task_251.yml
{'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'x_topo_feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-15 03:28:24,610 - training_jobs - DEBUG - training with: 
2019-09-15 03:28:24,610 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-15 03:28:24,611 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 03:28:24,611 - training_jobs - DEBUG - x_topo_feats
2019-09-15 03:28:24,611 - training_jobs - DEBUG - baseline training
2019-09-15 03:28:24,664 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 03:28:25,757 - training_jobs - DEBUG - training time: 1s
2019-09-15 03:28:25,758 - training_jobs - DEBUG - saving to results/20190915_032824_baseline_x_topo_feats.json
2019-09-15 03:28:25,758 - training_jobs - ERROR - Error with trains/task_251.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 176, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
  File "pipeline_train_model.py", line 46, in add_dataset_file
    for k2,v2 in v:
ValueError: too many values to unpack (expected 2)
2019-09-15 03:28:28,428 - training_jobs - DEBUG - training trains/task_188.yml
{'d1': 85,
 'd2': 20,
 'd3': 15,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'model': 'mlp2',
 'num_epochs': 30}
2019-09-15 03:28:28,457 - training_jobs - DEBUG - training with: 
2019-09-15 03:28:28,457 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-15 03:28:28,457 - training_jobs - DEBUG - mlp2
2019-09-15 03:28:28,457 - training_jobs - DEBUG - topo and code feats
2019-09-15 03:28:28,457 - training_jobs - DEBUG - baseline_nn training
2019-09-15 03:28:28,533 - training_jobs - DEBUG -  calling nn_train_models
2019-09-15 03:29:45,508 - training_jobs - DEBUG - training time: 77s
2019-09-15 03:29:45,616 - training_jobs - DEBUG - saving to results/20190915_032828_baseline_nn_topo_and_code_feats.json
2019-09-15 03:29:45,616 - training_jobs - ERROR - Error with trains/task_188.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 260, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
  File "pipeline_train_model.py", line 46, in add_dataset_file
    for k2,v2 in v:
ValueError: too many values to unpack (expected 2)
2019-09-15 03:29:54,044 - training_jobs - DEBUG - training trains/task_126.yml
{'d1': 85,
 'd2': 20,
 'd3': 15,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'model': 'mlp2',
 'num_epochs': 30}
2019-09-15 03:29:54,095 - training_jobs - DEBUG - training with: 
2019-09-15 03:29:54,095 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 03:29:54,095 - training_jobs - DEBUG - mlp2
2019-09-15 03:29:54,095 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 03:29:54,095 - training_jobs - DEBUG - nlp_nn training
2019-09-15 03:29:54,980 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-15 04:04:23,359 - training_jobs - DEBUG - training trains/task_67.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'max_depth': 150,
 'model': 'XGBClassifier'}
2019-09-15 04:04:23,394 - training_jobs - DEBUG - training with: 
2019-09-15 04:04:23,394 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 04:04:23,394 - training_jobs - DEBUG - XGBClassifier
2019-09-15 04:04:23,394 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 04:04:23,394 - training_jobs - DEBUG - nlp training
2019-09-15 04:04:24,023 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-15 04:51:01,872 - training_jobs - DEBUG - training time: 2798s
2019-09-15 04:51:01,872 - training_jobs - DEBUG - saving to results/20190915_040424_nlp_tfidf_and_topo_feats.json
2019-09-15 04:51:01,877 - training_jobs - DEBUG - moved jobdict to done_trainings/task_67.yml
2019-09-15 04:51:01,877 - training_jobs - DEBUG - Finished!

2019-09-15 04:51:03,980 - training_jobs - DEBUG - training trains/task_109.yml
{'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-15 04:51:04,017 - training_jobs - DEBUG - training with: 
2019-09-15 04:51:04,017 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 04:51:04,017 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 04:51:04,018 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 04:51:04,018 - training_jobs - DEBUG - nlp training
2019-09-15 04:51:04,618 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-15 04:51:08,576 - training_jobs - DEBUG - training time: 4s
2019-09-15 04:51:08,576 - training_jobs - DEBUG - saving to results/20190915_045104_nlp_tfidf_and_topo_feats.json
2019-09-15 04:51:08,612 - training_jobs - DEBUG - moved jobdict to done_trainings/task_109.yml
2019-09-15 04:51:08,612 - training_jobs - DEBUG - Finished!

2019-09-15 04:51:10,900 - training_jobs - DEBUG - training trains/task_139.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 04:51:11,208 - training_jobs - DEBUG - training with: 
2019-09-15 04:51:11,208 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 04:51:11,208 - training_jobs - DEBUG - GGNN1
2019-09-15 04:51:11,208 - training_jobs - DEBUG - 
2019-09-15 04:51:11,208 - training_jobs - DEBUG - ggnn training
2019-09-15 04:51:23,243 - training_jobs - DEBUG -  saving results to results/20190915_045123_ggnn_.json
2019-09-15 04:51:23,244 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 04:51:25,150 - training_jobs - ERROR - Error with trains/task_139.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 529, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 04:51:27,237 - training_jobs - DEBUG - training trains/task_119.yml
{'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-15 04:51:27,280 - training_jobs - DEBUG - training with: 
2019-09-15 04:51:27,280 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 04:51:27,280 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 04:51:27,280 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 04:51:27,280 - training_jobs - DEBUG - nlp training
2019-09-15 04:51:27,796 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-15 04:51:29,998 - training_jobs - DEBUG - training time: 2s
2019-09-15 04:51:29,998 - training_jobs - DEBUG - saving to results/20190915_045127_nlp_tfidf_and_topo_feats.json
2019-09-15 04:51:30,002 - training_jobs - DEBUG - moved jobdict to done_trainings/task_119.yml
2019-09-15 04:51:30,002 - training_jobs - DEBUG - Finished!

2019-09-15 04:51:32,096 - training_jobs - DEBUG - training trains/task_155.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 04:51:32,107 - training_jobs - DEBUG - training with: 
2019-09-15 04:51:32,107 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 04:51:32,108 - training_jobs - DEBUG - GGNN1
2019-09-15 04:51:32,108 - training_jobs - DEBUG - 
2019-09-15 04:51:32,108 - training_jobs - DEBUG - ggnn training
2019-09-15 04:51:32,108 - training_jobs - ERROR - Error with trains/task_155.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 470, in training_dispatcher
    train_dataset = FunctionsDataset(root=os.path.join(dataset_folder,'training_set'))
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 223, in __init__
    super(FunctionsDataset, self).__init__(root, transform, pre_transform)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 85, in __init__
    self._process()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 113, in _process
    if files_exist(self.processed_paths):  # pragma: no cover
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 102, in processed_paths
    files = to_list(self.processed_file_names)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 255, in processed_file_names
    for item in os.listdir(processed_path):
FileNotFoundError: [Errno 2] No such file or directory: 'tmp/symbols_dataset_1_precomp_split_undersample_max_half/training_set/processed'
2019-09-15 04:51:34,225 - training_jobs - DEBUG - training trains/task_127.yml
{'d1': 85,
 'd2': 20,
 'd3': 15,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'model': 'mlp2',
 'num_epochs': 100}
2019-09-15 04:51:34,616 - training_jobs - DEBUG - training with: 
2019-09-15 04:51:34,616 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 04:51:34,616 - training_jobs - DEBUG - mlp2
2019-09-15 04:51:34,616 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 04:51:34,616 - training_jobs - DEBUG - nlp_nn training
2019-09-15 04:51:35,213 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-15 04:58:32,497 - training_jobs - DEBUG - training time: 417s
2019-09-15 04:58:32,497 - training_jobs - DEBUG - saving to results/20190915_045135_nlp_nn_tfidf_and_topo_feats.json
2019-09-15 04:58:32,502 - training_jobs - DEBUG - moved jobdict to done_trainings/task_127.yml
2019-09-15 04:58:32,502 - training_jobs - DEBUG - Finished!

2019-09-15 04:58:34,695 - training_jobs - DEBUG - training trains/task_162.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 04:58:34,705 - training_jobs - DEBUG - training with: 
2019-09-15 04:58:34,705 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 04:58:34,705 - training_jobs - DEBUG - GGNN1
2019-09-15 04:58:34,705 - training_jobs - DEBUG - 
2019-09-15 04:58:34,705 - training_jobs - DEBUG - ggnn training
2019-09-15 04:58:36,742 - training_jobs - DEBUG -  saving results to results/20190915_045836_ggnn_.json
2019-09-15 04:58:36,742 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 04:58:38,672 - training_jobs - ERROR - Error with trains/task_162.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 529, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 04:58:40,773 - training_jobs - DEBUG - training trains/task_120.yml
{'d1': 85,
 'd2': 20,
 'd3': 15,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'model': 'mlp2',
 'num_epochs': 30}
2019-09-15 04:58:40,783 - training_jobs - DEBUG - training with: 
2019-09-15 04:58:40,783 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-15 04:58:40,783 - training_jobs - DEBUG - mlp2
2019-09-15 04:58:40,783 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 04:58:40,783 - training_jobs - DEBUG - nlp_nn training
2019-09-15 04:58:41,153 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-15 05:00:01,194 - training_jobs - DEBUG - training time: 80s
2019-09-15 05:00:01,194 - training_jobs - DEBUG - saving to results/20190915_045841_nlp_nn_tfidf_and_topo_feats.json
2019-09-15 05:00:01,197 - training_jobs - DEBUG - moved jobdict to done_trainings/task_120.yml
2019-09-15 05:00:01,197 - training_jobs - DEBUG - Finished!

2019-09-15 05:00:03,394 - training_jobs - DEBUG - training trains/task_243.yml
{'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'x_topo_feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-15 05:00:03,403 - training_jobs - DEBUG - training with: 
2019-09-15 05:00:03,403 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 05:00:03,403 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 05:00:03,404 - training_jobs - DEBUG - x_topo_feats
2019-09-15 05:00:03,404 - training_jobs - DEBUG - baseline training
2019-09-15 05:00:03,511 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 05:00:07,188 - training_jobs - DEBUG - training time: 4s
2019-09-15 05:00:07,188 - training_jobs - DEBUG - saving to results/20190915_050003_baseline_x_topo_feats.json
2019-09-15 05:00:07,199 - training_jobs - DEBUG - moved jobdict to done_trainings/task_243.yml
2019-09-15 05:00:07,199 - training_jobs - DEBUG - Finished!

2019-09-15 05:00:09,365 - training_jobs - DEBUG - training trains/task_152.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 05:00:09,375 - training_jobs - DEBUG - training with: 
2019-09-15 05:00:09,375 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-15 05:00:09,375 - training_jobs - DEBUG - GGNN1
2019-09-15 05:00:09,375 - training_jobs - DEBUG - 
2019-09-15 05:00:09,375 - training_jobs - DEBUG - ggnn training
2019-09-15 05:00:15,861 - training_jobs - DEBUG -  saving results to results/20190915_050015_ggnn_.json
2019-09-15 05:00:15,861 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 05:15:24,281 - training_jobs - DEBUG - test_multiple_models
2019-09-15 05:15:24,281 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-15 05:15:31,302 - training_jobs - DEBUG - training time: 915s
2019-09-15 05:15:31,302 - training_jobs - DEBUG - saving to results/20190915_050015_ggnn_.json
2019-09-15 05:15:31,307 - training_jobs - DEBUG - moved jobdict to done_trainings/task_152.yml
2019-09-15 05:15:31,307 - training_jobs - DEBUG - Finished!

2019-09-15 05:15:33,512 - training_jobs - DEBUG - training trains/task_19.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'max_depth': 150,
 'model': 'XGBClassifier'}
2019-09-15 05:15:33,521 - training_jobs - DEBUG - training with: 
2019-09-15 05:15:33,521 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-15 05:15:33,521 - training_jobs - DEBUG - XGBClassifier
2019-09-15 05:15:33,521 - training_jobs - DEBUG - topo and code feats
2019-09-15 05:15:33,521 - training_jobs - DEBUG - baseline training
2019-09-15 05:15:33,682 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 05:19:20,039 - training_jobs - DEBUG - training time: 226s
2019-09-15 05:19:20,039 - training_jobs - DEBUG - saving to results/20190915_051533_baseline_topo_and_code_feats.json
2019-09-15 05:19:20,050 - training_jobs - DEBUG - moved jobdict to done_trainings/task_19.yml
2019-09-15 05:19:20,050 - training_jobs - DEBUG - Finished!

2019-09-15 05:19:22,372 - training_jobs - DEBUG - training trains/task_280.yml
{'C': 1,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'max_iter': 100,
 'model': 'LogisticRegression',
 'multi_class': 'ovr',
 'penalty': 'l2',
 'solver': 'newton-cg'}
2019-09-15 05:19:22,382 - training_jobs - DEBUG - training with: 
2019-09-15 05:19:22,382 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-15 05:19:22,382 - training_jobs - DEBUG - LogisticRegression
2019-09-15 05:19:22,382 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 05:19:22,382 - training_jobs - DEBUG - nlp training
2019-09-15 05:19:22,685 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-15 05:19:28,716 - training_jobs - DEBUG - training time: 6s
2019-09-15 05:19:28,716 - training_jobs - DEBUG - saving to results/20190915_051922_nlp_tfidf_and_topo_feats.json
2019-09-15 05:19:28,719 - training_jobs - DEBUG - moved jobdict to done_trainings/task_280.yml
2019-09-15 05:19:28,719 - training_jobs - DEBUG - Finished!

2019-09-15 05:19:30,817 - training_jobs - DEBUG - training trains/task_132.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 05:19:30,828 - training_jobs - DEBUG - training with: 
2019-09-15 05:19:30,828 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-15 05:19:30,828 - training_jobs - DEBUG - GGNN1
2019-09-15 05:19:30,828 - training_jobs - DEBUG - 
2019-09-15 05:19:30,828 - training_jobs - DEBUG - ggnn training
2019-09-15 05:19:38,745 - training_jobs - DEBUG -  saving results to results/20190915_051938_ggnn_.json
2019-09-15 05:19:38,745 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 05:40:26,673 - training_jobs - DEBUG - test_multiple_models
2019-09-15 05:40:26,673 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-15 05:40:30,852 - training_jobs - DEBUG - training time: 1252s
2019-09-15 05:40:30,852 - training_jobs - DEBUG - saving to results/20190915_051938_ggnn_.json
2019-09-15 05:40:30,854 - training_jobs - DEBUG - moved jobdict to done_trainings/task_132.yml
2019-09-15 05:40:30,854 - training_jobs - DEBUG - Finished!

2019-09-15 05:40:33,035 - training_jobs - DEBUG - training trains/task_180.yml
{'d1': 85,
 'd2': 20,
 'd3': 15,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'model': 'mlp2',
 'num_epochs': 30}
2019-09-15 05:40:33,044 - training_jobs - DEBUG - training with: 
2019-09-15 05:40:33,044 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-15 05:40:33,045 - training_jobs - DEBUG - mlp2
2019-09-15 05:40:33,045 - training_jobs - DEBUG - topo and code feats
2019-09-15 05:40:33,045 - training_jobs - DEBUG - baseline_nn training
2019-09-15 05:40:33,122 - training_jobs - DEBUG -  calling nn_train_models
2019-09-15 05:41:52,210 - training_jobs - DEBUG - training time: 79s
2019-09-15 05:41:52,210 - training_jobs - DEBUG - saving to results/20190915_054033_baseline_nn_topo_and_code_feats.json
2019-09-15 05:41:52,214 - training_jobs - DEBUG - moved jobdict to done_trainings/task_180.yml
2019-09-15 05:41:52,214 - training_jobs - DEBUG - Finished!

2019-09-15 05:41:54,405 - training_jobs - DEBUG - training trains/task_241.yml
{'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'x_topo_feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-15 05:41:54,415 - training_jobs - DEBUG - training with: 
2019-09-15 05:41:54,415 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 05:41:54,415 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 05:41:54,415 - training_jobs - DEBUG - x_topo_feats
2019-09-15 05:41:54,415 - training_jobs - DEBUG - baseline training
2019-09-15 05:41:54,528 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 05:41:56,187 - training_jobs - DEBUG - training time: 2s
2019-09-15 05:41:56,187 - training_jobs - DEBUG - saving to results/20190915_054154_baseline_x_topo_feats.json
2019-09-15 05:41:56,198 - training_jobs - DEBUG - moved jobdict to done_trainings/task_241.yml
2019-09-15 05:41:56,198 - training_jobs - DEBUG - Finished!

2019-09-15 05:41:58,373 - training_jobs - DEBUG - training trains/task_250.yml
{'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'x_topo_feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-15 05:41:58,382 - training_jobs - DEBUG - training with: 
2019-09-15 05:41:58,382 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-15 05:41:58,382 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 05:41:58,383 - training_jobs - DEBUG - x_topo_feats
2019-09-15 05:41:58,383 - training_jobs - DEBUG - baseline training
2019-09-15 05:41:58,424 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 05:41:59,025 - training_jobs - DEBUG - training time: 1s
2019-09-15 05:41:59,025 - training_jobs - DEBUG - saving to results/20190915_054158_baseline_x_topo_feats.json
2019-09-15 05:41:59,031 - training_jobs - DEBUG - moved jobdict to done_trainings/task_250.yml
2019-09-15 05:41:59,031 - training_jobs - DEBUG - Finished!

2019-09-15 05:42:01,199 - training_jobs - DEBUG - training trains/task_233.yml
{'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'x_topo_feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-15 05:42:01,209 - training_jobs - DEBUG - training with: 
2019-09-15 05:42:01,209 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 05:42:01,209 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 05:42:01,209 - training_jobs - DEBUG - x_topo_feats
2019-09-15 05:42:01,209 - training_jobs - DEBUG - baseline training
2019-09-15 05:42:01,255 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 05:42:02,456 - training_jobs - DEBUG - training time: 1s
2019-09-15 05:42:02,456 - training_jobs - DEBUG - saving to results/20190915_054201_baseline_x_topo_feats.json
2019-09-15 05:42:02,463 - training_jobs - DEBUG - moved jobdict to done_trainings/task_233.yml
2019-09-15 05:42:02,463 - training_jobs - DEBUG - Finished!

2019-09-15 05:42:04,633 - training_jobs - DEBUG - training trains/task_261.yml
{'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-15 05:42:04,642 - training_jobs - DEBUG - training with: 
2019-09-15 05:42:04,643 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 05:42:04,643 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 05:42:04,643 - training_jobs - DEBUG - topo and code feats
2019-09-15 05:42:04,643 - training_jobs - DEBUG - baseline training
2019-09-15 05:42:04,783 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 05:42:05,645 - training_jobs - DEBUG - training time: 1s
2019-09-15 05:42:05,646 - training_jobs - DEBUG - saving to results/20190915_054204_baseline_topo_and_code_feats.json
2019-09-15 05:42:05,655 - training_jobs - DEBUG - moved jobdict to done_trainings/task_261.yml
2019-09-15 05:42:05,655 - training_jobs - DEBUG - Finished!

2019-09-15 05:42:07,825 - training_jobs - DEBUG - training trains/task_125.yml
{'d1': 85,
 'd2': 20,
 'd3': 15,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'model': 'mlp2',
 'num_epochs': 100}
2019-09-15 05:42:07,835 - training_jobs - DEBUG - training with: 
2019-09-15 05:42:07,835 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 05:42:07,835 - training_jobs - DEBUG - mlp2
2019-09-15 05:42:07,835 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 05:42:07,835 - training_jobs - DEBUG - nlp_nn training
2019-09-15 05:42:08,614 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-15 05:46:15,910 - training_jobs - DEBUG - training time: 247s
2019-09-15 05:46:15,910 - training_jobs - DEBUG - saving to results/20190915_054208_nlp_nn_tfidf_and_topo_feats.json
2019-09-15 05:46:15,913 - training_jobs - DEBUG - moved jobdict to done_trainings/task_125.yml
2019-09-15 05:46:15,913 - training_jobs - DEBUG - Finished!

2019-09-15 05:46:18,103 - training_jobs - DEBUG - training trains/task_47.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'max_depth': 150,
 'model': 'XGBClassifier'}
2019-09-15 05:46:18,112 - training_jobs - DEBUG - training with: 
2019-09-15 05:46:18,112 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-15 05:46:18,112 - training_jobs - DEBUG - XGBClassifier
2019-09-15 05:46:18,112 - training_jobs - DEBUG - topo and code feats
2019-09-15 05:46:18,112 - training_jobs - DEBUG - baseline training
2019-09-15 05:46:18,176 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 05:47:01,999 - training_jobs - DEBUG - training time: 44s
2019-09-15 05:47:01,999 - training_jobs - DEBUG - saving to results/20190915_054618_baseline_topo_and_code_feats.json
2019-09-15 05:47:02,004 - training_jobs - DEBUG - moved jobdict to done_trainings/task_47.yml
2019-09-15 05:47:02,004 - training_jobs - DEBUG - Finished!

2019-09-15 05:47:04,387 - training_jobs - DEBUG - training trains/task_159.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 05:47:04,398 - training_jobs - DEBUG - training with: 
2019-09-15 05:47:04,398 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-15 05:47:04,398 - training_jobs - DEBUG - GGNN1
2019-09-15 05:47:04,398 - training_jobs - DEBUG - 
2019-09-15 05:47:04,398 - training_jobs - DEBUG - ggnn training
2019-09-15 05:47:11,501 - training_jobs - DEBUG -  saving results to results/20190915_054711_ggnn_.json
2019-09-15 05:47:11,501 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 07:00:58,277 - training_jobs - DEBUG - test_multiple_models
2019-09-15 07:00:58,277 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-15 07:01:02,151 - training_jobs - DEBUG - training time: 4431s
2019-09-15 07:01:02,151 - training_jobs - DEBUG - saving to results/20190915_054711_ggnn_.json
2019-09-15 07:01:02,153 - training_jobs - DEBUG - moved jobdict to done_trainings/task_159.yml
2019-09-15 07:01:02,153 - training_jobs - DEBUG - Finished!

2019-09-15 07:01:04,347 - training_jobs - DEBUG - training trains/task_236.yml
{'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-15 07:01:04,357 - training_jobs - DEBUG - training with: 
2019-09-15 07:01:04,357 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 07:01:04,357 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 07:01:04,357 - training_jobs - DEBUG - topo and code feats
2019-09-15 07:01:04,357 - training_jobs - DEBUG - baseline training
2019-09-15 07:01:04,432 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 07:01:05,163 - training_jobs - DEBUG - training time: 1s
2019-09-15 07:01:05,163 - training_jobs - DEBUG - saving to results/20190915_070104_baseline_topo_and_code_feats.json
2019-09-15 07:01:05,170 - training_jobs - DEBUG - moved jobdict to done_trainings/task_236.yml
2019-09-15 07:01:05,170 - training_jobs - DEBUG - Finished!

2019-09-15 07:01:07,472 - training_jobs - DEBUG - training trains/task_268.yml
{'C': 1,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'x_topo_feats',
 'max_iter': 100,
 'model': 'LogisticRegression',
 'multi_class': 'ovr',
 'penalty': 'l2',
 'solver': 'newton-cg'}
2019-09-15 07:01:07,483 - training_jobs - DEBUG - training with: 
2019-09-15 07:01:07,483 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 07:01:07,483 - training_jobs - DEBUG - LogisticRegression
2019-09-15 07:01:07,483 - training_jobs - DEBUG - x_topo_feats
2019-09-15 07:01:07,483 - training_jobs - DEBUG - baseline training
2019-09-15 07:01:07,528 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 07:01:24,414 - training_jobs - DEBUG - training time: 17s
2019-09-15 07:01:24,414 - training_jobs - DEBUG - saving to results/20190915_070107_baseline_x_topo_feats.json
2019-09-15 07:01:24,427 - training_jobs - DEBUG - moved jobdict to done_trainings/task_268.yml
2019-09-15 07:01:24,428 - training_jobs - DEBUG - Finished!

2019-09-15 07:01:26,650 - training_jobs - DEBUG - training trains/task_187.yml
{'d1': 85,
 'd2': 20,
 'd3': 15,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'model': 'mlp2',
 'num_epochs': 100}
2019-09-15 07:01:26,660 - training_jobs - DEBUG - training with: 
2019-09-15 07:01:26,660 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 07:01:26,660 - training_jobs - DEBUG - mlp2
2019-09-15 07:01:26,660 - training_jobs - DEBUG - topo and code feats
2019-09-15 07:01:26,660 - training_jobs - DEBUG - baseline_nn training
2019-09-15 07:01:26,816 - training_jobs - DEBUG -  calling nn_train_models
2019-09-15 07:08:16,823 - training_jobs - DEBUG - training time: 410s
2019-09-15 07:08:16,823 - training_jobs - DEBUG - saving to results/20190915_070126_baseline_nn_topo_and_code_feats.json
2019-09-15 07:08:16,828 - training_jobs - DEBUG - moved jobdict to done_trainings/task_187.yml
2019-09-15 07:08:16,828 - training_jobs - DEBUG - Finished!

2019-09-15 07:08:18,996 - training_jobs - DEBUG - training trains/task_281.yml
{'C': 1,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'max_iter': 100,
 'model': 'LogisticRegression',
 'multi_class': 'ovr',
 'penalty': 'l2',
 'solver': 'newton-cg'}
2019-09-15 07:08:19,062 - training_jobs - DEBUG - training with: 
2019-09-15 07:08:19,062 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 07:08:19,062 - training_jobs - DEBUG - LogisticRegression
2019-09-15 07:08:19,062 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 07:08:19,062 - training_jobs - DEBUG - nlp training
2019-09-15 07:08:19,584 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-15 07:08:27,787 - training_jobs - DEBUG - training time: 8s
2019-09-15 07:08:27,787 - training_jobs - DEBUG - saving to results/20190915_070819_nlp_tfidf_and_topo_feats.json
2019-09-15 07:08:27,791 - training_jobs - DEBUG - moved jobdict to done_trainings/task_281.yml
2019-09-15 07:08:27,791 - training_jobs - DEBUG - Finished!

2019-09-15 07:08:29,882 - training_jobs - DEBUG - training trains/task_161.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 07:08:30,228 - training_jobs - DEBUG - training with: 
2019-09-15 07:08:30,228 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 07:08:30,228 - training_jobs - DEBUG - GGNN1
2019-09-15 07:08:30,228 - training_jobs - DEBUG - 
2019-09-15 07:08:30,229 - training_jobs - DEBUG - ggnn training
2019-09-15 07:08:31,466 - training_jobs - DEBUG -  saving results to results/20190915_070831_ggnn_.json
2019-09-15 07:08:31,466 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 07:08:32,615 - training_jobs - ERROR - Error with trains/task_161.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 529, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 07:08:34,708 - training_jobs - DEBUG - training trains/task_17.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'x_topo_feats',
 'max_depth': 150,
 'model': 'XGBClassifier'}
2019-09-15 07:08:34,737 - training_jobs - DEBUG - training with: 
2019-09-15 07:08:34,737 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-15 07:08:34,737 - training_jobs - DEBUG - XGBClassifier
2019-09-15 07:08:34,738 - training_jobs - DEBUG - x_topo_feats
2019-09-15 07:08:34,738 - training_jobs - DEBUG - baseline training
2019-09-15 07:08:34,849 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 07:12:35,475 - training_jobs - DEBUG - training time: 241s
2019-09-15 07:12:35,475 - training_jobs - DEBUG - saving to results/20190915_070834_baseline_x_topo_feats.json
2019-09-15 07:12:35,485 - training_jobs - DEBUG - moved jobdict to done_trainings/task_17.yml
2019-09-15 07:12:35,485 - training_jobs - DEBUG - Finished!

2019-09-15 07:12:37,743 - training_jobs - DEBUG - training trains/task_22.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'max_depth': 8,
 'model': 'XGBClassifier'}
2019-09-15 07:12:37,753 - training_jobs - DEBUG - training with: 
2019-09-15 07:12:37,753 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 07:12:37,753 - training_jobs - DEBUG - XGBClassifier
2019-09-15 07:12:37,753 - training_jobs - DEBUG - topo and code feats
2019-09-15 07:12:37,754 - training_jobs - DEBUG - baseline training
2019-09-15 07:12:37,829 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 07:15:03,717 - training_jobs - DEBUG - training time: 146s
2019-09-15 07:15:03,717 - training_jobs - DEBUG - saving to results/20190915_071237_baseline_topo_and_code_feats.json
2019-09-15 07:15:03,724 - training_jobs - DEBUG - moved jobdict to done_trainings/task_22.yml
2019-09-15 07:15:03,724 - training_jobs - DEBUG - Finished!

2019-09-15 07:15:05,975 - training_jobs - DEBUG - training trains/task_136.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 07:15:05,986 - training_jobs - DEBUG - training with: 
2019-09-15 07:15:05,986 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 07:15:05,986 - training_jobs - DEBUG - GGNN1
2019-09-15 07:15:05,986 - training_jobs - DEBUG - 
2019-09-15 07:15:05,987 - training_jobs - DEBUG - ggnn training
2019-09-15 07:15:07,202 - training_jobs - DEBUG -  saving results to results/20190915_071507_ggnn_.json
2019-09-15 07:15:07,202 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 07:15:08,352 - training_jobs - ERROR - Error with trains/task_136.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 529, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 07:15:10,459 - training_jobs - DEBUG - training trains/task_142.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 07:15:10,469 - training_jobs - DEBUG - training with: 
2019-09-15 07:15:10,469 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 07:15:10,470 - training_jobs - DEBUG - GGNN1
2019-09-15 07:15:10,470 - training_jobs - DEBUG - 
2019-09-15 07:15:10,470 - training_jobs - DEBUG - ggnn training
2019-09-15 07:15:10,470 - training_jobs - ERROR - Error with trains/task_142.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 664, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 470, in training_dispatcher
    train_dataset = FunctionsDataset(root=os.path.join(dataset_folder,'training_set'))
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 223, in __init__
    super(FunctionsDataset, self).__init__(root, transform, pre_transform)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 85, in __init__
    self._process()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 113, in _process
    if files_exist(self.processed_paths):  # pragma: no cover
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 102, in processed_paths
    files = to_list(self.processed_file_names)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 255, in processed_file_names
    for item in os.listdir(processed_path):
FileNotFoundError: [Errno 2] No such file or directory: 'tmp/symbols_dataset_1_precomp_split_undersample_max_half/training_set/processed'
2019-09-15 07:15:12,752 - training_jobs - DEBUG - training trains/task_133.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 07:15:12,803 - training_jobs - DEBUG - training with: 
2019-09-15 07:15:12,803 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-15 07:15:12,803 - training_jobs - DEBUG - GGNN1
2019-09-15 07:15:12,803 - training_jobs - DEBUG - 
2019-09-15 07:15:12,803 - training_jobs - DEBUG - ggnn training
2019-09-15 07:15:14,364 - training_jobs - DEBUG -  saving results to results/20190915_071514_ggnn_.json
2019-09-15 07:15:14,364 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 07:58:50,090 - training_jobs - DEBUG - test_multiple_models
2019-09-15 07:58:50,127 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-15 07:58:54,118 - training_jobs - DEBUG - training time: 2620s
2019-09-15 07:58:54,119 - training_jobs - DEBUG - saving to results/20190915_071514_ggnn_.json
2019-09-15 07:58:54,133 - training_jobs - DEBUG - moved jobdict to done_trainings/task_133.yml
2019-09-15 07:58:54,133 - training_jobs - DEBUG - Finished!

2019-09-15 07:59:06,098 - training_jobs - DEBUG - training trains/task_24.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'x_topo_feats',
 'max_depth': 8,
 'model': 'XGBClassifier'}
2019-09-15 07:59:06,138 - training_jobs - DEBUG - training with: 
2019-09-15 07:59:06,138 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 07:59:06,138 - training_jobs - DEBUG - XGBClassifier
2019-09-15 07:59:06,138 - training_jobs - DEBUG - x_topo_feats
2019-09-15 07:59:06,138 - training_jobs - DEBUG - baseline training
2019-09-15 07:59:06,761 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 07:59:48,892 - training_jobs - DEBUG - training trains/task_139.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 07:59:48,946 - training_jobs - DEBUG - training with: 
2019-09-15 07:59:48,946 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 07:59:48,946 - training_jobs - DEBUG - GGNN1
2019-09-15 07:59:48,946 - training_jobs - DEBUG - 
2019-09-15 07:59:48,946 - training_jobs - DEBUG - ggnn training
2019-09-15 08:00:03,040 - training_jobs - DEBUG -  saving results to results/20190915_080003_ggnn_.json
2019-09-15 08:00:03,040 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 08:00:04,928 - training_jobs - ERROR - Error with trains/task_139.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 666, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 531, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 08:00:07,076 - training_jobs - DEBUG - training trains/task_155.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 08:00:07,133 - training_jobs - DEBUG - training with: 
2019-09-15 08:00:07,133 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 08:00:07,133 - training_jobs - DEBUG - GGNN1
2019-09-15 08:00:07,133 - training_jobs - DEBUG - 
2019-09-15 08:00:07,133 - training_jobs - DEBUG - ggnn training
2019-09-15 08:00:19,911 - training_jobs - DEBUG -  saving results to results/20190915_080019_ggnn_.json
2019-09-15 08:00:19,911 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 08:51:08,387 - training_jobs - DEBUG - test_multiple_models
2019-09-15 08:51:08,387 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-15 08:51:24,425 - training_jobs - DEBUG - training time: 3065s
2019-09-15 08:51:24,425 - training_jobs - DEBUG - saving to results/20190915_080019_ggnn_.json
2019-09-15 08:51:24,425 - training_jobs - ERROR - Error with trains/task_155.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 666, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 548, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
TypeError: add_dataset_file() missing 1 required positional argument: 'thetime'
2019-09-15 08:51:26,670 - training_jobs - DEBUG - training trains/task_162.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 08:51:26,800 - training_jobs - DEBUG - training with: 
2019-09-15 08:51:26,800 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 08:51:26,800 - training_jobs - DEBUG - GGNN1
2019-09-15 08:51:26,800 - training_jobs - DEBUG - 
2019-09-15 08:51:26,801 - training_jobs - DEBUG - ggnn training
2019-09-15 08:51:28,822 - training_jobs - DEBUG -  saving results to results/20190915_085128_ggnn_.json
2019-09-15 08:51:28,822 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 08:51:30,694 - training_jobs - ERROR - Error with trains/task_162.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 666, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 531, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 08:51:32,772 - training_jobs - DEBUG - training trains/task_161.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 08:51:33,017 - training_jobs - DEBUG - training with: 
2019-09-15 08:51:33,017 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 08:51:33,017 - training_jobs - DEBUG - GGNN1
2019-09-15 08:51:33,017 - training_jobs - DEBUG - 
2019-09-15 08:51:33,017 - training_jobs - DEBUG - ggnn training
2019-09-15 08:51:35,867 - training_jobs - DEBUG -  saving results to results/20190915_085135_ggnn_.json
2019-09-15 08:51:35,867 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 08:51:37,007 - training_jobs - ERROR - Error with trains/task_161.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 666, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 531, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 08:51:39,075 - training_jobs - DEBUG - training trains/task_136.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 08:51:39,263 - training_jobs - DEBUG - training with: 
2019-09-15 08:51:39,263 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 08:51:39,263 - training_jobs - DEBUG - GGNN1
2019-09-15 08:51:39,263 - training_jobs - DEBUG - 
2019-09-15 08:51:39,263 - training_jobs - DEBUG - ggnn training
2019-09-15 08:51:40,482 - training_jobs - DEBUG -  saving results to results/20190915_085140_ggnn_.json
2019-09-15 08:51:40,482 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 08:51:41,615 - training_jobs - ERROR - Error with trains/task_136.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 666, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 531, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 08:51:43,683 - training_jobs - DEBUG - training trains/task_142.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 08:51:43,695 - training_jobs - DEBUG - training with: 
2019-09-15 08:51:43,695 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 08:51:43,695 - training_jobs - DEBUG - GGNN1
2019-09-15 08:51:43,695 - training_jobs - DEBUG - 
2019-09-15 08:51:43,695 - training_jobs - DEBUG - ggnn training
2019-09-15 08:51:45,795 - training_jobs - DEBUG -  saving results to results/20190915_085145_ggnn_.json
2019-09-15 08:51:45,795 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 09:17:06,434 - training_jobs - DEBUG - test_multiple_models
2019-09-15 09:17:06,434 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-15 09:17:07,244 - training_jobs - DEBUG - training time: 1521s
2019-09-15 09:17:07,244 - training_jobs - DEBUG - saving to results/20190915_085145_ggnn_.json
2019-09-15 09:17:07,244 - training_jobs - ERROR - Error with trains/task_142.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 666, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 548, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
TypeError: add_dataset_file() missing 1 required positional argument: 'thetime'
2019-09-15 09:17:09,460 - training_jobs - DEBUG - training trains/task_24.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'x_topo_feats',
 'max_depth': 8,
 'model': 'XGBClassifier'}
2019-09-15 09:17:09,469 - training_jobs - DEBUG - training with: 
2019-09-15 09:17:09,469 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 09:17:09,469 - training_jobs - DEBUG - XGBClassifier
2019-09-15 09:17:09,469 - training_jobs - DEBUG - x_topo_feats
2019-09-15 09:17:09,469 - training_jobs - DEBUG - baseline training
2019-09-15 09:17:09,577 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 09:20:58,436 - training_jobs - DEBUG - training time: 229s
2019-09-15 09:20:58,436 - training_jobs - DEBUG - saving to results/20190915_091709_baseline_x_topo_feats.json
2019-09-15 09:20:58,436 - training_jobs - ERROR - Error with trains/task_24.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 666, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 178, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
TypeError: add_dataset_file() missing 1 required positional argument: 'thetime'
2019-09-15 09:21:00,682 - training_jobs - DEBUG - training trains/task_108.yml
{'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-15 09:21:00,858 - training_jobs - DEBUG - training with: 
2019-09-15 09:21:00,858 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 09:21:00,858 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 09:21:00,858 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 09:21:00,858 - training_jobs - DEBUG - nlp training
2019-09-15 09:21:01,546 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-15 09:21:02,791 - training_jobs - DEBUG - training time: 1s
2019-09-15 09:21:02,791 - training_jobs - DEBUG - saving to results/20190915_092101_nlp_tfidf_and_topo_feats.json
2019-09-15 09:21:02,791 - training_jobs - ERROR - Error with trains/task_108.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 666, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 354, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
TypeError: add_dataset_file() missing 1 required positional argument: 'thetime'
2019-09-15 09:21:04,919 - training_jobs - DEBUG - training trains/task_259.yml
{'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'x_topo_feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-15 09:21:05,041 - training_jobs - DEBUG - training with: 
2019-09-15 09:21:05,041 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 09:21:05,041 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 09:21:05,041 - training_jobs - DEBUG - x_topo_feats
2019-09-15 09:21:05,041 - training_jobs - DEBUG - baseline training
2019-09-15 09:21:05,697 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 09:21:06,941 - training_jobs - DEBUG - training time: 1s
2019-09-15 09:21:06,941 - training_jobs - DEBUG - saving to results/20190915_092105_baseline_x_topo_feats.json
2019-09-15 09:21:06,941 - training_jobs - ERROR - Error with trains/task_259.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 666, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 178, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
TypeError: add_dataset_file() missing 1 required positional argument: 'thetime'
2019-09-15 09:21:09,198 - training_jobs - DEBUG - training trains/task_219.yml
{'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'x_topo_feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-15 09:21:09,499 - training_jobs - DEBUG - training with: 
2019-09-15 09:21:09,499 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-15 09:21:09,499 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 09:21:09,499 - training_jobs - DEBUG - x_topo_feats
2019-09-15 09:21:09,499 - training_jobs - DEBUG - baseline training
2019-09-15 09:21:09,892 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 09:21:11,150 - training_jobs - DEBUG - training time: 1s
2019-09-15 09:21:11,150 - training_jobs - DEBUG - saving to results/20190915_092109_baseline_x_topo_feats.json
2019-09-15 09:21:11,150 - training_jobs - ERROR - Error with trains/task_219.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 666, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 178, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
TypeError: add_dataset_file() missing 1 required positional argument: 'thetime'
2019-09-15 09:21:13,361 - training_jobs - DEBUG - training trains/task_251.yml
{'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'x_topo_feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-15 09:21:13,371 - training_jobs - DEBUG - training with: 
2019-09-15 09:21:13,371 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-15 09:21:13,371 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 09:21:13,371 - training_jobs - DEBUG - x_topo_feats
2019-09-15 09:21:13,371 - training_jobs - DEBUG - baseline training
2019-09-15 09:21:14,110 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 09:21:15,015 - training_jobs - DEBUG - training time: 1s
2019-09-15 09:21:15,015 - training_jobs - DEBUG - saving to results/20190915_092114_baseline_x_topo_feats.json
2019-09-15 09:21:15,015 - training_jobs - ERROR - Error with trains/task_251.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 666, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 178, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
TypeError: add_dataset_file() missing 1 required positional argument: 'thetime'
2019-09-15 09:21:17,183 - training_jobs - DEBUG - training trains/task_188.yml
{'d1': 85,
 'd2': 20,
 'd3': 15,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'model': 'mlp2',
 'num_epochs': 30}
2019-09-15 09:21:17,277 - training_jobs - DEBUG - training with: 
2019-09-15 09:21:17,277 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-15 09:21:17,277 - training_jobs - DEBUG - mlp2
2019-09-15 09:21:17,277 - training_jobs - DEBUG - topo and code feats
2019-09-15 09:21:17,277 - training_jobs - DEBUG - baseline_nn training
2019-09-15 09:21:17,345 - training_jobs - DEBUG -  calling nn_train_models
2019-09-15 09:22:22,689 - training_jobs - DEBUG - training time: 65s
2019-09-15 09:22:22,689 - training_jobs - DEBUG - saving to results/20190915_092117_baseline_nn_topo_and_code_feats.json
2019-09-15 09:22:22,690 - training_jobs - ERROR - Error with trains/task_188.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 666, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 262, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
TypeError: add_dataset_file() missing 1 required positional argument: 'thetime'
2019-09-15 09:22:24,893 - training_jobs - DEBUG - training trains/task_126.yml
{'d1': 85,
 'd2': 20,
 'd3': 15,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'model': 'mlp2',
 'num_epochs': 30}
2019-09-15 09:22:24,929 - training_jobs - DEBUG - training with: 
2019-09-15 09:22:24,929 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 09:22:24,929 - training_jobs - DEBUG - mlp2
2019-09-15 09:22:24,929 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 09:22:24,929 - training_jobs - DEBUG - nlp_nn training
2019-09-15 09:22:25,521 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-15 09:24:31,574 - training_jobs - DEBUG - training time: 126s
2019-09-15 09:24:31,574 - training_jobs - DEBUG - saving to results/20190915_092225_nlp_nn_tfidf_and_topo_feats.json
2019-09-15 09:24:31,574 - training_jobs - ERROR - Error with trains/task_126.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 666, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 451, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
TypeError: add_dataset_file() missing 1 required positional argument: 'thetime'
2019-09-15 09:24:33,755 - training_jobs - DEBUG - training trains/task_183.yml
{'d1': 85,
 'd2': 20,
 'd3': 15,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'model': 'mlp2',
 'num_epochs': 100}
2019-09-15 09:24:33,930 - training_jobs - DEBUG - training with: 
2019-09-15 09:24:33,930 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-15 09:24:33,930 - training_jobs - DEBUG - mlp2
2019-09-15 09:24:33,930 - training_jobs - DEBUG - topo and code feats
2019-09-15 09:24:33,930 - training_jobs - DEBUG - baseline_nn training
2019-09-15 09:24:34,574 - training_jobs - DEBUG -  calling nn_train_models
2019-09-15 09:31:42,065 - training_jobs - DEBUG - training time: 427s
2019-09-15 09:31:42,065 - training_jobs - DEBUG - saving to results/20190915_092434_baseline_nn_topo_and_code_feats.json
2019-09-15 09:31:42,065 - training_jobs - ERROR - Error with trains/task_183.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 666, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 262, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
TypeError: add_dataset_file() missing 1 required positional argument: 'thetime'
2019-09-15 09:31:44,276 - training_jobs - DEBUG - training trains/task_40.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'x_topo_feats',
 'max_depth': 8,
 'model': 'XGBClassifier'}
2019-09-15 09:31:44,286 - training_jobs - DEBUG - training with: 
2019-09-15 09:31:44,286 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 09:31:44,286 - training_jobs - DEBUG - XGBClassifier
2019-09-15 09:31:44,286 - training_jobs - DEBUG - x_topo_feats
2019-09-15 09:31:44,286 - training_jobs - DEBUG - baseline training
2019-09-15 09:31:44,617 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 09:33:44,036 - training_jobs - DEBUG - training time: 119s
2019-09-15 09:33:44,036 - training_jobs - DEBUG - saving to results/20190915_093144_baseline_x_topo_feats.json
2019-09-15 09:33:44,036 - training_jobs - ERROR - Error with trains/task_40.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 666, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 178, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
TypeError: add_dataset_file() missing 1 required positional argument: 'thetime'
2019-09-15 09:33:46,282 - training_jobs - DEBUG - training trains/task_35.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'max_depth': 150,
 'model': 'XGBClassifier'}
2019-09-15 09:33:46,293 - training_jobs - DEBUG - training with: 
2019-09-15 09:33:46,293 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 09:33:46,293 - training_jobs - DEBUG - XGBClassifier
2019-09-15 09:33:46,293 - training_jobs - DEBUG - topo and code feats
2019-09-15 09:33:46,293 - training_jobs - DEBUG - baseline training
2019-09-15 09:33:46,430 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 09:35:10,932 - training_jobs - DEBUG - training time: 85s
2019-09-15 09:35:10,932 - training_jobs - DEBUG - saving to results/20190915_093346_baseline_topo_and_code_feats.json
2019-09-15 09:35:10,932 - training_jobs - ERROR - Error with trains/task_35.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 666, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 178, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
TypeError: add_dataset_file() missing 1 required positional argument: 'thetime'
2019-09-15 09:35:13,238 - training_jobs - DEBUG - training trains/task_173.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 09:35:13,330 - training_jobs - DEBUG - training with: 
2019-09-15 09:35:13,331 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 09:35:13,331 - training_jobs - DEBUG - GGNN1
2019-09-15 09:35:13,331 - training_jobs - DEBUG - 
2019-09-15 09:35:13,331 - training_jobs - DEBUG - ggnn training
2019-09-15 09:35:14,543 - training_jobs - DEBUG -  saving results to results/20190915_093514_ggnn_.json
2019-09-15 09:35:14,543 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 09:35:15,691 - training_jobs - ERROR - Error with trains/task_173.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 666, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 531, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 09:35:17,757 - training_jobs - DEBUG - training trains/task_193.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'model': 'mlp4',
 'num_epochs': 100}
2019-09-15 09:35:18,134 - training_jobs - DEBUG - training with: 
2019-09-15 09:35:18,134 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-15 09:35:18,134 - training_jobs - DEBUG - mlp4
2019-09-15 09:35:18,134 - training_jobs - DEBUG - topo and code feats
2019-09-15 09:35:18,135 - training_jobs - DEBUG - moved jobdict to done_trainings/task_193.yml
2019-09-15 09:35:18,135 - training_jobs - DEBUG - Finished!

2019-09-15 09:35:20,189 - training_jobs - DEBUG - training trains/task_242.yml
{'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'x_topo_feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-15 09:35:20,199 - training_jobs - DEBUG - training with: 
2019-09-15 09:35:20,199 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 09:35:20,199 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 09:35:20,199 - training_jobs - DEBUG - x_topo_feats
2019-09-15 09:35:20,199 - training_jobs - DEBUG - baseline training
2019-09-15 09:35:20,306 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 09:35:21,637 - training_jobs - DEBUG - training time: 1s
2019-09-15 09:35:21,637 - training_jobs - DEBUG - saving to results/20190915_093520_baseline_x_topo_feats.json
2019-09-15 09:35:21,637 - training_jobs - ERROR - Error with trains/task_242.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 666, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 178, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
TypeError: add_dataset_file() missing 1 required positional argument: 'thetime'
2019-09-15 09:35:23,827 - training_jobs - DEBUG - training trains/task_28.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'x_topo_feats',
 'max_depth': 8,
 'model': 'XGBClassifier'}
2019-09-15 09:35:23,837 - training_jobs - DEBUG - training with: 
2019-09-15 09:35:23,837 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-15 09:35:23,837 - training_jobs - DEBUG - XGBClassifier
2019-09-15 09:35:23,837 - training_jobs - DEBUG - x_topo_feats
2019-09-15 09:35:23,837 - training_jobs - DEBUG - baseline training
2019-09-15 09:35:23,880 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 09:35:36,812 - training_jobs - DEBUG - training time: 13s
2019-09-15 09:35:36,812 - training_jobs - DEBUG - saving to results/20190915_093523_baseline_x_topo_feats.json
2019-09-15 09:35:36,812 - training_jobs - ERROR - Error with trains/task_28.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 666, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 178, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
TypeError: add_dataset_file() missing 1 required positional argument: 'thetime'
2019-09-15 09:35:39,204 - training_jobs - DEBUG - training trains/task_65.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'max_depth': 150,
 'model': 'XGBClassifier'}
2019-09-15 09:35:39,255 - training_jobs - DEBUG - training with: 
2019-09-15 09:35:39,255 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 09:35:39,255 - training_jobs - DEBUG - XGBClassifier
2019-09-15 09:35:39,255 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 09:35:39,255 - training_jobs - DEBUG - nlp training
2019-09-15 09:35:39,683 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-15 09:58:02,011 - training_jobs - DEBUG - training time: 1342s
2019-09-15 09:58:02,011 - training_jobs - DEBUG - saving to results/20190915_093539_nlp_tfidf_and_topo_feats.json
2019-09-15 09:58:02,011 - training_jobs - ERROR - Error with trains/task_65.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 666, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 354, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
TypeError: add_dataset_file() missing 1 required positional argument: 'thetime'
2019-09-15 09:58:04,094 - training_jobs - DEBUG - training trains/task_171.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 09:58:04,310 - training_jobs - DEBUG - training with: 
2019-09-15 09:58:04,310 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-15 09:58:04,310 - training_jobs - DEBUG - GGNN1
2019-09-15 09:58:04,310 - training_jobs - DEBUG - 
2019-09-15 09:58:04,310 - training_jobs - DEBUG - ggnn training
2019-09-15 09:58:14,961 - training_jobs - DEBUG -  saving results to results/20190915_095814_ggnn_.json
2019-09-15 09:58:14,961 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 11:11:49,454 - training_jobs - DEBUG - test_multiple_models
2019-09-15 11:11:49,455 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-15 11:11:52,787 - training_jobs - DEBUG - training time: 4418s
2019-09-15 11:11:52,788 - training_jobs - DEBUG - saving to results/20190915_095814_ggnn_.json
2019-09-15 11:11:52,788 - training_jobs - ERROR - Error with trains/task_171.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 666, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 548, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
TypeError: add_dataset_file() missing 1 required positional argument: 'thetime'
2019-09-15 11:11:54,967 - training_jobs - DEBUG - training trains/task_54.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'model': 'mlp3',
 'num_epochs': 30}
2019-09-15 11:11:55,019 - training_jobs - DEBUG - training with: 
2019-09-15 11:11:55,019 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 11:11:55,019 - training_jobs - DEBUG - mlp3
2019-09-15 11:11:55,020 - training_jobs - DEBUG - topo and code feats
2019-09-15 11:11:55,020 - training_jobs - DEBUG - moved jobdict to done_trainings/task_54.yml
2019-09-15 11:11:55,020 - training_jobs - DEBUG - Finished!

2019-09-15 11:11:57,123 - training_jobs - DEBUG - training trains/task_263.yml
{'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-15 11:11:57,236 - training_jobs - DEBUG - training with: 
2019-09-15 11:11:57,236 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 11:11:57,236 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 11:11:57,236 - training_jobs - DEBUG - topo and code feats
2019-09-15 11:11:57,236 - training_jobs - DEBUG - baseline training
2019-09-15 11:11:57,375 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 11:11:58,579 - training_jobs - DEBUG - training time: 1s
2019-09-15 11:11:58,580 - training_jobs - DEBUG - saving to results/20190915_111157_baseline_topo_and_code_feats.json
2019-09-15 11:11:58,580 - training_jobs - ERROR - Error with trains/task_263.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 666, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 178, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
TypeError: add_dataset_file() missing 1 required positional argument: 'thetime'
2019-09-15 11:12:00,762 - training_jobs - DEBUG - training trains/task_55.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'model': 'mlp3',
 'num_epochs': 100}
2019-09-15 11:12:00,862 - training_jobs - DEBUG - training with: 
2019-09-15 11:12:00,862 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 11:12:00,863 - training_jobs - DEBUG - mlp3
2019-09-15 11:12:00,863 - training_jobs - DEBUG - topo and code feats
2019-09-15 11:12:00,863 - training_jobs - DEBUG - moved jobdict to done_trainings/task_55.yml
2019-09-15 11:12:00,864 - training_jobs - DEBUG - Finished!

2019-09-15 11:12:02,930 - training_jobs - DEBUG - training trains/task_222.yml
{'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-15 11:12:03,026 - training_jobs - DEBUG - training with: 
2019-09-15 11:12:03,026 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-15 11:12:03,026 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 11:12:03,026 - training_jobs - DEBUG - topo and code feats
2019-09-15 11:12:03,026 - training_jobs - DEBUG - baseline training
2019-09-15 11:12:03,104 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 11:12:03,766 - training_jobs - DEBUG - training time: 1s
2019-09-15 11:12:03,766 - training_jobs - DEBUG - saving to results/20190915_111203_baseline_topo_and_code_feats.json
2019-09-15 11:12:03,766 - training_jobs - ERROR - Error with trains/task_222.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 666, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 178, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
TypeError: add_dataset_file() missing 1 required positional argument: 'thetime'
2019-09-15 11:12:05,958 - training_jobs - DEBUG - training trains/task_14.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'max_depth': 8,
 'model': 'XGBClassifier'}
2019-09-15 11:12:06,157 - training_jobs - DEBUG - training with: 
2019-09-15 11:12:06,157 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-15 11:12:06,157 - training_jobs - DEBUG - XGBClassifier
2019-09-15 11:12:06,157 - training_jobs - DEBUG - topo and code feats
2019-09-15 11:12:06,157 - training_jobs - DEBUG - baseline training
2019-09-15 11:12:06,245 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 11:12:55,207 - training_jobs - DEBUG - training time: 49s
2019-09-15 11:12:55,207 - training_jobs - DEBUG - saving to results/20190915_111206_baseline_topo_and_code_feats.json
2019-09-15 11:12:55,207 - training_jobs - ERROR - Error with trains/task_14.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 666, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 178, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
TypeError: add_dataset_file() missing 1 required positional argument: 'thetime'
2019-09-15 11:12:57,529 - training_jobs - DEBUG - training trains/task_228.yml
{'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-15 11:12:57,624 - training_jobs - DEBUG - training with: 
2019-09-15 11:12:57,624 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-15 11:12:57,624 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 11:12:57,624 - training_jobs - DEBUG - topo and code feats
2019-09-15 11:12:57,624 - training_jobs - DEBUG - baseline training
2019-09-15 11:12:57,792 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 11:12:58,552 - training_jobs - DEBUG - training time: 1s
2019-09-15 11:12:58,552 - training_jobs - DEBUG - saving to results/20190915_111257_baseline_topo_and_code_feats.json
2019-09-15 11:12:58,552 - training_jobs - ERROR - Error with trains/task_228.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 666, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 178, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
TypeError: add_dataset_file() missing 1 required positional argument: 'thetime'
2019-09-15 11:13:00,889 - training_jobs - DEBUG - training trains/task_238.yml
{'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-15 11:13:00,942 - training_jobs - DEBUG - training with: 
2019-09-15 11:13:00,942 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 11:13:00,943 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 11:13:00,943 - training_jobs - DEBUG - topo and code feats
2019-09-15 11:13:00,943 - training_jobs - DEBUG - baseline training
2019-09-15 11:13:01,028 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 11:13:02,326 - training_jobs - DEBUG - training time: 1s
2019-09-15 11:13:02,326 - training_jobs - DEBUG - saving to results/20190915_111301_baseline_topo_and_code_feats.json
2019-09-15 11:13:02,326 - training_jobs - ERROR - Error with trains/task_238.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 666, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 178, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
TypeError: add_dataset_file() missing 1 required positional argument: 'thetime'
2019-09-15 11:13:05,244 - training_jobs - DEBUG - training trains/task_254.yml
{'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-15 11:13:05,255 - training_jobs - DEBUG - training with: 
2019-09-15 11:13:05,255 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-15 11:13:05,255 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 11:13:05,255 - training_jobs - DEBUG - topo and code feats
2019-09-15 11:13:05,256 - training_jobs - DEBUG - baseline training
2019-09-15 11:13:05,330 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 11:13:05,928 - training_jobs - DEBUG - training time: 1s
2019-09-15 11:13:05,928 - training_jobs - DEBUG - saving to results/20190915_111305_baseline_topo_and_code_feats.json
2019-09-15 11:13:05,928 - training_jobs - ERROR - Error with trains/task_254.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 666, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 178, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
TypeError: add_dataset_file() missing 1 required positional argument: 'thetime'
2019-09-15 11:13:08,098 - training_jobs - DEBUG - training trains/task_23.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'max_depth': 150,
 'model': 'XGBClassifier'}
2019-09-15 11:13:08,313 - training_jobs - DEBUG - training with: 
2019-09-15 11:13:08,313 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 11:13:08,313 - training_jobs - DEBUG - XGBClassifier
2019-09-15 11:13:08,313 - training_jobs - DEBUG - topo and code feats
2019-09-15 11:13:08,313 - training_jobs - DEBUG - baseline training
2019-09-15 11:13:08,402 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 11:16:48,229 - training_jobs - DEBUG - training time: 220s
2019-09-15 11:16:48,229 - training_jobs - DEBUG - saving to results/20190915_111308_baseline_topo_and_code_feats.json
2019-09-15 11:16:48,229 - training_jobs - ERROR - Error with trains/task_23.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 666, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 178, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
TypeError: add_dataset_file() missing 1 required positional argument: 'thetime'
2019-09-15 11:16:50,486 - training_jobs - DEBUG - training trains/task_82.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 15,
 'd4': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 11:16:50,574 - training_jobs - DEBUG - training with: 
2019-09-15 11:16:50,574 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 11:16:50,575 - training_jobs - DEBUG - GGNN5
2019-09-15 11:16:50,575 - training_jobs - DEBUG - 
2019-09-15 11:16:50,576 - training_jobs - DEBUG - moved jobdict to done_trainings/task_82.yml
2019-09-15 11:16:50,576 - training_jobs - DEBUG - Finished!

2019-09-15 11:16:52,713 - training_jobs - DEBUG - training trains/task_257.yml
{'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'x_topo_feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-15 11:16:52,723 - training_jobs - DEBUG - training with: 
2019-09-15 11:16:52,723 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 11:16:52,723 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 11:16:52,724 - training_jobs - DEBUG - x_topo_feats
2019-09-15 11:16:52,724 - training_jobs - DEBUG - baseline training
2019-09-15 11:16:52,822 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 11:16:53,765 - training_jobs - DEBUG - training time: 1s
2019-09-15 11:16:53,765 - training_jobs - DEBUG - saving to results/20190915_111652_baseline_x_topo_feats.json
2019-09-15 11:16:53,765 - training_jobs - ERROR - Error with trains/task_257.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 666, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 178, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
TypeError: add_dataset_file() missing 1 required positional argument: 'thetime'
2019-09-15 11:16:55,944 - training_jobs - DEBUG - training trains/task_30.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'max_depth': 8,
 'model': 'XGBClassifier'}
2019-09-15 11:16:55,955 - training_jobs - DEBUG - training with: 
2019-09-15 11:16:55,955 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-15 11:16:55,955 - training_jobs - DEBUG - XGBClassifier
2019-09-15 11:16:55,955 - training_jobs - DEBUG - topo and code feats
2019-09-15 11:16:55,955 - training_jobs - DEBUG - baseline training
2019-09-15 11:16:56,020 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 11:17:13,715 - training_jobs - DEBUG - training time: 18s
2019-09-15 11:17:13,715 - training_jobs - DEBUG - saving to results/20190915_111656_baseline_topo_and_code_feats.json
2019-09-15 11:17:13,715 - training_jobs - ERROR - Error with trains/task_30.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 666, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 178, in training_dispatcher
    add_dataset_file(results_file, dataset_folder)
TypeError: add_dataset_file() missing 1 required positional argument: 'thetime'
2019-09-15 11:17:15,950 - training_jobs - DEBUG - training trains/task_3.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'model': 'mlp3',
 'num_epochs': 100}
2019-09-15 11:17:16,224 - training_jobs - DEBUG - training with: 
2019-09-15 11:17:16,224 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-15 11:17:16,224 - training_jobs - DEBUG - mlp3
2019-09-15 11:17:16,224 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 11:17:16,225 - training_jobs - DEBUG - moved jobdict to done_trainings/task_3.yml
2019-09-15 11:17:16,225 - training_jobs - DEBUG - Finished!

2019-09-15 11:17:18,311 - training_jobs - DEBUG - training trains/task_200.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'model': 'mlp4',
 'num_epochs': 30}
2019-09-15 11:17:18,347 - training_jobs - DEBUG - training with: 
2019-09-15 11:17:18,347 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-15 11:17:18,347 - training_jobs - DEBUG - mlp4
2019-09-15 11:17:18,347 - training_jobs - DEBUG - topo and code feats
2019-09-15 11:17:18,348 - training_jobs - DEBUG - moved jobdict to done_trainings/task_200.yml
2019-09-15 11:17:18,348 - training_jobs - DEBUG - Finished!

2019-09-15 11:17:20,478 - training_jobs - DEBUG - training trains/task_169.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 11:17:20,489 - training_jobs - DEBUG - training with: 
2019-09-15 11:17:20,489 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-15 11:17:20,489 - training_jobs - DEBUG - GGNN1
2019-09-15 11:17:20,489 - training_jobs - DEBUG - 
2019-09-15 11:17:20,489 - training_jobs - DEBUG - ggnn training
2019-09-15 11:17:21,925 - training_jobs - DEBUG -  saving results to results/20190915_111721_ggnn_.json
2019-09-15 11:17:21,925 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 11:45:09,514 - training_jobs - ERROR - Error with trains/task_169.yml
Traceback (most recent call last):
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 422, in get
    data = torch.load(filename)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/serialization.py", line 368, in load
    return _load(f, map_location, pickle_module)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/serialization.py", line 524, in _load
    return legacy_load(f)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/serialization.py", line 448, in legacy_load
    with closing(tarfile.open(fileobj=f, mode='r:', format=tarfile.PAX_FORMAT)) as tar, \
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/tarfile.py", line 1589, in open
    return func(name, filemode, fileobj, **kwargs)
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/tarfile.py", line 1619, in taropen
    return cls(name, mode, fileobj, **kwargs)
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/tarfile.py", line 1482, in __init__
    self.firstmember = self.next()
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/tarfile.py", line 2297, in next
    tarinfo = self.tarinfo.fromtarfile(self)
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/tarfile.py", line 1092, in fromtarfile
    buf = tarfile.fileobj.read(BLOCKSIZE)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "pipeline_train_model.py", line 666, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 531, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1541, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1368, in select_best_model
    best_model_acc = final_model_train(best_model_acc, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1123, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 941, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 828, in train_model_GGNN
    for batch in loader:
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 618, in __next__
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 618, in <listcomp>
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 126, in __getitem__
    data = self.get(idx)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 438, in get
    raise IndexError("error getting: ", idx)
IndexError: ('error getting: ', 2752)
2019-09-15 11:45:51,625 - training_jobs - DEBUG - training trains/task_139.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 11:45:51,933 - training_jobs - DEBUG - training with: 
2019-09-15 11:45:51,934 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 11:45:51,934 - training_jobs - DEBUG - GGNN1
2019-09-15 11:45:51,934 - training_jobs - DEBUG - 
2019-09-15 11:45:51,934 - training_jobs - DEBUG - ggnn training
2019-09-15 11:45:54,114 - training_jobs - DEBUG -  saving results to results/20190915_114554_ggnn_.json
2019-09-15 11:45:54,114 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 11:45:56,244 - training_jobs - ERROR - Error with trains/task_139.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 666, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 531, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 11:45:58,592 - training_jobs - DEBUG - training trains/task_155.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 11:45:58,759 - training_jobs - DEBUG - training with: 
2019-09-15 11:45:58,759 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 11:45:58,759 - training_jobs - DEBUG - GGNN1
2019-09-15 11:45:58,759 - training_jobs - DEBUG - 
2019-09-15 11:45:58,759 - training_jobs - DEBUG - ggnn training
2019-09-15 11:46:01,113 - training_jobs - DEBUG -  saving results to results/20190915_114601_ggnn_.json
2019-09-15 11:46:01,114 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 12:41:00,768 - training_jobs - DEBUG - test_multiple_models
2019-09-15 12:41:00,768 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-15 12:41:18,446 - training_jobs - DEBUG - training time: 3317s
2019-09-15 12:41:18,446 - training_jobs - DEBUG - saving to results/20190915_114601_ggnn_.json
2019-09-15 12:41:18,448 - training_jobs - DEBUG - moved jobdict to done_trainings/task_155.yml
2019-09-15 12:41:18,448 - training_jobs - DEBUG - Finished!

2019-09-15 12:41:24,191 - training_jobs - DEBUG - training trains/task_162.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 12:41:24,219 - training_jobs - DEBUG - training with: 
2019-09-15 12:41:24,219 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 12:41:24,219 - training_jobs - DEBUG - GGNN1
2019-09-15 12:41:24,219 - training_jobs - DEBUG - 
2019-09-15 12:41:24,219 - training_jobs - DEBUG - ggnn training
2019-09-15 12:41:27,341 - training_jobs - DEBUG -  saving results to results/20190915_124127_ggnn_.json
2019-09-15 12:41:27,341 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 12:41:29,757 - training_jobs - ERROR - Error with trains/task_162.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 666, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 531, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 12:41:32,594 - training_jobs - DEBUG - training trains/task_161.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 12:41:32,698 - training_jobs - DEBUG - training with: 
2019-09-15 12:41:32,699 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 12:41:32,699 - training_jobs - DEBUG - GGNN1
2019-09-15 12:41:32,699 - training_jobs - DEBUG - 
2019-09-15 12:41:32,699 - training_jobs - DEBUG - ggnn training
2019-09-15 12:41:34,886 - training_jobs - DEBUG -  saving results to results/20190915_124134_ggnn_.json
2019-09-15 12:41:34,886 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 12:41:36,340 - training_jobs - ERROR - Error with trains/task_161.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 666, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 531, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 12:41:38,979 - training_jobs - DEBUG - training trains/task_136.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 12:41:38,993 - training_jobs - DEBUG - training with: 
2019-09-15 12:41:38,993 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 12:41:38,993 - training_jobs - DEBUG - GGNN1
2019-09-15 12:41:38,993 - training_jobs - DEBUG - 
2019-09-15 12:41:38,993 - training_jobs - DEBUG - ggnn training
2019-09-15 12:41:40,553 - training_jobs - DEBUG -  saving results to results/20190915_124140_ggnn_.json
2019-09-15 12:41:40,553 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 12:41:41,938 - training_jobs - ERROR - Error with trains/task_136.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 666, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 531, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 12:41:44,500 - training_jobs - DEBUG - training trains/task_142.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 12:41:44,526 - training_jobs - DEBUG - training with: 
2019-09-15 12:41:44,526 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 12:41:44,526 - training_jobs - DEBUG - GGNN1
2019-09-15 12:41:44,526 - training_jobs - DEBUG - 
2019-09-15 12:41:44,526 - training_jobs - DEBUG - ggnn training
2019-09-15 12:41:46,801 - training_jobs - DEBUG -  saving results to results/20190915_124146_ggnn_.json
2019-09-15 12:41:46,801 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 13:03:33,804 - training_jobs - DEBUG - training trains/task_139.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 13:03:33,839 - training_jobs - DEBUG - training with: 
2019-09-15 13:03:33,839 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 13:03:33,839 - training_jobs - DEBUG - GGNN1
2019-09-15 13:03:33,839 - training_jobs - DEBUG - 
2019-09-15 13:03:33,839 - training_jobs - DEBUG - ggnn training
2019-09-15 13:03:37,272 - training_jobs - DEBUG -  saving results to results/20190915_130337_ggnn_.json
2019-09-15 13:03:37,272 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 13:03:40,608 - training_jobs - ERROR - Error with trains/task_139.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 667, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 532, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 13:03:43,993 - training_jobs - DEBUG - training trains/task_162.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 13:03:44,010 - training_jobs - DEBUG - training with: 
2019-09-15 13:03:44,010 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 13:03:44,010 - training_jobs - DEBUG - GGNN1
2019-09-15 13:03:44,010 - training_jobs - DEBUG - 
2019-09-15 13:03:44,010 - training_jobs - DEBUG - ggnn training
2019-09-15 13:03:47,238 - training_jobs - DEBUG -  saving results to results/20190915_130347_ggnn_.json
2019-09-15 13:03:47,239 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 13:03:50,137 - training_jobs - ERROR - Error with trains/task_162.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 667, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 532, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 13:03:53,192 - training_jobs - DEBUG - training trains/task_161.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 13:03:53,207 - training_jobs - DEBUG - training with: 
2019-09-15 13:03:53,207 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 13:03:53,208 - training_jobs - DEBUG - GGNN1
2019-09-15 13:03:53,208 - training_jobs - DEBUG - 
2019-09-15 13:03:53,208 - training_jobs - DEBUG - ggnn training
2019-09-15 13:03:55,071 - training_jobs - DEBUG -  saving results to results/20190915_130355_ggnn_.json
2019-09-15 13:03:55,071 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 13:03:56,735 - training_jobs - ERROR - Error with trains/task_161.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 667, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 532, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 13:03:59,669 - training_jobs - DEBUG - training trains/task_136.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 13:03:59,683 - training_jobs - DEBUG - training with: 
2019-09-15 13:03:59,683 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 13:03:59,683 - training_jobs - DEBUG - GGNN1
2019-09-15 13:03:59,683 - training_jobs - DEBUG - 
2019-09-15 13:03:59,683 - training_jobs - DEBUG - ggnn training
2019-09-15 13:04:01,348 - training_jobs - DEBUG -  saving results to results/20190915_130401_ggnn_.json
2019-09-15 13:04:01,348 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 13:04:02,792 - training_jobs - ERROR - Error with trains/task_136.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 667, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 532, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 13:04:05,673 - training_jobs - DEBUG - training trains/task_142.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 13:04:05,686 - training_jobs - DEBUG - training with: 
2019-09-15 13:04:05,686 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 13:04:05,686 - training_jobs - DEBUG - GGNN1
2019-09-15 13:04:05,686 - training_jobs - DEBUG - 
2019-09-15 13:04:05,686 - training_jobs - DEBUG - ggnn training
2019-09-15 13:04:08,175 - training_jobs - DEBUG -  saving results to results/20190915_130408_ggnn_.json
2019-09-15 13:04:08,176 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 13:17:32,141 - training_jobs - DEBUG - training trains/task_139.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 13:17:32,154 - training_jobs - DEBUG - training with: 
2019-09-15 13:17:32,154 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 13:17:32,154 - training_jobs - DEBUG - GGNN1
2019-09-15 13:17:32,155 - training_jobs - DEBUG - 
2019-09-15 13:17:32,155 - training_jobs - DEBUG - ggnn training
2019-09-15 13:17:37,333 - training_jobs - DEBUG -  saving results to results/20190915_131737_ggnn_.json
2019-09-15 13:17:37,333 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 13:17:40,410 - training_jobs - ERROR - Error with trains/task_139.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 13:17:43,370 - training_jobs - DEBUG - training trains/task_162.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 13:17:43,384 - training_jobs - DEBUG - training with: 
2019-09-15 13:17:43,385 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 13:17:43,385 - training_jobs - DEBUG - GGNN1
2019-09-15 13:17:43,385 - training_jobs - DEBUG - 
2019-09-15 13:17:43,385 - training_jobs - DEBUG - ggnn training
2019-09-15 13:17:46,941 - training_jobs - DEBUG -  saving results to results/20190915_131746_ggnn_.json
2019-09-15 13:17:46,941 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 13:17:49,761 - training_jobs - ERROR - Error with trains/task_162.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 13:17:55,882 - training_jobs - DEBUG - training trains/task_161.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 13:17:55,896 - training_jobs - DEBUG - training with: 
2019-09-15 13:17:55,896 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 13:17:55,896 - training_jobs - DEBUG - GGNN1
2019-09-15 13:17:55,896 - training_jobs - DEBUG - 
2019-09-15 13:17:55,896 - training_jobs - DEBUG - ggnn training
2019-09-15 13:17:57,749 - training_jobs - DEBUG -  saving results to results/20190915_131757_ggnn_.json
2019-09-15 13:17:57,749 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 13:17:59,219 - training_jobs - ERROR - Error with trains/task_161.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 13:18:01,681 - training_jobs - DEBUG - training trains/task_136.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 13:18:01,693 - training_jobs - DEBUG - training with: 
2019-09-15 13:18:01,694 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 13:18:01,694 - training_jobs - DEBUG - GGNN1
2019-09-15 13:18:01,694 - training_jobs - DEBUG - 
2019-09-15 13:18:01,694 - training_jobs - DEBUG - ggnn training
2019-09-15 13:18:03,234 - training_jobs - DEBUG -  saving results to results/20190915_131803_ggnn_.json
2019-09-15 13:18:03,234 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 13:18:04,930 - training_jobs - ERROR - Error with trains/task_136.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 13:18:08,066 - training_jobs - DEBUG - training trains/task_142.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 13:18:08,080 - training_jobs - DEBUG - training with: 
2019-09-15 13:18:08,080 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 13:18:08,080 - training_jobs - DEBUG - GGNN1
2019-09-15 13:18:08,080 - training_jobs - DEBUG - 
2019-09-15 13:18:08,080 - training_jobs - DEBUG - ggnn training
2019-09-15 13:18:17,872 - training_jobs - DEBUG -  saving results to results/20190915_131817_ggnn_.json
2019-09-15 13:18:17,873 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 13:31:52,961 - training_jobs - DEBUG - test_multiple_models
2019-09-15 13:31:53,019 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-15 13:32:20,965 - training_jobs - DEBUG - training time: 1693s
2019-09-15 13:32:20,965 - training_jobs - DEBUG - saving to results/20190915_130408_ggnn_.json
2019-09-15 13:32:20,966 - training_jobs - ERROR - Error with trains/task_142.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 667, in <module>
    # train!
  File "pipeline_train_model.py", line 549, in training_dispatcher
    logger.debug("saving to "+results_file)
  File "pipeline_train_model.py", line 49, in add_dataset_file
    f.seek(0)
NameError: name 'f' is not defined
2019-09-15 13:33:01,345 - training_jobs - DEBUG - training trains/task_139.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 13:33:01,603 - training_jobs - DEBUG - training with: 
2019-09-15 13:33:01,603 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 13:33:01,603 - training_jobs - DEBUG - GGNN1
2019-09-15 13:33:01,603 - training_jobs - DEBUG - 
2019-09-15 13:33:01,603 - training_jobs - DEBUG - ggnn training
2019-09-15 13:33:19,645 - training_jobs - DEBUG -  saving results to results/20190915_133319_ggnn_.json
2019-09-15 13:33:19,645 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 13:33:22,564 - training_jobs - ERROR - Error with trains/task_139.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 13:33:25,450 - training_jobs - DEBUG - training trains/task_162.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 13:33:26,645 - training_jobs - DEBUG - training with: 
2019-09-15 13:33:26,645 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 13:33:26,645 - training_jobs - DEBUG - GGNN1
2019-09-15 13:33:26,645 - training_jobs - DEBUG - 
2019-09-15 13:33:26,645 - training_jobs - DEBUG - ggnn training
2019-09-15 13:33:29,210 - training_jobs - DEBUG -  saving results to results/20190915_133329_ggnn_.json
2019-09-15 13:33:29,210 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 13:33:31,814 - training_jobs - ERROR - Error with trains/task_162.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 13:33:34,651 - training_jobs - DEBUG - training trains/task_161.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 13:33:34,840 - training_jobs - DEBUG - training with: 
2019-09-15 13:33:34,840 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 13:33:34,840 - training_jobs - DEBUG - GGNN1
2019-09-15 13:33:34,840 - training_jobs - DEBUG - 
2019-09-15 13:33:34,840 - training_jobs - DEBUG - ggnn training
2019-09-15 13:33:37,219 - training_jobs - DEBUG -  saving results to results/20190915_133337_ggnn_.json
2019-09-15 13:33:37,219 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 13:33:38,777 - training_jobs - ERROR - Error with trains/task_161.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 13:33:41,724 - training_jobs - DEBUG - training trains/task_136.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 13:33:41,755 - training_jobs - DEBUG - training with: 
2019-09-15 13:33:41,755 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 13:33:41,755 - training_jobs - DEBUG - GGNN1
2019-09-15 13:33:41,755 - training_jobs - DEBUG - 
2019-09-15 13:33:41,755 - training_jobs - DEBUG - ggnn training
2019-09-15 13:33:43,655 - training_jobs - DEBUG -  saving results to results/20190915_133343_ggnn_.json
2019-09-15 13:33:43,655 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 13:33:45,261 - training_jobs - ERROR - Error with trains/task_136.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 13:33:48,216 - training_jobs - DEBUG - training trains/task_142.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 13:33:48,269 - training_jobs - DEBUG - training with: 
2019-09-15 13:33:48,269 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 13:33:48,269 - training_jobs - DEBUG - GGNN1
2019-09-15 13:33:48,269 - training_jobs - DEBUG - 
2019-09-15 13:33:48,269 - training_jobs - DEBUG - ggnn training
2019-09-15 13:33:50,845 - training_jobs - DEBUG -  saving results to results/20190915_133350_ggnn_.json
2019-09-15 13:33:50,845 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 14:03:19,683 - training_jobs - DEBUG - test_multiple_models
2019-09-15 14:03:19,683 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-15 14:03:20,709 - training_jobs - DEBUG - training time: 1770s
2019-09-15 14:03:20,709 - training_jobs - DEBUG - saving to results/20190915_133350_ggnn_.json
2019-09-15 14:03:20,712 - training_jobs - DEBUG - moved jobdict to done_trainings/task_142.yml
2019-09-15 14:03:20,712 - training_jobs - DEBUG - Finished!

2019-09-15 14:03:23,987 - training_jobs - DEBUG - training trains/task_24.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'x_topo_feats',
 'max_depth': 8,
 'model': 'XGBClassifier'}
2019-09-15 14:03:24,004 - training_jobs - DEBUG - training with: 
2019-09-15 14:03:24,004 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 14:03:24,005 - training_jobs - DEBUG - XGBClassifier
2019-09-15 14:03:24,005 - training_jobs - DEBUG - x_topo_feats
2019-09-15 14:03:24,005 - training_jobs - DEBUG - baseline training
2019-09-15 14:03:24,508 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 14:07:31,673 - training_jobs - DEBUG - training time: 247s
2019-09-15 14:07:31,674 - training_jobs - DEBUG - saving to results/20190915_140324_baseline_x_topo_feats.json
2019-09-15 14:07:31,686 - training_jobs - DEBUG - moved jobdict to done_trainings/task_24.yml
2019-09-15 14:07:31,686 - training_jobs - DEBUG - Finished!

2019-09-15 14:07:34,342 - training_jobs - DEBUG - training trains/task_108.yml
{'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-15 14:07:34,374 - training_jobs - DEBUG - training with: 
2019-09-15 14:07:34,374 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 14:07:34,374 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 14:07:34,374 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 14:07:34,374 - training_jobs - DEBUG - nlp training
2019-09-15 14:07:35,180 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-15 14:07:36,846 - training_jobs - DEBUG - training time: 2s
2019-09-15 14:07:36,847 - training_jobs - DEBUG - saving to results/20190915_140735_nlp_tfidf_and_topo_feats.json
2019-09-15 14:07:36,857 - training_jobs - DEBUG - moved jobdict to done_trainings/task_108.yml
2019-09-15 14:07:36,857 - training_jobs - DEBUG - Finished!

2019-09-15 14:07:39,464 - training_jobs - DEBUG - training trains/task_259.yml
{'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'x_topo_feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-15 14:07:39,529 - training_jobs - DEBUG - training with: 
2019-09-15 14:07:39,530 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 14:07:39,530 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 14:07:39,530 - training_jobs - DEBUG - x_topo_feats
2019-09-15 14:07:39,530 - training_jobs - DEBUG - baseline training
2019-09-15 14:07:40,082 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 14:07:41,615 - training_jobs - DEBUG - training time: 2s
2019-09-15 14:07:41,615 - training_jobs - DEBUG - saving to results/20190915_140740_baseline_x_topo_feats.json
2019-09-15 14:07:41,841 - training_jobs - DEBUG - moved jobdict to done_trainings/task_259.yml
2019-09-15 14:07:41,841 - training_jobs - DEBUG - Finished!

2019-09-15 14:07:44,590 - training_jobs - DEBUG - training trains/task_219.yml
{'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'x_topo_feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-15 14:07:44,664 - training_jobs - DEBUG - training with: 
2019-09-15 14:07:44,664 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-15 14:07:44,664 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 14:07:44,664 - training_jobs - DEBUG - x_topo_feats
2019-09-15 14:07:44,664 - training_jobs - DEBUG - baseline training
2019-09-15 14:07:45,066 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 14:07:46,470 - training_jobs - DEBUG - training time: 1s
2019-09-15 14:07:46,471 - training_jobs - DEBUG - saving to results/20190915_140745_baseline_x_topo_feats.json
2019-09-15 14:07:46,479 - training_jobs - DEBUG - moved jobdict to done_trainings/task_219.yml
2019-09-15 14:07:46,479 - training_jobs - DEBUG - Finished!

2019-09-15 14:07:49,038 - training_jobs - DEBUG - training trains/task_251.yml
{'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'x_topo_feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-15 14:07:49,050 - training_jobs - DEBUG - training with: 
2019-09-15 14:07:49,050 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-15 14:07:49,050 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 14:07:49,050 - training_jobs - DEBUG - x_topo_feats
2019-09-15 14:07:49,050 - training_jobs - DEBUG - baseline training
2019-09-15 14:07:49,335 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 14:07:50,434 - training_jobs - DEBUG - training time: 1s
2019-09-15 14:07:50,434 - training_jobs - DEBUG - saving to results/20190915_140749_baseline_x_topo_feats.json
2019-09-15 14:07:50,441 - training_jobs - DEBUG - moved jobdict to done_trainings/task_251.yml
2019-09-15 14:07:50,441 - training_jobs - DEBUG - Finished!

2019-09-15 14:07:52,798 - training_jobs - DEBUG - training trains/task_188.yml
{'d1': 85,
 'd2': 20,
 'd3': 15,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'model': 'mlp2',
 'num_epochs': 30}
2019-09-15 14:07:52,856 - training_jobs - DEBUG - training with: 
2019-09-15 14:07:52,857 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-15 14:07:52,857 - training_jobs - DEBUG - mlp2
2019-09-15 14:07:52,857 - training_jobs - DEBUG - topo and code feats
2019-09-15 14:07:52,857 - training_jobs - DEBUG - baseline_nn training
2019-09-15 14:07:52,933 - training_jobs - DEBUG -  calling nn_train_models
2019-09-15 14:09:03,282 - training_jobs - DEBUG - training time: 70s
2019-09-15 14:09:03,282 - training_jobs - DEBUG - saving to results/20190915_140752_baseline_nn_topo_and_code_feats.json
2019-09-15 14:09:03,285 - training_jobs - DEBUG - moved jobdict to done_trainings/task_188.yml
2019-09-15 14:09:03,285 - training_jobs - DEBUG - Finished!

2019-09-15 14:09:05,665 - training_jobs - DEBUG - training trains/task_126.yml
{'d1': 85,
 'd2': 20,
 'd3': 15,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'model': 'mlp2',
 'num_epochs': 30}
2019-09-15 14:09:05,727 - training_jobs - DEBUG - training with: 
2019-09-15 14:09:05,727 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 14:09:05,727 - training_jobs - DEBUG - mlp2
2019-09-15 14:09:05,727 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 14:09:05,727 - training_jobs - DEBUG - nlp_nn training
2019-09-15 14:09:06,386 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-15 14:11:14,707 - training_jobs - DEBUG - training time: 128s
2019-09-15 14:11:14,708 - training_jobs - DEBUG - saving to results/20190915_140906_nlp_nn_tfidf_and_topo_feats.json
2019-09-15 14:11:14,715 - training_jobs - DEBUG - moved jobdict to done_trainings/task_126.yml
2019-09-15 14:11:14,715 - training_jobs - DEBUG - Finished!

2019-09-15 14:11:17,116 - training_jobs - DEBUG - training trains/task_183.yml
{'d1': 85,
 'd2': 20,
 'd3': 15,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'model': 'mlp2',
 'num_epochs': 100}
2019-09-15 14:11:17,149 - training_jobs - DEBUG - training with: 
2019-09-15 14:11:17,149 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-15 14:11:17,149 - training_jobs - DEBUG - mlp2
2019-09-15 14:11:17,149 - training_jobs - DEBUG - topo and code feats
2019-09-15 14:11:17,149 - training_jobs - DEBUG - baseline_nn training
2019-09-15 14:11:17,753 - training_jobs - DEBUG -  calling nn_train_models
2019-09-15 14:18:34,529 - training_jobs - DEBUG - training time: 437s
2019-09-15 14:18:34,529 - training_jobs - DEBUG - saving to results/20190915_141117_baseline_nn_topo_and_code_feats.json
2019-09-15 14:18:34,535 - training_jobs - DEBUG - moved jobdict to done_trainings/task_183.yml
2019-09-15 14:18:34,535 - training_jobs - DEBUG - Finished!

2019-09-15 14:18:36,936 - training_jobs - DEBUG - training trains/task_40.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'x_topo_feats',
 'max_depth': 8,
 'model': 'XGBClassifier'}
2019-09-15 14:18:36,947 - training_jobs - DEBUG - training with: 
2019-09-15 14:18:36,947 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 14:18:36,947 - training_jobs - DEBUG - XGBClassifier
2019-09-15 14:18:36,947 - training_jobs - DEBUG - x_topo_feats
2019-09-15 14:18:36,947 - training_jobs - DEBUG - baseline training
2019-09-15 14:18:38,590 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 14:20:40,860 - training_jobs - DEBUG - training time: 122s
2019-09-15 14:20:40,861 - training_jobs - DEBUG - saving to results/20190915_141838_baseline_x_topo_feats.json
2019-09-15 14:20:40,869 - training_jobs - DEBUG - moved jobdict to done_trainings/task_40.yml
2019-09-15 14:20:40,869 - training_jobs - DEBUG - Finished!

2019-09-15 14:20:43,581 - training_jobs - DEBUG - training trains/task_35.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'max_depth': 150,
 'model': 'XGBClassifier'}
2019-09-15 14:20:43,594 - training_jobs - DEBUG - training with: 
2019-09-15 14:20:43,594 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 14:20:43,594 - training_jobs - DEBUG - XGBClassifier
2019-09-15 14:20:43,594 - training_jobs - DEBUG - topo and code feats
2019-09-15 14:20:43,594 - training_jobs - DEBUG - baseline training
2019-09-15 14:20:43,766 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 14:22:12,998 - training_jobs - DEBUG - training time: 89s
2019-09-15 14:22:12,998 - training_jobs - DEBUG - saving to results/20190915_142043_baseline_topo_and_code_feats.json
2019-09-15 14:22:13,007 - training_jobs - DEBUG - moved jobdict to done_trainings/task_35.yml
2019-09-15 14:22:13,007 - training_jobs - DEBUG - Finished!

2019-09-15 14:22:16,128 - training_jobs - DEBUG - training trains/task_173.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 14:22:16,261 - training_jobs - DEBUG - training with: 
2019-09-15 14:22:16,262 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 14:22:16,262 - training_jobs - DEBUG - GGNN1
2019-09-15 14:22:16,262 - training_jobs - DEBUG - 
2019-09-15 14:22:16,262 - training_jobs - DEBUG - ggnn training
2019-09-15 14:22:18,057 - training_jobs - DEBUG -  saving results to results/20190915_142218_ggnn_.json
2019-09-15 14:22:18,058 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 14:22:19,737 - training_jobs - ERROR - Error with trains/task_173.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 14:22:22,653 - training_jobs - DEBUG - training trains/task_242.yml
{'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'x_topo_feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-15 14:22:22,774 - training_jobs - DEBUG - training with: 
2019-09-15 14:22:22,774 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 14:22:22,774 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 14:22:22,774 - training_jobs - DEBUG - x_topo_feats
2019-09-15 14:22:22,774 - training_jobs - DEBUG - baseline training
2019-09-15 14:22:22,914 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 14:22:24,637 - training_jobs - DEBUG - training time: 2s
2019-09-15 14:22:24,638 - training_jobs - DEBUG - saving to results/20190915_142222_baseline_x_topo_feats.json
2019-09-15 14:22:24,656 - training_jobs - DEBUG - moved jobdict to done_trainings/task_242.yml
2019-09-15 14:22:24,656 - training_jobs - DEBUG - Finished!

2019-09-15 14:22:27,537 - training_jobs - DEBUG - training trains/task_28.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'x_topo_feats',
 'max_depth': 8,
 'model': 'XGBClassifier'}
2019-09-15 14:22:27,709 - training_jobs - DEBUG - training with: 
2019-09-15 14:22:27,709 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-15 14:22:27,709 - training_jobs - DEBUG - XGBClassifier
2019-09-15 14:22:27,709 - training_jobs - DEBUG - x_topo_feats
2019-09-15 14:22:27,709 - training_jobs - DEBUG - baseline training
2019-09-15 14:22:27,758 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 14:22:42,808 - training_jobs - DEBUG - training time: 15s
2019-09-15 14:22:42,808 - training_jobs - DEBUG - saving to results/20190915_142227_baseline_x_topo_feats.json
2019-09-15 14:22:42,814 - training_jobs - DEBUG - moved jobdict to done_trainings/task_28.yml
2019-09-15 14:22:42,814 - training_jobs - DEBUG - Finished!

2019-09-15 14:22:45,621 - training_jobs - DEBUG - training trains/task_65.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'max_depth': 150,
 'model': 'XGBClassifier'}
2019-09-15 14:22:45,736 - training_jobs - DEBUG - training with: 
2019-09-15 14:22:45,736 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 14:22:45,736 - training_jobs - DEBUG - XGBClassifier
2019-09-15 14:22:45,736 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 14:22:45,736 - training_jobs - DEBUG - nlp training
2019-09-15 14:22:46,362 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-15 14:45:17,239 - training_jobs - DEBUG - training time: 1351s
2019-09-15 14:45:17,239 - training_jobs - DEBUG - saving to results/20190915_142246_nlp_tfidf_and_topo_feats.json
2019-09-15 14:45:17,245 - training_jobs - DEBUG - moved jobdict to done_trainings/task_65.yml
2019-09-15 14:45:17,245 - training_jobs - DEBUG - Finished!

2019-09-15 14:45:20,096 - training_jobs - DEBUG - training trains/task_171.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 14:45:20,128 - training_jobs - DEBUG - training with: 
2019-09-15 14:45:20,128 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-15 14:45:20,128 - training_jobs - DEBUG - GGNN1
2019-09-15 14:45:20,128 - training_jobs - DEBUG - 
2019-09-15 14:45:20,128 - training_jobs - DEBUG - ggnn training
2019-09-15 14:45:37,726 - training_jobs - DEBUG -  saving results to results/20190915_144537_ggnn_.json
2019-09-15 14:45:37,726 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 15:59:42,199 - training_jobs - DEBUG - test_multiple_models
2019-09-15 15:59:42,200 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-15 15:59:47,383 - training_jobs - DEBUG - training time: 4450s
2019-09-15 15:59:47,383 - training_jobs - DEBUG - saving to results/20190915_144537_ggnn_.json
2019-09-15 15:59:47,389 - training_jobs - DEBUG - moved jobdict to done_trainings/task_171.yml
2019-09-15 15:59:47,389 - training_jobs - DEBUG - Finished!

2019-09-15 15:59:50,452 - training_jobs - DEBUG - training trains/task_263.yml
{'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-15 15:59:50,515 - training_jobs - DEBUG - training with: 
2019-09-15 15:59:50,515 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 15:59:50,515 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 15:59:50,516 - training_jobs - DEBUG - topo and code feats
2019-09-15 15:59:50,516 - training_jobs - DEBUG - baseline training
2019-09-15 15:59:50,692 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 15:59:52,283 - training_jobs - DEBUG - training time: 2s
2019-09-15 15:59:52,283 - training_jobs - DEBUG - saving to results/20190915_155950_baseline_topo_and_code_feats.json
2019-09-15 15:59:52,297 - training_jobs - DEBUG - moved jobdict to done_trainings/task_263.yml
2019-09-15 15:59:52,297 - training_jobs - DEBUG - Finished!

2019-09-15 15:59:55,378 - training_jobs - DEBUG - training trains/task_222.yml
{'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-15 15:59:55,413 - training_jobs - DEBUG - training with: 
2019-09-15 15:59:55,414 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-15 15:59:55,414 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 15:59:55,414 - training_jobs - DEBUG - topo and code feats
2019-09-15 15:59:55,414 - training_jobs - DEBUG - baseline training
2019-09-15 15:59:55,515 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 15:59:56,424 - training_jobs - DEBUG - training time: 1s
2019-09-15 15:59:56,424 - training_jobs - DEBUG - saving to results/20190915_155955_baseline_topo_and_code_feats.json
2019-09-15 15:59:56,434 - training_jobs - DEBUG - moved jobdict to done_trainings/task_222.yml
2019-09-15 15:59:56,434 - training_jobs - DEBUG - Finished!

2019-09-15 15:59:59,314 - training_jobs - DEBUG - training trains/task_14.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'max_depth': 8,
 'model': 'XGBClassifier'}
2019-09-15 15:59:59,392 - training_jobs - DEBUG - training with: 
2019-09-15 15:59:59,392 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-15 15:59:59,392 - training_jobs - DEBUG - XGBClassifier
2019-09-15 15:59:59,392 - training_jobs - DEBUG - topo and code feats
2019-09-15 15:59:59,392 - training_jobs - DEBUG - baseline training
2019-09-15 15:59:59,486 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 16:00:50,765 - training_jobs - DEBUG - training time: 51s
2019-09-15 16:00:50,766 - training_jobs - DEBUG - saving to results/20190915_155959_baseline_topo_and_code_feats.json
2019-09-15 16:00:50,773 - training_jobs - DEBUG - moved jobdict to done_trainings/task_14.yml
2019-09-15 16:00:50,773 - training_jobs - DEBUG - Finished!

2019-09-15 16:00:53,655 - training_jobs - DEBUG - training trains/task_228.yml
{'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-15 16:00:53,755 - training_jobs - DEBUG - training with: 
2019-09-15 16:00:53,755 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-15 16:00:53,755 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 16:00:53,755 - training_jobs - DEBUG - topo and code feats
2019-09-15 16:00:53,755 - training_jobs - DEBUG - baseline training
2019-09-15 16:00:53,961 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 16:00:54,866 - training_jobs - DEBUG - training time: 1s
2019-09-15 16:00:54,866 - training_jobs - DEBUG - saving to results/20190915_160053_baseline_topo_and_code_feats.json
2019-09-15 16:00:55,157 - training_jobs - DEBUG - moved jobdict to done_trainings/task_228.yml
2019-09-15 16:00:55,157 - training_jobs - DEBUG - Finished!

2019-09-15 16:00:58,121 - training_jobs - DEBUG - training trains/task_238.yml
{'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-15 16:00:58,322 - training_jobs - DEBUG - training with: 
2019-09-15 16:00:58,322 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 16:00:58,322 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 16:00:58,322 - training_jobs - DEBUG - topo and code feats
2019-09-15 16:00:58,322 - training_jobs - DEBUG - baseline training
2019-09-15 16:00:58,414 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 16:00:59,604 - training_jobs - DEBUG - training time: 1s
2019-09-15 16:00:59,604 - training_jobs - DEBUG - saving to results/20190915_160058_baseline_topo_and_code_feats.json
2019-09-15 16:00:59,615 - training_jobs - DEBUG - moved jobdict to done_trainings/task_238.yml
2019-09-15 16:00:59,615 - training_jobs - DEBUG - Finished!

2019-09-15 16:01:02,504 - training_jobs - DEBUG - training trains/task_254.yml
{'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-15 16:01:02,546 - training_jobs - DEBUG - training with: 
2019-09-15 16:01:02,546 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-15 16:01:02,546 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 16:01:02,547 - training_jobs - DEBUG - topo and code feats
2019-09-15 16:01:02,547 - training_jobs - DEBUG - baseline training
2019-09-15 16:01:02,639 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 16:01:03,636 - training_jobs - DEBUG - training time: 1s
2019-09-15 16:01:03,636 - training_jobs - DEBUG - saving to results/20190915_160102_baseline_topo_and_code_feats.json
2019-09-15 16:01:03,644 - training_jobs - DEBUG - moved jobdict to done_trainings/task_254.yml
2019-09-15 16:01:03,645 - training_jobs - DEBUG - Finished!

2019-09-15 16:01:06,351 - training_jobs - DEBUG - training trains/task_23.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'max_depth': 150,
 'model': 'XGBClassifier'}
2019-09-15 16:01:06,580 - training_jobs - DEBUG - training with: 
2019-09-15 16:01:06,580 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 16:01:06,580 - training_jobs - DEBUG - XGBClassifier
2019-09-15 16:01:06,580 - training_jobs - DEBUG - topo and code feats
2019-09-15 16:01:06,580 - training_jobs - DEBUG - baseline training
2019-09-15 16:01:06,661 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 16:04:56,765 - training_jobs - DEBUG - training time: 230s
2019-09-15 16:04:56,765 - training_jobs - DEBUG - saving to results/20190915_160106_baseline_topo_and_code_feats.json
2019-09-15 16:04:56,773 - training_jobs - DEBUG - moved jobdict to done_trainings/task_23.yml
2019-09-15 16:04:56,773 - training_jobs - DEBUG - Finished!

2019-09-15 16:04:59,481 - training_jobs - DEBUG - training trains/task_257.yml
{'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'x_topo_feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-15 16:04:59,494 - training_jobs - DEBUG - training with: 
2019-09-15 16:04:59,495 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 16:04:59,495 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 16:04:59,495 - training_jobs - DEBUG - x_topo_feats
2019-09-15 16:04:59,495 - training_jobs - DEBUG - baseline training
2019-09-15 16:04:59,616 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 16:05:00,776 - training_jobs - DEBUG - training time: 1s
2019-09-15 16:05:00,777 - training_jobs - DEBUG - saving to results/20190915_160459_baseline_x_topo_feats.json
2019-09-15 16:05:00,789 - training_jobs - DEBUG - moved jobdict to done_trainings/task_257.yml
2019-09-15 16:05:00,789 - training_jobs - DEBUG - Finished!

2019-09-15 16:05:03,636 - training_jobs - DEBUG - training trains/task_30.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'max_depth': 8,
 'model': 'XGBClassifier'}
2019-09-15 16:05:03,649 - training_jobs - DEBUG - training with: 
2019-09-15 16:05:03,650 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-15 16:05:03,650 - training_jobs - DEBUG - XGBClassifier
2019-09-15 16:05:03,650 - training_jobs - DEBUG - topo and code feats
2019-09-15 16:05:03,650 - training_jobs - DEBUG - baseline training
2019-09-15 16:05:03,731 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 16:05:22,763 - training_jobs - DEBUG - training time: 19s
2019-09-15 16:05:22,763 - training_jobs - DEBUG - saving to results/20190915_160503_baseline_topo_and_code_feats.json
2019-09-15 16:05:22,769 - training_jobs - DEBUG - moved jobdict to done_trainings/task_30.yml
2019-09-15 16:05:22,769 - training_jobs - DEBUG - Finished!

2019-09-15 16:05:25,734 - training_jobs - DEBUG - training trains/task_169.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 16:05:25,783 - training_jobs - DEBUG - training with: 
2019-09-15 16:05:25,783 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-15 16:05:25,783 - training_jobs - DEBUG - GGNN1
2019-09-15 16:05:25,783 - training_jobs - DEBUG - 
2019-09-15 16:05:25,784 - training_jobs - DEBUG - ggnn training
2019-09-15 16:05:29,777 - training_jobs - DEBUG -  saving results to results/20190915_160529_ggnn_.json
2019-09-15 16:05:29,777 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 16:50:16,535 - training_jobs - DEBUG - test_multiple_models
2019-09-15 16:50:16,535 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-15 16:50:17,286 - training_jobs - DEBUG - training time: 2688s
2019-09-15 16:50:17,286 - training_jobs - DEBUG - saving to results/20190915_160529_ggnn_.json
2019-09-15 16:50:17,289 - training_jobs - DEBUG - moved jobdict to done_trainings/task_169.yml
2019-09-15 16:50:17,289 - training_jobs - DEBUG - Finished!

2019-09-15 16:50:20,185 - training_jobs - DEBUG - training trains/task_74.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 15,
 'd4': 10,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 16:50:20,264 - training_jobs - DEBUG - training with: 
2019-09-15 16:50:20,264 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-15 16:50:20,264 - training_jobs - DEBUG - GGNN5
2019-09-15 16:50:20,264 - training_jobs - DEBUG - 
2019-09-15 16:50:20,265 - training_jobs - DEBUG - moved jobdict to done_trainings/task_74.yml
2019-09-15 16:50:20,266 - training_jobs - DEBUG - Finished!

2019-09-15 16:50:23,226 - training_jobs - DEBUG - training trains/task_160.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 16:50:23,350 - training_jobs - DEBUG - training with: 
2019-09-15 16:50:23,350 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 16:50:23,350 - training_jobs - DEBUG - GGNN1
2019-09-15 16:50:23,350 - training_jobs - DEBUG - 
2019-09-15 16:50:23,350 - training_jobs - DEBUG - ggnn training
2019-09-15 16:50:25,771 - training_jobs - DEBUG -  saving results to results/20190915_165025_ggnn_.json
2019-09-15 16:50:25,771 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 16:50:27,304 - training_jobs - ERROR - Error with trains/task_160.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 16:50:30,114 - training_jobs - DEBUG - training trains/task_232.yml
{'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'x_topo_feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-15 16:50:30,382 - training_jobs - DEBUG - training with: 
2019-09-15 16:50:30,382 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 16:50:30,383 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 16:50:30,383 - training_jobs - DEBUG - x_topo_feats
2019-09-15 16:50:30,383 - training_jobs - DEBUG - baseline training
2019-09-15 16:50:30,442 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 16:50:31,459 - training_jobs - DEBUG - training time: 1s
2019-09-15 16:50:31,459 - training_jobs - DEBUG - saving to results/20190915_165030_baseline_x_topo_feats.json
2019-09-15 16:50:31,469 - training_jobs - DEBUG - moved jobdict to done_trainings/task_232.yml
2019-09-15 16:50:31,469 - training_jobs - DEBUG - Finished!

2019-09-15 16:50:34,245 - training_jobs - DEBUG - training trains/task_229.yml
{'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-15 16:50:34,276 - training_jobs - DEBUG - training with: 
2019-09-15 16:50:34,276 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-15 16:50:34,276 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 16:50:34,276 - training_jobs - DEBUG - topo and code feats
2019-09-15 16:50:34,276 - training_jobs - DEBUG - baseline training
2019-09-15 16:50:34,465 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 16:50:35,668 - training_jobs - DEBUG - training time: 1s
2019-09-15 16:50:35,668 - training_jobs - DEBUG - saving to results/20190915_165034_baseline_topo_and_code_feats.json
2019-09-15 16:50:35,683 - training_jobs - DEBUG - moved jobdict to done_trainings/task_229.yml
2019-09-15 16:50:35,683 - training_jobs - DEBUG - Finished!

2019-09-15 16:50:38,364 - training_jobs - DEBUG - training trains/task_76.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 15,
 'd4': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 16:50:39,158 - training_jobs - DEBUG - training with: 
2019-09-15 16:50:39,158 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 16:50:39,158 - training_jobs - DEBUG - GGNN5
2019-09-15 16:50:39,159 - training_jobs - DEBUG - 
2019-09-15 16:50:39,160 - training_jobs - DEBUG - moved jobdict to done_trainings/task_76.yml
2019-09-15 16:50:39,160 - training_jobs - DEBUG - Finished!

2019-09-15 16:50:41,820 - training_jobs - DEBUG - training trains/task_215.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'model': 'mlp4',
 'num_epochs': 100}
2019-09-15 16:50:41,841 - training_jobs - DEBUG - training with: 
2019-09-15 16:50:41,841 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 16:50:41,841 - training_jobs - DEBUG - mlp4
2019-09-15 16:50:41,841 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 16:50:41,842 - training_jobs - DEBUG - moved jobdict to done_trainings/task_215.yml
2019-09-15 16:50:41,842 - training_jobs - DEBUG - Finished!

2019-09-15 16:50:44,694 - training_jobs - DEBUG - training trains/task_86.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 16:50:44,981 - training_jobs - DEBUG - training with: 
2019-09-15 16:50:44,981 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-15 16:50:44,981 - training_jobs - DEBUG - GGNN6
2019-09-15 16:50:44,981 - training_jobs - DEBUG - 
2019-09-15 16:50:44,982 - training_jobs - DEBUG - moved jobdict to done_trainings/task_86.yml
2019-09-15 16:50:44,982 - training_jobs - DEBUG - Finished!

2019-09-15 16:50:47,755 - training_jobs - DEBUG - training trains/task_179.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 16:50:47,993 - training_jobs - DEBUG - training with: 
2019-09-15 16:50:47,993 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 16:50:47,993 - training_jobs - DEBUG - GGNN1
2019-09-15 16:50:47,993 - training_jobs - DEBUG - 
2019-09-15 16:50:47,993 - training_jobs - DEBUG - ggnn training
2019-09-15 16:50:52,967 - training_jobs - DEBUG -  saving results to results/20190915_165052_ggnn_.json
2019-09-15 16:50:52,967 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 17:45:46,506 - training_jobs - DEBUG - test_multiple_models
2019-09-15 17:45:46,507 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-15 17:45:48,296 - training_jobs - DEBUG - training time: 3295s
2019-09-15 17:45:48,296 - training_jobs - DEBUG - saving to results/20190915_165052_ggnn_.json
2019-09-15 17:45:48,298 - training_jobs - DEBUG - moved jobdict to done_trainings/task_179.yml
2019-09-15 17:45:48,298 - training_jobs - DEBUG - Finished!

2019-09-15 17:45:53,244 - training_jobs - DEBUG - training trains/task_203.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'model': 'mlp4',
 'num_epochs': 100}
2019-09-15 17:45:53,270 - training_jobs - DEBUG - training with: 
2019-09-15 17:45:53,270 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 17:45:53,270 - training_jobs - DEBUG - mlp4
2019-09-15 17:45:53,270 - training_jobs - DEBUG - topo and code feats
2019-09-15 17:45:53,271 - training_jobs - DEBUG - moved jobdict to done_trainings/task_203.yml
2019-09-15 17:45:53,271 - training_jobs - DEBUG - Finished!

2019-09-15 17:45:56,079 - training_jobs - DEBUG - training trains/task_87.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 17:45:56,118 - training_jobs - DEBUG - training with: 
2019-09-15 17:45:56,118 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-15 17:45:56,118 - training_jobs - DEBUG - GGNN6
2019-09-15 17:45:56,118 - training_jobs - DEBUG - 
2019-09-15 17:45:56,120 - training_jobs - DEBUG - moved jobdict to done_trainings/task_87.yml
2019-09-15 17:45:56,120 - training_jobs - DEBUG - Finished!

2019-09-15 17:45:59,089 - training_jobs - DEBUG - training trains/task_36.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max/',
 'features': 'x_topo_feats',
 'max_depth': 8,
 'model': 'XGBClassifier'}
2019-09-15 17:45:59,102 - training_jobs - DEBUG - training with: 
2019-09-15 17:45:59,103 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max/
2019-09-15 17:45:59,103 - training_jobs - DEBUG - XGBClassifier
2019-09-15 17:45:59,103 - training_jobs - DEBUG - x_topo_feats
2019-09-15 17:45:59,103 - training_jobs - DEBUG - baseline training
2019-09-15 17:45:59,931 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 17:48:17,709 - training_jobs - DEBUG - training time: 138s
2019-09-15 17:48:17,709 - training_jobs - DEBUG - saving to results/20190915_174559_baseline_x_topo_feats.json
2019-09-15 17:48:17,730 - training_jobs - DEBUG - moved jobdict to done_trainings/task_36.yml
2019-09-15 17:48:17,730 - training_jobs - DEBUG - Finished!

2019-09-15 17:48:20,206 - training_jobs - DEBUG - training trains/task_147.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 17:48:20,394 - training_jobs - DEBUG - training with: 
2019-09-15 17:48:20,394 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-15 17:48:20,394 - training_jobs - DEBUG - GGNN1
2019-09-15 17:48:20,394 - training_jobs - DEBUG - 
2019-09-15 17:48:20,394 - training_jobs - DEBUG - ggnn training
2019-09-15 17:48:23,046 - training_jobs - DEBUG -  saving results to results/20190915_174823_ggnn_.json
2019-09-15 17:48:23,047 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 18:59:00,742 - training_jobs - DEBUG - test_multiple_models
2019-09-15 18:59:00,742 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-15 18:59:02,216 - training_jobs - DEBUG - training time: 4239s
2019-09-15 18:59:02,216 - training_jobs - DEBUG - saving to results/20190915_174823_ggnn_.json
2019-09-15 18:59:02,218 - training_jobs - DEBUG - moved jobdict to done_trainings/task_147.yml
2019-09-15 18:59:02,218 - training_jobs - DEBUG - Finished!

2019-09-15 18:59:05,334 - training_jobs - DEBUG - training trains/task_62.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'max_depth': 8,
 'model': 'XGBClassifier'}
2019-09-15 18:59:05,370 - training_jobs - DEBUG - training with: 
2019-09-15 18:59:05,370 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-15 18:59:05,370 - training_jobs - DEBUG - XGBClassifier
2019-09-15 18:59:05,370 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 18:59:05,370 - training_jobs - DEBUG - nlp training
2019-09-15 18:59:06,340 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-15 19:10:36,583 - training_jobs - DEBUG - training time: 690s
2019-09-15 19:10:36,583 - training_jobs - DEBUG - saving to results/20190915_185906_nlp_tfidf_and_topo_feats.json
2019-09-15 19:10:36,589 - training_jobs - DEBUG - moved jobdict to done_trainings/task_62.yml
2019-09-15 19:10:36,589 - training_jobs - DEBUG - Finished!

2019-09-15 19:10:39,392 - training_jobs - DEBUG - training trains/task_113.yml
{'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-15 19:10:39,594 - training_jobs - DEBUG - training with: 
2019-09-15 19:10:39,594 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-15 19:10:39,594 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 19:10:39,594 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 19:10:39,594 - training_jobs - DEBUG - nlp training
2019-09-15 19:10:40,378 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-15 19:10:41,527 - training_jobs - DEBUG - training time: 1s
2019-09-15 19:10:41,527 - training_jobs - DEBUG - saving to results/20190915_191040_nlp_tfidf_and_topo_feats.json
2019-09-15 19:10:41,531 - training_jobs - DEBUG - moved jobdict to done_trainings/task_113.yml
2019-09-15 19:10:41,531 - training_jobs - DEBUG - Finished!

2019-09-15 19:10:44,578 - training_jobs - DEBUG - training trains/task_121.yml
{'d1': 85,
 'd2': 20,
 'd3': 15,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'model': 'mlp2',
 'num_epochs': 100}
2019-09-15 19:10:44,635 - training_jobs - DEBUG - training with: 
2019-09-15 19:10:44,635 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-15 19:10:44,635 - training_jobs - DEBUG - mlp2
2019-09-15 19:10:44,636 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 19:10:44,636 - training_jobs - DEBUG - nlp_nn training
2019-09-15 19:10:45,190 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-15 19:15:15,283 - training_jobs - DEBUG - training time: 270s
2019-09-15 19:15:15,347 - training_jobs - DEBUG - saving to results/20190915_191045_nlp_nn_tfidf_and_topo_feats.json
2019-09-15 19:15:15,409 - training_jobs - DEBUG - moved jobdict to done_trainings/task_121.yml
2019-09-15 19:15:15,409 - training_jobs - DEBUG - Finished!

2019-09-15 19:15:18,305 - training_jobs - DEBUG - training trains/task_92.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 19:15:18,424 - training_jobs - DEBUG - training with: 
2019-09-15 19:15:18,424 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-15 19:15:18,425 - training_jobs - DEBUG - GGNN6
2019-09-15 19:15:18,425 - training_jobs - DEBUG - 
2019-09-15 19:15:18,426 - training_jobs - DEBUG - moved jobdict to done_trainings/task_92.yml
2019-09-15 19:15:18,426 - training_jobs - DEBUG - Finished!

2019-09-15 19:15:20,755 - training_jobs - DEBUG - training trains/task_167.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 19:15:20,968 - training_jobs - DEBUG - training with: 
2019-09-15 19:15:20,968 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 19:15:20,968 - training_jobs - DEBUG - GGNN1
2019-09-15 19:15:20,968 - training_jobs - DEBUG - 
2019-09-15 19:15:20,968 - training_jobs - DEBUG - ggnn training
2019-09-15 19:15:28,207 - training_jobs - DEBUG -  saving results to results/20190915_191528_ggnn_.json
2019-09-15 19:15:28,207 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 19:47:32,773 - training_jobs - ERROR - Error with trains/task_167.yml
Traceback (most recent call last):
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 422, in get
    data = torch.load(filename)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/serialization.py", line 366, in load
    f = open(f, 'rb')
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1541, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1368, in select_best_model
    best_model_acc = final_model_train(best_model_acc, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1123, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 941, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 828, in train_model_GGNN
    for batch in loader:
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 618, in __next__
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 618, in <listcomp>
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 126, in __getitem__
    data = self.get(idx)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 438, in get
    raise IndexError("error getting: ", idx)
IndexError: ('error getting: ', 5870)
2019-09-15 19:48:32,906 - training_jobs - DEBUG - training trains/task_139.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 19:48:32,962 - training_jobs - DEBUG - training with: 
2019-09-15 19:48:32,962 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 19:48:32,962 - training_jobs - DEBUG - GGNN1
2019-09-15 19:48:32,962 - training_jobs - DEBUG - 
2019-09-15 19:48:32,962 - training_jobs - DEBUG - ggnn training
2019-09-15 19:48:42,663 - training_jobs - DEBUG -  saving results to results/20190915_194842_ggnn_.json
2019-09-15 19:48:42,663 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 19:48:45,302 - training_jobs - ERROR - Error with trains/task_139.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 19:48:48,584 - training_jobs - DEBUG - training trains/task_162.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 19:48:48,803 - training_jobs - DEBUG - training with: 
2019-09-15 19:48:48,803 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 19:48:48,803 - training_jobs - DEBUG - GGNN1
2019-09-15 19:48:48,803 - training_jobs - DEBUG - 
2019-09-15 19:48:48,803 - training_jobs - DEBUG - ggnn training
2019-09-15 19:48:51,729 - training_jobs - DEBUG -  saving results to results/20190915_194851_ggnn_.json
2019-09-15 19:48:51,730 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 19:48:54,417 - training_jobs - ERROR - Error with trains/task_162.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 19:48:57,248 - training_jobs - DEBUG - training trains/task_161.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 19:48:57,284 - training_jobs - DEBUG - training with: 
2019-09-15 19:48:57,285 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 19:48:57,285 - training_jobs - DEBUG - GGNN1
2019-09-15 19:48:57,285 - training_jobs - DEBUG - 
2019-09-15 19:48:57,285 - training_jobs - DEBUG - ggnn training
2019-09-15 19:48:58,863 - training_jobs - DEBUG -  saving results to results/20190915_194858_ggnn_.json
2019-09-15 19:48:58,863 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 19:49:00,325 - training_jobs - ERROR - Error with trains/task_161.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 19:49:02,917 - training_jobs - DEBUG - training trains/task_136.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 19:49:02,936 - training_jobs - DEBUG - training with: 
2019-09-15 19:49:02,937 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 19:49:02,937 - training_jobs - DEBUG - GGNN1
2019-09-15 19:49:02,937 - training_jobs - DEBUG - 
2019-09-15 19:49:02,937 - training_jobs - DEBUG - ggnn training
2019-09-15 19:49:04,372 - training_jobs - DEBUG -  saving results to results/20190915_194904_ggnn_.json
2019-09-15 19:49:04,372 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 19:49:05,774 - training_jobs - ERROR - Error with trains/task_136.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 19:49:08,294 - training_jobs - DEBUG - training trains/task_219.yml
{'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'x_topo_feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-15 19:49:08,352 - training_jobs - DEBUG - training with: 
2019-09-15 19:49:08,352 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-15 19:49:08,352 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 19:49:08,352 - training_jobs - DEBUG - x_topo_feats
2019-09-15 19:49:08,352 - training_jobs - DEBUG - baseline training
2019-09-15 19:49:08,409 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 19:49:09,889 - training_jobs - DEBUG - training time: 1s
2019-09-15 19:49:09,889 - training_jobs - DEBUG - saving to results/20190915_194908_baseline_x_topo_feats.json
2019-09-15 19:49:09,898 - training_jobs - DEBUG - moved jobdict to done_trainings/task_219.yml
2019-09-15 19:49:09,898 - training_jobs - DEBUG - Finished!

2019-09-15 19:49:12,693 - training_jobs - DEBUG - training trains/task_173.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 19:49:12,757 - training_jobs - DEBUG - training with: 
2019-09-15 19:49:12,757 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 19:49:12,758 - training_jobs - DEBUG - GGNN1
2019-09-15 19:49:12,758 - training_jobs - DEBUG - 
2019-09-15 19:49:12,758 - training_jobs - DEBUG - ggnn training
2019-09-15 19:49:14,202 - training_jobs - DEBUG -  saving results to results/20190915_194914_ggnn_.json
2019-09-15 19:49:14,202 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 19:49:15,591 - training_jobs - ERROR - Error with trains/task_173.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 19:49:18,345 - training_jobs - DEBUG - training trains/task_160.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 19:49:18,426 - training_jobs - DEBUG - training with: 
2019-09-15 19:49:18,427 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 19:49:18,427 - training_jobs - DEBUG - GGNN1
2019-09-15 19:49:18,427 - training_jobs - DEBUG - 
2019-09-15 19:49:18,427 - training_jobs - DEBUG - ggnn training
2019-09-15 19:49:19,839 - training_jobs - DEBUG -  saving results to results/20190915_194919_ggnn_.json
2019-09-15 19:49:19,839 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 19:49:21,182 - training_jobs - ERROR - Error with trains/task_160.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 19:49:23,835 - training_jobs - DEBUG - training trains/task_229.yml
{'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-15 19:49:23,847 - training_jobs - DEBUG - training with: 
2019-09-15 19:49:23,847 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-15 19:49:23,847 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 19:49:23,847 - training_jobs - DEBUG - topo and code feats
2019-09-15 19:49:23,847 - training_jobs - DEBUG - baseline training
2019-09-15 19:49:24,038 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 19:49:26,058 - training_jobs - DEBUG - training time: 2s
2019-09-15 19:49:26,059 - training_jobs - DEBUG - saving to results/20190915_194924_baseline_topo_and_code_feats.json
2019-09-15 19:49:26,096 - training_jobs - DEBUG - moved jobdict to done_trainings/task_229.yml
2019-09-15 19:49:26,096 - training_jobs - DEBUG - Finished!

2019-09-15 19:49:28,773 - training_jobs - DEBUG - training trains/task_76.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 15,
 'd4': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 19:49:28,786 - training_jobs - DEBUG - training with: 
2019-09-15 19:49:28,786 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 19:49:28,786 - training_jobs - DEBUG - GGNN5
2019-09-15 19:49:28,787 - training_jobs - DEBUG - 
2019-09-15 19:49:28,787 - training_jobs - DEBUG - ggnn training
2019-09-15 19:49:30,151 - training_jobs - DEBUG -  saving results to results/20190915_194930_ggnn_.json
2019-09-15 19:49:30,152 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 19:49:31,489 - training_jobs - ERROR - Error with trains/task_76.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 19:49:34,201 - training_jobs - DEBUG - training trains/task_92.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 19:49:34,215 - training_jobs - DEBUG - training with: 
2019-09-15 19:49:34,215 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-15 19:49:34,215 - training_jobs - DEBUG - GGNN6
2019-09-15 19:49:34,215 - training_jobs - DEBUG - 
2019-09-15 19:49:34,215 - training_jobs - DEBUG - ggnn training
2019-09-15 19:49:36,796 - training_jobs - DEBUG -  saving results to results/20190915_194936_ggnn_.json
2019-09-15 19:49:36,796 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 19:49:39,827 - training_jobs - ERROR - Error with trains/task_92.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1541, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1365, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1110, in final_model_train
    model = modelclass(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 242, in __init__
    super(GGNN3, self).__init__()
TypeError: super(type, obj): obj must be an instance or subtype of type
2019-09-15 19:49:42,520 - training_jobs - DEBUG - training trains/task_167.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 19:49:42,534 - training_jobs - DEBUG - training with: 
2019-09-15 19:49:42,534 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 19:49:42,534 - training_jobs - DEBUG - GGNN1
2019-09-15 19:49:42,534 - training_jobs - DEBUG - 
2019-09-15 19:49:42,534 - training_jobs - DEBUG - ggnn training
2019-09-15 19:49:44,783 - training_jobs - DEBUG -  saving results to results/20190915_194944_ggnn_.json
2019-09-15 19:49:44,784 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 20:41:28,090 - training_jobs - ERROR - Error with trains/task_167.yml
Traceback (most recent call last):
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/tarfile.py", line 189, in nti
    n = int(s.strip() or "0", 8)
ValueError: invalid literal for int() with base 8: 'ometric.'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/tarfile.py", line 2297, in next
    tarinfo = self.tarinfo.fromtarfile(self)
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/tarfile.py", line 1093, in fromtarfile
    obj = cls.frombuf(buf, tarfile.encoding, tarfile.errors)
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/tarfile.py", line 1035, in frombuf
    chksum = nti(buf[148:156])
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/tarfile.py", line 191, in nti
    raise InvalidHeaderError("invalid header")
tarfile.InvalidHeaderError: invalid header

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/serialization.py", line 524, in _load
    return legacy_load(f)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/serialization.py", line 448, in legacy_load
    with closing(tarfile.open(fileobj=f, mode='r:', format=tarfile.PAX_FORMAT)) as tar, \
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/tarfile.py", line 1589, in open
    return func(name, filemode, fileobj, **kwargs)
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/tarfile.py", line 1619, in taropen
    return cls(name, mode, fileobj, **kwargs)
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/tarfile.py", line 1482, in __init__
    self.firstmember = self.next()
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/tarfile.py", line 2309, in next
    raise ReadError(str(e))
tarfile.ReadError: invalid header

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 422, in get
    data = torch.load(filename)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/serialization.py", line 368, in load
    return _load(f, map_location, pickle_module)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/serialization.py", line 526, in _load
    if zipfile.is_zipfile(f):
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/zipfile.py", line 206, in is_zipfile
    result = _check_zipfile(fp=filename)
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/zipfile.py", line 192, in _check_zipfile
    if _EndRecData(fp):
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/zipfile.py", line 295, in _EndRecData
    fpin.seek(maxCommentStart, 0)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1541, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1376, in select_best_model
    best_model_macroF1  = final_model_train(best_model_macroF1 , train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1123, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 941, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 828, in train_model_GGNN
    for batch in loader:
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 618, in __next__
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 618, in <listcomp>
    batch = self.collate_fn([self.dataset[i] for i in indices])
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 126, in __getitem__
    data = self.get(idx)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 438, in get
    raise IndexError("error getting: ", idx)
IndexError: ('error getting: ', 7193)
2019-09-15 20:42:18,997 - training_jobs - DEBUG - training trains/task_139.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 20:42:19,018 - training_jobs - DEBUG - training with: 
2019-09-15 20:42:19,018 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 20:42:19,018 - training_jobs - DEBUG - GGNN1
2019-09-15 20:42:19,018 - training_jobs - DEBUG - 
2019-09-15 20:42:19,019 - training_jobs - DEBUG - ggnn training
2019-09-15 20:42:21,575 - training_jobs - DEBUG -  saving results to results/20190915_204221_ggnn_.json
2019-09-15 20:42:21,575 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 20:42:23,982 - training_jobs - ERROR - Error with trains/task_139.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 20:42:26,553 - training_jobs - DEBUG - training trains/task_162.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 20:42:26,565 - training_jobs - DEBUG - training with: 
2019-09-15 20:42:26,565 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 20:42:26,565 - training_jobs - DEBUG - GGNN1
2019-09-15 20:42:26,565 - training_jobs - DEBUG - 
2019-09-15 20:42:26,565 - training_jobs - DEBUG - ggnn training
2019-09-15 20:42:29,036 - training_jobs - DEBUG -  saving results to results/20190915_204229_ggnn_.json
2019-09-15 20:42:29,036 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 20:42:31,380 - training_jobs - ERROR - Error with trains/task_162.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 20:42:34,053 - training_jobs - DEBUG - training trains/task_161.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 20:42:34,066 - training_jobs - DEBUG - training with: 
2019-09-15 20:42:34,066 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 20:42:34,066 - training_jobs - DEBUG - GGNN1
2019-09-15 20:42:34,066 - training_jobs - DEBUG - 
2019-09-15 20:42:34,066 - training_jobs - DEBUG - ggnn training
2019-09-15 20:42:35,583 - training_jobs - DEBUG -  saving results to results/20190915_204235_ggnn_.json
2019-09-15 20:42:35,583 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 20:42:37,041 - training_jobs - ERROR - Error with trains/task_161.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 20:42:39,673 - training_jobs - DEBUG - training trains/task_136.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 20:42:39,687 - training_jobs - DEBUG - training with: 
2019-09-15 20:42:39,687 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 20:42:39,687 - training_jobs - DEBUG - GGNN1
2019-09-15 20:42:39,687 - training_jobs - DEBUG - 
2019-09-15 20:42:39,687 - training_jobs - DEBUG - ggnn training
2019-09-15 20:42:41,166 - training_jobs - DEBUG -  saving results to results/20190915_204241_ggnn_.json
2019-09-15 20:42:41,166 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 20:42:42,506 - training_jobs - ERROR - Error with trains/task_136.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 20:42:45,056 - training_jobs - DEBUG - training trains/task_173.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 20:42:45,069 - training_jobs - DEBUG - training with: 
2019-09-15 20:42:45,070 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 20:42:45,070 - training_jobs - DEBUG - GGNN1
2019-09-15 20:42:45,070 - training_jobs - DEBUG - 
2019-09-15 20:42:45,070 - training_jobs - DEBUG - ggnn training
2019-09-15 20:42:46,562 - training_jobs - DEBUG -  saving results to results/20190915_204246_ggnn_.json
2019-09-15 20:42:46,562 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 20:42:47,934 - training_jobs - ERROR - Error with trains/task_173.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 20:42:50,543 - training_jobs - DEBUG - training trains/task_160.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 20:42:50,556 - training_jobs - DEBUG - training with: 
2019-09-15 20:42:50,556 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 20:42:50,557 - training_jobs - DEBUG - GGNN1
2019-09-15 20:42:50,557 - training_jobs - DEBUG - 
2019-09-15 20:42:50,557 - training_jobs - DEBUG - ggnn training
2019-09-15 20:42:52,011 - training_jobs - DEBUG -  saving results to results/20190915_204252_ggnn_.json
2019-09-15 20:42:52,011 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 20:42:53,346 - training_jobs - ERROR - Error with trains/task_160.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 20:42:56,087 - training_jobs - DEBUG - training trains/task_76.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 15,
 'd4': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 20:42:56,100 - training_jobs - DEBUG - training with: 
2019-09-15 20:42:56,101 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 20:42:56,101 - training_jobs - DEBUG - GGNN5
2019-09-15 20:42:56,101 - training_jobs - DEBUG - 
2019-09-15 20:42:56,101 - training_jobs - DEBUG - ggnn training
2019-09-15 20:42:57,621 - training_jobs - DEBUG -  saving results to results/20190915_204257_ggnn_.json
2019-09-15 20:42:57,621 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 20:42:58,946 - training_jobs - ERROR - Error with trains/task_76.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 20:43:01,524 - training_jobs - DEBUG - training trains/task_92.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 20:43:01,537 - training_jobs - DEBUG - training with: 
2019-09-15 20:43:01,537 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-15 20:43:01,537 - training_jobs - DEBUG - GGNN6
2019-09-15 20:43:01,537 - training_jobs - DEBUG - 
2019-09-15 20:43:01,537 - training_jobs - DEBUG - ggnn training
2019-09-15 20:43:02,830 - training_jobs - DEBUG -  saving results to results/20190915_204302_ggnn_.json
2019-09-15 20:43:02,831 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 20:59:29,163 - training_jobs - DEBUG - test_multiple_models
2019-09-15 20:59:29,164 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-15 20:59:35,886 - training_jobs - DEBUG - training time: 993s
2019-09-15 20:59:35,887 - training_jobs - DEBUG - saving to results/20190915_204302_ggnn_.json
2019-09-15 20:59:35,889 - training_jobs - DEBUG - moved jobdict to done_trainings/task_92.yml
2019-09-15 20:59:35,889 - training_jobs - DEBUG - Finished!

2019-09-15 20:59:39,275 - training_jobs - DEBUG - training trains/task_167.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 20:59:39,288 - training_jobs - DEBUG - training with: 
2019-09-15 20:59:39,288 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 20:59:39,289 - training_jobs - DEBUG - GGNN1
2019-09-15 20:59:39,289 - training_jobs - DEBUG - 
2019-09-15 20:59:39,289 - training_jobs - DEBUG - ggnn training
2019-09-15 20:59:41,733 - training_jobs - DEBUG -  saving results to results/20190915_205941_ggnn_.json
2019-09-15 20:59:41,733 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 21:55:56,433 - training_jobs - DEBUG - test_multiple_models
2019-09-15 21:55:56,452 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-15 21:55:59,240 - training_jobs - DEBUG - training time: 3378s
2019-09-15 21:55:59,240 - training_jobs - DEBUG - saving to results/20190915_205941_ggnn_.json
2019-09-15 21:55:59,242 - training_jobs - DEBUG - moved jobdict to done_trainings/task_167.yml
2019-09-15 21:55:59,242 - training_jobs - DEBUG - Finished!

2019-09-15 21:56:03,299 - training_jobs - DEBUG - training trains/task_156.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 21:56:03,319 - training_jobs - DEBUG - training with: 
2019-09-15 21:56:03,319 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-15 21:56:03,319 - training_jobs - DEBUG - GGNN1
2019-09-15 21:56:03,320 - training_jobs - DEBUG - 
2019-09-15 21:56:03,320 - training_jobs - DEBUG - ggnn training
2019-09-15 21:56:09,345 - training_jobs - DEBUG -  saving results to results/20190915_215609_ggnn_.json
2019-09-15 21:56:09,345 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 22:19:19,458 - training_jobs - DEBUG - test_multiple_models
2019-09-15 22:19:19,458 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-15 22:19:21,885 - training_jobs - DEBUG - training time: 1393s
2019-09-15 22:19:21,885 - training_jobs - DEBUG - saving to results/20190915_215609_ggnn_.json
2019-09-15 22:19:21,888 - training_jobs - DEBUG - moved jobdict to done_trainings/task_156.yml
2019-09-15 22:19:21,888 - training_jobs - DEBUG - Finished!

2019-09-15 22:19:24,858 - training_jobs - DEBUG - training trains/task_265.yml
{'C': 1,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'max_iter': 100,
 'model': 'LogisticRegression',
 'multi_class': 'ovr',
 'penalty': 'l2',
 'solver': 'newton-cg'}
2019-09-15 22:19:24,898 - training_jobs - DEBUG - training with: 
2019-09-15 22:19:24,898 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-15 22:19:24,898 - training_jobs - DEBUG - LogisticRegression
2019-09-15 22:19:24,898 - training_jobs - DEBUG - topo and code feats
2019-09-15 22:19:24,898 - training_jobs - DEBUG - baseline training
2019-09-15 22:19:25,294 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 22:20:07,797 - training_jobs - DEBUG - training time: 43s
2019-09-15 22:20:07,797 - training_jobs - DEBUG - saving to results/20190915_221925_baseline_topo_and_code_feats.json
2019-09-15 22:20:07,814 - training_jobs - DEBUG - moved jobdict to done_trainings/task_265.yml
2019-09-15 22:20:07,814 - training_jobs - DEBUG - Finished!

2019-09-15 22:20:10,304 - training_jobs - DEBUG - training trains/task_101.yml
{'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-15 22:20:10,423 - training_jobs - DEBUG - training with: 
2019-09-15 22:20:10,424 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-15 22:20:10,424 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 22:20:10,424 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 22:20:10,424 - training_jobs - DEBUG - nlp training
2019-09-15 22:20:11,617 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-15 22:20:14,207 - training_jobs - DEBUG - training time: 3s
2019-09-15 22:20:14,207 - training_jobs - DEBUG - saving to results/20190915_222011_nlp_tfidf_and_topo_feats.json
2019-09-15 22:20:14,214 - training_jobs - DEBUG - moved jobdict to done_trainings/task_101.yml
2019-09-15 22:20:14,214 - training_jobs - DEBUG - Finished!

2019-09-15 22:20:16,977 - training_jobs - DEBUG - training trains/task_165.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 22:20:17,042 - training_jobs - DEBUG - training with: 
2019-09-15 22:20:17,043 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-15 22:20:17,043 - training_jobs - DEBUG - GGNN1
2019-09-15 22:20:17,043 - training_jobs - DEBUG - 
2019-09-15 22:20:17,043 - training_jobs - DEBUG - ggnn training
2019-09-15 22:20:18,453 - training_jobs - DEBUG -  saving results to results/20190915_222018_ggnn_.json
2019-09-15 22:20:18,453 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 22:53:25,169 - training_jobs - DEBUG - test_multiple_models
2019-09-15 22:53:25,169 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-15 22:53:25,735 - training_jobs - DEBUG - training time: 1987s
2019-09-15 22:53:25,735 - training_jobs - DEBUG - saving to results/20190915_222018_ggnn_.json
2019-09-15 22:53:25,737 - training_jobs - DEBUG - moved jobdict to done_trainings/task_165.yml
2019-09-15 22:53:25,737 - training_jobs - DEBUG - Finished!

2019-09-15 22:53:28,818 - training_jobs - DEBUG - training trains/task_32.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'x_topo_feats',
 'max_depth': 8,
 'model': 'XGBClassifier'}
2019-09-15 22:53:28,834 - training_jobs - DEBUG - training with: 
2019-09-15 22:53:28,834 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 22:53:28,834 - training_jobs - DEBUG - XGBClassifier
2019-09-15 22:53:28,834 - training_jobs - DEBUG - x_topo_feats
2019-09-15 22:53:28,834 - training_jobs - DEBUG - baseline training
2019-09-15 22:53:29,312 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 22:53:55,825 - training_jobs - DEBUG - training time: 27s
2019-09-15 22:53:55,825 - training_jobs - DEBUG - saving to results/20190915_225329_baseline_x_topo_feats.json
2019-09-15 22:53:55,834 - training_jobs - DEBUG - moved jobdict to done_trainings/task_32.yml
2019-09-15 22:53:55,835 - training_jobs - DEBUG - Finished!

2019-09-15 22:53:58,638 - training_jobs - DEBUG - training trains/task_63.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'max_depth': 150,
 'model': 'XGBClassifier'}
2019-09-15 22:53:58,873 - training_jobs - DEBUG - training with: 
2019-09-15 22:53:58,873 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-15 22:53:58,873 - training_jobs - DEBUG - XGBClassifier
2019-09-15 22:53:58,873 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 22:53:58,874 - training_jobs - DEBUG - nlp training
2019-09-15 22:53:59,628 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-15 23:16:23,215 - training_jobs - DEBUG - training time: 1344s
2019-09-15 23:16:23,215 - training_jobs - DEBUG - saving to results/20190915_225359_nlp_tfidf_and_topo_feats.json
2019-09-15 23:16:23,221 - training_jobs - DEBUG - moved jobdict to done_trainings/task_63.yml
2019-09-15 23:16:23,221 - training_jobs - DEBUG - Finished!

2019-09-15 23:16:25,792 - training_jobs - DEBUG - training trains/task_150.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 23:16:25,826 - training_jobs - DEBUG - training with: 
2019-09-15 23:16:25,826 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 23:16:25,826 - training_jobs - DEBUG - GGNN1
2019-09-15 23:16:25,826 - training_jobs - DEBUG - 
2019-09-15 23:16:25,826 - training_jobs - DEBUG - ggnn training
2019-09-15 23:16:28,939 - training_jobs - DEBUG -  saving results to results/20190915_231628_ggnn_.json
2019-09-15 23:16:28,939 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 23:16:31,396 - training_jobs - ERROR - Error with trains/task_150.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-15 23:16:34,208 - training_jobs - DEBUG - training trains/task_217.yml
{'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'x_topo_feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-15 23:16:34,256 - training_jobs - DEBUG - training with: 
2019-09-15 23:16:34,256 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-15 23:16:34,256 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 23:16:34,256 - training_jobs - DEBUG - x_topo_feats
2019-09-15 23:16:34,256 - training_jobs - DEBUG - baseline training
2019-09-15 23:16:34,312 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 23:16:35,280 - training_jobs - DEBUG - training time: 1s
2019-09-15 23:16:35,280 - training_jobs - DEBUG - saving to results/20190915_231634_baseline_x_topo_feats.json
2019-09-15 23:16:35,288 - training_jobs - DEBUG - moved jobdict to done_trainings/task_217.yml
2019-09-15 23:16:35,288 - training_jobs - DEBUG - Finished!

2019-09-15 23:16:37,786 - training_jobs - DEBUG - training trains/task_170.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 23:16:37,904 - training_jobs - DEBUG - training with: 
2019-09-15 23:16:37,905 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-15 23:16:37,905 - training_jobs - DEBUG - GGNN1
2019-09-15 23:16:37,905 - training_jobs - DEBUG - 
2019-09-15 23:16:37,905 - training_jobs - DEBUG - ggnn training
2019-09-15 23:16:43,526 - training_jobs - DEBUG -  saving results to results/20190915_231643_ggnn_.json
2019-09-15 23:16:43,526 - training_jobs - DEBUG -  calling modelSelection
2019-09-15 23:54:01,526 - training_jobs - DEBUG - test_multiple_models
2019-09-15 23:54:01,526 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-15 23:54:04,383 - training_jobs - DEBUG - training time: 2241s
2019-09-15 23:54:04,383 - training_jobs - DEBUG - saving to results/20190915_231643_ggnn_.json
2019-09-15 23:54:04,387 - training_jobs - DEBUG - moved jobdict to done_trainings/task_170.yml
2019-09-15 23:54:04,387 - training_jobs - DEBUG - Finished!

2019-09-15 23:54:07,863 - training_jobs - DEBUG - training trains/task_235.yml
{'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'x_topo_feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-15 23:54:07,888 - training_jobs - DEBUG - training with: 
2019-09-15 23:54:07,888 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 23:54:07,888 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 23:54:07,888 - training_jobs - DEBUG - x_topo_feats
2019-09-15 23:54:07,888 - training_jobs - DEBUG - baseline training
2019-09-15 23:54:08,204 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 23:54:11,510 - training_jobs - DEBUG - training time: 3s
2019-09-15 23:54:11,510 - training_jobs - DEBUG - saving to results/20190915_235408_baseline_x_topo_feats.json
2019-09-15 23:54:11,523 - training_jobs - DEBUG - moved jobdict to done_trainings/task_235.yml
2019-09-15 23:54:11,523 - training_jobs - DEBUG - Finished!

2019-09-15 23:54:14,709 - training_jobs - DEBUG - training trains/task_221.yml
{'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-15 23:54:14,741 - training_jobs - DEBUG - training with: 
2019-09-15 23:54:14,741 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-15 23:54:14,741 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 23:54:14,741 - training_jobs - DEBUG - topo and code feats
2019-09-15 23:54:14,741 - training_jobs - DEBUG - baseline training
2019-09-15 23:54:14,844 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 23:54:15,967 - training_jobs - DEBUG - training time: 1s
2019-09-15 23:54:15,968 - training_jobs - DEBUG - saving to results/20190915_235414_baseline_topo_and_code_feats.json
2019-09-15 23:54:15,996 - training_jobs - DEBUG - moved jobdict to done_trainings/task_221.yml
2019-09-15 23:54:15,996 - training_jobs - DEBUG - Finished!

2019-09-15 23:54:18,843 - training_jobs - DEBUG - training trains/task_234.yml
{'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'x_topo_feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-15 23:54:18,890 - training_jobs - DEBUG - training with: 
2019-09-15 23:54:18,890 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 23:54:18,890 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 23:54:18,890 - training_jobs - DEBUG - x_topo_feats
2019-09-15 23:54:18,890 - training_jobs - DEBUG - baseline training
2019-09-15 23:54:18,961 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 23:54:20,352 - training_jobs - DEBUG - training time: 1s
2019-09-15 23:54:20,352 - training_jobs - DEBUG - saving to results/20190915_235418_baseline_x_topo_feats.json
2019-09-15 23:54:20,373 - training_jobs - DEBUG - moved jobdict to done_trainings/task_234.yml
2019-09-15 23:54:20,373 - training_jobs - DEBUG - Finished!

2019-09-15 23:54:23,113 - training_jobs - DEBUG - training trains/task_208.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'model': 'mlp4',
 'num_epochs': 30}
2019-09-15 23:54:23,242 - training_jobs - DEBUG - training with: 
2019-09-15 23:54:23,242 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 23:54:23,242 - training_jobs - DEBUG - mlp4
2019-09-15 23:54:23,243 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 23:54:23,243 - training_jobs - DEBUG - nlp_nn training
2019-09-15 23:54:23,768 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-15 23:54:23,779 - training_jobs - ERROR - Error with trains/task_208.yml
Traceback (most recent call last):
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_nlp_models.py", line 955, in cv_train_nn_nlp_models_v2
    results_dict = train_nn_model_cv(X_train_embedding, y_train, X_test_embedding, y_test, param_set, scores, nclasses, numfolds=3 )
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1671, in train_nn_model_cv
    cv_fold_results_dict, _ =  train_nn_model_one_fold(X_train2, y_train, X_test2, y_test, nn_model_params, scores, nclasses )
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1488, in train_nn_model_one_fold
    **nn_model_params['model_kwargs']
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1167, in __init__
    super(mlp2, self).__init__()
TypeError: super(type, obj): obj must be an instance or subtype of type

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 448, in training_dispatcher
    results_dict = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_nlp_models.py", line 958, in cv_train_nn_nlp_models_v2
    print("Error with "+param_set+" and "+scores[0])
TypeError: must be str, not dict
2019-09-15 23:54:26,485 - training_jobs - DEBUG - training trains/task_105.yml
{'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-15 23:54:26,922 - training_jobs - DEBUG - training with: 
2019-09-15 23:54:26,922 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-15 23:54:26,922 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 23:54:26,922 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 23:54:26,923 - training_jobs - DEBUG - nlp training
2019-09-15 23:54:27,369 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-15 23:54:30,103 - training_jobs - DEBUG - training time: 3s
2019-09-15 23:54:30,103 - training_jobs - DEBUG - saving to results/20190915_235427_nlp_tfidf_and_topo_feats.json
2019-09-15 23:54:30,109 - training_jobs - DEBUG - moved jobdict to done_trainings/task_105.yml
2019-09-15 23:54:30,109 - training_jobs - DEBUG - Finished!

2019-09-15 23:54:32,791 - training_jobs - DEBUG - training trains/task_15.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'max_depth': 150,
 'model': 'XGBClassifier'}
2019-09-15 23:54:33,134 - training_jobs - DEBUG - training with: 
2019-09-15 23:54:33,134 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-15 23:54:33,134 - training_jobs - DEBUG - XGBClassifier
2019-09-15 23:54:33,134 - training_jobs - DEBUG - topo and code feats
2019-09-15 23:54:33,134 - training_jobs - DEBUG - baseline training
2019-09-15 23:54:33,220 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 23:56:30,865 - training_jobs - DEBUG - training time: 118s
2019-09-15 23:56:30,865 - training_jobs - DEBUG - saving to results/20190915_235433_baseline_topo_and_code_feats.json
2019-09-15 23:56:30,872 - training_jobs - DEBUG - moved jobdict to done_trainings/task_15.yml
2019-09-15 23:56:30,872 - training_jobs - DEBUG - Finished!

2019-09-15 23:56:34,184 - training_jobs - DEBUG - training trains/task_199.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'model': 'mlp4',
 'num_epochs': 100}
2019-09-15 23:56:34,222 - training_jobs - DEBUG - training with: 
2019-09-15 23:56:34,222 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 23:56:34,222 - training_jobs - DEBUG - mlp4
2019-09-15 23:56:34,222 - training_jobs - DEBUG - topo and code feats
2019-09-15 23:56:34,223 - training_jobs - DEBUG - moved jobdict to done_trainings/task_199.yml
2019-09-15 23:56:34,223 - training_jobs - DEBUG - Finished!

2019-09-15 23:56:37,501 - training_jobs - DEBUG - training trains/task_211.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'model': 'mlp4',
 'num_epochs': 100}
2019-09-15 23:56:37,516 - training_jobs - DEBUG - training with: 
2019-09-15 23:56:37,517 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 23:56:37,517 - training_jobs - DEBUG - mlp4
2019-09-15 23:56:37,517 - training_jobs - DEBUG - tfidf and topo feats
2019-09-15 23:56:37,517 - training_jobs - DEBUG - nlp_nn training
2019-09-15 23:56:39,497 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-15 23:56:39,517 - training_jobs - ERROR - Error with trains/task_211.yml
Traceback (most recent call last):
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_nlp_models.py", line 955, in cv_train_nn_nlp_models_v2
    results_dict = train_nn_model_cv(X_train_embedding, y_train, X_test_embedding, y_test, param_set, scores, nclasses, numfolds=3 )
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1671, in train_nn_model_cv
    cv_fold_results_dict, _ =  train_nn_model_one_fold(X_train2, y_train, X_test2, y_test, nn_model_params, scores, nclasses )
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1488, in train_nn_model_one_fold
    **nn_model_params['model_kwargs']
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1167, in __init__
    super(mlp2, self).__init__()
TypeError: super(type, obj): obj must be an instance or subtype of type

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 448, in training_dispatcher
    results_dict = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_nlp_models.py", line 958, in cv_train_nn_nlp_models_v2
    print("Error with "+param_set+" and "+scores[0])
TypeError: must be str, not dict
2019-09-15 23:56:42,806 - training_jobs - DEBUG - training trains/task_49.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'model': 'mlp3',
 'num_epochs': 100}
2019-09-15 23:56:42,824 - training_jobs - DEBUG - training with: 
2019-09-15 23:56:42,824 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-15 23:56:42,824 - training_jobs - DEBUG - mlp3
2019-09-15 23:56:42,825 - training_jobs - DEBUG - topo and code feats
2019-09-15 23:56:42,826 - training_jobs - DEBUG - moved jobdict to done_trainings/task_49.yml
2019-09-15 23:56:42,826 - training_jobs - DEBUG - Finished!

2019-09-15 23:56:45,976 - training_jobs - DEBUG - training trains/task_262.yml
{'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-15 23:56:46,102 - training_jobs - DEBUG - training with: 
2019-09-15 23:56:46,102 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 23:56:46,102 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 23:56:46,102 - training_jobs - DEBUG - topo and code feats
2019-09-15 23:56:46,102 - training_jobs - DEBUG - baseline training
2019-09-15 23:56:46,271 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 23:56:47,188 - training_jobs - DEBUG - training time: 1s
2019-09-15 23:56:47,188 - training_jobs - DEBUG - saving to results/20190915_235646_baseline_topo_and_code_feats.json
2019-09-15 23:56:47,199 - training_jobs - DEBUG - moved jobdict to done_trainings/task_262.yml
2019-09-15 23:56:47,199 - training_jobs - DEBUG - Finished!

2019-09-15 23:56:49,967 - training_jobs - DEBUG - training trains/task_240.yml
{'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'x_topo_feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-15 23:56:49,999 - training_jobs - DEBUG - training with: 
2019-09-15 23:56:49,999 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-15 23:56:49,999 - training_jobs - DEBUG - RandomForestClassifier
2019-09-15 23:56:49,999 - training_jobs - DEBUG - x_topo_feats
2019-09-15 23:56:49,999 - training_jobs - DEBUG - baseline training
2019-09-15 23:56:50,125 - training_jobs - DEBUG -  calling cv_train_models
2019-09-15 23:56:51,153 - training_jobs - DEBUG - training time: 1s
2019-09-15 23:56:51,153 - training_jobs - DEBUG - saving to results/20190915_235650_baseline_x_topo_feats.json
2019-09-15 23:56:51,169 - training_jobs - DEBUG - moved jobdict to done_trainings/task_240.yml
2019-09-15 23:56:51,169 - training_jobs - DEBUG - Finished!

2019-09-15 23:56:53,797 - training_jobs - DEBUG - training trains/task_166.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-15 23:56:53,822 - training_jobs - DEBUG - training with: 
2019-09-15 23:56:53,822 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-15 23:56:53,822 - training_jobs - DEBUG - GGNN1
2019-09-15 23:56:53,822 - training_jobs - DEBUG - 
2019-09-15 23:56:53,822 - training_jobs - DEBUG - ggnn training
2019-09-15 23:57:00,016 - training_jobs - DEBUG -  saving results to results/20190915_235700_ggnn_.json
2019-09-15 23:57:00,016 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 00:24:38,106 - training_jobs - DEBUG - test_multiple_models
2019-09-16 00:24:38,106 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-16 00:24:39,651 - training_jobs - DEBUG - training time: 1660s
2019-09-16 00:24:39,651 - training_jobs - DEBUG - saving to results/20190915_235700_ggnn_.json
2019-09-16 00:24:39,654 - training_jobs - DEBUG - moved jobdict to done_trainings/task_166.yml
2019-09-16 00:24:39,654 - training_jobs - DEBUG - Finished!

2019-09-16 00:24:42,999 - training_jobs - DEBUG - training trains/task_174.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 00:24:43,039 - training_jobs - DEBUG - training with: 
2019-09-16 00:24:43,039 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 00:24:43,039 - training_jobs - DEBUG - GGNN1
2019-09-16 00:24:43,039 - training_jobs - DEBUG - 
2019-09-16 00:24:43,039 - training_jobs - DEBUG - ggnn training
2019-09-16 00:24:46,131 - training_jobs - DEBUG -  saving results to results/20190916_002446_ggnn_.json
2019-09-16 00:24:46,131 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 00:24:48,591 - training_jobs - ERROR - Error with trains/task_174.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-16 00:24:51,577 - training_jobs - DEBUG - training trains/task_144.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 00:24:51,612 - training_jobs - DEBUG - training with: 
2019-09-16 00:24:51,612 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-16 00:24:51,613 - training_jobs - DEBUG - GGNN1
2019-09-16 00:24:51,613 - training_jobs - DEBUG - 
2019-09-16 00:24:51,613 - training_jobs - DEBUG - ggnn training
2019-09-16 00:24:53,497 - training_jobs - DEBUG -  saving results to results/20190916_002453_ggnn_.json
2019-09-16 00:24:53,497 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 00:46:18,437 - training_jobs - DEBUG - test_multiple_models
2019-09-16 00:46:18,437 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-16 00:46:19,113 - training_jobs - DEBUG - training time: 1286s
2019-09-16 00:46:19,113 - training_jobs - DEBUG - saving to results/20190916_002453_ggnn_.json
2019-09-16 00:46:19,115 - training_jobs - DEBUG - moved jobdict to done_trainings/task_144.yml
2019-09-16 00:46:19,115 - training_jobs - DEBUG - Finished!

2019-09-16 00:46:21,747 - training_jobs - DEBUG - training trains/task_71.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'max_depth': 150,
 'model': 'XGBClassifier'}
2019-09-16 00:46:21,779 - training_jobs - DEBUG - training with: 
2019-09-16 00:46:21,779 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-16 00:46:21,779 - training_jobs - DEBUG - XGBClassifier
2019-09-16 00:46:21,779 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 00:46:21,779 - training_jobs - DEBUG - nlp training
2019-09-16 00:46:22,508 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-16 00:54:59,225 - training_jobs - DEBUG - training time: 517s
2019-09-16 00:54:59,225 - training_jobs - DEBUG - saving to results/20190916_004622_nlp_tfidf_and_topo_feats.json
2019-09-16 00:54:59,230 - training_jobs - DEBUG - moved jobdict to done_trainings/task_71.yml
2019-09-16 00:54:59,230 - training_jobs - DEBUG - Finished!

2019-09-16 00:55:01,878 - training_jobs - DEBUG - training trains/task_110.yml
{'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-16 00:55:01,892 - training_jobs - DEBUG - training with: 
2019-09-16 00:55:01,892 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 00:55:01,892 - training_jobs - DEBUG - RandomForestClassifier
2019-09-16 00:55:01,892 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 00:55:01,892 - training_jobs - DEBUG - nlp training
2019-09-16 00:55:02,694 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-16 00:55:05,826 - training_jobs - DEBUG - training time: 3s
2019-09-16 00:55:05,826 - training_jobs - DEBUG - saving to results/20190916_005502_nlp_tfidf_and_topo_feats.json
2019-09-16 00:55:05,834 - training_jobs - DEBUG - moved jobdict to done_trainings/task_110.yml
2019-09-16 00:55:05,835 - training_jobs - DEBUG - Finished!

2019-09-16 00:55:08,538 - training_jobs - DEBUG - training trains/task_41.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'x_topo_feats',
 'max_depth': 150,
 'model': 'XGBClassifier'}
2019-09-16 00:55:08,552 - training_jobs - DEBUG - training with: 
2019-09-16 00:55:08,552 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 00:55:08,552 - training_jobs - DEBUG - XGBClassifier
2019-09-16 00:55:08,552 - training_jobs - DEBUG - x_topo_feats
2019-09-16 00:55:08,552 - training_jobs - DEBUG - baseline training
2019-09-16 00:55:08,608 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 00:58:23,831 - training_jobs - DEBUG - training time: 195s
2019-09-16 00:58:23,831 - training_jobs - DEBUG - saving to results/20190916_005508_baseline_x_topo_feats.json
2019-09-16 00:58:23,839 - training_jobs - DEBUG - moved jobdict to done_trainings/task_41.yml
2019-09-16 00:58:23,839 - training_jobs - DEBUG - Finished!

2019-09-16 00:58:26,534 - training_jobs - DEBUG - training trains/task_148.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 00:58:26,548 - training_jobs - DEBUG - training with: 
2019-09-16 00:58:26,548 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 00:58:26,548 - training_jobs - DEBUG - GGNN1
2019-09-16 00:58:26,548 - training_jobs - DEBUG - 
2019-09-16 00:58:26,548 - training_jobs - DEBUG - ggnn training
2019-09-16 00:58:28,502 - training_jobs - DEBUG -  saving results to results/20190916_005828_ggnn_.json
2019-09-16 00:58:28,502 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 00:58:29,935 - training_jobs - ERROR - Error with trains/task_148.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-16 00:58:32,457 - training_jobs - DEBUG - training trains/task_33.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'x_topo_feats',
 'max_depth': 150,
 'model': 'XGBClassifier'}
2019-09-16 00:58:32,470 - training_jobs - DEBUG - training with: 
2019-09-16 00:58:32,470 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-16 00:58:32,470 - training_jobs - DEBUG - XGBClassifier
2019-09-16 00:58:32,470 - training_jobs - DEBUG - x_topo_feats
2019-09-16 00:58:32,470 - training_jobs - DEBUG - baseline training
2019-09-16 00:58:32,585 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 01:00:06,014 - training_jobs - DEBUG - training time: 93s
2019-09-16 01:00:06,014 - training_jobs - DEBUG - saving to results/20190916_005832_baseline_x_topo_feats.json
2019-09-16 01:00:06,023 - training_jobs - DEBUG - moved jobdict to done_trainings/task_33.yml
2019-09-16 01:00:06,023 - training_jobs - DEBUG - Finished!

2019-09-16 01:00:09,344 - training_jobs - DEBUG - training trains/task_68.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'max_depth': 8,
 'model': 'XGBClassifier'}
2019-09-16 01:00:09,394 - training_jobs - DEBUG - training with: 
2019-09-16 01:00:09,395 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-16 01:00:09,395 - training_jobs - DEBUG - XGBClassifier
2019-09-16 01:00:09,395 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 01:00:09,395 - training_jobs - DEBUG - nlp training
2019-09-16 01:00:10,122 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-16 01:02:31,568 - training_jobs - DEBUG - training time: 141s
2019-09-16 01:02:31,568 - training_jobs - DEBUG - saving to results/20190916_010010_nlp_tfidf_and_topo_feats.json
2019-09-16 01:02:31,572 - training_jobs - DEBUG - moved jobdict to done_trainings/task_68.yml
2019-09-16 01:02:31,572 - training_jobs - DEBUG - Finished!

2019-09-16 01:02:34,353 - training_jobs - DEBUG - training trains/task_135.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 01:02:34,390 - training_jobs - DEBUG - training with: 
2019-09-16 01:02:34,390 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-16 01:02:34,390 - training_jobs - DEBUG - GGNN1
2019-09-16 01:02:34,390 - training_jobs - DEBUG - 
2019-09-16 01:02:34,390 - training_jobs - DEBUG - ggnn training
2019-09-16 01:02:37,797 - training_jobs - DEBUG -  saving results to results/20190916_010237_ggnn_.json
2019-09-16 01:02:37,797 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 02:13:06,306 - training_jobs - DEBUG - test_multiple_models
2019-09-16 02:13:06,306 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-16 02:13:07,578 - training_jobs - DEBUG - training time: 4230s
2019-09-16 02:13:07,578 - training_jobs - DEBUG - saving to results/20190916_010237_ggnn_.json
2019-09-16 02:13:07,580 - training_jobs - DEBUG - moved jobdict to done_trainings/task_135.yml
2019-09-16 02:13:07,580 - training_jobs - DEBUG - Finished!

2019-09-16 02:13:11,373 - training_jobs - DEBUG - training trains/task_12.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'x_topo_feats',
 'max_depth': 8,
 'model': 'XGBClassifier'}
2019-09-16 02:13:11,470 - training_jobs - DEBUG - training with: 
2019-09-16 02:13:11,470 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-16 02:13:11,470 - training_jobs - DEBUG - XGBClassifier
2019-09-16 02:13:11,470 - training_jobs - DEBUG - x_topo_feats
2019-09-16 02:13:11,470 - training_jobs - DEBUG - baseline training
2019-09-16 02:13:11,523 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 02:13:52,112 - training_jobs - DEBUG - training time: 41s
2019-09-16 02:13:52,112 - training_jobs - DEBUG - saving to results/20190916_021311_baseline_x_topo_feats.json
2019-09-16 02:13:52,119 - training_jobs - DEBUG - moved jobdict to done_trainings/task_12.yml
2019-09-16 02:13:52,119 - training_jobs - DEBUG - Finished!

2019-09-16 02:13:55,186 - training_jobs - DEBUG - training trains/task_189.yml
{'d1': 85,
 'd2': 20,
 'd3': 15,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'model': 'mlp2',
 'num_epochs': 100}
2019-09-16 02:13:55,395 - training_jobs - DEBUG - training with: 
2019-09-16 02:13:55,395 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-16 02:13:55,395 - training_jobs - DEBUG - mlp2
2019-09-16 02:13:55,395 - training_jobs - DEBUG - topo and code feats
2019-09-16 02:13:55,395 - training_jobs - DEBUG - baseline_nn training
2019-09-16 02:13:55,488 - training_jobs - DEBUG -  calling nn_train_models
2019-09-16 02:17:37,654 - training_jobs - DEBUG - training time: 222s
2019-09-16 02:17:37,655 - training_jobs - DEBUG - saving to results/20190916_021355_baseline_nn_topo_and_code_feats.json
2019-09-16 02:17:37,697 - training_jobs - DEBUG - moved jobdict to done_trainings/task_189.yml
2019-09-16 02:17:37,697 - training_jobs - DEBUG - Finished!

2019-09-16 02:17:41,920 - training_jobs - DEBUG - training trains/task_58.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'model': 'mlp3',
 'num_epochs': 30}
2019-09-16 02:17:41,941 - training_jobs - DEBUG - training with: 
2019-09-16 02:17:41,941 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-16 02:17:41,941 - training_jobs - DEBUG - mlp3
2019-09-16 02:17:41,942 - training_jobs - DEBUG - topo and code feats
2019-09-16 02:17:41,942 - training_jobs - DEBUG - moved jobdict to done_trainings/task_58.yml
2019-09-16 02:17:41,943 - training_jobs - DEBUG - Finished!

2019-09-16 02:17:44,300 - training_jobs - DEBUG - training trains/task_212.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'model': 'mlp4',
 'num_epochs': 30}
2019-09-16 02:17:44,335 - training_jobs - DEBUG - training with: 
2019-09-16 02:17:44,335 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-16 02:17:44,336 - training_jobs - DEBUG - mlp4
2019-09-16 02:17:44,336 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 02:17:44,336 - training_jobs - DEBUG - nlp_nn training
2019-09-16 02:17:44,675 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-16 02:17:44,687 - training_jobs - ERROR - Error with trains/task_212.yml
Traceback (most recent call last):
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_nlp_models.py", line 955, in cv_train_nn_nlp_models_v2
    results_dict = train_nn_model_cv(X_train_embedding, y_train, X_test_embedding, y_test, param_set, scores, nclasses, numfolds=3 )
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1671, in train_nn_model_cv
    cv_fold_results_dict, _ =  train_nn_model_one_fold(X_train2, y_train, X_test2, y_test, nn_model_params, scores, nclasses )
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1488, in train_nn_model_one_fold
    **nn_model_params['model_kwargs']
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1167, in __init__
    super(mlp2, self).__init__()
TypeError: super(type, obj): obj must be an instance or subtype of type

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 448, in training_dispatcher
    results_dict = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_nlp_models.py", line 958, in cv_train_nn_nlp_models_v2
    print("Error with "+param_set+" and "+scores[0])
TypeError: must be str, not dict
2019-09-16 02:17:47,142 - training_jobs - DEBUG - training trains/task_99.yml
{'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-16 02:17:47,450 - training_jobs - DEBUG - training with: 
2019-09-16 02:17:47,451 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-16 02:17:47,451 - training_jobs - DEBUG - RandomForestClassifier
2019-09-16 02:17:47,451 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 02:17:47,451 - training_jobs - DEBUG - nlp training
2019-09-16 02:17:47,914 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-16 02:17:50,119 - training_jobs - DEBUG - training time: 2s
2019-09-16 02:17:50,119 - training_jobs - DEBUG - saving to results/20190916_021747_nlp_tfidf_and_topo_feats.json
2019-09-16 02:17:50,123 - training_jobs - DEBUG - moved jobdict to done_trainings/task_99.yml
2019-09-16 02:17:50,123 - training_jobs - DEBUG - Finished!

2019-09-16 02:17:52,491 - training_jobs - DEBUG - training trains/task_94.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 02:17:52,504 - training_jobs - DEBUG - training with: 
2019-09-16 02:17:52,504 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-16 02:17:52,504 - training_jobs - DEBUG - GGNN6
2019-09-16 02:17:52,504 - training_jobs - DEBUG - 
2019-09-16 02:17:52,504 - training_jobs - DEBUG - ggnn training
2019-09-16 02:17:57,008 - training_jobs - DEBUG -  saving results to results/20190916_021757_ggnn_.json
2019-09-16 02:17:57,008 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 02:45:31,274 - training_jobs - DEBUG - test_multiple_models
2019-09-16 02:45:31,275 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-16 02:45:32,101 - training_jobs - DEBUG - training time: 1655s
2019-09-16 02:45:32,101 - training_jobs - DEBUG - saving to results/20190916_021757_ggnn_.json
2019-09-16 02:45:32,103 - training_jobs - DEBUG - moved jobdict to done_trainings/task_94.yml
2019-09-16 02:45:32,103 - training_jobs - DEBUG - Finished!

2019-09-16 02:45:34,571 - training_jobs - DEBUG - training trains/task_278.yml
{'C': 1,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'max_iter': 100,
 'model': 'LogisticRegression',
 'multi_class': 'ovr',
 'penalty': 'l2',
 'solver': 'newton-cg'}
2019-09-16 02:45:34,609 - training_jobs - DEBUG - training with: 
2019-09-16 02:45:34,609 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 02:45:34,609 - training_jobs - DEBUG - LogisticRegression
2019-09-16 02:45:34,609 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 02:45:34,609 - training_jobs - DEBUG - nlp training
2019-09-16 02:45:35,015 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-16 02:46:46,247 - training_jobs - DEBUG - training time: 71s
2019-09-16 02:46:46,247 - training_jobs - DEBUG - saving to results/20190916_024535_nlp_tfidf_and_topo_feats.json
2019-09-16 02:46:46,253 - training_jobs - DEBUG - moved jobdict to done_trainings/task_278.yml
2019-09-16 02:46:46,253 - training_jobs - DEBUG - Finished!

2019-09-16 02:46:48,538 - training_jobs - DEBUG - training trains/task_224.yml
{'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'x_topo_feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-16 02:46:48,549 - training_jobs - DEBUG - training with: 
2019-09-16 02:46:48,549 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-16 02:46:48,549 - training_jobs - DEBUG - RandomForestClassifier
2019-09-16 02:46:48,549 - training_jobs - DEBUG - x_topo_feats
2019-09-16 02:46:48,549 - training_jobs - DEBUG - baseline training
2019-09-16 02:46:48,674 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 02:46:49,488 - training_jobs - DEBUG - training time: 1s
2019-09-16 02:46:49,488 - training_jobs - DEBUG - saving to results/20190916_024648_baseline_x_topo_feats.json
2019-09-16 02:46:49,501 - training_jobs - DEBUG - moved jobdict to done_trainings/task_224.yml
2019-09-16 02:46:49,501 - training_jobs - DEBUG - Finished!

2019-09-16 02:46:52,109 - training_jobs - DEBUG - training trains/task_275.yml
{'C': 1,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'max_iter': 100,
 'model': 'LogisticRegression',
 'multi_class': 'ovr',
 'penalty': 'l2',
 'solver': 'newton-cg'}
2019-09-16 02:46:52,122 - training_jobs - DEBUG - training with: 
2019-09-16 02:46:52,122 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-16 02:46:52,122 - training_jobs - DEBUG - LogisticRegression
2019-09-16 02:46:52,122 - training_jobs - DEBUG - topo and code feats
2019-09-16 02:46:52,122 - training_jobs - DEBUG - baseline training
2019-09-16 02:46:52,273 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 02:47:14,942 - training_jobs - DEBUG - training time: 23s
2019-09-16 02:47:14,943 - training_jobs - DEBUG - saving to results/20190916_024652_baseline_topo_and_code_feats.json
2019-09-16 02:47:14,963 - training_jobs - DEBUG - moved jobdict to done_trainings/task_275.yml
2019-09-16 02:47:14,964 - training_jobs - DEBUG - Finished!

2019-09-16 02:47:17,513 - training_jobs - DEBUG - training trains/task_112.yml
{'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-16 02:47:17,544 - training_jobs - DEBUG - training with: 
2019-09-16 02:47:17,544 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-16 02:47:17,544 - training_jobs - DEBUG - RandomForestClassifier
2019-09-16 02:47:17,544 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 02:47:17,544 - training_jobs - DEBUG - nlp training
2019-09-16 02:47:17,903 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-16 02:47:18,259 - training_jobs - DEBUG - training time: 0s
2019-09-16 02:47:18,259 - training_jobs - DEBUG - saving to results/20190916_024717_nlp_tfidf_and_topo_feats.json
2019-09-16 02:47:18,263 - training_jobs - DEBUG - moved jobdict to done_trainings/task_112.yml
2019-09-16 02:47:18,263 - training_jobs - DEBUG - Finished!

2019-09-16 02:47:21,123 - training_jobs - DEBUG - training trains/task_80.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 15,
 'd4': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 02:47:21,138 - training_jobs - DEBUG - training with: 
2019-09-16 02:47:21,138 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-16 02:47:21,138 - training_jobs - DEBUG - GGNN5
2019-09-16 02:47:21,138 - training_jobs - DEBUG - 
2019-09-16 02:47:21,138 - training_jobs - DEBUG - ggnn training
2019-09-16 02:47:22,555 - training_jobs - DEBUG -  saving results to results/20190916_024722_ggnn_.json
2019-09-16 02:47:22,555 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 03:03:25,160 - training_jobs - DEBUG - test_multiple_models
2019-09-16 03:03:25,160 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-16 03:03:25,648 - training_jobs - DEBUG - training time: 963s
2019-09-16 03:03:25,648 - training_jobs - DEBUG - saving to results/20190916_024722_ggnn_.json
2019-09-16 03:03:25,650 - training_jobs - DEBUG - moved jobdict to done_trainings/task_80.yml
2019-09-16 03:03:25,650 - training_jobs - DEBUG - Finished!

2019-09-16 03:03:28,032 - training_jobs - DEBUG - training trains/task_247.yml
{'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-16 03:03:28,067 - training_jobs - DEBUG - training with: 
2019-09-16 03:03:28,067 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 03:03:28,067 - training_jobs - DEBUG - RandomForestClassifier
2019-09-16 03:03:28,067 - training_jobs - DEBUG - topo and code feats
2019-09-16 03:03:28,068 - training_jobs - DEBUG - baseline training
2019-09-16 03:03:28,235 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 03:03:31,794 - training_jobs - DEBUG - training time: 4s
2019-09-16 03:03:31,794 - training_jobs - DEBUG - saving to results/20190916_030328_baseline_topo_and_code_feats.json
2019-09-16 03:03:31,808 - training_jobs - DEBUG - moved jobdict to done_trainings/task_247.yml
2019-09-16 03:03:31,808 - training_jobs - DEBUG - Finished!

2019-09-16 03:03:34,351 - training_jobs - DEBUG - training trains/task_176.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 03:03:34,385 - training_jobs - DEBUG - training with: 
2019-09-16 03:03:34,385 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-16 03:03:34,385 - training_jobs - DEBUG - GGNN1
2019-09-16 03:03:34,386 - training_jobs - DEBUG - 
2019-09-16 03:03:34,386 - training_jobs - DEBUG - ggnn training
2019-09-16 03:03:35,582 - training_jobs - DEBUG -  saving results to results/20190916_030335_ggnn_.json
2019-09-16 03:03:35,582 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 03:20:17,660 - training_jobs - DEBUG - test_multiple_models
2019-09-16 03:20:17,660 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-16 03:20:18,167 - training_jobs - DEBUG - training time: 1003s
2019-09-16 03:20:18,167 - training_jobs - DEBUG - saving to results/20190916_030335_ggnn_.json
2019-09-16 03:20:18,169 - training_jobs - DEBUG - moved jobdict to done_trainings/task_176.yml
2019-09-16 03:20:18,169 - training_jobs - DEBUG - Finished!

2019-09-16 03:20:20,768 - training_jobs - DEBUG - training trains/task_111.yml
{'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-16 03:20:20,811 - training_jobs - DEBUG - training with: 
2019-09-16 03:20:20,811 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 03:20:20,811 - training_jobs - DEBUG - RandomForestClassifier
2019-09-16 03:20:20,811 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 03:20:20,811 - training_jobs - DEBUG - nlp training
2019-09-16 03:20:21,527 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-16 03:20:32,259 - training_jobs - DEBUG - training time: 11s
2019-09-16 03:20:32,259 - training_jobs - DEBUG - saving to results/20190916_032021_nlp_tfidf_and_topo_feats.json
2019-09-16 03:20:32,267 - training_jobs - DEBUG - moved jobdict to done_trainings/task_111.yml
2019-09-16 03:20:32,267 - training_jobs - DEBUG - Finished!

2019-09-16 03:20:34,724 - training_jobs - DEBUG - training trains/task_143.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 03:20:34,738 - training_jobs - DEBUG - training with: 
2019-09-16 03:20:34,738 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-16 03:20:34,738 - training_jobs - DEBUG - GGNN1
2019-09-16 03:20:34,738 - training_jobs - DEBUG - 
2019-09-16 03:20:34,738 - training_jobs - DEBUG - ggnn training
2019-09-16 03:20:37,046 - training_jobs - DEBUG -  saving results to results/20190916_032037_ggnn_.json
2019-09-16 03:20:37,046 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 04:12:25,196 - training_jobs - DEBUG - test_multiple_models
2019-09-16 04:12:25,196 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-16 04:12:26,102 - training_jobs - DEBUG - training time: 3109s
2019-09-16 04:12:26,103 - training_jobs - DEBUG - saving to results/20190916_032037_ggnn_.json
2019-09-16 04:12:26,104 - training_jobs - DEBUG - moved jobdict to done_trainings/task_143.yml
2019-09-16 04:12:26,104 - training_jobs - DEBUG - Finished!

2019-09-16 04:12:28,842 - training_jobs - DEBUG - training trains/task_196.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'model': 'mlp4',
 'num_epochs': 30}
2019-09-16 04:12:28,856 - training_jobs - DEBUG - training with: 
2019-09-16 04:12:28,856 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 04:12:28,856 - training_jobs - DEBUG - mlp4
2019-09-16 04:12:28,856 - training_jobs - DEBUG - topo and code feats
2019-09-16 04:12:28,857 - training_jobs - DEBUG - moved jobdict to done_trainings/task_196.yml
2019-09-16 04:12:28,857 - training_jobs - DEBUG - Finished!

2019-09-16 04:12:31,768 - training_jobs - DEBUG - training trains/task_114.yml
{'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-16 04:12:31,782 - training_jobs - DEBUG - training with: 
2019-09-16 04:12:31,782 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-16 04:12:31,782 - training_jobs - DEBUG - RandomForestClassifier
2019-09-16 04:12:31,782 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 04:12:31,782 - training_jobs - DEBUG - nlp training
2019-09-16 04:12:32,164 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-16 04:12:32,689 - training_jobs - DEBUG - training time: 1s
2019-09-16 04:12:32,689 - training_jobs - DEBUG - saving to results/20190916_041232_nlp_tfidf_and_topo_feats.json
2019-09-16 04:12:32,693 - training_jobs - DEBUG - moved jobdict to done_trainings/task_114.yml
2019-09-16 04:12:32,693 - training_jobs - DEBUG - Finished!

2019-09-16 04:12:35,421 - training_jobs - DEBUG - training trains/task_237.yml
{'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-16 04:12:35,463 - training_jobs - DEBUG - training with: 
2019-09-16 04:12:35,463 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 04:12:35,463 - training_jobs - DEBUG - RandomForestClassifier
2019-09-16 04:12:35,463 - training_jobs - DEBUG - topo and code feats
2019-09-16 04:12:35,463 - training_jobs - DEBUG - baseline training
2019-09-16 04:12:35,556 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 04:12:37,030 - training_jobs - DEBUG - training time: 1s
2019-09-16 04:12:37,030 - training_jobs - DEBUG - saving to results/20190916_041235_baseline_topo_and_code_feats.json
2019-09-16 04:12:37,041 - training_jobs - DEBUG - moved jobdict to done_trainings/task_237.yml
2019-09-16 04:12:37,041 - training_jobs - DEBUG - Finished!

2019-09-16 04:12:39,943 - training_jobs - DEBUG - training trains/task_128.yml
{'d1': 85,
 'd2': 20,
 'd3': 15,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'model': 'mlp2',
 'num_epochs': 30}
2019-09-16 04:12:39,966 - training_jobs - DEBUG - training with: 
2019-09-16 04:12:39,966 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-16 04:12:39,967 - training_jobs - DEBUG - mlp2
2019-09-16 04:12:39,967 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 04:12:39,967 - training_jobs - DEBUG - nlp_nn training
2019-09-16 04:12:40,367 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-16 04:13:50,214 - training_jobs - DEBUG - training time: 70s
2019-09-16 04:13:50,214 - training_jobs - DEBUG - saving to results/20190916_041240_nlp_nn_tfidf_and_topo_feats.json
2019-09-16 04:13:50,218 - training_jobs - DEBUG - moved jobdict to done_trainings/task_128.yml
2019-09-16 04:13:50,218 - training_jobs - DEBUG - Finished!

2019-09-16 04:13:52,705 - training_jobs - DEBUG - training trains/task_206.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'model': 'mlp4',
 'num_epochs': 30}
2019-09-16 04:13:52,923 - training_jobs - DEBUG - training with: 
2019-09-16 04:13:52,924 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-16 04:13:52,924 - training_jobs - DEBUG - mlp4
2019-09-16 04:13:52,924 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 04:13:52,924 - training_jobs - DEBUG - nlp_nn training
2019-09-16 04:13:53,588 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-16 04:13:53,603 - training_jobs - ERROR - Error with trains/task_206.yml
Traceback (most recent call last):
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_nlp_models.py", line 955, in cv_train_nn_nlp_models_v2
    results_dict = train_nn_model_cv(X_train_embedding, y_train, X_test_embedding, y_test, param_set, scores, nclasses, numfolds=3 )
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1671, in train_nn_model_cv
    cv_fold_results_dict, _ =  train_nn_model_one_fold(X_train2, y_train, X_test2, y_test, nn_model_params, scores, nclasses )
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1488, in train_nn_model_one_fold
    **nn_model_params['model_kwargs']
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1167, in __init__
    super(mlp2, self).__init__()
TypeError: super(type, obj): obj must be an instance or subtype of type

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 448, in training_dispatcher
    results_dict = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_nlp_models.py", line 958, in cv_train_nn_nlp_models_v2
    print("Error with "+param_set+" and "+scores[0])
TypeError: must be str, not dict
2019-09-16 04:13:56,058 - training_jobs - DEBUG - training trains/task_163.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 04:13:56,078 - training_jobs - DEBUG - training with: 
2019-09-16 04:13:56,078 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 04:13:56,078 - training_jobs - DEBUG - GGNN1
2019-09-16 04:13:56,078 - training_jobs - DEBUG - 
2019-09-16 04:13:56,078 - training_jobs - DEBUG - ggnn training
2019-09-16 04:14:11,112 - training_jobs - DEBUG -  saving results to results/20190916_041411_ggnn_.json
2019-09-16 04:14:11,112 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 04:14:13,446 - training_jobs - ERROR - Error with trains/task_163.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-16 04:14:16,607 - training_jobs - DEBUG - training trains/task_226.yml
{'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'x_topo_feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-16 04:14:16,635 - training_jobs - DEBUG - training with: 
2019-09-16 04:14:16,635 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-16 04:14:16,635 - training_jobs - DEBUG - RandomForestClassifier
2019-09-16 04:14:16,635 - training_jobs - DEBUG - x_topo_feats
2019-09-16 04:14:16,636 - training_jobs - DEBUG - baseline training
2019-09-16 04:14:16,778 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 04:14:17,859 - training_jobs - DEBUG - training time: 1s
2019-09-16 04:14:17,859 - training_jobs - DEBUG - saving to results/20190916_041416_baseline_x_topo_feats.json
2019-09-16 04:14:17,873 - training_jobs - DEBUG - moved jobdict to done_trainings/task_226.yml
2019-09-16 04:14:17,873 - training_jobs - DEBUG - Finished!

2019-09-16 04:14:20,727 - training_jobs - DEBUG - training trains/task_181.yml
{'d1': 85,
 'd2': 20,
 'd3': 15,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'model': 'mlp2',
 'num_epochs': 100}
2019-09-16 04:14:20,818 - training_jobs - DEBUG - training with: 
2019-09-16 04:14:20,818 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-16 04:14:20,818 - training_jobs - DEBUG - mlp2
2019-09-16 04:14:20,819 - training_jobs - DEBUG - topo and code feats
2019-09-16 04:14:20,819 - training_jobs - DEBUG - baseline_nn training
2019-09-16 04:14:20,923 - training_jobs - DEBUG -  calling nn_train_models
2019-09-16 04:18:46,780 - training_jobs - DEBUG - training time: 266s
2019-09-16 04:18:46,795 - training_jobs - DEBUG - saving to results/20190916_041420_baseline_nn_topo_and_code_feats.json
2019-09-16 04:18:46,802 - training_jobs - DEBUG - moved jobdict to done_trainings/task_181.yml
2019-09-16 04:18:46,802 - training_jobs - DEBUG - Finished!

2019-09-16 04:18:49,417 - training_jobs - DEBUG - training trains/task_10.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'model': 'mlp3',
 'num_epochs': 30}
2019-09-16 04:18:49,429 - training_jobs - DEBUG - training with: 
2019-09-16 04:18:49,429 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-16 04:18:49,429 - training_jobs - DEBUG - mlp3
2019-09-16 04:18:49,429 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 04:18:49,429 - training_jobs - DEBUG - nlp_nn training
2019-09-16 04:18:50,550 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-16 04:18:50,562 - training_jobs - ERROR - Error with trains/task_10.yml
Traceback (most recent call last):
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_nlp_models.py", line 955, in cv_train_nn_nlp_models_v2
    results_dict = train_nn_model_cv(X_train_embedding, y_train, X_test_embedding, y_test, param_set, scores, nclasses, numfolds=3 )
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1671, in train_nn_model_cv
    cv_fold_results_dict, _ =  train_nn_model_one_fold(X_train2, y_train, X_test2, y_test, nn_model_params, scores, nclasses )
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1488, in train_nn_model_one_fold
    **nn_model_params['model_kwargs']
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1147, in __init__
    super(mlp2, self).__init__()
TypeError: super(type, obj): obj must be an instance or subtype of type

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 448, in training_dispatcher
    results_dict = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_nlp_models.py", line 958, in cv_train_nn_nlp_models_v2
    print("Error with "+param_set+" and "+scores[0])
TypeError: must be str, not dict
2019-09-16 04:18:52,961 - training_jobs - DEBUG - training trains/task_124.yml
{'d1': 85,
 'd2': 20,
 'd3': 15,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'model': 'mlp2',
 'num_epochs': 30}
2019-09-16 04:18:52,972 - training_jobs - DEBUG - training with: 
2019-09-16 04:18:52,972 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 04:18:52,972 - training_jobs - DEBUG - mlp2
2019-09-16 04:18:52,972 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 04:18:52,973 - training_jobs - DEBUG - nlp_nn training
2019-09-16 04:18:53,443 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-16 04:20:13,449 - training_jobs - DEBUG - training time: 80s
2019-09-16 04:20:13,450 - training_jobs - DEBUG - saving to results/20190916_041853_nlp_nn_tfidf_and_topo_feats.json
2019-09-16 04:20:13,455 - training_jobs - DEBUG - moved jobdict to done_trainings/task_124.yml
2019-09-16 04:20:13,455 - training_jobs - DEBUG - Finished!

2019-09-16 04:20:15,846 - training_jobs - DEBUG - training trains/task_276.yml
{'C': 1,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'max_iter': 100,
 'model': 'LogisticRegression',
 'multi_class': 'ovr',
 'penalty': 'l2',
 'solver': 'newton-cg'}
2019-09-16 04:20:15,878 - training_jobs - DEBUG - training with: 
2019-09-16 04:20:15,878 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-16 04:20:15,878 - training_jobs - DEBUG - LogisticRegression
2019-09-16 04:20:15,878 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 04:20:15,878 - training_jobs - DEBUG - nlp training
2019-09-16 04:20:16,951 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-16 04:20:37,944 - training_jobs - DEBUG - training time: 21s
2019-09-16 04:20:37,944 - training_jobs - DEBUG - saving to results/20190916_042016_nlp_tfidf_and_topo_feats.json
2019-09-16 04:20:37,948 - training_jobs - DEBUG - moved jobdict to done_trainings/task_276.yml
2019-09-16 04:20:37,948 - training_jobs - DEBUG - Finished!

2019-09-16 04:20:40,294 - training_jobs - DEBUG - training trains/task_258.yml
{'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'x_topo_feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-16 04:20:40,323 - training_jobs - DEBUG - training with: 
2019-09-16 04:20:40,323 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-16 04:20:40,323 - training_jobs - DEBUG - RandomForestClassifier
2019-09-16 04:20:40,323 - training_jobs - DEBUG - x_topo_feats
2019-09-16 04:20:40,323 - training_jobs - DEBUG - baseline training
2019-09-16 04:20:40,438 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 04:20:41,337 - training_jobs - DEBUG - training time: 1s
2019-09-16 04:20:41,337 - training_jobs - DEBUG - saving to results/20190916_042040_baseline_x_topo_feats.json
2019-09-16 04:20:41,348 - training_jobs - DEBUG - moved jobdict to done_trainings/task_258.yml
2019-09-16 04:20:41,348 - training_jobs - DEBUG - Finished!

2019-09-16 04:20:44,048 - training_jobs - DEBUG - training trains/task_141.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 04:20:44,062 - training_jobs - DEBUG - training with: 
2019-09-16 04:20:44,062 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-16 04:20:44,062 - training_jobs - DEBUG - GGNN1
2019-09-16 04:20:44,062 - training_jobs - DEBUG - 
2019-09-16 04:20:44,062 - training_jobs - DEBUG - ggnn training
2019-09-16 04:20:46,545 - training_jobs - DEBUG -  saving results to results/20190916_042046_ggnn_.json
2019-09-16 04:20:46,545 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 04:51:46,934 - training_jobs - DEBUG - test_multiple_models
2019-09-16 04:51:46,934 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-16 04:51:51,323 - training_jobs - DEBUG - training time: 1865s
2019-09-16 04:51:51,323 - training_jobs - DEBUG - saving to results/20190916_042046_ggnn_.json
2019-09-16 04:51:51,325 - training_jobs - DEBUG - moved jobdict to done_trainings/task_141.yml
2019-09-16 04:51:51,325 - training_jobs - DEBUG - Finished!

2019-09-16 04:51:54,287 - training_jobs - DEBUG - training trains/task_85.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 04:51:54,392 - training_jobs - DEBUG - training with: 
2019-09-16 04:51:54,392 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-16 04:51:54,392 - training_jobs - DEBUG - GGNN6
2019-09-16 04:51:54,392 - training_jobs - DEBUG - 
2019-09-16 04:51:54,392 - training_jobs - DEBUG - ggnn training
2019-09-16 04:51:58,411 - training_jobs - DEBUG -  saving results to results/20190916_045158_ggnn_.json
2019-09-16 04:51:58,411 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 05:37:00,027 - training_jobs - DEBUG - test_multiple_models
2019-09-16 05:37:00,027 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-16 05:37:00,731 - training_jobs - DEBUG - training time: 2702s
2019-09-16 05:37:00,731 - training_jobs - DEBUG - saving to results/20190916_045158_ggnn_.json
2019-09-16 05:37:00,733 - training_jobs - DEBUG - moved jobdict to done_trainings/task_85.yml
2019-09-16 05:37:00,733 - training_jobs - DEBUG - Finished!

2019-09-16 05:37:03,256 - training_jobs - DEBUG - training trains/task_168.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 05:37:03,302 - training_jobs - DEBUG - training with: 
2019-09-16 05:37:03,302 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-16 05:37:03,302 - training_jobs - DEBUG - GGNN1
2019-09-16 05:37:03,302 - training_jobs - DEBUG - 
2019-09-16 05:37:03,302 - training_jobs - DEBUG - ggnn training
2019-09-16 05:37:05,162 - training_jobs - DEBUG -  saving results to results/20190916_053705_ggnn_.json
2019-09-16 05:37:05,162 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 05:59:46,180 - training_jobs - DEBUG - test_multiple_models
2019-09-16 05:59:46,180 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-16 05:59:46,907 - training_jobs - DEBUG - training time: 1362s
2019-09-16 05:59:46,907 - training_jobs - DEBUG - saving to results/20190916_053705_ggnn_.json
2019-09-16 05:59:46,909 - training_jobs - DEBUG - moved jobdict to done_trainings/task_168.yml
2019-09-16 05:59:46,909 - training_jobs - DEBUG - Finished!

2019-09-16 05:59:49,773 - training_jobs - DEBUG - training trains/task_96.yml
{'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-16 05:59:49,805 - training_jobs - DEBUG - training with: 
2019-09-16 05:59:49,806 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-16 05:59:49,806 - training_jobs - DEBUG - RandomForestClassifier
2019-09-16 05:59:49,806 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 05:59:49,806 - training_jobs - DEBUG - nlp training
2019-09-16 05:59:50,306 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-16 05:59:50,820 - training_jobs - DEBUG - training time: 1s
2019-09-16 05:59:50,820 - training_jobs - DEBUG - saving to results/20190916_055950_nlp_tfidf_and_topo_feats.json
2019-09-16 05:59:50,825 - training_jobs - DEBUG - moved jobdict to done_trainings/task_96.yml
2019-09-16 05:59:50,825 - training_jobs - DEBUG - Finished!

2019-09-16 05:59:53,962 - training_jobs - DEBUG - training trains/task_9.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'model': 'mlp3',
 'num_epochs': 100}
2019-09-16 05:59:54,042 - training_jobs - DEBUG - training with: 
2019-09-16 05:59:54,042 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-16 05:59:54,042 - training_jobs - DEBUG - mlp3
2019-09-16 05:59:54,042 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 05:59:54,042 - training_jobs - DEBUG - nlp_nn training
2019-09-16 05:59:54,519 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-16 05:59:54,533 - training_jobs - ERROR - Error with trains/task_9.yml
Traceback (most recent call last):
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_nlp_models.py", line 955, in cv_train_nn_nlp_models_v2
    results_dict = train_nn_model_cv(X_train_embedding, y_train, X_test_embedding, y_test, param_set, scores, nclasses, numfolds=3 )
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1671, in train_nn_model_cv
    cv_fold_results_dict, _ =  train_nn_model_one_fold(X_train2, y_train, X_test2, y_test, nn_model_params, scores, nclasses )
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1488, in train_nn_model_one_fold
    **nn_model_params['model_kwargs']
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1147, in __init__
    super(mlp2, self).__init__()
TypeError: super(type, obj): obj must be an instance or subtype of type

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 448, in training_dispatcher
    results_dict = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_nlp_models.py", line 958, in cv_train_nn_nlp_models_v2
    print("Error with "+param_set+" and "+scores[0])
TypeError: must be str, not dict
2019-09-16 05:59:57,218 - training_jobs - DEBUG - training trains/task_218.yml
{'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'x_topo_feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-16 05:59:57,248 - training_jobs - DEBUG - training with: 
2019-09-16 05:59:57,248 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-16 05:59:57,248 - training_jobs - DEBUG - RandomForestClassifier
2019-09-16 05:59:57,248 - training_jobs - DEBUG - x_topo_feats
2019-09-16 05:59:57,248 - training_jobs - DEBUG - baseline training
2019-09-16 05:59:57,306 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 05:59:58,370 - training_jobs - DEBUG - training time: 1s
2019-09-16 05:59:58,370 - training_jobs - DEBUG - saving to results/20190916_055957_baseline_x_topo_feats.json
2019-09-16 05:59:58,390 - training_jobs - DEBUG - moved jobdict to done_trainings/task_218.yml
2019-09-16 05:59:58,390 - training_jobs - DEBUG - Finished!

2019-09-16 06:00:01,209 - training_jobs - DEBUG - training trains/task_249.yml
{'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'x_topo_feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-16 06:00:01,222 - training_jobs - DEBUG - training with: 
2019-09-16 06:00:01,222 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-16 06:00:01,222 - training_jobs - DEBUG - RandomForestClassifier
2019-09-16 06:00:01,222 - training_jobs - DEBUG - x_topo_feats
2019-09-16 06:00:01,222 - training_jobs - DEBUG - baseline training
2019-09-16 06:00:01,268 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 06:00:02,181 - training_jobs - DEBUG - training time: 1s
2019-09-16 06:00:02,181 - training_jobs - DEBUG - saving to results/20190916_060001_baseline_x_topo_feats.json
2019-09-16 06:00:02,189 - training_jobs - DEBUG - moved jobdict to done_trainings/task_249.yml
2019-09-16 06:00:02,189 - training_jobs - DEBUG - Finished!

2019-09-16 06:00:04,672 - training_jobs - DEBUG - training trains/task_117.yml
{'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-16 06:00:04,704 - training_jobs - DEBUG - training with: 
2019-09-16 06:00:04,704 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-16 06:00:04,704 - training_jobs - DEBUG - RandomForestClassifier
2019-09-16 06:00:04,704 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 06:00:04,704 - training_jobs - DEBUG - nlp training
2019-09-16 06:00:05,317 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-16 06:00:06,930 - training_jobs - DEBUG - training time: 2s
2019-09-16 06:00:06,930 - training_jobs - DEBUG - saving to results/20190916_060005_nlp_tfidf_and_topo_feats.json
2019-09-16 06:00:06,936 - training_jobs - DEBUG - moved jobdict to done_trainings/task_117.yml
2019-09-16 06:00:06,936 - training_jobs - DEBUG - Finished!

2019-09-16 06:00:09,587 - training_jobs - DEBUG - training trains/task_53.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'model': 'mlp3',
 'num_epochs': 100}
2019-09-16 06:00:09,599 - training_jobs - DEBUG - training with: 
2019-09-16 06:00:09,599 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 06:00:09,599 - training_jobs - DEBUG - mlp3
2019-09-16 06:00:09,599 - training_jobs - DEBUG - topo and code feats
2019-09-16 06:00:09,600 - training_jobs - DEBUG - moved jobdict to done_trainings/task_53.yml
2019-09-16 06:00:09,600 - training_jobs - DEBUG - Finished!

2019-09-16 06:00:12,167 - training_jobs - DEBUG - training trains/task_34.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'max_depth': 8,
 'model': 'XGBClassifier'}
2019-09-16 06:00:12,181 - training_jobs - DEBUG - training with: 
2019-09-16 06:00:12,181 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-16 06:00:12,181 - training_jobs - DEBUG - XGBClassifier
2019-09-16 06:00:12,181 - training_jobs - DEBUG - topo and code feats
2019-09-16 06:00:12,181 - training_jobs - DEBUG - baseline training
2019-09-16 06:00:12,357 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 06:00:42,423 - training_jobs - DEBUG - training time: 30s
2019-09-16 06:00:42,423 - training_jobs - DEBUG - saving to results/20190916_060012_baseline_topo_and_code_feats.json
2019-09-16 06:00:42,432 - training_jobs - DEBUG - moved jobdict to done_trainings/task_34.yml
2019-09-16 06:00:42,432 - training_jobs - DEBUG - Finished!

2019-09-16 06:00:45,088 - training_jobs - DEBUG - training trains/task_131.yml
{'d1': 85,
 'd2': 20,
 'd3': 15,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'model': 'mlp2',
 'num_epochs': 100}
2019-09-16 06:00:45,122 - training_jobs - DEBUG - training with: 
2019-09-16 06:00:45,123 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-16 06:00:45,123 - training_jobs - DEBUG - mlp2
2019-09-16 06:00:45,123 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 06:00:45,123 - training_jobs - DEBUG - nlp_nn training
2019-09-16 06:00:45,736 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-16 06:06:50,277 - training_jobs - DEBUG - training time: 365s
2019-09-16 06:06:50,277 - training_jobs - DEBUG - saving to results/20190916_060045_nlp_nn_tfidf_and_topo_feats.json
2019-09-16 06:06:50,282 - training_jobs - DEBUG - moved jobdict to done_trainings/task_131.yml
2019-09-16 06:06:50,282 - training_jobs - DEBUG - Finished!

2019-09-16 06:06:52,715 - training_jobs - DEBUG - training trains/task_158.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 06:06:52,755 - training_jobs - DEBUG - training with: 
2019-09-16 06:06:52,755 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-16 06:06:52,755 - training_jobs - DEBUG - GGNN1
2019-09-16 06:06:52,755 - training_jobs - DEBUG - 
2019-09-16 06:06:52,755 - training_jobs - DEBUG - ggnn training
2019-09-16 06:06:58,599 - training_jobs - DEBUG -  saving results to results/20190916_060658_ggnn_.json
2019-09-16 06:06:58,599 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 06:44:36,602 - training_jobs - DEBUG - test_multiple_models
2019-09-16 06:44:36,602 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-16 06:44:38,233 - training_jobs - DEBUG - training time: 2260s
2019-09-16 06:44:38,233 - training_jobs - DEBUG - saving to results/20190916_060658_ggnn_.json
2019-09-16 06:44:38,236 - training_jobs - DEBUG - moved jobdict to done_trainings/task_158.yml
2019-09-16 06:44:38,236 - training_jobs - DEBUG - Finished!

2019-09-16 06:44:41,654 - training_jobs - DEBUG - training trains/task_8.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'model': 'mlp3',
 'num_epochs': 30}
2019-09-16 06:44:41,702 - training_jobs - DEBUG - training with: 
2019-09-16 06:44:41,702 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-16 06:44:41,702 - training_jobs - DEBUG - mlp3
2019-09-16 06:44:41,702 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 06:44:41,702 - training_jobs - DEBUG - nlp_nn training
2019-09-16 06:44:42,202 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-16 06:44:42,217 - training_jobs - ERROR - Error with trains/task_8.yml
Traceback (most recent call last):
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_nlp_models.py", line 955, in cv_train_nn_nlp_models_v2
    results_dict = train_nn_model_cv(X_train_embedding, y_train, X_test_embedding, y_test, param_set, scores, nclasses, numfolds=3 )
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1671, in train_nn_model_cv
    cv_fold_results_dict, _ =  train_nn_model_one_fold(X_train2, y_train, X_test2, y_test, nn_model_params, scores, nclasses )
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1488, in train_nn_model_one_fold
    **nn_model_params['model_kwargs']
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1147, in __init__
    super(mlp2, self).__init__()
TypeError: super(type, obj): obj must be an instance or subtype of type

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 448, in training_dispatcher
    results_dict = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_nlp_models.py", line 958, in cv_train_nn_nlp_models_v2
    print("Error with "+param_set+" and "+scores[0])
TypeError: must be str, not dict
2019-09-16 06:44:45,339 - training_jobs - DEBUG - training trains/task_201.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'model': 'mlp4',
 'num_epochs': 100}
2019-09-16 06:44:45,373 - training_jobs - DEBUG - training with: 
2019-09-16 06:44:45,373 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-16 06:44:45,373 - training_jobs - DEBUG - mlp4
2019-09-16 06:44:45,373 - training_jobs - DEBUG - topo and code feats
2019-09-16 06:44:45,374 - training_jobs - DEBUG - moved jobdict to done_trainings/task_201.yml
2019-09-16 06:44:45,374 - training_jobs - DEBUG - Finished!

2019-09-16 06:44:48,517 - training_jobs - DEBUG - training trains/task_52.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'model': 'mlp3',
 'num_epochs': 30}
2019-09-16 06:44:48,531 - training_jobs - DEBUG - training with: 
2019-09-16 06:44:48,531 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 06:44:48,531 - training_jobs - DEBUG - mlp3
2019-09-16 06:44:48,531 - training_jobs - DEBUG - topo and code feats
2019-09-16 06:44:48,532 - training_jobs - DEBUG - moved jobdict to done_trainings/task_52.yml
2019-09-16 06:44:48,532 - training_jobs - DEBUG - Finished!

2019-09-16 06:44:51,599 - training_jobs - DEBUG - training trains/task_210.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'model': 'mlp4',
 'num_epochs': 30}
2019-09-16 06:44:51,632 - training_jobs - DEBUG - training with: 
2019-09-16 06:44:51,632 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 06:44:51,632 - training_jobs - DEBUG - mlp4
2019-09-16 06:44:51,632 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 06:44:51,632 - training_jobs - DEBUG - nlp_nn training
2019-09-16 06:44:52,418 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-16 06:44:52,434 - training_jobs - ERROR - Error with trains/task_210.yml
Traceback (most recent call last):
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_nlp_models.py", line 955, in cv_train_nn_nlp_models_v2
    results_dict = train_nn_model_cv(X_train_embedding, y_train, X_test_embedding, y_test, param_set, scores, nclasses, numfolds=3 )
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1671, in train_nn_model_cv
    cv_fold_results_dict, _ =  train_nn_model_one_fold(X_train2, y_train, X_test2, y_test, nn_model_params, scores, nclasses )
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1488, in train_nn_model_one_fold
    **nn_model_params['model_kwargs']
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1167, in __init__
    super(mlp2, self).__init__()
TypeError: super(type, obj): obj must be an instance or subtype of type

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 448, in training_dispatcher
    results_dict = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_nlp_models.py", line 958, in cv_train_nn_nlp_models_v2
    print("Error with "+param_set+" and "+scores[0])
TypeError: must be str, not dict
2019-09-16 06:44:55,126 - training_jobs - DEBUG - training trains/task_39.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max/',
 'features': 'topo and code feats',
 'max_depth': 150,
 'model': 'XGBClassifier'}
2019-09-16 06:44:55,138 - training_jobs - DEBUG - training with: 
2019-09-16 06:44:55,138 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max/
2019-09-16 06:44:55,138 - training_jobs - DEBUG - XGBClassifier
2019-09-16 06:44:55,138 - training_jobs - DEBUG - topo and code feats
2019-09-16 06:44:55,138 - training_jobs - DEBUG - baseline training
2019-09-16 06:44:56,076 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 06:54:56,843 - training_jobs - DEBUG - training time: 601s
2019-09-16 06:54:56,843 - training_jobs - DEBUG - saving to results/20190916_064456_baseline_topo_and_code_feats.json
2019-09-16 06:54:56,867 - training_jobs - DEBUG - moved jobdict to done_trainings/task_39.yml
2019-09-16 06:54:56,868 - training_jobs - DEBUG - Finished!

2019-09-16 06:54:59,577 - training_jobs - DEBUG - training trains/task_214.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'model': 'mlp4',
 'num_epochs': 30}
2019-09-16 06:54:59,592 - training_jobs - DEBUG - training with: 
2019-09-16 06:54:59,592 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-16 06:54:59,592 - training_jobs - DEBUG - mlp4
2019-09-16 06:54:59,592 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 06:54:59,592 - training_jobs - DEBUG - nlp_nn training
2019-09-16 06:55:00,236 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-16 06:55:00,250 - training_jobs - ERROR - Error with trains/task_214.yml
Traceback (most recent call last):
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_nlp_models.py", line 955, in cv_train_nn_nlp_models_v2
    results_dict = train_nn_model_cv(X_train_embedding, y_train, X_test_embedding, y_test, param_set, scores, nclasses, numfolds=3 )
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1671, in train_nn_model_cv
    cv_fold_results_dict, _ =  train_nn_model_one_fold(X_train2, y_train, X_test2, y_test, nn_model_params, scores, nclasses )
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1488, in train_nn_model_one_fold
    **nn_model_params['model_kwargs']
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1167, in __init__
    super(mlp2, self).__init__()
TypeError: super(type, obj): obj must be an instance or subtype of type

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 448, in training_dispatcher
    results_dict = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_nlp_models.py", line 958, in cv_train_nn_nlp_models_v2
    print("Error with "+param_set+" and "+scores[0])
TypeError: must be str, not dict
2019-09-16 06:55:03,044 - training_jobs - DEBUG - training trains/task_273.yml
{'C': 1,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'max_iter': 100,
 'model': 'LogisticRegression',
 'multi_class': 'ovr',
 'penalty': 'l2',
 'solver': 'newton-cg'}
2019-09-16 06:55:03,057 - training_jobs - DEBUG - training with: 
2019-09-16 06:55:03,057 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-16 06:55:03,057 - training_jobs - DEBUG - LogisticRegression
2019-09-16 06:55:03,057 - training_jobs - DEBUG - topo and code feats
2019-09-16 06:55:03,057 - training_jobs - DEBUG - baseline training
2019-09-16 06:55:03,135 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 06:55:18,628 - training_jobs - DEBUG - training time: 15s
2019-09-16 06:55:18,628 - training_jobs - DEBUG - saving to results/20190916_065503_baseline_topo_and_code_feats.json
2019-09-16 06:55:18,646 - training_jobs - DEBUG - moved jobdict to done_trainings/task_273.yml
2019-09-16 06:55:18,646 - training_jobs - DEBUG - Finished!

2019-09-16 06:55:21,032 - training_jobs - DEBUG - training trains/task_204.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'model': 'mlp4',
 'num_epochs': 30}
2019-09-16 06:55:21,237 - training_jobs - DEBUG - training with: 
2019-09-16 06:55:21,237 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-16 06:55:21,237 - training_jobs - DEBUG - mlp4
2019-09-16 06:55:21,237 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 06:55:21,237 - training_jobs - DEBUG - nlp_nn training
2019-09-16 06:55:21,676 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-16 06:55:21,686 - training_jobs - ERROR - Error with trains/task_204.yml
Traceback (most recent call last):
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_nlp_models.py", line 955, in cv_train_nn_nlp_models_v2
    results_dict = train_nn_model_cv(X_train_embedding, y_train, X_test_embedding, y_test, param_set, scores, nclasses, numfolds=3 )
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1671, in train_nn_model_cv
    cv_fold_results_dict, _ =  train_nn_model_one_fold(X_train2, y_train, X_test2, y_test, nn_model_params, scores, nclasses )
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1488, in train_nn_model_one_fold
    **nn_model_params['model_kwargs']
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1167, in __init__
    super(mlp2, self).__init__()
TypeError: super(type, obj): obj must be an instance or subtype of type

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 448, in training_dispatcher
    results_dict = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_nlp_models.py", line 958, in cv_train_nn_nlp_models_v2
    print("Error with "+param_set+" and "+scores[0])
TypeError: must be str, not dict
2019-09-16 06:55:24,033 - training_jobs - DEBUG - training trains/task_50.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'model': 'mlp3',
 'num_epochs': 30}
2019-09-16 06:55:24,157 - training_jobs - DEBUG - training with: 
2019-09-16 06:55:24,157 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-16 06:55:24,157 - training_jobs - DEBUG - mlp3
2019-09-16 06:55:24,157 - training_jobs - DEBUG - topo and code feats
2019-09-16 06:55:24,158 - training_jobs - DEBUG - moved jobdict to done_trainings/task_50.yml
2019-09-16 06:55:24,158 - training_jobs - DEBUG - Finished!

2019-09-16 06:55:26,435 - training_jobs - DEBUG - training trains/task_191.yml
{'d1': 85,
 'd2': 20,
 'd3': 15,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'model': 'mlp2',
 'num_epochs': 100}
2019-09-16 06:55:26,477 - training_jobs - DEBUG - training with: 
2019-09-16 06:55:26,477 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-16 06:55:26,477 - training_jobs - DEBUG - mlp2
2019-09-16 06:55:26,477 - training_jobs - DEBUG - topo and code feats
2019-09-16 06:55:26,477 - training_jobs - DEBUG - baseline_nn training
2019-09-16 06:55:26,636 - training_jobs - DEBUG -  calling nn_train_models
2019-09-16 07:01:39,886 - training_jobs - DEBUG - training time: 373s
2019-09-16 07:01:39,886 - training_jobs - DEBUG - saving to results/20190916_065526_baseline_nn_topo_and_code_feats.json
2019-09-16 07:01:39,890 - training_jobs - DEBUG - moved jobdict to done_trainings/task_191.yml
2019-09-16 07:01:39,891 - training_jobs - DEBUG - Finished!

2019-09-16 07:01:42,309 - training_jobs - DEBUG - training trains/task_100.yml
{'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-16 07:01:42,341 - training_jobs - DEBUG - training with: 
2019-09-16 07:01:42,341 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-16 07:01:42,341 - training_jobs - DEBUG - RandomForestClassifier
2019-09-16 07:01:42,341 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 07:01:42,341 - training_jobs - DEBUG - nlp training
2019-09-16 07:01:43,002 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-16 07:01:43,640 - training_jobs - DEBUG - training time: 1s
2019-09-16 07:01:43,640 - training_jobs - DEBUG - saving to results/20190916_070143_nlp_tfidf_and_topo_feats.json
2019-09-16 07:01:43,646 - training_jobs - DEBUG - moved jobdict to done_trainings/task_100.yml
2019-09-16 07:01:43,646 - training_jobs - DEBUG - Finished!

2019-09-16 07:01:45,971 - training_jobs - DEBUG - training trains/task_182.yml
{'d1': 85,
 'd2': 20,
 'd3': 15,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'model': 'mlp2',
 'num_epochs': 30}
2019-09-16 07:01:46,003 - training_jobs - DEBUG - training with: 
2019-09-16 07:01:46,003 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-16 07:01:46,003 - training_jobs - DEBUG - mlp2
2019-09-16 07:01:46,003 - training_jobs - DEBUG - topo and code feats
2019-09-16 07:01:46,003 - training_jobs - DEBUG - baseline_nn training
2019-09-16 07:01:46,180 - training_jobs - DEBUG -  calling nn_train_models
2019-09-16 07:04:04,379 - training_jobs - DEBUG - training time: 138s
2019-09-16 07:04:04,379 - training_jobs - DEBUG - saving to results/20190916_070146_baseline_nn_topo_and_code_feats.json
2019-09-16 07:04:04,385 - training_jobs - DEBUG - moved jobdict to done_trainings/task_182.yml
2019-09-16 07:04:04,385 - training_jobs - DEBUG - Finished!

2019-09-16 07:04:06,778 - training_jobs - DEBUG - training trains/task_6.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'model': 'mlp3',
 'num_epochs': 30}
2019-09-16 07:04:06,790 - training_jobs - DEBUG - training with: 
2019-09-16 07:04:06,790 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 07:04:06,790 - training_jobs - DEBUG - mlp3
2019-09-16 07:04:06,790 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 07:04:06,790 - training_jobs - DEBUG - nlp_nn training
2019-09-16 07:04:07,434 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-16 07:04:07,447 - training_jobs - ERROR - Error with trains/task_6.yml
Traceback (most recent call last):
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_nlp_models.py", line 955, in cv_train_nn_nlp_models_v2
    results_dict = train_nn_model_cv(X_train_embedding, y_train, X_test_embedding, y_test, param_set, scores, nclasses, numfolds=3 )
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1671, in train_nn_model_cv
    cv_fold_results_dict, _ =  train_nn_model_one_fold(X_train2, y_train, X_test2, y_test, nn_model_params, scores, nclasses )
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1488, in train_nn_model_one_fold
    **nn_model_params['model_kwargs']
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1147, in __init__
    super(mlp2, self).__init__()
TypeError: super(type, obj): obj must be an instance or subtype of type

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 448, in training_dispatcher
    results_dict = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_nlp_models.py", line 958, in cv_train_nn_nlp_models_v2
    print("Error with "+param_set+" and "+scores[0])
TypeError: must be str, not dict
2019-09-16 07:04:09,797 - training_jobs - DEBUG - training trains/task_48.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'model': 'mlp3',
 'num_epochs': 30}
2019-09-16 07:04:09,814 - training_jobs - DEBUG - training with: 
2019-09-16 07:04:09,814 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-16 07:04:09,814 - training_jobs - DEBUG - mlp3
2019-09-16 07:04:09,814 - training_jobs - DEBUG - topo and code feats
2019-09-16 07:04:09,815 - training_jobs - DEBUG - moved jobdict to done_trainings/task_48.yml
2019-09-16 07:04:09,815 - training_jobs - DEBUG - Finished!

2019-09-16 07:04:12,201 - training_jobs - DEBUG - training trains/task_271.yml
{'C': 1,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'max_iter': 100,
 'model': 'LogisticRegression',
 'multi_class': 'ovr',
 'penalty': 'l2',
 'solver': 'newton-cg'}
2019-09-16 07:04:12,214 - training_jobs - DEBUG - training with: 
2019-09-16 07:04:12,214 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 07:04:12,214 - training_jobs - DEBUG - LogisticRegression
2019-09-16 07:04:12,214 - training_jobs - DEBUG - topo and code feats
2019-09-16 07:04:12,214 - training_jobs - DEBUG - baseline training
2019-09-16 07:04:12,387 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 07:07:58,878 - training_jobs - DEBUG - training time: 226s
2019-09-16 07:07:58,878 - training_jobs - DEBUG - saving to results/20190916_070412_baseline_topo_and_code_feats.json
2019-09-16 07:07:58,905 - training_jobs - DEBUG - moved jobdict to done_trainings/task_271.yml
2019-09-16 07:07:58,905 - training_jobs - DEBUG - Finished!

2019-09-16 07:08:01,322 - training_jobs - DEBUG - training trains/task_31.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'max_depth': 150,
 'model': 'XGBClassifier'}
2019-09-16 07:08:01,343 - training_jobs - DEBUG - training with: 
2019-09-16 07:08:01,343 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-16 07:08:01,343 - training_jobs - DEBUG - XGBClassifier
2019-09-16 07:08:01,343 - training_jobs - DEBUG - topo and code feats
2019-09-16 07:08:01,343 - training_jobs - DEBUG - baseline training
2019-09-16 07:08:01,413 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 07:08:47,269 - training_jobs - DEBUG - training time: 46s
2019-09-16 07:08:47,269 - training_jobs - DEBUG - saving to results/20190916_070801_baseline_topo_and_code_feats.json
2019-09-16 07:08:47,275 - training_jobs - DEBUG - moved jobdict to done_trainings/task_31.yml
2019-09-16 07:08:47,275 - training_jobs - DEBUG - Finished!

2019-09-16 07:08:50,059 - training_jobs - DEBUG - training trains/task_103.yml
{'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-16 07:08:50,074 - training_jobs - DEBUG - training with: 
2019-09-16 07:08:50,074 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-16 07:08:50,074 - training_jobs - DEBUG - RandomForestClassifier
2019-09-16 07:08:50,074 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 07:08:50,074 - training_jobs - DEBUG - nlp training
2019-09-16 07:08:50,869 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-16 07:08:55,439 - training_jobs - DEBUG - training time: 5s
2019-09-16 07:08:55,439 - training_jobs - DEBUG - saving to results/20190916_070850_nlp_tfidf_and_topo_feats.json
2019-09-16 07:08:55,445 - training_jobs - DEBUG - moved jobdict to done_trainings/task_103.yml
2019-09-16 07:08:55,445 - training_jobs - DEBUG - Finished!

2019-09-16 07:08:58,299 - training_jobs - DEBUG - training trains/task_91.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 07:08:58,337 - training_jobs - DEBUG - training with: 
2019-09-16 07:08:58,337 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 07:08:58,337 - training_jobs - DEBUG - GGNN6
2019-09-16 07:08:58,337 - training_jobs - DEBUG - 
2019-09-16 07:08:58,338 - training_jobs - DEBUG - ggnn training
2019-09-16 07:09:02,832 - training_jobs - DEBUG -  saving results to results/20190916_070902_ggnn_.json
2019-09-16 07:09:02,832 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 07:09:05,037 - training_jobs - ERROR - Error with trains/task_91.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-16 07:09:07,500 - training_jobs - DEBUG - training trains/task_45.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'x_topo_feats',
 'max_depth': 150,
 'model': 'XGBClassifier'}
2019-09-16 07:09:07,512 - training_jobs - DEBUG - training with: 
2019-09-16 07:09:07,512 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-16 07:09:07,512 - training_jobs - DEBUG - XGBClassifier
2019-09-16 07:09:07,512 - training_jobs - DEBUG - x_topo_feats
2019-09-16 07:09:07,512 - training_jobs - DEBUG - baseline training
2019-09-16 07:09:07,553 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 07:09:56,252 - training_jobs - DEBUG - training time: 49s
2019-09-16 07:09:56,252 - training_jobs - DEBUG - saving to results/20190916_070907_baseline_x_topo_feats.json
2019-09-16 07:09:56,259 - training_jobs - DEBUG - moved jobdict to done_trainings/task_45.yml
2019-09-16 07:09:56,259 - training_jobs - DEBUG - Finished!

2019-09-16 07:09:58,747 - training_jobs - DEBUG - training trains/task_153.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 07:09:58,853 - training_jobs - DEBUG - training with: 
2019-09-16 07:09:58,853 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-16 07:09:58,853 - training_jobs - DEBUG - GGNN1
2019-09-16 07:09:58,853 - training_jobs - DEBUG - 
2019-09-16 07:09:58,853 - training_jobs - DEBUG - ggnn training
2019-09-16 07:10:00,013 - training_jobs - DEBUG -  saving results to results/20190916_071000_ggnn_.json
2019-09-16 07:10:00,013 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 07:40:48,971 - training_jobs - DEBUG - test_multiple_models
2019-09-16 07:40:48,971 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-16 07:40:49,464 - training_jobs - DEBUG - training time: 1849s
2019-09-16 07:40:49,464 - training_jobs - DEBUG - saving to results/20190916_071000_ggnn_.json
2019-09-16 07:40:49,466 - training_jobs - DEBUG - moved jobdict to done_trainings/task_153.yml
2019-09-16 07:40:49,466 - training_jobs - DEBUG - Finished!

2019-09-16 07:40:51,938 - training_jobs - DEBUG - training trains/task_270.yml
{'C': 1,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'x_topo_feats',
 'max_iter': 100,
 'model': 'LogisticRegression',
 'multi_class': 'ovr',
 'penalty': 'l2',
 'solver': 'newton-cg'}
2019-09-16 07:40:51,963 - training_jobs - DEBUG - training with: 
2019-09-16 07:40:51,963 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 07:40:51,963 - training_jobs - DEBUG - LogisticRegression
2019-09-16 07:40:51,963 - training_jobs - DEBUG - x_topo_feats
2019-09-16 07:40:51,964 - training_jobs - DEBUG - baseline training
2019-09-16 07:40:52,086 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 07:41:37,625 - training_jobs - DEBUG - training time: 46s
2019-09-16 07:41:37,626 - training_jobs - DEBUG - saving to results/20190916_074052_baseline_x_topo_feats.json
2019-09-16 07:41:37,656 - training_jobs - DEBUG - moved jobdict to done_trainings/task_270.yml
2019-09-16 07:41:37,656 - training_jobs - DEBUG - Finished!

2019-09-16 07:41:39,991 - training_jobs - DEBUG - training trains/task_274.yml
{'C': 1,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'x_topo_feats',
 'max_iter': 100,
 'model': 'LogisticRegression',
 'multi_class': 'ovr',
 'penalty': 'l2',
 'solver': 'newton-cg'}
2019-09-16 07:41:40,002 - training_jobs - DEBUG - training with: 
2019-09-16 07:41:40,002 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-16 07:41:40,002 - training_jobs - DEBUG - LogisticRegression
2019-09-16 07:41:40,002 - training_jobs - DEBUG - x_topo_feats
2019-09-16 07:41:40,002 - training_jobs - DEBUG - baseline training
2019-09-16 07:41:40,115 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 07:41:45,420 - training_jobs - DEBUG - training time: 5s
2019-09-16 07:41:45,420 - training_jobs - DEBUG - saving to results/20190916_074140_baseline_x_topo_feats.json
2019-09-16 07:41:45,440 - training_jobs - DEBUG - moved jobdict to done_trainings/task_274.yml
2019-09-16 07:41:45,441 - training_jobs - DEBUG - Finished!

2019-09-16 07:41:48,151 - training_jobs - DEBUG - training trains/task_98.yml
{'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-16 07:41:48,188 - training_jobs - DEBUG - training with: 
2019-09-16 07:41:48,188 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-16 07:41:48,188 - training_jobs - DEBUG - RandomForestClassifier
2019-09-16 07:41:48,188 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 07:41:48,189 - training_jobs - DEBUG - nlp training
2019-09-16 07:41:48,644 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-16 07:41:49,411 - training_jobs - DEBUG - training time: 1s
2019-09-16 07:41:49,411 - training_jobs - DEBUG - saving to results/20190916_074148_nlp_tfidf_and_topo_feats.json
2019-09-16 07:41:49,415 - training_jobs - DEBUG - moved jobdict to done_trainings/task_98.yml
2019-09-16 07:41:49,416 - training_jobs - DEBUG - Finished!

2019-09-16 07:41:52,232 - training_jobs - DEBUG - training trains/task_73.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 15,
 'd4': 10,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 07:41:52,273 - training_jobs - DEBUG - training with: 
2019-09-16 07:41:52,273 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-16 07:41:52,273 - training_jobs - DEBUG - GGNN5
2019-09-16 07:41:52,273 - training_jobs - DEBUG - 
2019-09-16 07:41:52,273 - training_jobs - DEBUG - ggnn training
2019-09-16 07:41:53,888 - training_jobs - DEBUG -  saving results to results/20190916_074153_ggnn_.json
2019-09-16 07:41:53,888 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 08:25:20,878 - training_jobs - DEBUG - test_multiple_models
2019-09-16 08:25:20,878 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-16 08:25:21,522 - training_jobs - DEBUG - training time: 2608s
2019-09-16 08:25:21,522 - training_jobs - DEBUG - saving to results/20190916_074153_ggnn_.json
2019-09-16 08:25:21,524 - training_jobs - DEBUG - moved jobdict to done_trainings/task_73.yml
2019-09-16 08:25:21,524 - training_jobs - DEBUG - Finished!

2019-09-16 08:25:23,911 - training_jobs - DEBUG - training trains/task_260.yml
{'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-16 08:25:23,932 - training_jobs - DEBUG - training with: 
2019-09-16 08:25:23,932 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-16 08:25:23,932 - training_jobs - DEBUG - RandomForestClassifier
2019-09-16 08:25:23,932 - training_jobs - DEBUG - topo and code feats
2019-09-16 08:25:23,932 - training_jobs - DEBUG - baseline training
2019-09-16 08:25:24,082 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 08:25:24,738 - training_jobs - DEBUG - training time: 1s
2019-09-16 08:25:24,738 - training_jobs - DEBUG - saving to results/20190916_082524_baseline_topo_and_code_feats.json
2019-09-16 08:25:24,749 - training_jobs - DEBUG - moved jobdict to done_trainings/task_260.yml
2019-09-16 08:25:24,749 - training_jobs - DEBUG - Finished!

2019-09-16 08:25:27,180 - training_jobs - DEBUG - training trains/task_93.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 08:25:27,215 - training_jobs - DEBUG - training with: 
2019-09-16 08:25:27,216 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-16 08:25:27,216 - training_jobs - DEBUG - GGNN6
2019-09-16 08:25:27,216 - training_jobs - DEBUG - 
2019-09-16 08:25:27,216 - training_jobs - DEBUG - ggnn training
2019-09-16 08:25:28,386 - training_jobs - DEBUG -  saving results to results/20190916_082528_ggnn_.json
2019-09-16 08:25:28,386 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 08:58:02,330 - training_jobs - DEBUG - test_multiple_models
2019-09-16 08:58:02,330 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-16 08:58:02,822 - training_jobs - DEBUG - training time: 1954s
2019-09-16 08:58:02,822 - training_jobs - DEBUG - saving to results/20190916_082528_ggnn_.json
2019-09-16 08:58:02,824 - training_jobs - DEBUG - moved jobdict to done_trainings/task_93.yml
2019-09-16 08:58:02,824 - training_jobs - DEBUG - Finished!

2019-09-16 08:58:05,559 - training_jobs - DEBUG - training trains/task_248.yml
{'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'x_topo_feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-16 08:58:05,595 - training_jobs - DEBUG - training with: 
2019-09-16 08:58:05,595 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-16 08:58:05,595 - training_jobs - DEBUG - RandomForestClassifier
2019-09-16 08:58:05,595 - training_jobs - DEBUG - x_topo_feats
2019-09-16 08:58:05,595 - training_jobs - DEBUG - baseline training
2019-09-16 08:58:05,637 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 08:58:06,294 - training_jobs - DEBUG - training time: 1s
2019-09-16 08:58:06,295 - training_jobs - DEBUG - saving to results/20190916_085805_baseline_x_topo_feats.json
2019-09-16 08:58:06,301 - training_jobs - DEBUG - moved jobdict to done_trainings/task_248.yml
2019-09-16 08:58:06,301 - training_jobs - DEBUG - Finished!

2019-09-16 08:58:08,745 - training_jobs - DEBUG - training trains/task_81.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 15,
 'd4': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 08:58:08,778 - training_jobs - DEBUG - training with: 
2019-09-16 08:58:08,779 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-16 08:58:08,779 - training_jobs - DEBUG - GGNN5
2019-09-16 08:58:08,779 - training_jobs - DEBUG - 
2019-09-16 08:58:08,779 - training_jobs - DEBUG - ggnn training
2019-09-16 08:58:09,972 - training_jobs - DEBUG -  saving results to results/20190916_085809_ggnn_.json
2019-09-16 08:58:09,972 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 09:30:03,253 - training_jobs - DEBUG - test_multiple_models
2019-09-16 09:30:03,254 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-16 09:30:03,743 - training_jobs - DEBUG - training time: 1914s
2019-09-16 09:30:03,743 - training_jobs - DEBUG - saving to results/20190916_085809_ggnn_.json
2019-09-16 09:30:03,745 - training_jobs - DEBUG - moved jobdict to done_trainings/task_81.yml
2019-09-16 09:30:03,745 - training_jobs - DEBUG - Finished!

2019-09-16 09:30:06,130 - training_jobs - DEBUG - training trains/task_252.yml
{'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-16 09:30:06,141 - training_jobs - DEBUG - training with: 
2019-09-16 09:30:06,141 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-16 09:30:06,141 - training_jobs - DEBUG - RandomForestClassifier
2019-09-16 09:30:06,141 - training_jobs - DEBUG - topo and code feats
2019-09-16 09:30:06,141 - training_jobs - DEBUG - baseline training
2019-09-16 09:30:06,211 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 09:30:06,849 - training_jobs - DEBUG - training time: 1s
2019-09-16 09:30:06,849 - training_jobs - DEBUG - saving to results/20190916_093006_baseline_topo_and_code_feats.json
2019-09-16 09:30:06,893 - training_jobs - DEBUG - moved jobdict to done_trainings/task_252.yml
2019-09-16 09:30:06,893 - training_jobs - DEBUG - Finished!

2019-09-16 09:30:09,340 - training_jobs - DEBUG - training trains/task_138.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 09:30:09,415 - training_jobs - DEBUG - training with: 
2019-09-16 09:30:09,415 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 09:30:09,415 - training_jobs - DEBUG - GGNN1
2019-09-16 09:30:09,415 - training_jobs - DEBUG - 
2019-09-16 09:30:09,415 - training_jobs - DEBUG - ggnn training
2019-09-16 09:30:11,682 - training_jobs - DEBUG -  saving results to results/20190916_093011_ggnn_.json
2019-09-16 09:30:11,682 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 09:30:13,783 - training_jobs - ERROR - Error with trains/task_138.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-16 09:30:16,227 - training_jobs - DEBUG - training trains/task_42.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'max_depth': 8,
 'model': 'XGBClassifier'}
2019-09-16 09:30:16,247 - training_jobs - DEBUG - training with: 
2019-09-16 09:30:16,248 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 09:30:16,248 - training_jobs - DEBUG - XGBClassifier
2019-09-16 09:30:16,248 - training_jobs - DEBUG - topo and code feats
2019-09-16 09:30:16,248 - training_jobs - DEBUG - baseline training
2019-09-16 09:30:16,343 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 09:32:52,314 - training_jobs - DEBUG - training time: 156s
2019-09-16 09:32:52,314 - training_jobs - DEBUG - saving to results/20190916_093016_baseline_topo_and_code_feats.json
2019-09-16 09:32:52,322 - training_jobs - DEBUG - moved jobdict to done_trainings/task_42.yml
2019-09-16 09:32:52,322 - training_jobs - DEBUG - Finished!

2019-09-16 09:32:55,313 - training_jobs - DEBUG - training trains/task_25.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'x_topo_feats',
 'max_depth': 150,
 'model': 'XGBClassifier'}
2019-09-16 09:32:55,329 - training_jobs - DEBUG - training with: 
2019-09-16 09:32:55,329 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 09:32:55,329 - training_jobs - DEBUG - XGBClassifier
2019-09-16 09:32:55,329 - training_jobs - DEBUG - x_topo_feats
2019-09-16 09:32:55,329 - training_jobs - DEBUG - baseline training
2019-09-16 09:32:55,474 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 09:40:21,771 - training_jobs - DEBUG - training time: 446s
2019-09-16 09:40:21,771 - training_jobs - DEBUG - saving to results/20190916_093255_baseline_x_topo_feats.json
2019-09-16 09:40:21,784 - training_jobs - DEBUG - moved jobdict to done_trainings/task_25.yml
2019-09-16 09:40:21,784 - training_jobs - DEBUG - Finished!

2019-09-16 09:40:24,285 - training_jobs - DEBUG - training trains/task_26.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'max_depth': 8,
 'model': 'XGBClassifier'}
2019-09-16 09:40:24,298 - training_jobs - DEBUG - training with: 
2019-09-16 09:40:24,298 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 09:40:24,298 - training_jobs - DEBUG - XGBClassifier
2019-09-16 09:40:24,298 - training_jobs - DEBUG - topo and code feats
2019-09-16 09:40:24,298 - training_jobs - DEBUG - baseline training
2019-09-16 09:40:24,484 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 09:45:04,715 - training_jobs - DEBUG - training time: 280s
2019-09-16 09:45:04,715 - training_jobs - DEBUG - saving to results/20190916_094024_baseline_topo_and_code_feats.json
2019-09-16 09:45:04,727 - training_jobs - DEBUG - moved jobdict to done_trainings/task_26.yml
2019-09-16 09:45:04,727 - training_jobs - DEBUG - Finished!

2019-09-16 09:45:07,400 - training_jobs - DEBUG - training trains/task_184.yml
{'d1': 85,
 'd2': 20,
 'd3': 15,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'model': 'mlp2',
 'num_epochs': 30}
2019-09-16 09:45:07,413 - training_jobs - DEBUG - training with: 
2019-09-16 09:45:07,413 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 09:45:07,413 - training_jobs - DEBUG - mlp2
2019-09-16 09:45:07,413 - training_jobs - DEBUG - topo and code feats
2019-09-16 09:45:07,414 - training_jobs - DEBUG - baseline_nn training
2019-09-16 09:45:07,504 - training_jobs - DEBUG -  calling nn_train_models
2019-09-16 09:46:26,454 - training_jobs - DEBUG - training time: 79s
2019-09-16 09:46:26,454 - training_jobs - DEBUG - saving to results/20190916_094507_baseline_nn_topo_and_code_feats.json
2019-09-16 09:46:26,459 - training_jobs - DEBUG - moved jobdict to done_trainings/task_184.yml
2019-09-16 09:46:26,459 - training_jobs - DEBUG - Finished!

2019-09-16 09:46:28,863 - training_jobs - DEBUG - training trains/task_256.yml
{'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'x_topo_feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-16 09:46:28,883 - training_jobs - DEBUG - training with: 
2019-09-16 09:46:28,883 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-16 09:46:28,883 - training_jobs - DEBUG - RandomForestClassifier
2019-09-16 09:46:28,883 - training_jobs - DEBUG - x_topo_feats
2019-09-16 09:46:28,883 - training_jobs - DEBUG - baseline training
2019-09-16 09:46:28,994 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 09:46:29,655 - training_jobs - DEBUG - training time: 1s
2019-09-16 09:46:29,655 - training_jobs - DEBUG - saving to results/20190916_094628_baseline_x_topo_feats.json
2019-09-16 09:46:29,666 - training_jobs - DEBUG - moved jobdict to done_trainings/task_256.yml
2019-09-16 09:46:29,666 - training_jobs - DEBUG - Finished!

2019-09-16 09:46:32,077 - training_jobs - DEBUG - training trains/task_116.yml
{'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-16 09:46:32,089 - training_jobs - DEBUG - training with: 
2019-09-16 09:46:32,089 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-16 09:46:32,089 - training_jobs - DEBUG - RandomForestClassifier
2019-09-16 09:46:32,089 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 09:46:32,089 - training_jobs - DEBUG - nlp training
2019-09-16 09:46:32,641 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-16 09:46:33,106 - training_jobs - DEBUG - training time: 0s
2019-09-16 09:46:33,106 - training_jobs - DEBUG - saving to results/20190916_094632_nlp_tfidf_and_topo_feats.json
2019-09-16 09:46:33,111 - training_jobs - DEBUG - moved jobdict to done_trainings/task_116.yml
2019-09-16 09:46:33,111 - training_jobs - DEBUG - Finished!

2019-09-16 09:46:35,449 - training_jobs - DEBUG - training trains/task_272.yml
{'C': 1,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'x_topo_feats',
 'max_iter': 100,
 'model': 'LogisticRegression',
 'multi_class': 'ovr',
 'penalty': 'l2',
 'solver': 'newton-cg'}
2019-09-16 09:46:35,627 - training_jobs - DEBUG - training with: 
2019-09-16 09:46:35,627 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-16 09:46:35,627 - training_jobs - DEBUG - LogisticRegression
2019-09-16 09:46:35,627 - training_jobs - DEBUG - x_topo_feats
2019-09-16 09:46:35,628 - training_jobs - DEBUG - baseline training
2019-09-16 09:46:35,669 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 09:46:38,187 - training_jobs - DEBUG - training time: 3s
2019-09-16 09:46:38,199 - training_jobs - DEBUG - saving to results/20190916_094635_baseline_x_topo_feats.json
2019-09-16 09:46:38,223 - training_jobs - DEBUG - moved jobdict to done_trainings/task_272.yml
2019-09-16 09:46:38,224 - training_jobs - DEBUG - Finished!

2019-09-16 09:46:40,738 - training_jobs - DEBUG - training trains/task_5.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'model': 'mlp3',
 'num_epochs': 100}
2019-09-16 09:46:40,761 - training_jobs - DEBUG - training with: 
2019-09-16 09:46:40,761 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 09:46:40,761 - training_jobs - DEBUG - mlp3
2019-09-16 09:46:40,761 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 09:46:40,761 - training_jobs - DEBUG - nlp_nn training
2019-09-16 09:46:41,187 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-16 09:46:41,198 - training_jobs - ERROR - Error with trains/task_5.yml
Traceback (most recent call last):
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_nlp_models.py", line 955, in cv_train_nn_nlp_models_v2
    results_dict = train_nn_model_cv(X_train_embedding, y_train, X_test_embedding, y_test, param_set, scores, nclasses, numfolds=3 )
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1671, in train_nn_model_cv
    cv_fold_results_dict, _ =  train_nn_model_one_fold(X_train2, y_train, X_test2, y_test, nn_model_params, scores, nclasses )
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1488, in train_nn_model_one_fold
    **nn_model_params['model_kwargs']
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1147, in __init__
    super(mlp2, self).__init__()
TypeError: super(type, obj): obj must be an instance or subtype of type

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 448, in training_dispatcher
    results_dict = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_nlp_models.py", line 958, in cv_train_nn_nlp_models_v2
    print("Error with "+param_set+" and "+scores[0])
TypeError: must be str, not dict
2019-09-16 09:46:43,729 - training_jobs - DEBUG - training trains/task_83.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 15,
 'd4': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 09:46:43,764 - training_jobs - DEBUG - training with: 
2019-09-16 09:46:43,764 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-16 09:46:43,764 - training_jobs - DEBUG - GGNN5
2019-09-16 09:46:43,764 - training_jobs - DEBUG - 
2019-09-16 09:46:43,764 - training_jobs - DEBUG - ggnn training
2019-09-16 09:46:47,778 - training_jobs - DEBUG -  saving results to results/20190916_094647_ggnn_.json
2019-09-16 09:46:47,778 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 10:39:40,129 - training_jobs - DEBUG - test_multiple_models
2019-09-16 10:39:40,129 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-16 10:39:42,465 - training_jobs - DEBUG - training time: 3175s
2019-09-16 10:39:42,465 - training_jobs - DEBUG - saving to results/20190916_094647_ggnn_.json
2019-09-16 10:39:42,467 - training_jobs - DEBUG - moved jobdict to done_trainings/task_83.yml
2019-09-16 10:39:42,467 - training_jobs - DEBUG - Finished!

2019-09-16 10:39:45,031 - training_jobs - DEBUG - training trains/task_118.yml
{'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-16 10:39:45,052 - training_jobs - DEBUG - training with: 
2019-09-16 10:39:45,052 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-16 10:39:45,052 - training_jobs - DEBUG - RandomForestClassifier
2019-09-16 10:39:45,052 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 10:39:45,052 - training_jobs - DEBUG - nlp training
2019-09-16 10:39:45,615 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-16 10:39:46,322 - training_jobs - DEBUG - training time: 1s
2019-09-16 10:39:46,322 - training_jobs - DEBUG - saving to results/20190916_103945_nlp_tfidf_and_topo_feats.json
2019-09-16 10:39:46,326 - training_jobs - DEBUG - moved jobdict to done_trainings/task_118.yml
2019-09-16 10:39:46,326 - training_jobs - DEBUG - Finished!

2019-09-16 10:39:48,748 - training_jobs - DEBUG - training trains/task_66.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'max_depth': 8,
 'model': 'XGBClassifier'}
2019-09-16 10:39:48,762 - training_jobs - DEBUG - training with: 
2019-09-16 10:39:48,762 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 10:39:48,762 - training_jobs - DEBUG - XGBClassifier
2019-09-16 10:39:48,762 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 10:39:48,762 - training_jobs - DEBUG - nlp training
2019-09-16 10:39:49,436 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-16 11:15:22,904 - training_jobs - DEBUG - training time: 2133s
2019-09-16 11:15:22,904 - training_jobs - DEBUG - saving to results/20190916_103949_nlp_tfidf_and_topo_feats.json
2019-09-16 11:15:22,911 - training_jobs - DEBUG - moved jobdict to done_trainings/task_66.yml
2019-09-16 11:15:22,911 - training_jobs - DEBUG - Finished!

2019-09-16 11:15:25,515 - training_jobs - DEBUG - training trains/task_1.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'model': 'mlp3',
 'num_epochs': 100}
2019-09-16 11:15:25,550 - training_jobs - DEBUG - training with: 
2019-09-16 11:15:25,550 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-16 11:15:25,551 - training_jobs - DEBUG - mlp3
2019-09-16 11:15:25,551 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 11:15:25,551 - training_jobs - DEBUG - nlp_nn training
2019-09-16 11:15:26,043 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-16 11:15:26,055 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_nlp_models.py", line 955, in cv_train_nn_nlp_models_v2
    results_dict = train_nn_model_cv(X_train_embedding, y_train, X_test_embedding, y_test, param_set, scores, nclasses, numfolds=3 )
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1671, in train_nn_model_cv
    cv_fold_results_dict, _ =  train_nn_model_one_fold(X_train2, y_train, X_test2, y_test, nn_model_params, scores, nclasses )
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1488, in train_nn_model_one_fold
    **nn_model_params['model_kwargs']
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1147, in __init__
    super(mlp2, self).__init__()
TypeError: super(type, obj): obj must be an instance or subtype of type

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 448, in training_dispatcher
    results_dict = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_nlp_models.py", line 958, in cv_train_nn_nlp_models_v2
    print("Error with "+param_set+" and "+scores[0])
TypeError: must be str, not dict
2019-09-16 11:15:28,835 - training_jobs - DEBUG - training trains/task_77.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 15,
 'd4': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 11:15:28,904 - training_jobs - DEBUG - training with: 
2019-09-16 11:15:28,904 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 11:15:28,904 - training_jobs - DEBUG - GGNN5
2019-09-16 11:15:28,904 - training_jobs - DEBUG - 
2019-09-16 11:15:28,905 - training_jobs - DEBUG - ggnn training
2019-09-16 11:15:30,889 - training_jobs - DEBUG -  saving results to results/20190916_111530_ggnn_.json
2019-09-16 11:15:30,890 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 11:15:32,324 - training_jobs - ERROR - Error with trains/task_77.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-16 11:15:35,142 - training_jobs - DEBUG - training trains/task_27.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'max_depth': 150,
 'model': 'XGBClassifier'}
2019-09-16 11:15:35,156 - training_jobs - DEBUG - training with: 
2019-09-16 11:15:35,156 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 11:15:35,156 - training_jobs - DEBUG - XGBClassifier
2019-09-16 11:15:35,156 - training_jobs - DEBUG - topo and code feats
2019-09-16 11:15:35,156 - training_jobs - DEBUG - baseline training
2019-09-16 11:15:35,368 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 11:23:14,710 - training_jobs - DEBUG - training time: 459s
2019-09-16 11:23:14,710 - training_jobs - DEBUG - saving to results/20190916_111535_baseline_topo_and_code_feats.json
2019-09-16 11:23:14,722 - training_jobs - DEBUG - moved jobdict to done_trainings/task_27.yml
2019-09-16 11:23:14,722 - training_jobs - DEBUG - Finished!

2019-09-16 11:23:17,330 - training_jobs - DEBUG - training trains/task_209.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'model': 'mlp4',
 'num_epochs': 100}
2019-09-16 11:23:17,364 - training_jobs - DEBUG - training with: 
2019-09-16 11:23:17,364 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 11:23:17,364 - training_jobs - DEBUG - mlp4
2019-09-16 11:23:17,364 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 11:23:17,364 - training_jobs - DEBUG - nlp_nn training
2019-09-16 11:23:17,818 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-16 11:23:17,830 - training_jobs - ERROR - Error with trains/task_209.yml
Traceback (most recent call last):
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_nlp_models.py", line 955, in cv_train_nn_nlp_models_v2
    results_dict = train_nn_model_cv(X_train_embedding, y_train, X_test_embedding, y_test, param_set, scores, nclasses, numfolds=3 )
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1671, in train_nn_model_cv
    cv_fold_results_dict, _ =  train_nn_model_one_fold(X_train2, y_train, X_test2, y_test, nn_model_params, scores, nclasses )
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1488, in train_nn_model_one_fold
    **nn_model_params['model_kwargs']
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1167, in __init__
    super(mlp2, self).__init__()
TypeError: super(type, obj): obj must be an instance or subtype of type

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 448, in training_dispatcher
    results_dict = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_nlp_models.py", line 958, in cv_train_nn_nlp_models_v2
    print("Error with "+param_set+" and "+scores[0])
TypeError: must be str, not dict
2019-09-16 11:23:20,489 - training_jobs - DEBUG - training trains/task_44.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'x_topo_feats',
 'max_depth': 8,
 'model': 'XGBClassifier'}
2019-09-16 11:23:20,502 - training_jobs - DEBUG - training with: 
2019-09-16 11:23:20,502 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-16 11:23:20,502 - training_jobs - DEBUG - XGBClassifier
2019-09-16 11:23:20,502 - training_jobs - DEBUG - x_topo_feats
2019-09-16 11:23:20,502 - training_jobs - DEBUG - baseline training
2019-09-16 11:23:20,548 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 11:23:34,778 - training_jobs - DEBUG - training time: 14s
2019-09-16 11:23:34,779 - training_jobs - DEBUG - saving to results/20190916_112320_baseline_x_topo_feats.json
2019-09-16 11:23:34,784 - training_jobs - DEBUG - moved jobdict to done_trainings/task_44.yml
2019-09-16 11:23:34,784 - training_jobs - DEBUG - Finished!

2019-09-16 11:23:37,478 - training_jobs - DEBUG - training trains/task_244.yml
{'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-16 11:23:37,507 - training_jobs - DEBUG - training with: 
2019-09-16 11:23:37,507 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 11:23:37,507 - training_jobs - DEBUG - RandomForestClassifier
2019-09-16 11:23:37,507 - training_jobs - DEBUG - topo and code feats
2019-09-16 11:23:37,508 - training_jobs - DEBUG - baseline training
2019-09-16 11:23:37,708 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 11:23:38,759 - training_jobs - DEBUG - training time: 1s
2019-09-16 11:23:38,759 - training_jobs - DEBUG - saving to results/20190916_112337_baseline_topo_and_code_feats.json
2019-09-16 11:23:38,775 - training_jobs - DEBUG - moved jobdict to done_trainings/task_244.yml
2019-09-16 11:23:38,775 - training_jobs - DEBUG - Finished!

2019-09-16 11:23:41,605 - training_jobs - DEBUG - training trains/task_177.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 11:23:41,637 - training_jobs - DEBUG - training with: 
2019-09-16 11:23:41,638 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-16 11:23:41,638 - training_jobs - DEBUG - GGNN1
2019-09-16 11:23:41,638 - training_jobs - DEBUG - 
2019-09-16 11:23:41,638 - training_jobs - DEBUG - ggnn training
2019-09-16 11:23:43,011 - training_jobs - DEBUG -  saving results to results/20190916_112343_ggnn_.json
2019-09-16 11:23:43,012 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 11:56:17,143 - training_jobs - DEBUG - test_multiple_models
2019-09-16 11:56:17,143 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-16 11:56:17,688 - training_jobs - DEBUG - training time: 1955s
2019-09-16 11:56:17,688 - training_jobs - DEBUG - saving to results/20190916_112343_ggnn_.json
2019-09-16 11:56:17,690 - training_jobs - DEBUG - moved jobdict to done_trainings/task_177.yml
2019-09-16 11:56:17,690 - training_jobs - DEBUG - Finished!

2019-09-16 11:56:20,517 - training_jobs - DEBUG - training trains/task_64.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'max_depth': 8,
 'model': 'XGBClassifier'}
2019-09-16 11:56:20,549 - training_jobs - DEBUG - training with: 
2019-09-16 11:56:20,549 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 11:56:20,549 - training_jobs - DEBUG - XGBClassifier
2019-09-16 11:56:20,549 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 11:56:20,549 - training_jobs - DEBUG - nlp training
2019-09-16 11:56:21,022 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-16 12:14:33,766 - training_jobs - DEBUG - training time: 1093s
2019-09-16 12:14:33,766 - training_jobs - DEBUG - saving to results/20190916_115621_nlp_tfidf_and_topo_feats.json
2019-09-16 12:14:33,771 - training_jobs - DEBUG - moved jobdict to done_trainings/task_64.yml
2019-09-16 12:14:33,772 - training_jobs - DEBUG - Finished!

2019-09-16 12:14:36,344 - training_jobs - DEBUG - training trains/task_72.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 15,
 'd4': 10,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 12:14:36,360 - training_jobs - DEBUG - training with: 
2019-09-16 12:14:36,360 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-16 12:14:36,360 - training_jobs - DEBUG - GGNN5
2019-09-16 12:14:36,360 - training_jobs - DEBUG - 
2019-09-16 12:14:36,360 - training_jobs - DEBUG - ggnn training
2019-09-16 12:14:38,432 - training_jobs - DEBUG -  saving results to results/20190916_121438_ggnn_.json
2019-09-16 12:14:38,432 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 12:36:18,481 - training_jobs - DEBUG - test_multiple_models
2019-09-16 12:36:18,481 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-16 12:36:19,123 - training_jobs - DEBUG - training time: 1301s
2019-09-16 12:36:19,123 - training_jobs - DEBUG - saving to results/20190916_121438_ggnn_.json
2019-09-16 12:36:19,126 - training_jobs - DEBUG - moved jobdict to done_trainings/task_72.yml
2019-09-16 12:36:19,126 - training_jobs - DEBUG - Finished!

2019-09-16 12:36:21,566 - training_jobs - DEBUG - training trains/task_151.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 12:36:21,844 - training_jobs - DEBUG - training with: 
2019-09-16 12:36:21,845 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 12:36:21,845 - training_jobs - DEBUG - GGNN1
2019-09-16 12:36:21,845 - training_jobs - DEBUG - 
2019-09-16 12:36:21,845 - training_jobs - DEBUG - ggnn training
2019-09-16 12:36:24,091 - training_jobs - DEBUG -  saving results to results/20190916_123624_ggnn_.json
2019-09-16 12:36:24,091 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 12:36:26,213 - training_jobs - ERROR - Error with trains/task_151.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-16 12:36:28,595 - training_jobs - DEBUG - training trains/task_266.yml
{'C': 1,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'x_topo_feats',
 'max_iter': 100,
 'model': 'LogisticRegression',
 'multi_class': 'ovr',
 'penalty': 'l2',
 'solver': 'newton-cg'}
2019-09-16 12:36:28,606 - training_jobs - DEBUG - training with: 
2019-09-16 12:36:28,607 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-16 12:36:28,607 - training_jobs - DEBUG - LogisticRegression
2019-09-16 12:36:28,607 - training_jobs - DEBUG - x_topo_feats
2019-09-16 12:36:28,607 - training_jobs - DEBUG - baseline training
2019-09-16 12:36:28,735 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 12:36:37,796 - training_jobs - DEBUG - training time: 9s
2019-09-16 12:36:37,796 - training_jobs - DEBUG - saving to results/20190916_123628_baseline_x_topo_feats.json
2019-09-16 12:36:37,817 - training_jobs - DEBUG - moved jobdict to done_trainings/task_266.yml
2019-09-16 12:36:37,817 - training_jobs - DEBUG - Finished!

2019-09-16 12:36:40,516 - training_jobs - DEBUG - training trains/task_56.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'model': 'mlp3',
 'num_epochs': 30}
2019-09-16 12:36:40,547 - training_jobs - DEBUG - training with: 
2019-09-16 12:36:40,547 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-16 12:36:40,547 - training_jobs - DEBUG - mlp3
2019-09-16 12:36:40,547 - training_jobs - DEBUG - topo and code feats
2019-09-16 12:36:40,548 - training_jobs - DEBUG - moved jobdict to done_trainings/task_56.yml
2019-09-16 12:36:40,548 - training_jobs - DEBUG - Finished!

2019-09-16 12:36:43,000 - training_jobs - DEBUG - training trains/task_78.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 15,
 'd4': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 12:36:43,014 - training_jobs - DEBUG - training with: 
2019-09-16 12:36:43,014 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 12:36:43,014 - training_jobs - DEBUG - GGNN5
2019-09-16 12:36:43,014 - training_jobs - DEBUG - 
2019-09-16 12:36:43,014 - training_jobs - DEBUG - ggnn training
2019-09-16 12:36:45,597 - training_jobs - DEBUG -  saving results to results/20190916_123645_ggnn_.json
2019-09-16 12:36:45,598 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 12:36:48,152 - training_jobs - ERROR - Error with trains/task_78.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-16 12:36:51,033 - training_jobs - DEBUG - training trains/task_164.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 12:36:51,049 - training_jobs - DEBUG - training with: 
2019-09-16 12:36:51,049 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-16 12:36:51,049 - training_jobs - DEBUG - GGNN1
2019-09-16 12:36:51,049 - training_jobs - DEBUG - 
2019-09-16 12:36:51,049 - training_jobs - DEBUG - ggnn training
2019-09-16 12:36:52,454 - training_jobs - DEBUG -  saving results to results/20190916_123652_ggnn_.json
2019-09-16 12:36:52,454 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 12:53:10,110 - training_jobs - DEBUG - test_multiple_models
2019-09-16 12:53:10,110 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-16 12:53:10,670 - training_jobs - DEBUG - training time: 978s
2019-09-16 12:53:10,670 - training_jobs - DEBUG - saving to results/20190916_123652_ggnn_.json
2019-09-16 12:53:10,672 - training_jobs - DEBUG - moved jobdict to done_trainings/task_164.yml
2019-09-16 12:53:10,672 - training_jobs - DEBUG - Finished!

2019-09-16 12:53:13,596 - training_jobs - DEBUG - training trains/task_61.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'max_depth': 150,
 'model': 'XGBClassifier'}
2019-09-16 12:53:13,629 - training_jobs - DEBUG - training with: 
2019-09-16 12:53:13,629 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-16 12:53:13,629 - training_jobs - DEBUG - XGBClassifier
2019-09-16 12:53:13,629 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 12:53:13,630 - training_jobs - DEBUG - nlp training
2019-09-16 12:53:14,151 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-16 13:04:30,408 - training_jobs - DEBUG - training time: 676s
2019-09-16 13:04:30,408 - training_jobs - DEBUG - saving to results/20190916_125314_nlp_tfidf_and_topo_feats.json
2019-09-16 13:04:30,412 - training_jobs - DEBUG - moved jobdict to done_trainings/task_61.yml
2019-09-16 13:04:30,412 - training_jobs - DEBUG - Finished!

2019-09-16 13:04:33,262 - training_jobs - DEBUG - training trains/task_225.yml
{'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'x_topo_feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-16 13:04:33,277 - training_jobs - DEBUG - training with: 
2019-09-16 13:04:33,277 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-16 13:04:33,278 - training_jobs - DEBUG - RandomForestClassifier
2019-09-16 13:04:33,278 - training_jobs - DEBUG - x_topo_feats
2019-09-16 13:04:33,278 - training_jobs - DEBUG - baseline training
2019-09-16 13:04:33,426 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 13:04:34,968 - training_jobs - DEBUG - training time: 2s
2019-09-16 13:04:34,969 - training_jobs - DEBUG - saving to results/20190916_130433_baseline_x_topo_feats.json
2019-09-16 13:04:35,003 - training_jobs - DEBUG - moved jobdict to done_trainings/task_225.yml
2019-09-16 13:04:35,003 - training_jobs - DEBUG - Finished!

2019-09-16 13:04:38,209 - training_jobs - DEBUG - training trains/task_175.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 13:04:38,225 - training_jobs - DEBUG - training with: 
2019-09-16 13:04:38,226 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 13:04:38,226 - training_jobs - DEBUG - GGNN1
2019-09-16 13:04:38,226 - training_jobs - DEBUG - 
2019-09-16 13:04:38,226 - training_jobs - DEBUG - ggnn training
2019-09-16 13:04:41,204 - training_jobs - DEBUG -  saving results to results/20190916_130441_ggnn_.json
2019-09-16 13:04:41,204 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 13:04:43,997 - training_jobs - ERROR - Error with trains/task_175.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-16 13:04:46,800 - training_jobs - DEBUG - training trains/task_186.yml
{'d1': 85,
 'd2': 20,
 'd3': 15,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'model': 'mlp2',
 'num_epochs': 30}
2019-09-16 13:04:47,089 - training_jobs - DEBUG - training with: 
2019-09-16 13:04:47,089 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 13:04:47,089 - training_jobs - DEBUG - mlp2
2019-09-16 13:04:47,089 - training_jobs - DEBUG - topo and code feats
2019-09-16 13:04:47,089 - training_jobs - DEBUG - baseline_nn training
2019-09-16 13:04:47,282 - training_jobs - DEBUG -  calling nn_train_models
2019-09-16 13:06:52,612 - training_jobs - DEBUG - training time: 125s
2019-09-16 13:06:52,612 - training_jobs - DEBUG - saving to results/20190916_130447_baseline_nn_topo_and_code_feats.json
2019-09-16 13:06:52,619 - training_jobs - DEBUG - moved jobdict to done_trainings/task_186.yml
2019-09-16 13:06:52,619 - training_jobs - DEBUG - Finished!

2019-09-16 13:06:54,997 - training_jobs - DEBUG - training trains/task_122.yml
{'d1': 85,
 'd2': 20,
 'd3': 15,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'model': 'mlp2',
 'num_epochs': 30}
2019-09-16 13:06:55,027 - training_jobs - DEBUG - training with: 
2019-09-16 13:06:55,027 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-16 13:06:55,028 - training_jobs - DEBUG - mlp2
2019-09-16 13:06:55,028 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 13:06:55,028 - training_jobs - DEBUG - nlp_nn training
2019-09-16 13:06:55,924 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-16 13:09:11,324 - training_jobs - DEBUG - training time: 135s
2019-09-16 13:09:11,324 - training_jobs - DEBUG - saving to results/20190916_130655_nlp_nn_tfidf_and_topo_feats.json
2019-09-16 13:09:11,330 - training_jobs - DEBUG - moved jobdict to done_trainings/task_122.yml
2019-09-16 13:09:11,330 - training_jobs - DEBUG - Finished!

2019-09-16 13:09:13,740 - training_jobs - DEBUG - training trains/task_223.yml
{'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-16 13:09:13,753 - training_jobs - DEBUG - training with: 
2019-09-16 13:09:13,753 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-16 13:09:13,754 - training_jobs - DEBUG - RandomForestClassifier
2019-09-16 13:09:13,754 - training_jobs - DEBUG - topo and code feats
2019-09-16 13:09:13,754 - training_jobs - DEBUG - baseline training
2019-09-16 13:09:13,838 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 13:09:15,087 - training_jobs - DEBUG - training time: 1s
2019-09-16 13:09:15,087 - training_jobs - DEBUG - saving to results/20190916_130913_baseline_topo_and_code_feats.json
2019-09-16 13:09:15,096 - training_jobs - DEBUG - moved jobdict to done_trainings/task_223.yml
2019-09-16 13:09:15,096 - training_jobs - DEBUG - Finished!

2019-09-16 13:09:17,693 - training_jobs - DEBUG - training trains/task_38.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max/',
 'features': 'topo and code feats',
 'max_depth': 8,
 'model': 'XGBClassifier'}
2019-09-16 13:09:17,714 - training_jobs - DEBUG - training with: 
2019-09-16 13:09:17,714 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max/
2019-09-16 13:09:17,714 - training_jobs - DEBUG - XGBClassifier
2019-09-16 13:09:17,714 - training_jobs - DEBUG - topo and code feats
2019-09-16 13:09:17,714 - training_jobs - DEBUG - baseline training
2019-09-16 13:09:18,672 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 13:11:59,988 - training_jobs - DEBUG - training time: 161s
2019-09-16 13:11:59,989 - training_jobs - DEBUG - saving to results/20190916_130918_baseline_topo_and_code_feats.json
2019-09-16 13:12:00,009 - training_jobs - DEBUG - moved jobdict to done_trainings/task_38.yml
2019-09-16 13:12:00,010 - training_jobs - DEBUG - Finished!

2019-09-16 13:12:02,806 - training_jobs - DEBUG - training trains/task_178.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 13:12:02,849 - training_jobs - DEBUG - training with: 
2019-09-16 13:12:02,849 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-16 13:12:02,849 - training_jobs - DEBUG - GGNN1
2019-09-16 13:12:02,849 - training_jobs - DEBUG - 
2019-09-16 13:12:02,849 - training_jobs - DEBUG - ggnn training
2019-09-16 13:12:06,868 - training_jobs - DEBUG -  saving results to results/20190916_131206_ggnn_.json
2019-09-16 13:12:06,868 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 13:39:17,359 - training_jobs - DEBUG - test_multiple_models
2019-09-16 13:39:17,359 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-16 13:39:18,227 - training_jobs - DEBUG - training time: 1631s
2019-09-16 13:39:18,227 - training_jobs - DEBUG - saving to results/20190916_131206_ggnn_.json
2019-09-16 13:39:18,229 - training_jobs - DEBUG - moved jobdict to done_trainings/task_178.yml
2019-09-16 13:39:18,229 - training_jobs - DEBUG - Finished!

2019-09-16 13:39:20,983 - training_jobs - DEBUG - training trains/task_97.yml
{'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-16 13:39:21,090 - training_jobs - DEBUG - training with: 
2019-09-16 13:39:21,090 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-16 13:39:21,091 - training_jobs - DEBUG - RandomForestClassifier
2019-09-16 13:39:21,091 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 13:39:21,091 - training_jobs - DEBUG - nlp training
2019-09-16 13:39:21,578 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-16 13:39:23,111 - training_jobs - DEBUG - training time: 2s
2019-09-16 13:39:23,111 - training_jobs - DEBUG - saving to results/20190916_133921_nlp_tfidf_and_topo_feats.json
2019-09-16 13:39:23,188 - training_jobs - DEBUG - moved jobdict to done_trainings/task_97.yml
2019-09-16 13:39:23,188 - training_jobs - DEBUG - Finished!

2019-09-16 13:39:26,028 - training_jobs - DEBUG - training trains/task_16.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'x_topo_feats',
 'max_depth': 8,
 'model': 'XGBClassifier'}
2019-09-16 13:39:26,354 - training_jobs - DEBUG - training with: 
2019-09-16 13:39:26,355 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-16 13:39:26,355 - training_jobs - DEBUG - XGBClassifier
2019-09-16 13:39:26,355 - training_jobs - DEBUG - x_topo_feats
2019-09-16 13:39:26,355 - training_jobs - DEBUG - baseline training
2019-09-16 13:39:26,485 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 13:40:34,907 - training_jobs - DEBUG - training time: 68s
2019-09-16 13:40:34,907 - training_jobs - DEBUG - saving to results/20190916_133926_baseline_x_topo_feats.json
2019-09-16 13:40:34,918 - training_jobs - DEBUG - moved jobdict to done_trainings/task_16.yml
2019-09-16 13:40:34,918 - training_jobs - DEBUG - Finished!

2019-09-16 13:40:37,404 - training_jobs - DEBUG - training trains/task_102.yml
{'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-16 13:40:37,425 - training_jobs - DEBUG - training with: 
2019-09-16 13:40:37,425 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-16 13:40:37,425 - training_jobs - DEBUG - RandomForestClassifier
2019-09-16 13:40:37,425 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 13:40:37,425 - training_jobs - DEBUG - nlp training
2019-09-16 13:40:38,085 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-16 13:40:39,168 - training_jobs - DEBUG - training time: 1s
2019-09-16 13:40:39,168 - training_jobs - DEBUG - saving to results/20190916_134038_nlp_tfidf_and_topo_feats.json
2019-09-16 13:40:39,174 - training_jobs - DEBUG - moved jobdict to done_trainings/task_102.yml
2019-09-16 13:40:39,174 - training_jobs - DEBUG - Finished!

2019-09-16 13:40:41,559 - training_jobs - DEBUG - training trains/task_107.yml
{'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-16 13:40:41,570 - training_jobs - DEBUG - training with: 
2019-09-16 13:40:41,570 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 13:40:41,570 - training_jobs - DEBUG - RandomForestClassifier
2019-09-16 13:40:41,570 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 13:40:41,570 - training_jobs - DEBUG - nlp training
2019-09-16 13:40:41,971 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-16 13:40:47,360 - training_jobs - DEBUG - training time: 5s
2019-09-16 13:40:47,360 - training_jobs - DEBUG - saving to results/20190916_134041_nlp_tfidf_and_topo_feats.json
2019-09-16 13:40:47,366 - training_jobs - DEBUG - moved jobdict to done_trainings/task_107.yml
2019-09-16 13:40:47,366 - training_jobs - DEBUG - Finished!

2019-09-16 13:40:49,763 - training_jobs - DEBUG - training trains/task_195.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'model': 'mlp4',
 'num_epochs': 100}
2019-09-16 13:40:49,798 - training_jobs - DEBUG - training with: 
2019-09-16 13:40:49,798 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-16 13:40:49,798 - training_jobs - DEBUG - mlp4
2019-09-16 13:40:49,798 - training_jobs - DEBUG - topo and code feats
2019-09-16 13:40:49,799 - training_jobs - DEBUG - moved jobdict to done_trainings/task_195.yml
2019-09-16 13:40:49,799 - training_jobs - DEBUG - Finished!

2019-09-16 13:40:52,335 - training_jobs - DEBUG - training trains/task_70.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'max_depth': 8,
 'model': 'XGBClassifier'}
2019-09-16 13:40:52,370 - training_jobs - DEBUG - training with: 
2019-09-16 13:40:52,370 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-16 13:40:52,370 - training_jobs - DEBUG - XGBClassifier
2019-09-16 13:40:52,370 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 13:40:52,371 - training_jobs - DEBUG - nlp training
2019-09-16 13:40:52,984 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-16 13:44:49,863 - training_jobs - DEBUG - training time: 237s
2019-09-16 13:44:49,863 - training_jobs - DEBUG - saving to results/20190916_134052_nlp_tfidf_and_topo_feats.json
2019-09-16 13:44:49,867 - training_jobs - DEBUG - moved jobdict to done_trainings/task_70.yml
2019-09-16 13:44:49,868 - training_jobs - DEBUG - Finished!

2019-09-16 13:44:52,693 - training_jobs - DEBUG - training trains/task_246.yml
{'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-16 13:44:52,757 - training_jobs - DEBUG - training with: 
2019-09-16 13:44:52,757 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 13:44:52,757 - training_jobs - DEBUG - RandomForestClassifier
2019-09-16 13:44:52,757 - training_jobs - DEBUG - topo and code feats
2019-09-16 13:44:52,757 - training_jobs - DEBUG - baseline training
2019-09-16 13:44:52,966 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 13:44:54,773 - training_jobs - DEBUG - training time: 2s
2019-09-16 13:44:54,773 - training_jobs - DEBUG - saving to results/20190916_134452_baseline_topo_and_code_feats.json
2019-09-16 13:44:54,792 - training_jobs - DEBUG - moved jobdict to done_trainings/task_246.yml
2019-09-16 13:44:54,792 - training_jobs - DEBUG - Finished!

2019-09-16 13:44:57,841 - training_jobs - DEBUG - training trains/task_192.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'model': 'mlp4',
 'num_epochs': 30}
2019-09-16 13:44:58,082 - training_jobs - DEBUG - training with: 
2019-09-16 13:44:58,082 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-16 13:44:58,082 - training_jobs - DEBUG - mlp4
2019-09-16 13:44:58,082 - training_jobs - DEBUG - topo and code feats
2019-09-16 13:44:58,083 - training_jobs - DEBUG - moved jobdict to done_trainings/task_192.yml
2019-09-16 13:44:58,083 - training_jobs - DEBUG - Finished!

2019-09-16 13:45:01,077 - training_jobs - DEBUG - training trains/task_123.yml
{'d1': 85,
 'd2': 20,
 'd3': 15,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'model': 'mlp2',
 'num_epochs': 100}
2019-09-16 13:45:01,129 - training_jobs - DEBUG - training with: 
2019-09-16 13:45:01,129 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-16 13:45:01,129 - training_jobs - DEBUG - mlp2
2019-09-16 13:45:01,129 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 13:45:01,129 - training_jobs - DEBUG - nlp_nn training
2019-09-16 13:45:01,905 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-16 13:52:18,256 - training_jobs - DEBUG - training time: 436s
2019-09-16 13:52:18,256 - training_jobs - DEBUG - saving to results/20190916_134501_nlp_nn_tfidf_and_topo_feats.json
2019-09-16 13:52:18,262 - training_jobs - DEBUG - moved jobdict to done_trainings/task_123.yml
2019-09-16 13:52:18,262 - training_jobs - DEBUG - Finished!

2019-09-16 13:52:20,695 - training_jobs - DEBUG - training trains/task_137.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 13:52:20,718 - training_jobs - DEBUG - training with: 
2019-09-16 13:52:20,718 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 13:52:20,718 - training_jobs - DEBUG - GGNN1
2019-09-16 13:52:20,718 - training_jobs - DEBUG - 
2019-09-16 13:52:20,718 - training_jobs - DEBUG - ggnn training
2019-09-16 13:52:22,052 - training_jobs - DEBUG -  saving results to results/20190916_135222_ggnn_.json
2019-09-16 13:52:22,052 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 13:52:23,340 - training_jobs - ERROR - Error with trains/task_137.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-16 13:52:25,722 - training_jobs - DEBUG - training trains/task_21.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'x_topo_feats',
 'max_depth': 150,
 'model': 'XGBClassifier'}
2019-09-16 13:52:25,735 - training_jobs - DEBUG - training with: 
2019-09-16 13:52:25,735 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 13:52:25,735 - training_jobs - DEBUG - XGBClassifier
2019-09-16 13:52:25,736 - training_jobs - DEBUG - x_topo_feats
2019-09-16 13:52:25,736 - training_jobs - DEBUG - baseline training
2019-09-16 13:52:25,795 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 13:55:40,879 - training_jobs - DEBUG - training time: 195s
2019-09-16 13:55:40,879 - training_jobs - DEBUG - saving to results/20190916_135225_baseline_x_topo_feats.json
2019-09-16 13:55:40,888 - training_jobs - DEBUG - moved jobdict to done_trainings/task_21.yml
2019-09-16 13:55:40,888 - training_jobs - DEBUG - Finished!

2019-09-16 13:55:43,600 - training_jobs - DEBUG - training trains/task_89.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 13:55:43,636 - training_jobs - DEBUG - training with: 
2019-09-16 13:55:43,636 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 13:55:43,636 - training_jobs - DEBUG - GGNN6
2019-09-16 13:55:43,636 - training_jobs - DEBUG - 
2019-09-16 13:55:43,636 - training_jobs - DEBUG - ggnn training
2019-09-16 13:55:45,214 - training_jobs - DEBUG -  saving results to results/20190916_135545_ggnn_.json
2019-09-16 13:55:45,214 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 13:55:46,741 - training_jobs - ERROR - Error with trains/task_89.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-16 13:55:49,331 - training_jobs - DEBUG - training trains/task_277.yml
{'C': 1,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'max_iter': 100,
 'model': 'LogisticRegression',
 'multi_class': 'ovr',
 'penalty': 'l2',
 'solver': 'newton-cg'}
2019-09-16 13:55:49,346 - training_jobs - DEBUG - training with: 
2019-09-16 13:55:49,346 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-16 13:55:49,346 - training_jobs - DEBUG - LogisticRegression
2019-09-16 13:55:49,346 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 13:55:49,346 - training_jobs - DEBUG - nlp training
2019-09-16 13:55:50,102 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-16 13:56:24,512 - training_jobs - DEBUG - training time: 34s
2019-09-16 13:56:24,513 - training_jobs - DEBUG - saving to results/20190916_135550_nlp_tfidf_and_topo_feats.json
2019-09-16 13:56:24,518 - training_jobs - DEBUG - moved jobdict to done_trainings/task_277.yml
2019-09-16 13:56:24,518 - training_jobs - DEBUG - Finished!

2019-09-16 13:56:26,840 - training_jobs - DEBUG - training trains/task_190.yml
{'d1': 85,
 'd2': 20,
 'd3': 15,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'model': 'mlp2',
 'num_epochs': 30}
2019-09-16 13:56:26,988 - training_jobs - DEBUG - training with: 
2019-09-16 13:56:26,989 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-16 13:56:26,989 - training_jobs - DEBUG - mlp2
2019-09-16 13:56:26,989 - training_jobs - DEBUG - topo and code feats
2019-09-16 13:56:26,989 - training_jobs - DEBUG - baseline_nn training
2019-09-16 13:56:27,145 - training_jobs - DEBUG -  calling nn_train_models
2019-09-16 13:58:17,272 - training_jobs - DEBUG - training time: 110s
2019-09-16 13:58:17,272 - training_jobs - DEBUG - saving to results/20190916_135627_baseline_nn_topo_and_code_feats.json
2019-09-16 13:58:17,277 - training_jobs - DEBUG - moved jobdict to done_trainings/task_190.yml
2019-09-16 13:58:17,277 - training_jobs - DEBUG - Finished!

2019-09-16 13:58:19,636 - training_jobs - DEBUG - training trains/task_29.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'x_topo_feats',
 'max_depth': 150,
 'model': 'XGBClassifier'}
2019-09-16 13:58:19,678 - training_jobs - DEBUG - training with: 
2019-09-16 13:58:19,678 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-16 13:58:19,678 - training_jobs - DEBUG - XGBClassifier
2019-09-16 13:58:19,678 - training_jobs - DEBUG - x_topo_feats
2019-09-16 13:58:19,678 - training_jobs - DEBUG - baseline training
2019-09-16 13:58:19,740 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 13:59:08,668 - training_jobs - DEBUG - training time: 49s
2019-09-16 13:59:08,668 - training_jobs - DEBUG - saving to results/20190916_135819_baseline_x_topo_feats.json
2019-09-16 13:59:08,673 - training_jobs - DEBUG - moved jobdict to done_trainings/task_29.yml
2019-09-16 13:59:08,674 - training_jobs - DEBUG - Finished!

2019-09-16 13:59:11,729 - training_jobs - DEBUG - training trains/task_269.yml
{'C': 1,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'max_iter': 100,
 'model': 'LogisticRegression',
 'multi_class': 'ovr',
 'penalty': 'l2',
 'solver': 'newton-cg'}
2019-09-16 13:59:11,744 - training_jobs - DEBUG - training with: 
2019-09-16 13:59:11,744 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 13:59:11,744 - training_jobs - DEBUG - LogisticRegression
2019-09-16 13:59:11,744 - training_jobs - DEBUG - topo and code feats
2019-09-16 13:59:11,744 - training_jobs - DEBUG - baseline training
2019-09-16 13:59:11,849 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 14:01:45,041 - training_jobs - DEBUG - training time: 153s
2019-09-16 14:01:45,042 - training_jobs - DEBUG - saving to results/20190916_135911_baseline_topo_and_code_feats.json
2019-09-16 14:01:45,082 - training_jobs - DEBUG - moved jobdict to done_trainings/task_269.yml
2019-09-16 14:01:45,082 - training_jobs - DEBUG - Finished!

2019-09-16 14:01:47,471 - training_jobs - DEBUG - training trains/task_239.yml
{'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-16 14:01:47,506 - training_jobs - DEBUG - training with: 
2019-09-16 14:01:47,506 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 14:01:47,506 - training_jobs - DEBUG - RandomForestClassifier
2019-09-16 14:01:47,506 - training_jobs - DEBUG - topo and code feats
2019-09-16 14:01:47,507 - training_jobs - DEBUG - baseline training
2019-09-16 14:01:47,589 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 14:01:49,880 - training_jobs - DEBUG - training time: 2s
2019-09-16 14:01:49,880 - training_jobs - DEBUG - saving to results/20190916_140147_baseline_topo_and_code_feats.json
2019-09-16 14:01:49,891 - training_jobs - DEBUG - moved jobdict to done_trainings/task_239.yml
2019-09-16 14:01:49,891 - training_jobs - DEBUG - Finished!

2019-09-16 14:01:52,509 - training_jobs - DEBUG - training trains/task_216.yml
{'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'x_topo_feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-16 14:01:52,522 - training_jobs - DEBUG - training with: 
2019-09-16 14:01:52,522 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-16 14:01:52,523 - training_jobs - DEBUG - RandomForestClassifier
2019-09-16 14:01:52,523 - training_jobs - DEBUG - x_topo_feats
2019-09-16 14:01:52,523 - training_jobs - DEBUG - baseline training
2019-09-16 14:01:52,579 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 14:01:53,376 - training_jobs - DEBUG - training time: 1s
2019-09-16 14:01:53,376 - training_jobs - DEBUG - saving to results/20190916_140152_baseline_x_topo_feats.json
2019-09-16 14:01:53,385 - training_jobs - DEBUG - moved jobdict to done_trainings/task_216.yml
2019-09-16 14:01:53,385 - training_jobs - DEBUG - Finished!

2019-09-16 14:01:56,229 - training_jobs - DEBUG - training trains/task_43.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'max_depth': 150,
 'model': 'XGBClassifier'}
2019-09-16 14:01:56,242 - training_jobs - DEBUG - training with: 
2019-09-16 14:01:56,242 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 14:01:56,242 - training_jobs - DEBUG - XGBClassifier
2019-09-16 14:01:56,242 - training_jobs - DEBUG - topo and code feats
2019-09-16 14:01:56,242 - training_jobs - DEBUG - baseline training
2019-09-16 14:01:56,326 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 14:05:38,027 - training_jobs - DEBUG - training time: 222s
2019-09-16 14:05:38,028 - training_jobs - DEBUG - saving to results/20190916_140156_baseline_topo_and_code_feats.json
2019-09-16 14:05:38,036 - training_jobs - DEBUG - moved jobdict to done_trainings/task_43.yml
2019-09-16 14:05:38,036 - training_jobs - DEBUG - Finished!

2019-09-16 14:05:40,762 - training_jobs - DEBUG - training trains/task_140.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 14:05:40,810 - training_jobs - DEBUG - training with: 
2019-09-16 14:05:40,810 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-16 14:05:40,810 - training_jobs - DEBUG - GGNN1
2019-09-16 14:05:40,810 - training_jobs - DEBUG - 
2019-09-16 14:05:40,810 - training_jobs - DEBUG - ggnn training
2019-09-16 14:05:42,178 - training_jobs - DEBUG -  saving results to results/20190916_140542_ggnn_.json
2019-09-16 14:05:42,178 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 14:20:57,897 - training_jobs - DEBUG - test_multiple_models
2019-09-16 14:20:57,897 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-16 14:20:58,398 - training_jobs - DEBUG - training time: 916s
2019-09-16 14:20:58,398 - training_jobs - DEBUG - saving to results/20190916_140542_ggnn_.json
2019-09-16 14:20:58,400 - training_jobs - DEBUG - moved jobdict to done_trainings/task_140.yml
2019-09-16 14:20:58,400 - training_jobs - DEBUG - Finished!

2019-09-16 14:21:00,906 - training_jobs - DEBUG - training trains/task_267.yml
{'C': 1,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'max_iter': 100,
 'model': 'LogisticRegression',
 'multi_class': 'ovr',
 'penalty': 'l2',
 'solver': 'newton-cg'}
2019-09-16 14:21:00,919 - training_jobs - DEBUG - training with: 
2019-09-16 14:21:00,920 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-16 14:21:00,920 - training_jobs - DEBUG - LogisticRegression
2019-09-16 14:21:00,920 - training_jobs - DEBUG - topo and code feats
2019-09-16 14:21:00,920 - training_jobs - DEBUG - baseline training
2019-09-16 14:21:01,105 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 14:21:58,164 - training_jobs - DEBUG - training time: 57s
2019-09-16 14:21:58,165 - training_jobs - DEBUG - saving to results/20190916_142101_baseline_topo_and_code_feats.json
2019-09-16 14:21:58,190 - training_jobs - DEBUG - moved jobdict to done_trainings/task_267.yml
2019-09-16 14:21:58,192 - training_jobs - DEBUG - Finished!

2019-09-16 14:22:00,618 - training_jobs - DEBUG - training trains/task_59.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'model': 'mlp3',
 'num_epochs': 100}
2019-09-16 14:22:00,651 - training_jobs - DEBUG - training with: 
2019-09-16 14:22:00,651 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-16 14:22:00,651 - training_jobs - DEBUG - mlp3
2019-09-16 14:22:00,651 - training_jobs - DEBUG - topo and code feats
2019-09-16 14:22:00,652 - training_jobs - DEBUG - moved jobdict to done_trainings/task_59.yml
2019-09-16 14:22:00,652 - training_jobs - DEBUG - Finished!

2019-09-16 14:22:03,098 - training_jobs - DEBUG - training trains/task_37.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max/',
 'features': 'x_topo_feats',
 'max_depth': 150,
 'model': 'XGBClassifier'}
2019-09-16 14:22:03,112 - training_jobs - DEBUG - training with: 
2019-09-16 14:22:03,112 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max/
2019-09-16 14:22:03,112 - training_jobs - DEBUG - XGBClassifier
2019-09-16 14:22:03,112 - training_jobs - DEBUG - x_topo_feats
2019-09-16 14:22:03,113 - training_jobs - DEBUG - baseline training
2019-09-16 14:22:03,336 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 14:33:31,205 - training_jobs - DEBUG - training time: 688s
2019-09-16 14:33:31,205 - training_jobs - DEBUG - saving to results/20190916_142203_baseline_x_topo_feats.json
2019-09-16 14:33:31,227 - training_jobs - DEBUG - moved jobdict to done_trainings/task_37.yml
2019-09-16 14:33:31,227 - training_jobs - DEBUG - Finished!

2019-09-16 14:33:33,629 - training_jobs - DEBUG - training trains/task_46.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'max_depth': 8,
 'model': 'XGBClassifier'}
2019-09-16 14:33:33,641 - training_jobs - DEBUG - training with: 
2019-09-16 14:33:33,642 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-16 14:33:33,642 - training_jobs - DEBUG - XGBClassifier
2019-09-16 14:33:33,642 - training_jobs - DEBUG - topo and code feats
2019-09-16 14:33:33,642 - training_jobs - DEBUG - baseline training
2019-09-16 14:33:33,724 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 14:33:52,245 - training_jobs - DEBUG - training time: 19s
2019-09-16 14:33:52,245 - training_jobs - DEBUG - saving to results/20190916_143333_baseline_topo_and_code_feats.json
2019-09-16 14:33:52,277 - training_jobs - DEBUG - moved jobdict to done_trainings/task_46.yml
2019-09-16 14:33:52,277 - training_jobs - DEBUG - Finished!

2019-09-16 14:33:54,774 - training_jobs - DEBUG - training trains/task_145.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 14:33:54,786 - training_jobs - DEBUG - training with: 
2019-09-16 14:33:54,787 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-16 14:33:54,787 - training_jobs - DEBUG - GGNN1
2019-09-16 14:33:54,787 - training_jobs - DEBUG - 
2019-09-16 14:33:54,787 - training_jobs - DEBUG - ggnn training
2019-09-16 14:33:56,952 - training_jobs - DEBUG -  saving results to results/20190916_143356_ggnn_.json
2019-09-16 14:33:56,952 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 15:15:52,722 - training_jobs - DEBUG - test_multiple_models
2019-09-16 15:15:52,722 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-16 15:15:53,421 - training_jobs - DEBUG - training time: 2516s
2019-09-16 15:15:53,421 - training_jobs - DEBUG - saving to results/20190916_143356_ggnn_.json
2019-09-16 15:15:53,423 - training_jobs - DEBUG - moved jobdict to done_trainings/task_145.yml
2019-09-16 15:15:53,423 - training_jobs - DEBUG - Finished!

2019-09-16 15:15:56,392 - training_jobs - DEBUG - training trains/task_129.yml
{'d1': 85,
 'd2': 20,
 'd3': 15,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'model': 'mlp2',
 'num_epochs': 100}
2019-09-16 15:15:56,440 - training_jobs - DEBUG - training with: 
2019-09-16 15:15:56,440 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-16 15:15:56,441 - training_jobs - DEBUG - mlp2
2019-09-16 15:15:56,441 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 15:15:56,441 - training_jobs - DEBUG - nlp_nn training
2019-09-16 15:15:56,845 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-16 15:19:37,239 - training_jobs - DEBUG - training time: 220s
2019-09-16 15:19:37,240 - training_jobs - DEBUG - saving to results/20190916_151556_nlp_nn_tfidf_and_topo_feats.json
2019-09-16 15:19:37,243 - training_jobs - DEBUG - moved jobdict to done_trainings/task_129.yml
2019-09-16 15:19:37,243 - training_jobs - DEBUG - Finished!

2019-09-16 15:19:41,626 - training_jobs - DEBUG - training trains/task_134.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 15:19:41,650 - training_jobs - DEBUG - training with: 
2019-09-16 15:19:41,650 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-16 15:19:41,650 - training_jobs - DEBUG - GGNN1
2019-09-16 15:19:41,650 - training_jobs - DEBUG - 
2019-09-16 15:19:41,650 - training_jobs - DEBUG - ggnn training
2019-09-16 15:19:51,009 - training_jobs - DEBUG -  saving results to results/20190916_151951_ggnn_.json
2019-09-16 15:19:51,009 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 15:54:43,808 - training_jobs - DEBUG - test_multiple_models
2019-09-16 15:54:43,808 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-16 15:54:46,590 - training_jobs - DEBUG - training time: 2096s
2019-09-16 15:54:46,590 - training_jobs - DEBUG - saving to results/20190916_151951_ggnn_.json
2019-09-16 15:54:46,593 - training_jobs - DEBUG - moved jobdict to done_trainings/task_134.yml
2019-09-16 15:54:46,593 - training_jobs - DEBUG - Finished!

2019-09-16 15:54:49,526 - training_jobs - DEBUG - training trains/task_231.yml
{'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-16 15:54:49,773 - training_jobs - DEBUG - training with: 
2019-09-16 15:54:49,773 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-16 15:54:49,773 - training_jobs - DEBUG - RandomForestClassifier
2019-09-16 15:54:49,773 - training_jobs - DEBUG - topo and code feats
2019-09-16 15:54:49,774 - training_jobs - DEBUG - baseline training
2019-09-16 15:54:49,972 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 15:54:52,027 - training_jobs - DEBUG - training time: 2s
2019-09-16 15:54:52,027 - training_jobs - DEBUG - saving to results/20190916_155449_baseline_topo_and_code_feats.json
2019-09-16 15:54:52,041 - training_jobs - DEBUG - moved jobdict to done_trainings/task_231.yml
2019-09-16 15:54:52,041 - training_jobs - DEBUG - Finished!

2019-09-16 15:54:54,926 - training_jobs - DEBUG - training trains/task_245.yml
{'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-16 15:54:54,971 - training_jobs - DEBUG - training with: 
2019-09-16 15:54:54,972 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 15:54:54,972 - training_jobs - DEBUG - RandomForestClassifier
2019-09-16 15:54:54,972 - training_jobs - DEBUG - topo and code feats
2019-09-16 15:54:54,972 - training_jobs - DEBUG - baseline training
2019-09-16 15:54:55,153 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 15:54:57,131 - training_jobs - DEBUG - training time: 2s
2019-09-16 15:54:57,131 - training_jobs - DEBUG - saving to results/20190916_155455_baseline_topo_and_code_feats.json
2019-09-16 15:54:57,150 - training_jobs - DEBUG - moved jobdict to done_trainings/task_245.yml
2019-09-16 15:54:57,150 - training_jobs - DEBUG - Finished!

2019-09-16 15:54:59,963 - training_jobs - DEBUG - training trains/task_157.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 15:54:59,993 - training_jobs - DEBUG - training with: 
2019-09-16 15:54:59,993 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-16 15:54:59,993 - training_jobs - DEBUG - GGNN1
2019-09-16 15:54:59,993 - training_jobs - DEBUG - 
2019-09-16 15:54:59,993 - training_jobs - DEBUG - ggnn training
2019-09-16 15:55:01,761 - training_jobs - DEBUG -  saving results to results/20190916_155501_ggnn_.json
2019-09-16 15:55:01,761 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 16:39:49,465 - training_jobs - DEBUG - test_multiple_models
2019-09-16 16:39:49,465 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-16 16:39:50,350 - training_jobs - DEBUG - training time: 2689s
2019-09-16 16:39:50,351 - training_jobs - DEBUG - saving to results/20190916_155501_ggnn_.json
2019-09-16 16:39:50,353 - training_jobs - DEBUG - moved jobdict to done_trainings/task_157.yml
2019-09-16 16:39:50,354 - training_jobs - DEBUG - Finished!

2019-09-16 16:39:54,064 - training_jobs - DEBUG - training trains/task_51.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'model': 'mlp3',
 'num_epochs': 100}
2019-09-16 16:39:54,102 - training_jobs - DEBUG - training with: 
2019-09-16 16:39:54,102 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-16 16:39:54,102 - training_jobs - DEBUG - mlp3
2019-09-16 16:39:54,102 - training_jobs - DEBUG - topo and code feats
2019-09-16 16:39:54,104 - training_jobs - DEBUG - moved jobdict to done_trainings/task_51.yml
2019-09-16 16:39:54,104 - training_jobs - DEBUG - Finished!

2019-09-16 16:39:57,800 - training_jobs - DEBUG - training trains/task_279.yml
{'C': 1,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'max_iter': 100,
 'model': 'LogisticRegression',
 'multi_class': 'ovr',
 'penalty': 'l2',
 'solver': 'newton-cg'}
2019-09-16 16:39:57,837 - training_jobs - DEBUG - training with: 
2019-09-16 16:39:57,837 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 16:39:57,838 - training_jobs - DEBUG - LogisticRegression
2019-09-16 16:39:57,838 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 16:39:57,838 - training_jobs - DEBUG - nlp training
2019-09-16 16:39:58,811 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-16 16:41:57,678 - training_jobs - DEBUG - training time: 119s
2019-09-16 16:41:57,678 - training_jobs - DEBUG - saving to results/20190916_163958_nlp_tfidf_and_topo_feats.json
2019-09-16 16:41:57,685 - training_jobs - DEBUG - moved jobdict to done_trainings/task_279.yml
2019-09-16 16:41:57,685 - training_jobs - DEBUG - Finished!

2019-09-16 16:42:00,002 - training_jobs - DEBUG - training trains/task_149.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 16:42:00,035 - training_jobs - DEBUG - training with: 
2019-09-16 16:42:00,035 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 16:42:00,035 - training_jobs - DEBUG - GGNN1
2019-09-16 16:42:00,035 - training_jobs - DEBUG - 
2019-09-16 16:42:00,035 - training_jobs - DEBUG - ggnn training
2019-09-16 16:42:02,605 - training_jobs - DEBUG -  saving results to results/20190916_164202_ggnn_.json
2019-09-16 16:42:02,605 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 16:42:03,855 - training_jobs - ERROR - Error with trains/task_149.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-16 16:42:06,250 - training_jobs - DEBUG - training trains/task_60.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'max_depth': 8,
 'model': 'XGBClassifier'}
2019-09-16 16:42:06,332 - training_jobs - DEBUG - training with: 
2019-09-16 16:42:06,332 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-16 16:42:06,332 - training_jobs - DEBUG - XGBClassifier
2019-09-16 16:42:06,332 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 16:42:06,332 - training_jobs - DEBUG - nlp training
2019-09-16 16:42:06,819 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-16 16:48:41,035 - training_jobs - DEBUG - training time: 394s
2019-09-16 16:48:41,035 - training_jobs - DEBUG - saving to results/20190916_164206_nlp_tfidf_and_topo_feats.json
2019-09-16 16:48:41,039 - training_jobs - DEBUG - moved jobdict to done_trainings/task_60.yml
2019-09-16 16:48:41,039 - training_jobs - DEBUG - Finished!

2019-09-16 16:48:43,686 - training_jobs - DEBUG - training trains/task_84.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 16:48:43,701 - training_jobs - DEBUG - training with: 
2019-09-16 16:48:43,701 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-16 16:48:43,701 - training_jobs - DEBUG - GGNN6
2019-09-16 16:48:43,701 - training_jobs - DEBUG - 
2019-09-16 16:48:43,701 - training_jobs - DEBUG - ggnn training
2019-09-16 16:48:45,653 - training_jobs - DEBUG -  saving results to results/20190916_164845_ggnn_.json
2019-09-16 16:48:45,653 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 17:10:59,230 - training_jobs - DEBUG - test_multiple_models
2019-09-16 17:10:59,230 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-16 17:10:59,876 - training_jobs - DEBUG - training time: 1334s
2019-09-16 17:10:59,877 - training_jobs - DEBUG - saving to results/20190916_164845_ggnn_.json
2019-09-16 17:10:59,879 - training_jobs - DEBUG - moved jobdict to done_trainings/task_84.yml
2019-09-16 17:10:59,879 - training_jobs - DEBUG - Finished!

2019-09-16 17:11:02,298 - training_jobs - DEBUG - training trains/task_220.yml
{'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-16 17:11:02,309 - training_jobs - DEBUG - training with: 
2019-09-16 17:11:02,309 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-16 17:11:02,310 - training_jobs - DEBUG - RandomForestClassifier
2019-09-16 17:11:02,310 - training_jobs - DEBUG - topo and code feats
2019-09-16 17:11:02,310 - training_jobs - DEBUG - baseline training
2019-09-16 17:11:02,395 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 17:11:03,054 - training_jobs - DEBUG - training time: 1s
2019-09-16 17:11:03,054 - training_jobs - DEBUG - saving to results/20190916_171102_baseline_topo_and_code_feats.json
2019-09-16 17:11:03,062 - training_jobs - DEBUG - moved jobdict to done_trainings/task_220.yml
2019-09-16 17:11:03,062 - training_jobs - DEBUG - Finished!

2019-09-16 17:11:05,590 - training_jobs - DEBUG - training trains/task_253.yml
{'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'max_depth': 8,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-16 17:11:05,633 - training_jobs - DEBUG - training with: 
2019-09-16 17:11:05,633 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-16 17:11:05,633 - training_jobs - DEBUG - RandomForestClassifier
2019-09-16 17:11:05,633 - training_jobs - DEBUG - topo and code feats
2019-09-16 17:11:05,633 - training_jobs - DEBUG - baseline training
2019-09-16 17:11:05,708 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 17:11:06,567 - training_jobs - DEBUG - training time: 1s
2019-09-16 17:11:06,568 - training_jobs - DEBUG - saving to results/20190916_171105_baseline_topo_and_code_feats.json
2019-09-16 17:11:06,574 - training_jobs - DEBUG - moved jobdict to done_trainings/task_253.yml
2019-09-16 17:11:06,574 - training_jobs - DEBUG - Finished!

2019-09-16 17:11:09,119 - training_jobs - DEBUG - training trains/task_13.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'x_topo_feats',
 'max_depth': 150,
 'model': 'XGBClassifier'}
2019-09-16 17:11:09,224 - training_jobs - DEBUG - training with: 
2019-09-16 17:11:09,224 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-16 17:11:09,224 - training_jobs - DEBUG - XGBClassifier
2019-09-16 17:11:09,224 - training_jobs - DEBUG - x_topo_feats
2019-09-16 17:11:09,224 - training_jobs - DEBUG - baseline training
2019-09-16 17:11:09,276 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 17:12:59,371 - training_jobs - DEBUG - training time: 110s
2019-09-16 17:12:59,371 - training_jobs - DEBUG - saving to results/20190916_171109_baseline_x_topo_feats.json
2019-09-16 17:12:59,378 - training_jobs - DEBUG - moved jobdict to done_trainings/task_13.yml
2019-09-16 17:12:59,378 - training_jobs - DEBUG - Finished!

2019-09-16 17:13:02,370 - training_jobs - DEBUG - training trains/task_75.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 15,
 'd4': 10,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 17:13:02,413 - training_jobs - DEBUG - training with: 
2019-09-16 17:13:02,413 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-16 17:13:02,413 - training_jobs - DEBUG - GGNN5
2019-09-16 17:13:02,413 - training_jobs - DEBUG - 
2019-09-16 17:13:02,413 - training_jobs - DEBUG - ggnn training
2019-09-16 17:13:05,801 - training_jobs - DEBUG -  saving results to results/20190916_171305_ggnn_.json
2019-09-16 17:13:05,801 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 18:24:37,122 - training_jobs - DEBUG - test_multiple_models
2019-09-16 18:24:37,122 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-16 18:24:38,278 - training_jobs - DEBUG - training time: 4292s
2019-09-16 18:24:38,278 - training_jobs - DEBUG - saving to results/20190916_171305_ggnn_.json
2019-09-16 18:24:38,280 - training_jobs - DEBUG - moved jobdict to done_trainings/task_75.yml
2019-09-16 18:24:38,280 - training_jobs - DEBUG - Finished!

2019-09-16 18:24:40,807 - training_jobs - DEBUG - training trains/task_154.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 18:24:40,848 - training_jobs - DEBUG - training with: 
2019-09-16 18:24:40,848 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-16 18:24:40,848 - training_jobs - DEBUG - GGNN1
2019-09-16 18:24:40,848 - training_jobs - DEBUG - 
2019-09-16 18:24:40,848 - training_jobs - DEBUG - ggnn training
2019-09-16 18:24:50,284 - training_jobs - DEBUG -  saving results to results/20190916_182450_ggnn_.json
2019-09-16 18:24:50,284 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 18:50:26,015 - training_jobs - DEBUG - test_multiple_models
2019-09-16 18:50:26,015 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-16 18:50:26,972 - training_jobs - DEBUG - training time: 1537s
2019-09-16 18:50:26,972 - training_jobs - DEBUG - saving to results/20190916_182450_ggnn_.json
2019-09-16 18:50:26,975 - training_jobs - DEBUG - moved jobdict to done_trainings/task_154.yml
2019-09-16 18:50:26,975 - training_jobs - DEBUG - Finished!

2019-09-16 18:50:29,991 - training_jobs - DEBUG - training trains/task_88.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 18:50:30,030 - training_jobs - DEBUG - training with: 
2019-09-16 18:50:30,030 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 18:50:30,030 - training_jobs - DEBUG - GGNN6
2019-09-16 18:50:30,031 - training_jobs - DEBUG - 
2019-09-16 18:50:30,031 - training_jobs - DEBUG - ggnn training
2019-09-16 18:50:33,321 - training_jobs - DEBUG -  saving results to results/20190916_185033_ggnn_.json
2019-09-16 18:50:33,321 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 18:50:34,883 - training_jobs - ERROR - Error with trains/task_88.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-16 18:50:37,787 - training_jobs - DEBUG - training trains/task_227.yml
{'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'x_topo_feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 16}
2019-09-16 18:50:37,831 - training_jobs - DEBUG - training with: 
2019-09-16 18:50:37,832 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-16 18:50:37,832 - training_jobs - DEBUG - RandomForestClassifier
2019-09-16 18:50:37,832 - training_jobs - DEBUG - x_topo_feats
2019-09-16 18:50:37,832 - training_jobs - DEBUG - baseline training
2019-09-16 18:50:37,978 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 18:50:40,351 - training_jobs - DEBUG - training time: 2s
2019-09-16 18:50:40,351 - training_jobs - DEBUG - saving to results/20190916_185037_baseline_x_topo_feats.json
2019-09-16 18:50:40,368 - training_jobs - DEBUG - moved jobdict to done_trainings/task_227.yml
2019-09-16 18:50:40,368 - training_jobs - DEBUG - Finished!

2019-09-16 18:50:43,096 - training_jobs - DEBUG - training trains/task_230.yml
{'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'topo and code feats',
 'max_depth': 150,
 'model': 'RandomForestClassifier',
 'n_estimators': 4}
2019-09-16 18:50:43,116 - training_jobs - DEBUG - training with: 
2019-09-16 18:50:43,117 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-16 18:50:43,117 - training_jobs - DEBUG - RandomForestClassifier
2019-09-16 18:50:43,117 - training_jobs - DEBUG - topo and code feats
2019-09-16 18:50:43,117 - training_jobs - DEBUG - baseline training
2019-09-16 18:50:43,307 - training_jobs - DEBUG -  calling cv_train_models
2019-09-16 18:50:44,282 - training_jobs - DEBUG - training time: 1s
2019-09-16 18:50:44,282 - training_jobs - DEBUG - saving to results/20190916_185043_baseline_topo_and_code_feats.json
2019-09-16 18:50:44,296 - training_jobs - DEBUG - moved jobdict to done_trainings/task_230.yml
2019-09-16 18:50:44,296 - training_jobs - DEBUG - Finished!

2019-09-16 18:50:46,964 - training_jobs - DEBUG - training trains/task_185.yml
{'d1': 85,
 'd2': 20,
 'd3': 15,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'topo and code feats',
 'model': 'mlp2',
 'num_epochs': 100}
2019-09-16 18:50:47,004 - training_jobs - DEBUG - training with: 
2019-09-16 18:50:47,004 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 18:50:47,004 - training_jobs - DEBUG - mlp2
2019-09-16 18:50:47,004 - training_jobs - DEBUG - topo and code feats
2019-09-16 18:50:47,004 - training_jobs - DEBUG - baseline_nn training
2019-09-16 18:50:47,092 - training_jobs - DEBUG -  calling nn_train_models
2019-09-16 18:54:53,598 - training_jobs - DEBUG - training time: 247s
2019-09-16 18:54:53,598 - training_jobs - DEBUG - saving to results/20190916_185047_baseline_nn_topo_and_code_feats.json
2019-09-16 18:54:53,603 - training_jobs - DEBUG - moved jobdict to done_trainings/task_185.yml
2019-09-16 18:54:53,603 - training_jobs - DEBUG - Finished!

2019-09-16 18:54:55,963 - training_jobs - DEBUG - training trains/task_69.yml
{'booster': 'gbtree',
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'max_depth': 150,
 'model': 'XGBClassifier'}
2019-09-16 18:54:56,002 - training_jobs - DEBUG - training with: 
2019-09-16 18:54:56,002 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-16 18:54:56,002 - training_jobs - DEBUG - XGBClassifier
2019-09-16 18:54:56,002 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 18:54:56,002 - training_jobs - DEBUG - nlp training
2019-09-16 18:54:56,331 - training_jobs - DEBUG -  calling cv_train_nlp_models_v2
2019-09-16 18:59:18,132 - training_jobs - DEBUG - training time: 262s
2019-09-16 18:59:18,132 - training_jobs - DEBUG - saving to results/20190916_185456_nlp_tfidf_and_topo_feats.json
2019-09-16 18:59:18,135 - training_jobs - DEBUG - moved jobdict to done_trainings/task_69.yml
2019-09-16 18:59:18,136 - training_jobs - DEBUG - Finished!

2019-09-16 18:59:21,108 - training_jobs - DEBUG - training trains/task_172.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 18:59:21,149 - training_jobs - DEBUG - training with: 
2019-09-16 18:59:21,149 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 18:59:21,149 - training_jobs - DEBUG - GGNN1
2019-09-16 18:59:21,150 - training_jobs - DEBUG - 
2019-09-16 18:59:21,150 - training_jobs - DEBUG - ggnn training
2019-09-16 18:59:22,925 - training_jobs - DEBUG -  saving results to results/20190916_185922_ggnn_.json
2019-09-16 18:59:22,925 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 18:59:24,630 - training_jobs - ERROR - Error with trains/task_172.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1460, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 709, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 602, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 169
2019-09-16 18:59:27,701 - training_jobs - DEBUG - training trains/task_146.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 18:59:27,728 - training_jobs - DEBUG - training with: 
2019-09-16 18:59:27,728 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-16 18:59:27,728 - training_jobs - DEBUG - GGNN1
2019-09-16 18:59:27,728 - training_jobs - DEBUG - 
2019-09-16 18:59:27,728 - training_jobs - DEBUG - ggnn training
2019-09-16 18:59:30,945 - training_jobs - DEBUG -  saving results to results/20190916_185930_ggnn_.json
2019-09-16 18:59:30,945 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 19:34:28,319 - training_jobs - DEBUG - test_multiple_models
2019-09-16 19:34:28,319 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-16 19:34:29,846 - training_jobs - DEBUG - training time: 2099s
2019-09-16 19:34:29,846 - training_jobs - DEBUG - saving to results/20190916_185930_ggnn_.json
2019-09-16 19:34:29,849 - training_jobs - DEBUG - moved jobdict to done_trainings/task_146.yml
2019-09-16 19:34:29,850 - training_jobs - DEBUG - Finished!

2019-09-16 19:34:33,430 - training_jobs - DEBUG - training trains/task_205.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'model': 'mlp4',
 'num_epochs': 100}
2019-09-16 19:34:33,459 - training_jobs - DEBUG - training with: 
2019-09-16 19:34:33,459 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-16 19:34:33,459 - training_jobs - DEBUG - mlp4
2019-09-16 19:34:33,459 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 19:34:33,459 - training_jobs - DEBUG - nlp_nn training
2019-09-16 19:34:34,086 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-16 19:34:34,102 - training_jobs - ERROR - Error with trains/task_205.yml
Traceback (most recent call last):
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_nlp_models.py", line 955, in cv_train_nn_nlp_models_v2
    results_dict = train_nn_model_cv(X_train_embedding, y_train, X_test_embedding, y_test, param_set, scores, nclasses, numfolds=3 )
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1671, in train_nn_model_cv
    cv_fold_results_dict, _ =  train_nn_model_one_fold(X_train2, y_train, X_test2, y_test, nn_model_params, scores, nclasses )
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1488, in train_nn_model_one_fold
    **nn_model_params['model_kwargs']
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_baseline_models.py", line 1167, in __init__
    super(mlp2, self).__init__()
TypeError: super(type, obj): obj must be an instance or subtype of type

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 448, in training_dispatcher
    results_dict = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_nlp_models.py", line 958, in cv_train_nn_nlp_models_v2
    print("Error with "+param_set+" and "+scores[0])
TypeError: must be str, not dict
2019-09-16 20:54:23,759 - training_jobs - DEBUG - training trains/task_139.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 20:54:23,771 - training_jobs - DEBUG - training with: 
2019-09-16 20:54:23,771 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 20:54:23,771 - training_jobs - DEBUG - GGNN1
2019-09-16 20:54:23,771 - training_jobs - DEBUG - 
2019-09-16 20:54:23,771 - training_jobs - DEBUG - ggnn training
2019-09-16 20:54:26,922 - training_jobs - DEBUG -  saving results to results/20190916_205426_ggnn_.json
2019-09-16 20:54:26,922 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 20:54:29,064 - training_jobs - ERROR - Error with trains/task_139.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1461, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 710, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 599, in balancedDatasetKfoldSplit_slice
    list_classes = list(set(graph.y.tolist()))
UnboundLocalError: local variable 'graph' referenced before assignment
2019-09-16 20:54:31,627 - training_jobs - DEBUG - training trains/task_162.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 20:54:31,639 - training_jobs - DEBUG - training with: 
2019-09-16 20:54:31,639 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 20:54:31,639 - training_jobs - DEBUG - GGNN1
2019-09-16 20:54:31,639 - training_jobs - DEBUG - 
2019-09-16 20:54:31,639 - training_jobs - DEBUG - ggnn training
2019-09-16 20:54:33,908 - training_jobs - DEBUG -  saving results to results/20190916_205433_ggnn_.json
2019-09-16 20:54:33,908 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 20:54:36,004 - training_jobs - ERROR - Error with trains/task_162.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1461, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 710, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 599, in balancedDatasetKfoldSplit_slice
    list_classes = list(set(graph.y.tolist()))
UnboundLocalError: local variable 'graph' referenced before assignment
2019-09-16 20:54:38,331 - training_jobs - DEBUG - training trains/task_161.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 20:54:38,343 - training_jobs - DEBUG - training with: 
2019-09-16 20:54:38,343 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 20:54:38,343 - training_jobs - DEBUG - GGNN1
2019-09-16 20:54:38,343 - training_jobs - DEBUG - 
2019-09-16 20:54:38,343 - training_jobs - DEBUG - ggnn training
2019-09-16 20:54:39,678 - training_jobs - DEBUG -  saving results to results/20190916_205439_ggnn_.json
2019-09-16 20:54:39,678 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 20:54:40,928 - training_jobs - ERROR - Error with trains/task_161.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1461, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 710, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 599, in balancedDatasetKfoldSplit_slice
    list_classes = list(set(graph.y.tolist()))
UnboundLocalError: local variable 'graph' referenced before assignment
2019-09-16 20:54:43,259 - training_jobs - DEBUG - training trains/task_136.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 20:54:43,270 - training_jobs - DEBUG - training with: 
2019-09-16 20:54:43,270 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 20:54:43,270 - training_jobs - DEBUG - GGNN1
2019-09-16 20:54:43,270 - training_jobs - DEBUG - 
2019-09-16 20:54:43,270 - training_jobs - DEBUG - ggnn training
2019-09-16 20:54:44,602 - training_jobs - DEBUG -  saving results to results/20190916_205444_ggnn_.json
2019-09-16 20:54:44,602 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 20:54:45,849 - training_jobs - ERROR - Error with trains/task_136.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1461, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 710, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 599, in balancedDatasetKfoldSplit_slice
    list_classes = list(set(graph.y.tolist()))
UnboundLocalError: local variable 'graph' referenced before assignment
2019-09-16 20:54:48,174 - training_jobs - DEBUG - training trains/task_173.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 20:54:48,187 - training_jobs - DEBUG - training with: 
2019-09-16 20:54:48,187 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 20:54:48,187 - training_jobs - DEBUG - GGNN1
2019-09-16 20:54:48,187 - training_jobs - DEBUG - 
2019-09-16 20:54:48,187 - training_jobs - DEBUG - ggnn training
2019-09-16 20:54:49,522 - training_jobs - DEBUG -  saving results to results/20190916_205449_ggnn_.json
2019-09-16 20:54:49,522 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 20:54:50,792 - training_jobs - ERROR - Error with trains/task_173.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1461, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 710, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 599, in balancedDatasetKfoldSplit_slice
    list_classes = list(set(graph.y.tolist()))
UnboundLocalError: local variable 'graph' referenced before assignment
2019-09-16 20:54:53,180 - training_jobs - DEBUG - training trains/task_160.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 20:54:53,192 - training_jobs - DEBUG - training with: 
2019-09-16 20:54:53,192 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 20:54:53,192 - training_jobs - DEBUG - GGNN1
2019-09-16 20:54:53,192 - training_jobs - DEBUG - 
2019-09-16 20:54:53,192 - training_jobs - DEBUG - ggnn training
2019-09-16 20:54:54,526 - training_jobs - DEBUG -  saving results to results/20190916_205454_ggnn_.json
2019-09-16 20:54:54,527 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 20:54:55,769 - training_jobs - ERROR - Error with trains/task_160.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1461, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 710, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 599, in balancedDatasetKfoldSplit_slice
    list_classes = list(set(graph.y.tolist()))
UnboundLocalError: local variable 'graph' referenced before assignment
2019-09-16 20:54:58,077 - training_jobs - DEBUG - training trains/task_76.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 15,
 'd4': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 20:54:58,089 - training_jobs - DEBUG - training with: 
2019-09-16 20:54:58,089 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 20:54:58,090 - training_jobs - DEBUG - GGNN5
2019-09-16 20:54:58,090 - training_jobs - DEBUG - 
2019-09-16 20:54:58,090 - training_jobs - DEBUG - ggnn training
2019-09-16 20:54:59,421 - training_jobs - DEBUG -  saving results to results/20190916_205459_ggnn_.json
2019-09-16 20:54:59,421 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 20:55:00,675 - training_jobs - ERROR - Error with trains/task_76.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1461, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 710, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 599, in balancedDatasetKfoldSplit_slice
    list_classes = list(set(graph.y.tolist()))
UnboundLocalError: local variable 'graph' referenced before assignment
2019-09-16 20:55:03,021 - training_jobs - DEBUG - training trains/task_150.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 20:55:03,033 - training_jobs - DEBUG - training with: 
2019-09-16 20:55:03,033 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 20:55:03,033 - training_jobs - DEBUG - GGNN1
2019-09-16 20:55:03,033 - training_jobs - DEBUG - 
2019-09-16 20:55:03,033 - training_jobs - DEBUG - ggnn training
2019-09-16 20:55:05,236 - training_jobs - DEBUG -  saving results to results/20190916_205505_ggnn_.json
2019-09-16 20:55:05,236 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 20:55:07,315 - training_jobs - ERROR - Error with trains/task_150.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1461, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 710, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 599, in balancedDatasetKfoldSplit_slice
    list_classes = list(set(graph.y.tolist()))
UnboundLocalError: local variable 'graph' referenced before assignment
2019-09-16 20:55:09,728 - training_jobs - DEBUG - training trains/task_208.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'model': 'mlp4',
 'num_epochs': 30}
2019-09-16 20:55:09,740 - training_jobs - DEBUG - training with: 
2019-09-16 20:55:09,740 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 20:55:09,740 - training_jobs - DEBUG - mlp4
2019-09-16 20:55:09,741 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 20:55:09,741 - training_jobs - DEBUG - nlp_nn training
2019-09-16 20:55:10,142 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-16 20:56:53,990 - training_jobs - DEBUG - training time: 104s
2019-09-16 20:56:53,990 - training_jobs - DEBUG - saving to results/20190916_205510_nlp_nn_tfidf_and_topo_feats.json
2019-09-16 20:56:53,996 - training_jobs - DEBUG - moved jobdict to done_trainings/task_208.yml
2019-09-16 20:56:53,996 - training_jobs - DEBUG - Finished!

2019-09-16 20:56:56,535 - training_jobs - DEBUG - training trains/task_211.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'model': 'mlp4',
 'num_epochs': 100}
2019-09-16 20:56:56,546 - training_jobs - DEBUG - training with: 
2019-09-16 20:56:56,546 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 20:56:56,546 - training_jobs - DEBUG - mlp4
2019-09-16 20:56:56,546 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 20:56:56,546 - training_jobs - DEBUG - nlp_nn training
2019-09-16 20:56:57,198 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-16 21:06:07,075 - training_jobs - DEBUG - training time: 550s
2019-09-16 21:06:07,075 - training_jobs - DEBUG - saving to results/20190916_205657_nlp_nn_tfidf_and_topo_feats.json
2019-09-16 21:06:07,083 - training_jobs - DEBUG - moved jobdict to done_trainings/task_211.yml
2019-09-16 21:06:07,083 - training_jobs - DEBUG - Finished!

2019-09-16 21:06:09,685 - training_jobs - DEBUG - training trains/task_174.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 21:06:09,734 - training_jobs - DEBUG - training with: 
2019-09-16 21:06:09,734 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 21:06:09,734 - training_jobs - DEBUG - GGNN1
2019-09-16 21:06:09,734 - training_jobs - DEBUG - 
2019-09-16 21:06:09,734 - training_jobs - DEBUG - ggnn training
2019-09-16 21:06:14,808 - training_jobs - DEBUG -  saving results to results/20190916_210614_ggnn_.json
2019-09-16 21:06:14,808 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 21:06:16,899 - training_jobs - ERROR - Error with trains/task_174.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1461, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 710, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 599, in balancedDatasetKfoldSplit_slice
    list_classes = list(set(graph.y.tolist()))
UnboundLocalError: local variable 'graph' referenced before assignment
2019-09-16 21:06:19,323 - training_jobs - DEBUG - training trains/task_148.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 21:06:19,353 - training_jobs - DEBUG - training with: 
2019-09-16 21:06:19,353 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 21:06:19,353 - training_jobs - DEBUG - GGNN1
2019-09-16 21:06:19,353 - training_jobs - DEBUG - 
2019-09-16 21:06:19,353 - training_jobs - DEBUG - ggnn training
2019-09-16 21:06:20,675 - training_jobs - DEBUG -  saving results to results/20190916_210620_ggnn_.json
2019-09-16 21:06:20,676 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 21:06:21,925 - training_jobs - ERROR - Error with trains/task_148.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1461, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 710, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 599, in balancedDatasetKfoldSplit_slice
    list_classes = list(set(graph.y.tolist()))
UnboundLocalError: local variable 'graph' referenced before assignment
2019-09-16 21:06:24,269 - training_jobs - DEBUG - training trains/task_212.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'model': 'mlp4',
 'num_epochs': 50}
2019-09-16 21:06:24,555 - training_jobs - DEBUG - training with: 
2019-09-16 21:06:24,555 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-16 21:06:24,556 - training_jobs - DEBUG - mlp4
2019-09-16 21:06:24,556 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 21:06:24,556 - training_jobs - DEBUG - nlp_nn training
2019-09-16 21:06:24,926 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-16 21:08:54,719 - training_jobs - DEBUG - training time: 150s
2019-09-16 21:08:54,719 - training_jobs - DEBUG - saving to results/20190916_210624_nlp_nn_tfidf_and_topo_feats.json
2019-09-16 21:08:54,723 - training_jobs - DEBUG - moved jobdict to done_trainings/task_212.yml
2019-09-16 21:08:54,723 - training_jobs - DEBUG - Finished!

2019-09-16 21:08:57,305 - training_jobs - DEBUG - training trains/task_206.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'model': 'mlp4',
 'num_epochs': 30}
2019-09-16 21:08:57,358 - training_jobs - DEBUG - training with: 
2019-09-16 21:08:57,358 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-16 21:08:57,358 - training_jobs - DEBUG - mlp4
2019-09-16 21:08:57,358 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 21:08:57,358 - training_jobs - DEBUG - nlp_nn training
2019-09-16 21:08:58,080 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-16 21:12:04,747 - training_jobs - DEBUG - training time: 187s
2019-09-16 21:12:04,747 - training_jobs - DEBUG - saving to results/20190916_210858_nlp_nn_tfidf_and_topo_feats.json
2019-09-16 21:12:04,754 - training_jobs - DEBUG - moved jobdict to done_trainings/task_206.yml
2019-09-16 21:12:04,754 - training_jobs - DEBUG - Finished!

2019-09-16 21:12:07,244 - training_jobs - DEBUG - training trains/task_163.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 21:12:07,284 - training_jobs - DEBUG - training with: 
2019-09-16 21:12:07,284 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 21:12:07,284 - training_jobs - DEBUG - GGNN1
2019-09-16 21:12:07,284 - training_jobs - DEBUG - 
2019-09-16 21:12:07,284 - training_jobs - DEBUG - ggnn training
2019-09-16 21:12:09,884 - training_jobs - DEBUG -  saving results to results/20190916_211209_ggnn_.json
2019-09-16 21:12:09,884 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 21:12:14,412 - training_jobs - ERROR - Error with trains/task_163.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1461, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 710, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 603, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 38
2019-09-16 21:12:17,192 - training_jobs - DEBUG - training trains/task_9.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'model': 'mlp3',
 'num_epochs': 100}
2019-09-16 21:12:17,205 - training_jobs - DEBUG - training with: 
2019-09-16 21:12:17,205 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-16 21:12:17,205 - training_jobs - DEBUG - mlp3
2019-09-16 21:12:17,206 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 21:12:17,206 - training_jobs - DEBUG - nlp_nn training
2019-09-16 21:12:17,561 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-16 21:16:54,440 - training_jobs - DEBUG - training time: 277s
2019-09-16 21:16:54,440 - training_jobs - DEBUG - saving to results/20190916_211217_nlp_nn_tfidf_and_topo_feats.json
2019-09-16 21:16:54,450 - training_jobs - DEBUG - moved jobdict to done_trainings/task_9.yml
2019-09-16 21:16:54,450 - training_jobs - DEBUG - Finished!

2019-09-16 21:16:57,227 - training_jobs - DEBUG - training trains/task_8.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'model': 'mlp3',
 'num_epochs': 200}
2019-09-16 21:16:57,420 - training_jobs - DEBUG - training with: 
2019-09-16 21:16:57,420 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-16 21:16:57,420 - training_jobs - DEBUG - mlp3
2019-09-16 21:16:57,420 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 21:16:57,420 - training_jobs - DEBUG - nlp_nn training
2019-09-16 21:16:57,779 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-16 21:25:34,484 - training_jobs - DEBUG - training time: 517s
2019-09-16 21:25:34,484 - training_jobs - DEBUG - saving to results/20190916_211657_nlp_nn_tfidf_and_topo_feats.json
2019-09-16 21:25:34,487 - training_jobs - DEBUG - moved jobdict to done_trainings/task_8.yml
2019-09-16 21:25:34,487 - training_jobs - DEBUG - Finished!

2019-09-16 21:25:36,893 - training_jobs - DEBUG - training trains/task_210.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'model': 'mlp4',
 'num_epochs': 50}
2019-09-16 21:25:36,933 - training_jobs - DEBUG - training with: 
2019-09-16 21:25:36,934 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 21:25:36,934 - training_jobs - DEBUG - mlp4
2019-09-16 21:25:36,934 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 21:25:36,934 - training_jobs - DEBUG - nlp_nn training
2019-09-16 21:25:37,578 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-16 21:30:13,859 - training_jobs - DEBUG - training time: 276s
2019-09-16 21:30:13,860 - training_jobs - DEBUG - saving to results/20190916_212537_nlp_nn_tfidf_and_topo_feats.json
2019-09-16 21:30:13,867 - training_jobs - DEBUG - moved jobdict to done_trainings/task_210.yml
2019-09-16 21:30:13,867 - training_jobs - DEBUG - Finished!

2019-09-16 21:30:16,267 - training_jobs - DEBUG - training trains/task_214.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'model': 'mlp4',
 'num_epochs': 300}
2019-09-16 21:30:16,279 - training_jobs - DEBUG - training with: 
2019-09-16 21:30:16,279 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-16 21:30:16,279 - training_jobs - DEBUG - mlp4
2019-09-16 21:30:16,279 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 21:30:16,279 - training_jobs - DEBUG - nlp_nn training
2019-09-16 21:30:18,934 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-16 21:54:48,967 - training_jobs - DEBUG - training time: 1470s
2019-09-16 21:54:49,004 - training_jobs - DEBUG - saving to results/20190916_213018_nlp_nn_tfidf_and_topo_feats.json
2019-09-16 21:54:49,009 - training_jobs - DEBUG - moved jobdict to done_trainings/task_214.yml
2019-09-16 21:54:49,009 - training_jobs - DEBUG - Finished!

2019-09-16 21:54:55,012 - training_jobs - DEBUG - training trains/task_204.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'model': 'mlp4',
 'num_epochs': 30}
2019-09-16 21:54:55,066 - training_jobs - DEBUG - training with: 
2019-09-16 21:54:55,066 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-16 21:54:55,066 - training_jobs - DEBUG - mlp4
2019-09-16 21:54:55,066 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 21:54:55,066 - training_jobs - DEBUG - nlp_nn training
2019-09-16 21:54:55,816 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-16 21:56:42,629 - training_jobs - DEBUG - training time: 107s
2019-09-16 21:56:42,630 - training_jobs - DEBUG - saving to results/20190916_215455_nlp_nn_tfidf_and_topo_feats.json
2019-09-16 21:56:42,640 - training_jobs - DEBUG - moved jobdict to done_trainings/task_204.yml
2019-09-16 21:56:42,640 - training_jobs - DEBUG - Finished!

2019-09-16 21:56:45,096 - training_jobs - DEBUG - training trains/task_6.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'model': 'mlp3',
 'num_epochs': 300}
2019-09-16 21:56:45,128 - training_jobs - DEBUG - training with: 
2019-09-16 21:56:45,128 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 21:56:45,128 - training_jobs - DEBUG - mlp3
2019-09-16 21:56:45,128 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 21:56:45,128 - training_jobs - DEBUG - nlp_nn training
2019-09-16 21:56:46,057 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-16 22:21:12,332 - training_jobs - DEBUG - training time: 1466s
2019-09-16 22:21:12,348 - training_jobs - DEBUG - saving to results/20190916_215646_nlp_nn_tfidf_and_topo_feats.json
2019-09-16 22:21:12,362 - training_jobs - DEBUG - moved jobdict to done_trainings/task_6.yml
2019-09-16 22:21:12,362 - training_jobs - DEBUG - Finished!

2019-09-16 22:21:22,832 - training_jobs - DEBUG - training trains/task_91.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 22:21:22,900 - training_jobs - DEBUG - training with: 
2019-09-16 22:21:22,900 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 22:21:22,900 - training_jobs - DEBUG - GGNN6
2019-09-16 22:21:22,900 - training_jobs - DEBUG - 
2019-09-16 22:21:22,900 - training_jobs - DEBUG - ggnn training
2019-09-16 22:21:42,429 - training_jobs - DEBUG -  saving results to results/20190916_222142_ggnn_.json
2019-09-16 22:21:42,429 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 22:21:47,689 - training_jobs - ERROR - Error with trains/task_91.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1461, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 710, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 603, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 38
2019-09-16 22:21:50,779 - training_jobs - DEBUG - training trains/task_138.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 22:21:50,794 - training_jobs - DEBUG - training with: 
2019-09-16 22:21:50,794 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 22:21:50,794 - training_jobs - DEBUG - GGNN1
2019-09-16 22:21:50,795 - training_jobs - DEBUG - 
2019-09-16 22:21:50,795 - training_jobs - DEBUG - ggnn training
2019-09-16 22:21:53,696 - training_jobs - DEBUG -  saving results to results/20190916_222153_ggnn_.json
2019-09-16 22:21:53,696 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 22:21:58,639 - training_jobs - ERROR - Error with trains/task_138.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1461, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 710, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 603, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 38
2019-09-16 22:22:01,395 - training_jobs - DEBUG - training trains/task_5.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'model': 'mlp3',
 'num_epochs': 100}
2019-09-16 22:22:01,431 - training_jobs - DEBUG - training with: 
2019-09-16 22:22:01,431 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 22:22:01,431 - training_jobs - DEBUG - mlp3
2019-09-16 22:22:01,431 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 22:22:01,431 - training_jobs - DEBUG - nlp_nn training
2019-09-16 22:22:03,276 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-16 22:26:51,259 - training_jobs - DEBUG - training time: 288s
2019-09-16 22:26:51,259 - training_jobs - DEBUG - saving to results/20190916_222203_nlp_nn_tfidf_and_topo_feats.json
2019-09-16 22:26:51,265 - training_jobs - DEBUG - moved jobdict to done_trainings/task_5.yml
2019-09-16 22:26:51,265 - training_jobs - DEBUG - Finished!

2019-09-16 22:26:53,693 - training_jobs - DEBUG - training trains/task_1.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'model': 'mlp3',
 'num_epochs': 100}
2019-09-16 22:26:53,904 - training_jobs - DEBUG - training with: 
2019-09-16 22:26:53,904 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-16 22:26:53,904 - training_jobs - DEBUG - mlp3
2019-09-16 22:26:53,904 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 22:26:53,904 - training_jobs - DEBUG - nlp_nn training
2019-09-16 22:26:54,445 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-16 22:31:57,292 - training_jobs - DEBUG - training time: 303s
2019-09-16 22:31:57,293 - training_jobs - DEBUG - saving to results/20190916_222654_nlp_nn_tfidf_and_topo_feats.json
2019-09-16 22:31:57,297 - training_jobs - DEBUG - moved jobdict to done_trainings/task_1.yml
2019-09-16 22:31:57,297 - training_jobs - DEBUG - Finished!

2019-09-16 22:31:59,689 - training_jobs - DEBUG - training trains/task_77.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 15,
 'd4': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 22:31:59,947 - training_jobs - DEBUG - training with: 
2019-09-16 22:31:59,947 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 22:31:59,947 - training_jobs - DEBUG - GGNN5
2019-09-16 22:31:59,947 - training_jobs - DEBUG - 
2019-09-16 22:31:59,947 - training_jobs - DEBUG - ggnn training
2019-09-16 22:32:01,608 - training_jobs - DEBUG -  saving results to results/20190916_223201_ggnn_.json
2019-09-16 22:32:01,609 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 22:32:04,079 - training_jobs - ERROR - Error with trains/task_77.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1461, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 710, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 603, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 38
2019-09-16 22:32:06,552 - training_jobs - DEBUG - training trains/task_209.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'model': 'mlp4',
 'num_epochs': 100}
2019-09-16 22:32:06,627 - training_jobs - DEBUG - training with: 
2019-09-16 22:32:06,627 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 22:32:06,627 - training_jobs - DEBUG - mlp4
2019-09-16 22:32:06,627 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 22:32:06,627 - training_jobs - DEBUG - nlp_nn training
2019-09-16 22:32:07,050 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-16 22:37:24,968 - training_jobs - DEBUG - training time: 318s
2019-09-16 22:37:24,969 - training_jobs - DEBUG - saving to results/20190916_223207_nlp_nn_tfidf_and_topo_feats.json
2019-09-16 22:37:25,136 - training_jobs - DEBUG - moved jobdict to done_trainings/task_209.yml
2019-09-16 22:37:25,137 - training_jobs - DEBUG - Finished!

2019-09-16 22:37:27,543 - training_jobs - DEBUG - training trains/task_151.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 22:37:27,593 - training_jobs - DEBUG - training with: 
2019-09-16 22:37:27,594 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 22:37:27,594 - training_jobs - DEBUG - GGNN1
2019-09-16 22:37:27,594 - training_jobs - DEBUG - 
2019-09-16 22:37:27,594 - training_jobs - DEBUG - ggnn training
2019-09-16 22:37:29,828 - training_jobs - DEBUG -  saving results to results/20190916_223729_ggnn_.json
2019-09-16 22:37:29,828 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 22:37:33,992 - training_jobs - ERROR - Error with trains/task_151.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1461, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 710, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 603, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 38
2019-09-16 22:37:36,352 - training_jobs - DEBUG - training trains/task_78.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 15,
 'd4': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 300,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 22:37:36,393 - training_jobs - DEBUG - training with: 
2019-09-16 22:37:36,393 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 22:37:36,393 - training_jobs - DEBUG - GGNN5
2019-09-16 22:37:36,393 - training_jobs - DEBUG - 
2019-09-16 22:37:36,393 - training_jobs - DEBUG - ggnn training
2019-09-16 22:37:38,642 - training_jobs - DEBUG -  saving results to results/20190916_223738_ggnn_.json
2019-09-16 22:37:38,642 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 22:37:43,255 - training_jobs - ERROR - Error with trains/task_78.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1461, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 710, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 603, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 38
2019-09-16 22:37:45,844 - training_jobs - DEBUG - training trains/task_175.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 22:37:45,870 - training_jobs - DEBUG - training with: 
2019-09-16 22:37:45,870 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 22:37:45,870 - training_jobs - DEBUG - GGNN1
2019-09-16 22:37:45,870 - training_jobs - DEBUG - 
2019-09-16 22:37:45,871 - training_jobs - DEBUG - ggnn training
2019-09-16 22:37:48,429 - training_jobs - DEBUG -  saving results to results/20190916_223748_ggnn_.json
2019-09-16 22:37:48,429 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 22:37:53,098 - training_jobs - ERROR - Error with trains/task_175.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1461, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 710, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 603, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 38
2019-09-16 22:37:55,675 - training_jobs - DEBUG - training trains/task_137.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 22:37:55,686 - training_jobs - DEBUG - training with: 
2019-09-16 22:37:55,687 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 22:37:55,687 - training_jobs - DEBUG - GGNN1
2019-09-16 22:37:55,687 - training_jobs - DEBUG - 
2019-09-16 22:37:55,687 - training_jobs - DEBUG - ggnn training
2019-09-16 22:37:57,070 - training_jobs - DEBUG -  saving results to results/20190916_223757_ggnn_.json
2019-09-16 22:37:57,070 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 22:37:59,742 - training_jobs - ERROR - Error with trains/task_137.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1461, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 710, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 603, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 38
2019-09-16 22:38:02,437 - training_jobs - DEBUG - training trains/task_89.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 22:38:02,594 - training_jobs - DEBUG - training with: 
2019-09-16 22:38:02,594 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 22:38:02,594 - training_jobs - DEBUG - GGNN6
2019-09-16 22:38:02,594 - training_jobs - DEBUG - 
2019-09-16 22:38:02,594 - training_jobs - DEBUG - ggnn training
2019-09-16 22:38:03,983 - training_jobs - DEBUG -  saving results to results/20190916_223803_ggnn_.json
2019-09-16 22:38:03,983 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 22:38:06,481 - training_jobs - ERROR - Error with trains/task_89.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1461, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 710, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 603, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 38
2019-09-16 22:38:08,958 - training_jobs - DEBUG - training trains/task_149.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 22:38:09,080 - training_jobs - DEBUG - training with: 
2019-09-16 22:38:09,080 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 22:38:09,080 - training_jobs - DEBUG - GGNN1
2019-09-16 22:38:09,080 - training_jobs - DEBUG - 
2019-09-16 22:38:09,080 - training_jobs - DEBUG - ggnn training
2019-09-16 22:38:10,472 - training_jobs - DEBUG -  saving results to results/20190916_223810_ggnn_.json
2019-09-16 22:38:10,472 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 22:38:13,145 - training_jobs - ERROR - Error with trains/task_149.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1461, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 710, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 603, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 38
2019-09-16 22:38:15,749 - training_jobs - DEBUG - training trains/task_88.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 22:38:15,774 - training_jobs - DEBUG - training with: 
2019-09-16 22:38:15,774 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 22:38:15,774 - training_jobs - DEBUG - GGNN6
2019-09-16 22:38:15,774 - training_jobs - DEBUG - 
2019-09-16 22:38:15,774 - training_jobs - DEBUG - ggnn training
2019-09-16 22:38:17,272 - training_jobs - DEBUG -  saving results to results/20190916_223817_ggnn_.json
2019-09-16 22:38:17,272 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 22:38:20,114 - training_jobs - ERROR - Error with trains/task_88.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1461, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 710, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 603, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 38
2019-09-16 22:38:22,797 - training_jobs - DEBUG - training trains/task_172.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 22:38:22,830 - training_jobs - DEBUG - training with: 
2019-09-16 22:38:22,830 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 22:38:22,830 - training_jobs - DEBUG - GGNN1
2019-09-16 22:38:22,830 - training_jobs - DEBUG - 
2019-09-16 22:38:22,830 - training_jobs - DEBUG - ggnn training
2019-09-16 22:38:24,278 - training_jobs - DEBUG -  saving results to results/20190916_223824_ggnn_.json
2019-09-16 22:38:24,278 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 22:38:26,780 - training_jobs - ERROR - Error with trains/task_172.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1461, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 710, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 603, in balancedDatasetKfoldSplit_slice
    datasets_byclass[int(graph.y.item())].append(i)
KeyError: 38
2019-09-16 22:38:29,388 - training_jobs - DEBUG - training trains/task_205.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'features': 'tfidf and topo feats',
 'model': 'mlp4',
 'num_epochs': 100}
2019-09-16 22:38:29,400 - training_jobs - DEBUG - training with: 
2019-09-16 22:38:29,400 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-16 22:38:29,400 - training_jobs - DEBUG - mlp4
2019-09-16 22:38:29,400 - training_jobs - DEBUG - tfidf and topo feats
2019-09-16 22:38:29,400 - training_jobs - DEBUG - nlp_nn training
2019-09-16 22:38:29,831 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-16 22:44:05,119 - training_jobs - DEBUG - training time: 335s
2019-09-16 22:44:05,120 - training_jobs - DEBUG - saving to results/20190916_223829_nlp_nn_tfidf_and_topo_feats.json
2019-09-16 22:44:05,124 - training_jobs - DEBUG - moved jobdict to done_trainings/task_205.yml
2019-09-16 22:44:05,124 - training_jobs - DEBUG - Finished!

2019-09-16 23:09:48,793 - training_jobs - DEBUG - training trains/task_139.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 23:09:48,827 - training_jobs - DEBUG - training with: 
2019-09-16 23:09:48,827 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 23:09:48,828 - training_jobs - DEBUG - GGNN1
2019-09-16 23:09:48,828 - training_jobs - DEBUG - 
2019-09-16 23:09:48,828 - training_jobs - DEBUG - ggnn training
2019-09-16 23:09:51,096 - training_jobs - DEBUG -  saving results to results/20190916_230951_ggnn_.json
2019-09-16 23:09:51,096 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 23:09:55,332 - training_jobs - ERROR - Error with trains/task_139.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1476, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 725, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 609, in balancedDatasetKfoldSplit_slice
    if key_y not in dataset_byclass.keys():
NameError: name 'dataset_byclass' is not defined
2019-09-16 23:09:57,757 - training_jobs - DEBUG - training trains/task_162.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 23:09:57,795 - training_jobs - DEBUG - training with: 
2019-09-16 23:09:57,795 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 23:09:57,795 - training_jobs - DEBUG - GGNN1
2019-09-16 23:09:57,795 - training_jobs - DEBUG - 
2019-09-16 23:09:57,795 - training_jobs - DEBUG - ggnn training
2019-09-16 23:10:00,124 - training_jobs - DEBUG -  saving results to results/20190916_231000_ggnn_.json
2019-09-16 23:10:00,124 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 23:10:04,376 - training_jobs - ERROR - Error with trains/task_162.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1476, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 725, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 609, in balancedDatasetKfoldSplit_slice
    if key_y not in dataset_byclass.keys():
NameError: name 'dataset_byclass' is not defined
2019-09-16 23:10:06,818 - training_jobs - DEBUG - training trains/task_161.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 23:10:06,831 - training_jobs - DEBUG - training with: 
2019-09-16 23:10:06,831 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 23:10:06,831 - training_jobs - DEBUG - GGNN1
2019-09-16 23:10:06,831 - training_jobs - DEBUG - 
2019-09-16 23:10:06,831 - training_jobs - DEBUG - ggnn training
2019-09-16 23:10:08,199 - training_jobs - DEBUG -  saving results to results/20190916_231008_ggnn_.json
2019-09-16 23:10:08,199 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 23:10:10,728 - training_jobs - ERROR - Error with trains/task_161.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1476, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 725, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 609, in balancedDatasetKfoldSplit_slice
    if key_y not in dataset_byclass.keys():
NameError: name 'dataset_byclass' is not defined
2019-09-16 23:10:13,164 - training_jobs - DEBUG - training trains/task_136.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 23:10:13,185 - training_jobs - DEBUG - training with: 
2019-09-16 23:10:13,185 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 23:10:13,185 - training_jobs - DEBUG - GGNN1
2019-09-16 23:10:13,185 - training_jobs - DEBUG - 
2019-09-16 23:10:13,185 - training_jobs - DEBUG - ggnn training
2019-09-16 23:10:14,528 - training_jobs - DEBUG -  saving results to results/20190916_231014_ggnn_.json
2019-09-16 23:10:14,528 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 23:10:17,011 - training_jobs - ERROR - Error with trains/task_136.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1476, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 725, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 609, in balancedDatasetKfoldSplit_slice
    if key_y not in dataset_byclass.keys():
NameError: name 'dataset_byclass' is not defined
2019-09-16 23:10:19,438 - training_jobs - DEBUG - training trains/task_173.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 23:10:19,451 - training_jobs - DEBUG - training with: 
2019-09-16 23:10:19,451 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 23:10:19,451 - training_jobs - DEBUG - GGNN1
2019-09-16 23:10:19,451 - training_jobs - DEBUG - 
2019-09-16 23:10:19,451 - training_jobs - DEBUG - ggnn training
2019-09-16 23:10:20,802 - training_jobs - DEBUG -  saving results to results/20190916_231020_ggnn_.json
2019-09-16 23:10:20,803 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 23:10:23,263 - training_jobs - ERROR - Error with trains/task_173.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1476, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 725, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 609, in balancedDatasetKfoldSplit_slice
    if key_y not in dataset_byclass.keys():
NameError: name 'dataset_byclass' is not defined
2019-09-16 23:10:25,725 - training_jobs - DEBUG - training trains/task_160.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 23:10:25,738 - training_jobs - DEBUG - training with: 
2019-09-16 23:10:25,738 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 23:10:25,738 - training_jobs - DEBUG - GGNN1
2019-09-16 23:10:25,738 - training_jobs - DEBUG - 
2019-09-16 23:10:25,738 - training_jobs - DEBUG - ggnn training
2019-09-16 23:10:27,112 - training_jobs - DEBUG -  saving results to results/20190916_231027_ggnn_.json
2019-09-16 23:10:27,112 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 23:10:29,608 - training_jobs - ERROR - Error with trains/task_160.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1476, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 725, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 609, in balancedDatasetKfoldSplit_slice
    if key_y not in dataset_byclass.keys():
NameError: name 'dataset_byclass' is not defined
2019-09-16 23:10:31,963 - training_jobs - DEBUG - training trains/task_76.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 15,
 'd4': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 23:10:31,976 - training_jobs - DEBUG - training with: 
2019-09-16 23:10:31,976 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 23:10:31,976 - training_jobs - DEBUG - GGNN5
2019-09-16 23:10:31,976 - training_jobs - DEBUG - 
2019-09-16 23:10:31,976 - training_jobs - DEBUG - ggnn training
2019-09-16 23:10:33,311 - training_jobs - DEBUG -  saving results to results/20190916_231033_ggnn_.json
2019-09-16 23:10:33,311 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 23:10:35,809 - training_jobs - ERROR - Error with trains/task_76.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1476, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 725, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 609, in balancedDatasetKfoldSplit_slice
    if key_y not in dataset_byclass.keys():
NameError: name 'dataset_byclass' is not defined
2019-09-16 23:10:38,217 - training_jobs - DEBUG - training trains/task_150.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 23:10:38,468 - training_jobs - DEBUG - training with: 
2019-09-16 23:10:38,468 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 23:10:38,468 - training_jobs - DEBUG - GGNN1
2019-09-16 23:10:38,468 - training_jobs - DEBUG - 
2019-09-16 23:10:38,468 - training_jobs - DEBUG - ggnn training
2019-09-16 23:10:40,738 - training_jobs - DEBUG -  saving results to results/20190916_231040_ggnn_.json
2019-09-16 23:10:40,738 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 23:10:44,948 - training_jobs - ERROR - Error with trains/task_150.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1476, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 725, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 609, in balancedDatasetKfoldSplit_slice
    if key_y not in dataset_byclass.keys():
NameError: name 'dataset_byclass' is not defined
2019-09-16 23:10:47,299 - training_jobs - DEBUG - training trains/task_174.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 23:10:47,312 - training_jobs - DEBUG - training with: 
2019-09-16 23:10:47,312 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 23:10:47,312 - training_jobs - DEBUG - GGNN1
2019-09-16 23:10:47,312 - training_jobs - DEBUG - 
2019-09-16 23:10:47,312 - training_jobs - DEBUG - ggnn training
2019-09-16 23:10:49,575 - training_jobs - DEBUG -  saving results to results/20190916_231049_ggnn_.json
2019-09-16 23:10:49,575 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 23:10:53,909 - training_jobs - ERROR - Error with trains/task_174.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1476, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 725, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 609, in balancedDatasetKfoldSplit_slice
    if key_y not in dataset_byclass.keys():
NameError: name 'dataset_byclass' is not defined
2019-09-16 23:10:56,590 - training_jobs - DEBUG - training trains/task_148.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 23:10:56,626 - training_jobs - DEBUG - training with: 
2019-09-16 23:10:56,626 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 23:10:56,626 - training_jobs - DEBUG - GGNN1
2019-09-16 23:10:56,626 - training_jobs - DEBUG - 
2019-09-16 23:10:56,626 - training_jobs - DEBUG - ggnn training
2019-09-16 23:10:57,865 - training_jobs - ERROR - Error with trains/task_148.yml
Traceback (most recent call last):
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 422, in get
    data = torch.load(filename)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/serialization.py", line 368, in load
    return _load(f, map_location, pickle_module)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/serialization.py", line 542, in _load
    result = unpickler.load()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/_utils.py", line 135, in _rebuild_tensor_v2
    tensor = _rebuild_tensor(storage, storage_offset, size, stride)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/_utils.py", line 131, in _rebuild_tensor
    return tensor_class().set_(storage, storage_offset, size, stride)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 477, in training_dispatcher
    num_classes = train_dataset.num_classes
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 275, in num_classes
    [self.get(j).y[0].item() for j in range(self.__len__() )]
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 275, in <listcomp>
    [self.get(j).y[0].item() for j in range(self.__len__() )]
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 438, in get
    raise IndexError("error getting: ", idx)
IndexError: ('error getting: ', 3909)
2019-09-16 23:11:37,359 - training_jobs - DEBUG - training trains/task_139.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 23:11:37,371 - training_jobs - DEBUG - training with: 
2019-09-16 23:11:37,371 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 23:11:37,371 - training_jobs - DEBUG - GGNN1
2019-09-16 23:11:37,371 - training_jobs - DEBUG - 
2019-09-16 23:11:37,371 - training_jobs - DEBUG - ggnn training
2019-09-16 23:11:39,585 - training_jobs - DEBUG -  saving results to results/20190916_231139_ggnn_.json
2019-09-16 23:11:39,585 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 23:19:15,964 - training_jobs - ERROR - Error with trains/task_139.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA error: device-side assert triggered
2019-09-16 23:19:19,865 - training_jobs - DEBUG - training trains/task_162.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 23:19:19,880 - training_jobs - DEBUG - training with: 
2019-09-16 23:19:19,881 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 23:19:19,881 - training_jobs - DEBUG - GGNN1
2019-09-16 23:19:19,881 - training_jobs - DEBUG - 
2019-09-16 23:19:19,881 - training_jobs - DEBUG - ggnn training
2019-09-16 23:19:22,585 - training_jobs - DEBUG -  saving results to results/20190916_231922_ggnn_.json
2019-09-16 23:19:22,585 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 23:23:23,108 - training_jobs - ERROR - Error with trains/task_162.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: cublas runtime error : the GPU program failed to execute at /pytorch/aten/src/THC/THCBlas.cu:258
2019-09-16 23:23:27,137 - training_jobs - DEBUG - training trains/task_161.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 23:23:27,151 - training_jobs - DEBUG - training with: 
2019-09-16 23:23:27,151 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 23:23:27,151 - training_jobs - DEBUG - GGNN1
2019-09-16 23:23:27,151 - training_jobs - DEBUG - 
2019-09-16 23:23:27,151 - training_jobs - DEBUG - ggnn training
2019-09-16 23:23:28,856 - training_jobs - DEBUG -  saving results to results/20190916_232328_ggnn_.json
2019-09-16 23:23:28,856 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 23:27:24,206 - training_jobs - ERROR - Error with trains/task_161.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: cublas runtime error : the GPU program failed to execute at /pytorch/aten/src/THC/THCBlas.cu:258
2019-09-16 23:27:28,760 - training_jobs - DEBUG - training trains/task_136.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 23:27:28,777 - training_jobs - DEBUG - training with: 
2019-09-16 23:27:28,777 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 23:27:28,777 - training_jobs - DEBUG - GGNN1
2019-09-16 23:27:28,777 - training_jobs - DEBUG - 
2019-09-16 23:27:28,777 - training_jobs - DEBUG - ggnn training
2019-09-16 23:27:30,791 - training_jobs - DEBUG -  saving results to results/20190916_232730_ggnn_.json
2019-09-16 23:27:30,791 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 23:29:28,988 - training_jobs - ERROR - Error with trains/task_136.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA error: device-side assert triggered
2019-09-16 23:29:33,184 - training_jobs - DEBUG - training trains/task_173.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 23:29:33,200 - training_jobs - DEBUG - training with: 
2019-09-16 23:29:33,200 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 23:29:33,200 - training_jobs - DEBUG - GGNN1
2019-09-16 23:29:33,200 - training_jobs - DEBUG - 
2019-09-16 23:29:33,200 - training_jobs - DEBUG - ggnn training
2019-09-16 23:29:35,088 - training_jobs - DEBUG -  saving results to results/20190916_232935_ggnn_.json
2019-09-16 23:29:35,088 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 23:33:40,808 - training_jobs - ERROR - Error with trains/task_173.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/ATen/native/cuda/SoftMax.cu:617
2019-09-16 23:33:45,073 - training_jobs - DEBUG - training trains/task_160.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 23:33:45,091 - training_jobs - DEBUG - training with: 
2019-09-16 23:33:45,091 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 23:33:45,091 - training_jobs - DEBUG - GGNN1
2019-09-16 23:33:45,091 - training_jobs - DEBUG - 
2019-09-16 23:33:45,091 - training_jobs - DEBUG - ggnn training
2019-09-16 23:33:46,915 - training_jobs - DEBUG -  saving results to results/20190916_233346_ggnn_.json
2019-09-16 23:33:46,915 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 23:35:53,011 - training_jobs - ERROR - Error with trains/task_160.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA error: device-side assert triggered
2019-09-16 23:35:57,012 - training_jobs - DEBUG - training trains/task_76.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 15,
 'd4': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 23:35:57,028 - training_jobs - DEBUG - training with: 
2019-09-16 23:35:57,028 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-16 23:35:57,028 - training_jobs - DEBUG - GGNN5
2019-09-16 23:35:57,028 - training_jobs - DEBUG - 
2019-09-16 23:35:57,028 - training_jobs - DEBUG - ggnn training
2019-09-16 23:35:58,709 - training_jobs - DEBUG -  saving results to results/20190916_233558_ggnn_.json
2019-09-16 23:35:58,709 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 23:38:03,411 - training_jobs - ERROR - Error with trains/task_76.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: cublas runtime error : the GPU program failed to execute at /pytorch/aten/src/THC/THCBlas.cu:258
2019-09-16 23:38:07,164 - training_jobs - DEBUG - training trains/task_150.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 23:38:07,177 - training_jobs - DEBUG - training with: 
2019-09-16 23:38:07,177 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 23:38:07,177 - training_jobs - DEBUG - GGNN1
2019-09-16 23:38:07,177 - training_jobs - DEBUG - 
2019-09-16 23:38:07,177 - training_jobs - DEBUG - ggnn training
2019-09-16 23:38:10,164 - training_jobs - DEBUG -  saving results to results/20190916_233810_ggnn_.json
2019-09-16 23:38:10,164 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 23:42:19,559 - training_jobs - ERROR - Error with trains/task_150.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA error: device-side assert triggered
2019-09-16 23:42:23,469 - training_jobs - DEBUG - training trains/task_174.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 23:42:23,484 - training_jobs - DEBUG - training with: 
2019-09-16 23:42:23,484 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 23:42:23,484 - training_jobs - DEBUG - GGNN1
2019-09-16 23:42:23,484 - training_jobs - DEBUG - 
2019-09-16 23:42:23,484 - training_jobs - DEBUG - ggnn training
2019-09-16 23:42:26,323 - training_jobs - DEBUG -  saving results to results/20190916_234226_ggnn_.json
2019-09-16 23:42:26,323 - training_jobs - DEBUG -  calling modelSelection
2019-09-16 23:54:21,494 - training_jobs - DEBUG - training trains/task_139.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-16 23:54:21,570 - training_jobs - DEBUG - training with: 
2019-09-16 23:54:21,570 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-16 23:54:21,570 - training_jobs - DEBUG - GGNN1
2019-09-16 23:54:21,570 - training_jobs - DEBUG - 
2019-09-16 23:54:21,570 - training_jobs - DEBUG - ggnn training
2019-09-16 23:54:43,067 - training_jobs - DEBUG -  saving results to results/20190916_235443_ggnn_.json
2019-09-16 23:54:43,068 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 00:01:47,637 - training_jobs - ERROR - Error with trains/task_139.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: cublas runtime error : the GPU program failed to execute at /pytorch/aten/src/THC/THCBlas.cu:258
2019-09-17 00:01:50,859 - training_jobs - DEBUG - training trains/task_162.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 00:01:50,883 - training_jobs - DEBUG - training with: 
2019-09-17 00:01:50,883 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-17 00:01:50,883 - training_jobs - DEBUG - GGNN1
2019-09-17 00:01:50,883 - training_jobs - DEBUG - 
2019-09-17 00:01:50,883 - training_jobs - DEBUG - ggnn training
2019-09-17 00:01:52,834 - training_jobs - DEBUG -  saving results to results/20190917_000152_ggnn_.json
2019-09-17 00:01:52,834 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 00:05:34,398 - training_jobs - ERROR - Error with trains/task_162.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA error: device-side assert triggered
2019-09-17 00:05:37,583 - training_jobs - DEBUG - training trains/task_161.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 00:05:37,595 - training_jobs - DEBUG - training with: 
2019-09-17 00:05:37,595 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-17 00:05:37,595 - training_jobs - DEBUG - GGNN1
2019-09-17 00:05:37,595 - training_jobs - DEBUG - 
2019-09-17 00:05:37,595 - training_jobs - DEBUG - ggnn training
2019-09-17 00:05:39,979 - training_jobs - DEBUG -  saving results to results/20190917_000539_ggnn_.json
2019-09-17 00:05:39,979 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 00:09:17,018 - training_jobs - ERROR - Error with trains/task_161.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: cublas runtime error : the GPU program failed to execute at /pytorch/aten/src/THC/THCBlas.cu:258
2019-09-17 00:09:20,214 - training_jobs - DEBUG - training trains/task_136.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 00:09:20,225 - training_jobs - DEBUG - training with: 
2019-09-17 00:09:20,225 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-17 00:09:20,225 - training_jobs - DEBUG - GGNN1
2019-09-17 00:09:20,225 - training_jobs - DEBUG - 
2019-09-17 00:09:20,226 - training_jobs - DEBUG - ggnn training
2019-09-17 00:09:21,433 - training_jobs - DEBUG -  saving results to results/20190917_000921_ggnn_.json
2019-09-17 00:09:21,433 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 00:11:05,055 - training_jobs - ERROR - Error with trains/task_136.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA error: device-side assert triggered
2019-09-17 00:11:08,286 - training_jobs - DEBUG - training trains/task_173.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 00:11:08,298 - training_jobs - DEBUG - training with: 
2019-09-17 00:11:08,298 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-17 00:11:08,298 - training_jobs - DEBUG - GGNN1
2019-09-17 00:11:08,298 - training_jobs - DEBUG - 
2019-09-17 00:11:08,298 - training_jobs - DEBUG - ggnn training
2019-09-17 00:11:09,520 - training_jobs - DEBUG -  saving results to results/20190917_001109_ggnn_.json
2019-09-17 00:11:09,520 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 00:14:43,961 - training_jobs - ERROR - Error with trains/task_173.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: cublas runtime error : the GPU program failed to execute at /pytorch/aten/src/THC/THCBlas.cu:258
2019-09-17 00:14:47,112 - training_jobs - DEBUG - training trains/task_160.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 00:14:47,124 - training_jobs - DEBUG - training with: 
2019-09-17 00:14:47,124 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-17 00:14:47,124 - training_jobs - DEBUG - GGNN1
2019-09-17 00:14:47,124 - training_jobs - DEBUG - 
2019-09-17 00:14:47,124 - training_jobs - DEBUG - ggnn training
2019-09-17 00:14:48,348 - training_jobs - DEBUG -  saving results to results/20190917_001448_ggnn_.json
2019-09-17 00:14:48,348 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 00:16:39,930 - training_jobs - ERROR - Error with trains/task_160.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: cublas runtime error : the GPU program failed to execute at /pytorch/aten/src/THC/THCBlas.cu:258
2019-09-17 00:16:43,177 - training_jobs - DEBUG - training trains/task_76.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 15,
 'd4': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 00:16:43,189 - training_jobs - DEBUG - training with: 
2019-09-17 00:16:43,189 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-17 00:16:43,189 - training_jobs - DEBUG - GGNN5
2019-09-17 00:16:43,189 - training_jobs - DEBUG - 
2019-09-17 00:16:43,189 - training_jobs - DEBUG - ggnn training
2019-09-17 00:16:44,414 - training_jobs - DEBUG -  saving results to results/20190917_001644_ggnn_.json
2019-09-17 00:16:44,414 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 00:18:32,359 - training_jobs - ERROR - Error with trains/task_76.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: cuDNN error: CUDNN_STATUS_MAPPING_ERROR
2019-09-17 00:18:35,596 - training_jobs - DEBUG - training trains/task_150.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 00:18:35,608 - training_jobs - DEBUG - training with: 
2019-09-17 00:18:35,608 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-17 00:18:35,608 - training_jobs - DEBUG - GGNN1
2019-09-17 00:18:35,608 - training_jobs - DEBUG - 
2019-09-17 00:18:35,608 - training_jobs - DEBUG - ggnn training
2019-09-17 00:18:37,686 - training_jobs - DEBUG -  saving results to results/20190917_001837_ggnn_.json
2019-09-17 00:18:37,686 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 00:22:09,200 - training_jobs - ERROR - Error with trains/task_150.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: cublas runtime error : the GPU program failed to execute at /pytorch/aten/src/THC/THCBlas.cu:258
2019-09-17 00:22:12,581 - training_jobs - DEBUG - training trains/task_174.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 00:22:12,594 - training_jobs - DEBUG - training with: 
2019-09-17 00:22:12,594 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-17 00:22:12,594 - training_jobs - DEBUG - GGNN1
2019-09-17 00:22:12,594 - training_jobs - DEBUG - 
2019-09-17 00:22:12,594 - training_jobs - DEBUG - ggnn training
2019-09-17 00:22:14,830 - training_jobs - DEBUG -  saving results to results/20190917_002214_ggnn_.json
2019-09-17 00:22:14,830 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 00:25:58,103 - training_jobs - ERROR - Error with trains/task_174.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA error: device-side assert triggered
2019-09-17 00:26:01,297 - training_jobs - DEBUG - training trains/task_148.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 00:26:01,309 - training_jobs - DEBUG - training with: 
2019-09-17 00:26:01,309 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-17 00:26:01,309 - training_jobs - DEBUG - GGNN1
2019-09-17 00:26:01,309 - training_jobs - DEBUG - 
2019-09-17 00:26:01,309 - training_jobs - DEBUG - ggnn training
2019-09-17 00:26:02,514 - training_jobs - DEBUG -  saving results to results/20190917_002602_ggnn_.json
2019-09-17 00:26:02,514 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 00:27:48,864 - training_jobs - ERROR - Error with trains/task_148.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA error: device-side assert triggered
2019-09-17 00:27:52,099 - training_jobs - DEBUG - training trains/task_163.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 00:27:52,110 - training_jobs - DEBUG - training with: 
2019-09-17 00:27:52,110 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-17 00:27:52,110 - training_jobs - DEBUG - GGNN1
2019-09-17 00:27:52,111 - training_jobs - DEBUG - 
2019-09-17 00:27:52,111 - training_jobs - DEBUG - ggnn training
2019-09-17 00:27:54,126 - training_jobs - DEBUG -  saving results to results/20190917_002754_ggnn_.json
2019-09-17 00:27:54,126 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 00:35:13,641 - training_jobs - ERROR - Error with trains/task_163.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: cublas runtime error : the GPU program failed to execute at /pytorch/aten/src/THC/THCBlas.cu:258
2019-09-17 00:35:16,986 - training_jobs - DEBUG - training trains/task_10.yml
{'d1': 55,
 'd2': 10,
 'd3': 10,
 'd4': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'features': 'tfidf and topo feats',
 'model': 'mlp3',
 'num_epochs': 30}
2019-09-17 00:35:17,024 - training_jobs - DEBUG - training with: 
2019-09-17 00:35:17,024 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-17 00:35:17,024 - training_jobs - DEBUG - mlp3
2019-09-17 00:35:17,024 - training_jobs - DEBUG - tfidf and topo feats
2019-09-17 00:35:17,024 - training_jobs - DEBUG - nlp_nn training
2019-09-17 00:35:18,053 - training_jobs - DEBUG -  calling cv_train_nn_nlp_models_v2
2019-09-17 00:37:20,547 - training_jobs - DEBUG - training time: 122s
2019-09-17 00:37:20,547 - training_jobs - DEBUG - saving to results/20190917_003518_nlp_nn_tfidf_and_topo_feats.json
2019-09-17 00:37:20,572 - training_jobs - DEBUG - moved jobdict to done_trainings/task_10.yml
2019-09-17 00:37:20,573 - training_jobs - DEBUG - Finished!

2019-09-17 00:37:22,729 - training_jobs - DEBUG - training trains/task_91.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 00:37:22,758 - training_jobs - DEBUG - training with: 
2019-09-17 00:37:22,758 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-17 00:37:22,758 - training_jobs - DEBUG - GGNN6
2019-09-17 00:37:22,758 - training_jobs - DEBUG - 
2019-09-17 00:37:22,758 - training_jobs - DEBUG - ggnn training
2019-09-17 00:37:24,729 - training_jobs - DEBUG -  saving results to results/20190917_003724_ggnn_.json
2019-09-17 00:37:24,729 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 00:44:39,768 - training_jobs - ERROR - Error with trains/task_91.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA error: device-side assert triggered
2019-09-17 00:44:42,883 - training_jobs - DEBUG - training trains/task_138.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 00:44:42,895 - training_jobs - DEBUG - training with: 
2019-09-17 00:44:42,895 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-17 00:44:42,895 - training_jobs - DEBUG - GGNN1
2019-09-17 00:44:42,895 - training_jobs - DEBUG - 
2019-09-17 00:44:42,895 - training_jobs - DEBUG - ggnn training
2019-09-17 00:44:44,843 - training_jobs - DEBUG -  saving results to results/20190917_004444_ggnn_.json
2019-09-17 00:44:44,843 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 00:48:16,713 - training_jobs - ERROR - Error with trains/task_138.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA error: device-side assert triggered
2019-09-17 00:48:20,009 - training_jobs - DEBUG - training trains/task_77.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 15,
 'd4': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 00:48:20,058 - training_jobs - DEBUG - training with: 
2019-09-17 00:48:20,058 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-17 00:48:20,058 - training_jobs - DEBUG - GGNN5
2019-09-17 00:48:20,058 - training_jobs - DEBUG - 
2019-09-17 00:48:20,058 - training_jobs - DEBUG - ggnn training
2019-09-17 00:48:21,283 - training_jobs - DEBUG -  saving results to results/20190917_004821_ggnn_.json
2019-09-17 00:48:21,284 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 00:51:52,832 - training_jobs - ERROR - Error with trains/task_77.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA error: device-side assert triggered
2019-09-17 00:51:55,951 - training_jobs - DEBUG - training trains/task_151.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 00:51:55,963 - training_jobs - DEBUG - training with: 
2019-09-17 00:51:55,963 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-17 00:51:55,963 - training_jobs - DEBUG - GGNN1
2019-09-17 00:51:55,963 - training_jobs - DEBUG - 
2019-09-17 00:51:55,963 - training_jobs - DEBUG - ggnn training
2019-09-17 00:51:57,923 - training_jobs - DEBUG -  saving results to results/20190917_005157_ggnn_.json
2019-09-17 00:51:57,923 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 00:58:51,286 - training_jobs - ERROR - Error with trains/task_151.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: cublas runtime error : the GPU program failed to execute at /pytorch/aten/src/THC/THCBlas.cu:258
2019-09-17 00:58:54,417 - training_jobs - DEBUG - training trains/task_78.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 15,
 'd4': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 300,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 00:58:54,459 - training_jobs - DEBUG - training with: 
2019-09-17 00:58:54,460 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-17 00:58:54,460 - training_jobs - DEBUG - GGNN5
2019-09-17 00:58:54,460 - training_jobs - DEBUG - 
2019-09-17 00:58:54,460 - training_jobs - DEBUG - ggnn training
2019-09-17 00:58:56,423 - training_jobs - DEBUG -  saving results to results/20190917_005856_ggnn_.json
2019-09-17 00:58:56,424 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 01:09:32,457 - training_jobs - ERROR - Error with trains/task_78.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA error: device-side assert triggered
2019-09-17 01:09:35,589 - training_jobs - DEBUG - training trains/task_175.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 01:09:35,600 - training_jobs - DEBUG - training with: 
2019-09-17 01:09:35,600 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-17 01:09:35,601 - training_jobs - DEBUG - GGNN1
2019-09-17 01:09:35,601 - training_jobs - DEBUG - 
2019-09-17 01:09:35,601 - training_jobs - DEBUG - ggnn training
2019-09-17 01:09:37,545 - training_jobs - DEBUG -  saving results to results/20190917_010937_ggnn_.json
2019-09-17 01:09:37,545 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 01:16:51,553 - training_jobs - ERROR - Error with trains/task_175.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: cublas runtime error : the GPU program failed to execute at /pytorch/aten/src/THC/THCBlas.cu:258
2019-09-17 01:16:54,936 - training_jobs - DEBUG - training trains/task_137.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 01:16:54,948 - training_jobs - DEBUG - training with: 
2019-09-17 01:16:54,948 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-17 01:16:54,948 - training_jobs - DEBUG - GGNN1
2019-09-17 01:16:54,948 - training_jobs - DEBUG - 
2019-09-17 01:16:54,948 - training_jobs - DEBUG - ggnn training
2019-09-17 01:16:56,206 - training_jobs - DEBUG -  saving results to results/20190917_011656_ggnn_.json
2019-09-17 01:16:56,206 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 01:20:19,922 - training_jobs - ERROR - Error with trains/task_137.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA error: device-side assert triggered
2019-09-17 01:20:23,229 - training_jobs - DEBUG - training trains/task_89.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 01:20:23,261 - training_jobs - DEBUG - training with: 
2019-09-17 01:20:23,261 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-17 01:20:23,261 - training_jobs - DEBUG - GGNN6
2019-09-17 01:20:23,261 - training_jobs - DEBUG - 
2019-09-17 01:20:23,261 - training_jobs - DEBUG - ggnn training
2019-09-17 01:20:24,604 - training_jobs - DEBUG -  saving results to results/20190917_012024_ggnn_.json
2019-09-17 01:20:24,604 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 01:23:57,752 - training_jobs - ERROR - Error with trains/task_89.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA error: device-side assert triggered
2019-09-17 01:24:00,880 - training_jobs - DEBUG - training trains/task_149.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 60,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 01:24:00,892 - training_jobs - DEBUG - training with: 
2019-09-17 01:24:00,892 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-17 01:24:00,892 - training_jobs - DEBUG - GGNN1
2019-09-17 01:24:00,892 - training_jobs - DEBUG - 
2019-09-17 01:24:00,892 - training_jobs - DEBUG - ggnn training
2019-09-17 01:24:02,084 - training_jobs - DEBUG -  saving results to results/20190917_012402_ggnn_.json
2019-09-17 01:24:02,085 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 01:27:27,412 - training_jobs - ERROR - Error with trains/task_149.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: cublas runtime error : the GPU program failed to execute at /pytorch/aten/src/THC/THCBlas.cu:258
2019-09-17 01:27:30,531 - training_jobs - DEBUG - training trains/task_88.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 01:27:30,543 - training_jobs - DEBUG - training with: 
2019-09-17 01:27:30,544 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-17 01:27:30,544 - training_jobs - DEBUG - GGNN6
2019-09-17 01:27:30,544 - training_jobs - DEBUG - 
2019-09-17 01:27:30,544 - training_jobs - DEBUG - ggnn training
2019-09-17 01:27:31,741 - training_jobs - DEBUG -  saving results to results/20190917_012731_ggnn_.json
2019-09-17 01:27:31,741 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 01:29:20,537 - training_jobs - ERROR - Error with trains/task_88.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED
2019-09-17 01:29:23,691 - training_jobs - DEBUG - training trains/task_172.yml
{'aggr_type': 'mean',
 'batch_size': 32,
 'd1': 70,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 01:29:23,703 - training_jobs - DEBUG - training with: 
2019-09-17 01:29:23,703 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-17 01:29:23,703 - training_jobs - DEBUG - GGNN1
2019-09-17 01:29:23,703 - training_jobs - DEBUG - 
2019-09-17 01:29:23,703 - training_jobs - DEBUG - ggnn training
2019-09-17 01:29:24,882 - training_jobs - DEBUG -  saving results to results/20190917_012924_ggnn_.json
2019-09-17 01:29:24,882 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 01:31:13,178 - training_jobs - ERROR - Error with trains/task_172.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA error: device-side assert triggered
2019-09-17 08:14:48,740 - training_jobs - DEBUG - ('tasks/gnn_ggnn1_complete_dataset', '.yaml')
2019-09-17 08:14:48,774 - training_jobs - DEBUG - ('tasks/gnn_ggnn5_all_datasets', '.yaml')
2019-09-17 08:14:48,876 - training_jobs - DEBUG - ('tasks/gnn_ggnn6_all_datasets', '.yaml')
2019-09-17 08:14:48,986 - training_jobs - DEBUG - ('tasks/gnn_ggnn1_all_datasets', '.yaml')
2019-09-17 08:14:49,120 - training_jobs - DEBUG - ('tasks/gnn_ggnn5_complete_dataset', '.yaml')
2019-09-17 08:14:51,577 - training_jobs - DEBUG - training trains/task_115.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 08:14:51,589 - training_jobs - DEBUG - training with: 
2019-09-17 08:14:51,589 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-17 08:14:51,589 - training_jobs - DEBUG - GGNN6
2019-09-17 08:14:51,590 - training_jobs - DEBUG - 
2019-09-17 08:14:51,590 - training_jobs - DEBUG - ggnn training
2019-09-17 08:15:01,289 - training_jobs - DEBUG -  saving results to results/20190917_081501_ggnn_.json
2019-09-17 08:15:01,289 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 08:15:10,850 - training_jobs - ERROR - Error with trains/task_115.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-17 08:15:13,381 - training_jobs - DEBUG - training trains/task_20.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 20,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-17 08:15:13,394 - training_jobs - DEBUG - training with: 
2019-09-17 08:15:13,394 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-17 08:15:13,394 - training_jobs - DEBUG - GGNN5
2019-09-17 08:15:13,394 - training_jobs - DEBUG - 
2019-09-17 08:15:13,394 - training_jobs - DEBUG - ggnn training
2019-09-17 08:15:15,962 - training_jobs - DEBUG -  saving results to results/20190917_081515_ggnn_.json
2019-09-17 08:15:15,962 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 08:24:02,598 - training_jobs - DEBUG - test_multiple_models
2019-09-17 08:24:02,598 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 08:24:06,853 - training_jobs - DEBUG - training time: 531s
2019-09-17 08:24:06,854 - training_jobs - DEBUG - saving to results/20190917_081515_ggnn_.json
2019-09-17 08:24:06,856 - training_jobs - DEBUG - moved jobdict to done_trainings/task_20.yml
2019-09-17 08:24:06,856 - training_jobs - DEBUG - Finished!

2019-09-17 08:24:09,286 - training_jobs - DEBUG - training trains/task_104.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-17 08:24:09,299 - training_jobs - DEBUG - training with: 
2019-09-17 08:24:09,299 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-17 08:24:09,299 - training_jobs - DEBUG - GGNN6
2019-09-17 08:24:09,299 - training_jobs - DEBUG - 
2019-09-17 08:24:09,300 - training_jobs - DEBUG - ggnn training
2019-09-17 08:24:10,860 - training_jobs - DEBUG -  saving results to results/20190917_082410_ggnn_.json
2019-09-17 08:24:10,860 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 08:24:17,379 - training_jobs - ERROR - Error with trains/task_104.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-17 08:24:19,732 - training_jobs - DEBUG - training trains/task_294.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-17 08:24:19,745 - training_jobs - DEBUG - training with: 
2019-09-17 08:24:19,745 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-17 08:24:19,745 - training_jobs - DEBUG - GGNN1
2019-09-17 08:24:19,745 - training_jobs - DEBUG - 
2019-09-17 08:24:19,745 - training_jobs - DEBUG - ggnn training
2019-09-17 08:24:25,482 - training_jobs - DEBUG -  saving results to results/20190917_082425_ggnn_.json
2019-09-17 08:24:25,482 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 08:32:24,553 - training_jobs - DEBUG - test_multiple_models
2019-09-17 08:32:24,553 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 08:32:29,337 - training_jobs - DEBUG - training time: 484s
2019-09-17 08:32:29,337 - training_jobs - DEBUG - saving to results/20190917_082425_ggnn_.json
2019-09-17 08:32:29,339 - training_jobs - DEBUG - moved jobdict to done_trainings/task_294.yml
2019-09-17 08:32:29,339 - training_jobs - DEBUG - Finished!

2019-09-17 08:32:31,787 - training_jobs - DEBUG - training trains/task_130.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-17 08:32:31,800 - training_jobs - DEBUG - training with: 
2019-09-17 08:32:31,800 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-17 08:32:31,800 - training_jobs - DEBUG - GGNN6
2019-09-17 08:32:31,800 - training_jobs - DEBUG - 
2019-09-17 08:32:31,800 - training_jobs - DEBUG - ggnn training
2019-09-17 08:32:34,387 - training_jobs - DEBUG -  saving results to results/20190917_083234_ggnn_.json
2019-09-17 08:32:34,387 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 08:32:43,682 - training_jobs - ERROR - Error with trains/task_130.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-17 08:32:46,036 - training_jobs - DEBUG - training trains/task_106.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-17 08:32:46,048 - training_jobs - DEBUG - training with: 
2019-09-17 08:32:46,048 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-17 08:32:46,048 - training_jobs - DEBUG - GGNN6
2019-09-17 08:32:46,048 - training_jobs - DEBUG - 
2019-09-17 08:32:46,048 - training_jobs - DEBUG - ggnn training
2019-09-17 08:32:47,581 - training_jobs - DEBUG -  saving results to results/20190917_083247_ggnn_.json
2019-09-17 08:32:47,581 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 08:32:54,099 - training_jobs - ERROR - Error with trains/task_106.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-17 08:32:56,442 - training_jobs - DEBUG - training trains/task_207.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 08:32:56,454 - training_jobs - DEBUG - training with: 
2019-09-17 08:32:56,454 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-17 08:32:56,454 - training_jobs - DEBUG - GGNN1
2019-09-17 08:32:56,455 - training_jobs - DEBUG - 
2019-09-17 08:32:56,455 - training_jobs - DEBUG - ggnn training
2019-09-17 08:32:58,052 - training_jobs - DEBUG -  saving results to results/20190917_083258_ggnn_.json
2019-09-17 08:32:58,052 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 08:49:06,532 - training_jobs - DEBUG - test_multiple_models
2019-09-17 08:49:06,532 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 08:49:07,179 - training_jobs - DEBUG - training time: 969s
2019-09-17 08:49:07,179 - training_jobs - DEBUG - saving to results/20190917_083258_ggnn_.json
2019-09-17 08:49:07,181 - training_jobs - DEBUG - moved jobdict to done_trainings/task_207.yml
2019-09-17 08:49:07,181 - training_jobs - DEBUG - Finished!

2019-09-17 08:49:09,536 - training_jobs - DEBUG - training trains/task_4.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-17 08:49:09,548 - training_jobs - DEBUG - training with: 
2019-09-17 08:49:09,548 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-17 08:49:09,548 - training_jobs - DEBUG - GGNN5
2019-09-17 08:49:09,549 - training_jobs - DEBUG - 
2019-09-17 08:49:09,549 - training_jobs - DEBUG - ggnn training
2019-09-17 08:49:11,084 - training_jobs - DEBUG -  saving results to results/20190917_084911_ggnn_.json
2019-09-17 08:49:11,085 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 08:54:33,089 - training_jobs - DEBUG - test_multiple_models
2019-09-17 08:54:33,089 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 08:54:33,715 - training_jobs - DEBUG - training time: 323s
2019-09-17 08:54:33,715 - training_jobs - DEBUG - saving to results/20190917_084911_ggnn_.json
2019-09-17 08:54:33,717 - training_jobs - DEBUG - moved jobdict to done_trainings/task_4.yml
2019-09-17 08:54:33,717 - training_jobs - DEBUG - Finished!

2019-09-17 08:54:36,041 - training_jobs - DEBUG - training trains/task_255.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 08:54:36,052 - training_jobs - DEBUG - training with: 
2019-09-17 08:54:36,052 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-17 08:54:36,052 - training_jobs - DEBUG - GGNN1
2019-09-17 08:54:36,052 - training_jobs - DEBUG - 
2019-09-17 08:54:36,052 - training_jobs - DEBUG - ggnn training
2019-09-17 08:54:37,363 - training_jobs - DEBUG -  saving results to results/20190917_085437_ggnn_.json
2019-09-17 08:54:37,363 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 08:56:01,602 - training_jobs - ERROR - Error with trains/task_255.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: cublas runtime error : the GPU program failed to execute at /pytorch/aten/src/THC/THCBlas.cu:258
2019-09-17 08:56:05,032 - training_jobs - DEBUG - training trains/task_287.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '5e-4'}
2019-09-17 08:56:05,043 - training_jobs - DEBUG - training with: 
2019-09-17 08:56:05,044 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-17 08:56:05,044 - training_jobs - DEBUG - GGNN1
2019-09-17 08:56:05,044 - training_jobs - DEBUG - 
2019-09-17 08:56:05,044 - training_jobs - DEBUG - ggnn training
2019-09-17 08:56:07,227 - training_jobs - DEBUG -  saving results to results/20190917_085607_ggnn_.json
2019-09-17 08:56:07,227 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 08:57:09,070 - training_jobs - ERROR - Error with trains/task_287.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: cublas runtime error : the GPU program failed to execute at /pytorch/aten/src/THC/THCBlas.cu:258
2019-09-17 08:57:12,563 - training_jobs - DEBUG - training trains/task_264.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-17 08:57:12,576 - training_jobs - DEBUG - training with: 
2019-09-17 08:57:12,576 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-17 08:57:12,576 - training_jobs - DEBUG - GGNN1
2019-09-17 08:57:12,576 - training_jobs - DEBUG - 
2019-09-17 08:57:12,576 - training_jobs - DEBUG - ggnn training
2019-09-17 08:57:14,017 - training_jobs - DEBUG -  saving results to results/20190917_085714_ggnn_.json
2019-09-17 08:57:14,017 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 08:58:34,911 - training_jobs - ERROR - Error with trains/task_264.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: cublas runtime error : the GPU program failed to execute at /pytorch/aten/src/THC/THCBlas.cu:258
2019-09-17 08:58:38,324 - training_jobs - DEBUG - training trains/task_18.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '1e-4'}
2019-09-17 08:58:38,337 - training_jobs - DEBUG - training with: 
2019-09-17 08:58:38,337 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-17 08:58:38,337 - training_jobs - DEBUG - GGNN5
2019-09-17 08:58:38,338 - training_jobs - DEBUG - 
2019-09-17 08:58:38,338 - training_jobs - DEBUG - ggnn training
2019-09-17 08:58:39,913 - training_jobs - DEBUG -  saving results to results/20190917_085839_ggnn_.json
2019-09-17 08:58:39,913 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 09:12:59,930 - training_jobs - DEBUG - test_multiple_models
2019-09-17 09:12:59,930 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 09:13:00,586 - training_jobs - DEBUG - training time: 861s
2019-09-17 09:13:00,586 - training_jobs - DEBUG - saving to results/20190917_085839_ggnn_.json
2019-09-17 09:13:00,588 - training_jobs - DEBUG - moved jobdict to done_trainings/task_18.yml
2019-09-17 09:13:00,588 - training_jobs - DEBUG - Finished!

2019-09-17 09:13:02,951 - training_jobs - DEBUG - training trains/task_67.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 09:13:02,964 - training_jobs - DEBUG - training with: 
2019-09-17 09:13:02,964 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-17 09:13:02,964 - training_jobs - DEBUG - GGNN5
2019-09-17 09:13:02,964 - training_jobs - DEBUG - 
2019-09-17 09:13:02,964 - training_jobs - DEBUG - ggnn training
2019-09-17 09:13:04,101 - training_jobs - DEBUG -  saving results to results/20190917_091304_ggnn_.json
2019-09-17 09:13:04,101 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 09:17:01,825 - training_jobs - DEBUG - test_multiple_models
2019-09-17 09:17:01,825 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 09:17:02,271 - training_jobs - DEBUG - training time: 238s
2019-09-17 09:17:02,271 - training_jobs - DEBUG - saving to results/20190917_091304_ggnn_.json
2019-09-17 09:17:02,273 - training_jobs - DEBUG - moved jobdict to done_trainings/task_67.yml
2019-09-17 09:17:02,273 - training_jobs - DEBUG - Finished!

2019-09-17 09:17:04,671 - training_jobs - DEBUG - training trains/task_109.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '5e-4'}
2019-09-17 09:17:04,684 - training_jobs - DEBUG - training with: 
2019-09-17 09:17:04,684 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-17 09:17:04,684 - training_jobs - DEBUG - GGNN6
2019-09-17 09:17:04,684 - training_jobs - DEBUG - 
2019-09-17 09:17:04,684 - training_jobs - DEBUG - ggnn training
2019-09-17 09:17:06,221 - training_jobs - DEBUG -  saving results to results/20190917_091706_ggnn_.json
2019-09-17 09:17:06,221 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 09:17:12,831 - training_jobs - ERROR - Error with trains/task_109.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-17 09:17:15,183 - training_jobs - DEBUG - training trains/task_194.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-17 09:17:15,195 - training_jobs - DEBUG - training with: 
2019-09-17 09:17:15,195 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-17 09:17:15,195 - training_jobs - DEBUG - GGNN6
2019-09-17 09:17:15,195 - training_jobs - DEBUG - 
2019-09-17 09:17:15,195 - training_jobs - DEBUG - ggnn training
2019-09-17 09:17:22,943 - training_jobs - DEBUG -  saving results to results/20190917_091722_ggnn_.json
2019-09-17 09:17:22,943 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 09:17:31,161 - training_jobs - ERROR - Error with trains/task_194.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-17 09:17:33,848 - training_jobs - DEBUG - training trains/task_139.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 09:17:33,862 - training_jobs - DEBUG - training with: 
2019-09-17 09:17:33,862 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-17 09:17:33,862 - training_jobs - DEBUG - GGNN6
2019-09-17 09:17:33,862 - training_jobs - DEBUG - 
2019-09-17 09:17:33,862 - training_jobs - DEBUG - ggnn training
2019-09-17 09:17:35,320 - training_jobs - DEBUG -  saving results to results/20190917_091735_ggnn_.json
2019-09-17 09:17:35,320 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 09:17:41,759 - training_jobs - ERROR - Error with trains/task_139.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-17 09:17:44,320 - training_jobs - DEBUG - training trains/task_302.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '1e-4'}
2019-09-17 09:17:44,332 - training_jobs - DEBUG - training with: 
2019-09-17 09:17:44,333 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-17 09:17:44,333 - training_jobs - DEBUG - GGNN1
2019-09-17 09:17:44,333 - training_jobs - DEBUG - 
2019-09-17 09:17:44,333 - training_jobs - DEBUG - ggnn training
2019-09-17 09:17:45,578 - training_jobs - DEBUG -  saving results to results/20190917_091745_ggnn_.json
2019-09-17 09:17:45,579 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 09:27:23,807 - training_jobs - DEBUG - test_multiple_models
2019-09-17 09:27:23,808 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 09:27:24,293 - training_jobs - DEBUG - training time: 579s
2019-09-17 09:27:24,294 - training_jobs - DEBUG - saving to results/20190917_091745_ggnn_.json
2019-09-17 09:27:24,296 - training_jobs - DEBUG - moved jobdict to done_trainings/task_302.yml
2019-09-17 09:27:24,296 - training_jobs - DEBUG - Finished!

2019-09-17 09:27:26,697 - training_jobs - DEBUG - training trains/task_119.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 09:27:26,709 - training_jobs - DEBUG - training with: 
2019-09-17 09:27:26,710 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-17 09:27:26,710 - training_jobs - DEBUG - GGNN6
2019-09-17 09:27:26,710 - training_jobs - DEBUG - 
2019-09-17 09:27:26,710 - training_jobs - DEBUG - ggnn training
2019-09-17 09:27:29,284 - training_jobs - DEBUG -  saving results to results/20190917_092729_ggnn_.json
2019-09-17 09:27:29,284 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 09:27:38,646 - training_jobs - ERROR - Error with trains/task_119.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-17 09:27:41,057 - training_jobs - DEBUG - training trains/task_2.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-17 09:27:41,070 - training_jobs - DEBUG - training with: 
2019-09-17 09:27:41,070 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max/
2019-09-17 09:27:41,070 - training_jobs - DEBUG - GGNN1
2019-09-17 09:27:41,070 - training_jobs - DEBUG - 
2019-09-17 09:27:41,070 - training_jobs - DEBUG - ggnn training
2019-09-17 09:28:00,305 - training_jobs - DEBUG -  saving results to results/20190917_092800_ggnn_.json
2019-09-17 09:28:00,305 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 10:15:48,531 - training_jobs - DEBUG - test_multiple_models
2019-09-17 10:15:48,531 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 10:16:18,540 - training_jobs - DEBUG - training time: 2898s
2019-09-17 10:16:18,540 - training_jobs - DEBUG - saving to results/20190917_092800_ggnn_.json
2019-09-17 10:16:18,580 - training_jobs - DEBUG - moved jobdict to done_trainings/task_2.yml
2019-09-17 10:16:18,581 - training_jobs - DEBUG - Finished!

2019-09-17 10:16:20,927 - training_jobs - DEBUG - training trains/task_213.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 10:16:20,939 - training_jobs - DEBUG - training with: 
2019-09-17 10:16:20,939 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-17 10:16:20,939 - training_jobs - DEBUG - GGNN1
2019-09-17 10:16:20,939 - training_jobs - DEBUG - 
2019-09-17 10:16:20,939 - training_jobs - DEBUG - ggnn training
2019-09-17 10:16:22,473 - training_jobs - DEBUG -  saving results to results/20190917_101622_ggnn_.json
2019-09-17 10:16:22,473 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 10:32:25,985 - training_jobs - DEBUG - test_multiple_models
2019-09-17 10:32:25,985 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 10:32:26,630 - training_jobs - DEBUG - training time: 964s
2019-09-17 10:32:26,630 - training_jobs - DEBUG - saving to results/20190917_101622_ggnn_.json
2019-09-17 10:32:26,633 - training_jobs - DEBUG - moved jobdict to done_trainings/task_213.yml
2019-09-17 10:32:26,633 - training_jobs - DEBUG - Finished!

2019-09-17 10:32:29,045 - training_jobs - DEBUG - training trains/task_197.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '5e-4'}
2019-09-17 10:32:29,057 - training_jobs - DEBUG - training with: 
2019-09-17 10:32:29,057 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-17 10:32:29,057 - training_jobs - DEBUG - GGNN1
2019-09-17 10:32:29,057 - training_jobs - DEBUG - 
2019-09-17 10:32:29,057 - training_jobs - DEBUG - ggnn training
2019-09-17 10:32:30,607 - training_jobs - DEBUG -  saving results to results/20190917_103230_ggnn_.json
2019-09-17 10:32:30,607 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 10:43:26,209 - training_jobs - DEBUG - test_multiple_models
2019-09-17 10:43:26,209 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 10:43:26,842 - training_jobs - DEBUG - training time: 656s
2019-09-17 10:43:26,842 - training_jobs - DEBUG - saving to results/20190917_103230_ggnn_.json
2019-09-17 10:43:26,877 - training_jobs - DEBUG - moved jobdict to done_trainings/task_197.yml
2019-09-17 10:43:26,878 - training_jobs - DEBUG - Finished!

2019-09-17 10:43:29,232 - training_jobs - DEBUG - training trains/task_79.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 10:43:29,244 - training_jobs - DEBUG - training with: 
2019-09-17 10:43:29,244 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-17 10:43:29,244 - training_jobs - DEBUG - GGNN5
2019-09-17 10:43:29,244 - training_jobs - DEBUG - 
2019-09-17 10:43:29,244 - training_jobs - DEBUG - ggnn training
2019-09-17 10:43:30,388 - training_jobs - DEBUG -  saving results to results/20190917_104330_ggnn_.json
2019-09-17 10:43:30,388 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 10:53:17,685 - training_jobs - DEBUG - test_multiple_models
2019-09-17 10:53:17,685 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 10:53:18,123 - training_jobs - DEBUG - training time: 588s
2019-09-17 10:53:18,123 - training_jobs - DEBUG - saving to results/20190917_104330_ggnn_.json
2019-09-17 10:53:18,125 - training_jobs - DEBUG - moved jobdict to done_trainings/task_79.yml
2019-09-17 10:53:18,125 - training_jobs - DEBUG - Finished!

2019-09-17 10:53:20,466 - training_jobs - DEBUG - training trains/task_155.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 10:53:20,479 - training_jobs - DEBUG - training with: 
2019-09-17 10:53:20,479 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-17 10:53:20,479 - training_jobs - DEBUG - GGNN6
2019-09-17 10:53:20,479 - training_jobs - DEBUG - 
2019-09-17 10:53:20,479 - training_jobs - DEBUG - ggnn training
2019-09-17 10:53:22,678 - training_jobs - DEBUG -  saving results to results/20190917_105322_ggnn_.json
2019-09-17 10:53:22,678 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 10:53:30,938 - training_jobs - ERROR - Error with trains/task_155.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-17 10:53:33,235 - training_jobs - DEBUG - training trains/task_127.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 10:53:33,247 - training_jobs - DEBUG - training with: 
2019-09-17 10:53:33,247 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-17 10:53:33,247 - training_jobs - DEBUG - GGNN6
2019-09-17 10:53:33,247 - training_jobs - DEBUG - 
2019-09-17 10:53:33,247 - training_jobs - DEBUG - ggnn training
2019-09-17 10:53:35,820 - training_jobs - DEBUG -  saving results to results/20190917_105335_ggnn_.json
2019-09-17 10:53:35,820 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 10:53:45,646 - training_jobs - ERROR - Error with trains/task_127.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-17 10:53:47,979 - training_jobs - DEBUG - training trains/task_7.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 10:53:47,991 - training_jobs - DEBUG - training with: 
2019-09-17 10:53:47,991 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-17 10:53:47,991 - training_jobs - DEBUG - GGNN5
2019-09-17 10:53:47,991 - training_jobs - DEBUG - 
2019-09-17 10:53:47,991 - training_jobs - DEBUG - ggnn training
2019-09-17 10:53:49,655 - training_jobs - DEBUG -  saving results to results/20190917_105349_ggnn_.json
2019-09-17 10:53:49,655 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 10:59:07,309 - training_jobs - DEBUG - test_multiple_models
2019-09-17 10:59:07,309 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 10:59:07,931 - training_jobs - DEBUG - training time: 318s
2019-09-17 10:59:07,932 - training_jobs - DEBUG - saving to results/20190917_105349_ggnn_.json
2019-09-17 10:59:07,934 - training_jobs - DEBUG - moved jobdict to done_trainings/task_7.yml
2019-09-17 10:59:07,934 - training_jobs - DEBUG - Finished!

2019-09-17 10:59:10,268 - training_jobs - DEBUG - training trains/task_162.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-17 10:59:10,280 - training_jobs - DEBUG - training with: 
2019-09-17 10:59:10,280 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-17 10:59:10,280 - training_jobs - DEBUG - GGNN6
2019-09-17 10:59:10,280 - training_jobs - DEBUG - 
2019-09-17 10:59:10,280 - training_jobs - DEBUG - ggnn training
2019-09-17 10:59:12,514 - training_jobs - DEBUG -  saving results to results/20190917_105912_ggnn_.json
2019-09-17 10:59:12,514 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 10:59:20,789 - training_jobs - ERROR - Error with trains/task_162.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-17 10:59:23,143 - training_jobs - DEBUG - training trains/task_120.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-17 10:59:23,156 - training_jobs - DEBUG - training with: 
2019-09-17 10:59:23,156 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-17 10:59:23,156 - training_jobs - DEBUG - GGNN6
2019-09-17 10:59:23,156 - training_jobs - DEBUG - 
2019-09-17 10:59:23,156 - training_jobs - DEBUG - ggnn training
2019-09-17 10:59:25,876 - training_jobs - DEBUG -  saving results to results/20190917_105925_ggnn_.json
2019-09-17 10:59:25,876 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 10:59:35,726 - training_jobs - ERROR - Error with trains/task_120.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-17 10:59:38,080 - training_jobs - DEBUG - training trains/task_243.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 10:59:38,093 - training_jobs - DEBUG - training with: 
2019-09-17 10:59:38,094 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-17 10:59:38,094 - training_jobs - DEBUG - GGNN1
2019-09-17 10:59:38,094 - training_jobs - DEBUG - 
2019-09-17 10:59:38,094 - training_jobs - DEBUG - ggnn training
2019-09-17 10:59:39,491 - training_jobs - DEBUG -  saving results to results/20190917_105939_ggnn_.json
2019-09-17 10:59:39,492 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 11:00:42,053 - training_jobs - ERROR - Error with trains/task_243.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: cublas runtime error : the GPU program failed to execute at /pytorch/aten/src/THC/THCBlas.cu:258
2019-09-17 11:00:45,524 - training_jobs - DEBUG - training trains/task_152.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-17 11:00:45,537 - training_jobs - DEBUG - training with: 
2019-09-17 11:00:45,537 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-17 11:00:45,537 - training_jobs - DEBUG - GGNN6
2019-09-17 11:00:45,537 - training_jobs - DEBUG - 
2019-09-17 11:00:45,537 - training_jobs - DEBUG - ggnn training
2019-09-17 11:00:47,967 - training_jobs - DEBUG -  saving results to results/20190917_110047_ggnn_.json
2019-09-17 11:00:47,967 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 11:00:56,503 - training_jobs - ERROR - Error with trains/task_152.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-17 11:00:59,051 - training_jobs - DEBUG - training trains/task_19.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 20,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 11:00:59,065 - training_jobs - DEBUG - training with: 
2019-09-17 11:00:59,065 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-17 11:00:59,065 - training_jobs - DEBUG - GGNN5
2019-09-17 11:00:59,065 - training_jobs - DEBUG - 
2019-09-17 11:00:59,065 - training_jobs - DEBUG - ggnn training
2019-09-17 11:01:02,023 - training_jobs - DEBUG -  saving results to results/20190917_110102_ggnn_.json
2019-09-17 11:01:02,023 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 11:09:56,568 - training_jobs - DEBUG - test_multiple_models
2019-09-17 11:09:56,568 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 11:09:57,658 - training_jobs - DEBUG - training time: 536s
2019-09-17 11:09:57,658 - training_jobs - DEBUG - saving to results/20190917_110102_ggnn_.json
2019-09-17 11:09:57,661 - training_jobs - DEBUG - moved jobdict to done_trainings/task_19.yml
2019-09-17 11:09:57,661 - training_jobs - DEBUG - Finished!

2019-09-17 11:10:00,079 - training_jobs - DEBUG - training trains/task_325.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '5e-4'}
2019-09-17 11:10:00,091 - training_jobs - DEBUG - training with: 
2019-09-17 11:10:00,091 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-17 11:10:00,092 - training_jobs - DEBUG - GGNN1
2019-09-17 11:10:00,092 - training_jobs - DEBUG - 
2019-09-17 11:10:00,092 - training_jobs - DEBUG - ggnn training
2019-09-17 11:10:02,010 - training_jobs - DEBUG -  saving results to results/20190917_111002_ggnn_.json
2019-09-17 11:10:02,010 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 11:21:54,432 - training_jobs - DEBUG - test_multiple_models
2019-09-17 11:21:54,432 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 11:21:55,295 - training_jobs - DEBUG - training time: 713s
2019-09-17 11:21:55,295 - training_jobs - DEBUG - saving to results/20190917_111002_ggnn_.json
2019-09-17 11:21:55,297 - training_jobs - DEBUG - moved jobdict to done_trainings/task_325.yml
2019-09-17 11:21:55,297 - training_jobs - DEBUG - Finished!

2019-09-17 11:21:57,659 - training_jobs - DEBUG - training trains/task_338.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '1e-4'}
2019-09-17 11:21:57,671 - training_jobs - DEBUG - training with: 
2019-09-17 11:21:57,671 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-17 11:21:57,672 - training_jobs - DEBUG - GGNN1
2019-09-17 11:21:57,672 - training_jobs - DEBUG - 
2019-09-17 11:21:57,672 - training_jobs - DEBUG - ggnn training
2019-09-17 11:21:59,602 - training_jobs - DEBUG -  saving results to results/20190917_112159_ggnn_.json
2019-09-17 11:21:59,602 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 11:38:52,797 - training_jobs - DEBUG - test_multiple_models
2019-09-17 11:38:52,797 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 11:38:53,731 - training_jobs - DEBUG - training time: 1014s
2019-09-17 11:38:53,732 - training_jobs - DEBUG - saving to results/20190917_112159_ggnn_.json
2019-09-17 11:38:53,733 - training_jobs - DEBUG - moved jobdict to done_trainings/task_338.yml
2019-09-17 11:38:53,733 - training_jobs - DEBUG - Finished!

2019-09-17 11:38:56,565 - training_jobs - DEBUG - training trains/task_202.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-17 11:38:56,580 - training_jobs - DEBUG - training with: 
2019-09-17 11:38:56,580 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-17 11:38:56,580 - training_jobs - DEBUG - GGNN1
2019-09-17 11:38:56,580 - training_jobs - DEBUG - 
2019-09-17 11:38:56,580 - training_jobs - DEBUG - ggnn training
2019-09-17 11:38:58,539 - training_jobs - DEBUG -  saving results to results/20190917_113858_ggnn_.json
2019-09-17 11:38:58,539 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 11:50:36,771 - training_jobs - DEBUG - test_multiple_models
2019-09-17 11:50:36,771 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 11:50:37,442 - training_jobs - DEBUG - training time: 699s
2019-09-17 11:50:37,443 - training_jobs - DEBUG - saving to results/20190917_113858_ggnn_.json
2019-09-17 11:50:37,445 - training_jobs - DEBUG - moved jobdict to done_trainings/task_202.yml
2019-09-17 11:50:37,445 - training_jobs - DEBUG - Finished!

2019-09-17 11:50:39,833 - training_jobs - DEBUG - training trains/task_319.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '5e-4'}
2019-09-17 11:50:39,845 - training_jobs - DEBUG - training with: 
2019-09-17 11:50:39,845 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-17 11:50:39,845 - training_jobs - DEBUG - GGNN1
2019-09-17 11:50:39,845 - training_jobs - DEBUG - 
2019-09-17 11:50:39,845 - training_jobs - DEBUG - ggnn training
2019-09-17 11:50:41,826 - training_jobs - DEBUG -  saving results to results/20190917_115041_ggnn_.json
2019-09-17 11:50:41,826 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 12:02:40,766 - training_jobs - DEBUG - test_multiple_models
2019-09-17 12:02:40,766 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 12:02:41,634 - training_jobs - DEBUG - training time: 720s
2019-09-17 12:02:41,634 - training_jobs - DEBUG - saving to results/20190917_115041_ggnn_.json
2019-09-17 12:02:41,636 - training_jobs - DEBUG - moved jobdict to done_trainings/task_319.yml
2019-09-17 12:02:41,636 - training_jobs - DEBUG - Finished!

2019-09-17 12:02:44,189 - training_jobs - DEBUG - training trains/task_280.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-17 12:02:44,202 - training_jobs - DEBUG - training with: 
2019-09-17 12:02:44,202 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-17 12:02:44,202 - training_jobs - DEBUG - GGNN1
2019-09-17 12:02:44,202 - training_jobs - DEBUG - 
2019-09-17 12:02:44,202 - training_jobs - DEBUG - ggnn training
2019-09-17 12:02:46,734 - training_jobs - DEBUG -  saving results to results/20190917_120246_ggnn_.json
2019-09-17 12:02:46,734 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 12:03:50,447 - training_jobs - ERROR - Error with trains/task_280.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA error: device-side assert triggered
2019-09-17 12:03:54,262 - training_jobs - DEBUG - training trains/task_132.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-17 12:03:54,277 - training_jobs - DEBUG - training with: 
2019-09-17 12:03:54,278 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-17 12:03:54,278 - training_jobs - DEBUG - GGNN6
2019-09-17 12:03:54,278 - training_jobs - DEBUG - 
2019-09-17 12:03:54,278 - training_jobs - DEBUG - ggnn training
2019-09-17 12:03:55,914 - training_jobs - DEBUG -  saving results to results/20190917_120355_ggnn_.json
2019-09-17 12:03:55,914 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 12:04:02,747 - training_jobs - ERROR - Error with trains/task_132.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-17 12:04:05,290 - training_jobs - DEBUG - training trains/task_180.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-17 12:04:05,303 - training_jobs - DEBUG - training with: 
2019-09-17 12:04:05,303 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-17 12:04:05,303 - training_jobs - DEBUG - GGNN6
2019-09-17 12:04:05,303 - training_jobs - DEBUG - 
2019-09-17 12:04:05,303 - training_jobs - DEBUG - ggnn training
2019-09-17 12:04:07,388 - training_jobs - DEBUG -  saving results to results/20190917_120407_ggnn_.json
2019-09-17 12:04:07,388 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 12:04:15,289 - training_jobs - ERROR - Error with trains/task_180.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-17 12:04:17,834 - training_jobs - DEBUG - training trains/task_333.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 12:04:17,849 - training_jobs - DEBUG - training with: 
2019-09-17 12:04:17,849 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-17 12:04:17,849 - training_jobs - DEBUG - GGNN1
2019-09-17 12:04:17,849 - training_jobs - DEBUG - 
2019-09-17 12:04:17,849 - training_jobs - DEBUG - ggnn training
2019-09-17 12:04:20,206 - training_jobs - DEBUG -  saving results to results/20190917_120420_ggnn_.json
2019-09-17 12:04:20,206 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 12:19:03,443 - training_jobs - DEBUG - test_multiple_models
2019-09-17 12:19:03,443 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 12:19:04,289 - training_jobs - DEBUG - training time: 884s
2019-09-17 12:19:04,289 - training_jobs - DEBUG - saving to results/20190917_120420_ggnn_.json
2019-09-17 12:19:04,291 - training_jobs - DEBUG - moved jobdict to done_trainings/task_333.yml
2019-09-17 12:19:04,291 - training_jobs - DEBUG - Finished!

2019-09-17 12:19:06,693 - training_jobs - DEBUG - training trains/task_346.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-17 12:19:06,705 - training_jobs - DEBUG - training with: 
2019-09-17 12:19:06,705 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max/
2019-09-17 12:19:06,705 - training_jobs - DEBUG - GGNN5
2019-09-17 12:19:06,706 - training_jobs - DEBUG - 
2019-09-17 12:19:06,706 - training_jobs - DEBUG - ggnn training
2019-09-17 12:19:24,493 - training_jobs - DEBUG -  saving results to results/20190917_121924_ggnn_.json
2019-09-17 12:19:24,494 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 12:19:36,694 - training_jobs - ERROR - Error with trains/task_346.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1126, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'd5'
2019-09-17 12:19:38,955 - training_jobs - DEBUG - training trains/task_300.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-17 12:19:38,967 - training_jobs - DEBUG - training with: 
2019-09-17 12:19:38,967 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-17 12:19:38,967 - training_jobs - DEBUG - GGNN1
2019-09-17 12:19:38,967 - training_jobs - DEBUG - 
2019-09-17 12:19:38,967 - training_jobs - DEBUG - ggnn training
2019-09-17 12:19:40,171 - training_jobs - DEBUG -  saving results to results/20190917_121940_ggnn_.json
2019-09-17 12:19:40,171 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 12:27:41,617 - training_jobs - DEBUG - test_multiple_models
2019-09-17 12:27:41,617 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 12:27:42,066 - training_jobs - DEBUG - training time: 482s
2019-09-17 12:27:42,066 - training_jobs - DEBUG - saving to results/20190917_121940_ggnn_.json
2019-09-17 12:27:42,068 - training_jobs - DEBUG - moved jobdict to done_trainings/task_300.yml
2019-09-17 12:27:42,068 - training_jobs - DEBUG - Finished!

2019-09-17 12:27:44,423 - training_jobs - DEBUG - training trains/task_354.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-17 12:27:44,434 - training_jobs - DEBUG - training with: 
2019-09-17 12:27:44,434 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max/
2019-09-17 12:27:44,435 - training_jobs - DEBUG - GGNN5
2019-09-17 12:27:44,435 - training_jobs - DEBUG - 
2019-09-17 12:27:44,435 - training_jobs - DEBUG - ggnn training
2019-09-17 12:27:48,228 - training_jobs - DEBUG -  saving results to results/20190917_122748_ggnn_.json
2019-09-17 12:27:48,228 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 12:27:59,806 - training_jobs - ERROR - Error with trains/task_354.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1126, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'd5'
2019-09-17 12:28:03,396 - training_jobs - DEBUG - training trains/task_241.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '5e-4'}
2019-09-17 12:28:03,417 - training_jobs - DEBUG - training with: 
2019-09-17 12:28:03,417 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-17 12:28:03,417 - training_jobs - DEBUG - GGNN1
2019-09-17 12:28:03,417 - training_jobs - DEBUG - 
2019-09-17 12:28:03,417 - training_jobs - DEBUG - ggnn training
2019-09-17 12:28:07,815 - training_jobs - DEBUG -  saving results to results/20190917_122807_ggnn_.json
2019-09-17 12:28:07,815 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 12:58:31,071 - training_jobs - DEBUG - test_multiple_models
2019-09-17 12:58:31,071 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 12:58:32,302 - training_jobs - DEBUG - training time: 1824s
2019-09-17 12:58:32,302 - training_jobs - DEBUG - saving to results/20190917_122807_ggnn_.json
2019-09-17 12:58:32,305 - training_jobs - DEBUG - moved jobdict to done_trainings/task_241.yml
2019-09-17 12:58:32,305 - training_jobs - DEBUG - Finished!

2019-09-17 12:58:35,062 - training_jobs - DEBUG - training trains/task_250.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-17 12:58:35,077 - training_jobs - DEBUG - training with: 
2019-09-17 12:58:35,077 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-17 12:58:35,077 - training_jobs - DEBUG - GGNN1
2019-09-17 12:58:35,077 - training_jobs - DEBUG - 
2019-09-17 12:58:35,077 - training_jobs - DEBUG - ggnn training
2019-09-17 12:58:36,619 - training_jobs - DEBUG -  saving results to results/20190917_125836_ggnn_.json
2019-09-17 12:58:36,619 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 12:59:40,646 - training_jobs - ERROR - Error with trains/task_250.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: cublas runtime error : the GPU program failed to execute at /pytorch/aten/src/THC/THCBlas.cu:258
2019-09-17 12:59:44,266 - training_jobs - DEBUG - training trains/task_314.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '1e-4'}
2019-09-17 12:59:44,279 - training_jobs - DEBUG - training with: 
2019-09-17 12:59:44,279 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-17 12:59:44,279 - training_jobs - DEBUG - GGNN1
2019-09-17 12:59:44,279 - training_jobs - DEBUG - 
2019-09-17 12:59:44,279 - training_jobs - DEBUG - ggnn training
2019-09-17 12:59:45,555 - training_jobs - DEBUG -  saving results to results/20190917_125945_ggnn_.json
2019-09-17 12:59:45,555 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 13:13:22,182 - training_jobs - DEBUG - test_multiple_models
2019-09-17 13:13:22,183 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 13:13:22,671 - training_jobs - DEBUG - training time: 817s
2019-09-17 13:13:22,671 - training_jobs - DEBUG - saving to results/20190917_125945_ggnn_.json
2019-09-17 13:13:22,673 - training_jobs - DEBUG - moved jobdict to done_trainings/task_314.yml
2019-09-17 13:13:22,673 - training_jobs - DEBUG - Finished!

2019-09-17 13:13:25,032 - training_jobs - DEBUG - training trains/task_286.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-17 13:13:25,044 - training_jobs - DEBUG - training with: 
2019-09-17 13:13:25,044 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-17 13:13:25,044 - training_jobs - DEBUG - GGNN1
2019-09-17 13:13:25,044 - training_jobs - DEBUG - 
2019-09-17 13:13:25,044 - training_jobs - DEBUG - ggnn training
2019-09-17 13:13:27,287 - training_jobs - DEBUG -  saving results to results/20190917_131327_ggnn_.json
2019-09-17 13:13:27,287 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 13:14:31,632 - training_jobs - ERROR - Error with trains/task_286.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: cublas runtime error : the GPU program failed to execute at /pytorch/aten/src/THC/THCBlas.cu:258
2019-09-17 13:14:35,477 - training_jobs - DEBUG - training trains/task_290.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '1e-4'}
2019-09-17 13:14:35,491 - training_jobs - DEBUG - training with: 
2019-09-17 13:14:35,492 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-17 13:14:35,492 - training_jobs - DEBUG - GGNN1
2019-09-17 13:14:35,492 - training_jobs - DEBUG - 
2019-09-17 13:14:35,492 - training_jobs - DEBUG - ggnn training
2019-09-17 13:14:38,181 - training_jobs - DEBUG -  saving results to results/20190917_131438_ggnn_.json
2019-09-17 13:14:38,181 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 13:15:46,505 - training_jobs - ERROR - Error with trains/task_290.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA error: device-side assert triggered
2019-09-17 13:15:50,267 - training_jobs - DEBUG - training trains/task_233.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '5e-4'}
2019-09-17 13:15:50,281 - training_jobs - DEBUG - training with: 
2019-09-17 13:15:50,281 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-17 13:15:50,281 - training_jobs - DEBUG - GGNN1
2019-09-17 13:15:50,281 - training_jobs - DEBUG - 
2019-09-17 13:15:50,282 - training_jobs - DEBUG - ggnn training
2019-09-17 13:15:53,310 - training_jobs - DEBUG -  saving results to results/20190917_131553_ggnn_.json
2019-09-17 13:15:53,310 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 13:41:06,260 - training_jobs - DEBUG - test_multiple_models
2019-09-17 13:41:06,260 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 13:41:07,328 - training_jobs - DEBUG - training time: 1514s
2019-09-17 13:41:07,328 - training_jobs - DEBUG - saving to results/20190917_131553_ggnn_.json
2019-09-17 13:41:07,330 - training_jobs - DEBUG - moved jobdict to done_trainings/task_233.yml
2019-09-17 13:41:07,330 - training_jobs - DEBUG - Finished!

2019-09-17 13:41:09,757 - training_jobs - DEBUG - training trains/task_261.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 13:41:09,769 - training_jobs - DEBUG - training with: 
2019-09-17 13:41:09,769 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-17 13:41:09,769 - training_jobs - DEBUG - GGNN1
2019-09-17 13:41:09,769 - training_jobs - DEBUG - 
2019-09-17 13:41:09,769 - training_jobs - DEBUG - ggnn training
2019-09-17 13:41:11,118 - training_jobs - DEBUG -  saving results to results/20190917_134111_ggnn_.json
2019-09-17 13:41:11,118 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 13:42:39,663 - training_jobs - ERROR - Error with trains/task_261.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA error: device-side assert triggered
2019-09-17 13:42:43,059 - training_jobs - DEBUG - training trains/task_125.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '5e-4'}
2019-09-17 13:42:43,071 - training_jobs - DEBUG - training with: 
2019-09-17 13:42:43,071 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-17 13:42:43,071 - training_jobs - DEBUG - GGNN6
2019-09-17 13:42:43,071 - training_jobs - DEBUG - 
2019-09-17 13:42:43,071 - training_jobs - DEBUG - ggnn training
2019-09-17 13:42:45,633 - training_jobs - DEBUG -  saving results to results/20190917_134245_ggnn_.json
2019-09-17 13:42:45,633 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 13:42:54,857 - training_jobs - ERROR - Error with trains/task_125.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-17 13:42:57,165 - training_jobs - DEBUG - training trains/task_47.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 13:42:57,176 - training_jobs - DEBUG - training with: 
2019-09-17 13:42:57,176 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-17 13:42:57,176 - training_jobs - DEBUG - GGNN5
2019-09-17 13:42:57,177 - training_jobs - DEBUG - 
2019-09-17 13:42:57,177 - training_jobs - DEBUG - ggnn training
2019-09-17 13:42:58,632 - training_jobs - DEBUG -  saving results to results/20190917_134258_ggnn_.json
2019-09-17 13:42:58,632 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 13:44:10,534 - training_jobs - ERROR - Error with trains/task_47.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: cublas runtime error : the GPU program failed to execute at /pytorch/aten/src/THC/THCBlas.cu:258
2019-09-17 13:44:14,481 - training_jobs - DEBUG - training trains/task_159.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 13:44:14,496 - training_jobs - DEBUG - training with: 
2019-09-17 13:44:14,496 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-17 13:44:14,496 - training_jobs - DEBUG - GGNN6
2019-09-17 13:44:14,496 - training_jobs - DEBUG - 
2019-09-17 13:44:14,496 - training_jobs - DEBUG - ggnn training
2019-09-17 13:44:17,306 - training_jobs - DEBUG -  saving results to results/20190917_134417_ggnn_.json
2019-09-17 13:44:17,306 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 13:44:26,525 - training_jobs - ERROR - Error with trains/task_159.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-17 13:44:29,101 - training_jobs - DEBUG - training trains/task_236.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '1e-4'}
2019-09-17 13:44:29,115 - training_jobs - DEBUG - training with: 
2019-09-17 13:44:29,115 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-17 13:44:29,115 - training_jobs - DEBUG - GGNN1
2019-09-17 13:44:29,115 - training_jobs - DEBUG - 
2019-09-17 13:44:29,115 - training_jobs - DEBUG - ggnn training
2019-09-17 13:44:31,998 - training_jobs - DEBUG -  saving results to results/20190917_134431_ggnn_.json
2019-09-17 13:44:31,998 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 14:14:51,250 - training_jobs - DEBUG - test_multiple_models
2019-09-17 14:14:51,251 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 14:14:52,418 - training_jobs - DEBUG - training time: 1820s
2019-09-17 14:14:52,418 - training_jobs - DEBUG - saving to results/20190917_134431_ggnn_.json
2019-09-17 14:14:52,421 - training_jobs - DEBUG - moved jobdict to done_trainings/task_236.yml
2019-09-17 14:14:52,421 - training_jobs - DEBUG - Finished!

2019-09-17 14:14:55,004 - training_jobs - DEBUG - training trains/task_268.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-17 14:14:55,017 - training_jobs - DEBUG - training with: 
2019-09-17 14:14:55,018 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-17 14:14:55,018 - training_jobs - DEBUG - GGNN1
2019-09-17 14:14:55,018 - training_jobs - DEBUG - 
2019-09-17 14:14:55,018 - training_jobs - DEBUG - ggnn training
2019-09-17 14:14:57,415 - training_jobs - DEBUG -  saving results to results/20190917_141457_ggnn_.json
2019-09-17 14:14:57,415 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 14:15:46,548 - training_jobs - ERROR - Error with trains/task_268.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: cublas runtime error : the GPU program failed to execute at /pytorch/aten/src/THC/THCBlas.cu:258
2019-09-17 14:15:50,389 - training_jobs - DEBUG - training trains/task_187.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 14:15:50,403 - training_jobs - DEBUG - training with: 
2019-09-17 14:15:50,403 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-17 14:15:50,403 - training_jobs - DEBUG - GGNN6
2019-09-17 14:15:50,403 - training_jobs - DEBUG - 
2019-09-17 14:15:50,403 - training_jobs - DEBUG - ggnn training
2019-09-17 14:15:52,780 - training_jobs - DEBUG -  saving results to results/20190917_141552_ggnn_.json
2019-09-17 14:15:52,780 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 14:16:01,125 - training_jobs - ERROR - Error with trains/task_187.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-17 14:16:03,507 - training_jobs - DEBUG - training trains/task_198.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-17 14:16:03,520 - training_jobs - DEBUG - training with: 
2019-09-17 14:16:03,520 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-17 14:16:03,520 - training_jobs - DEBUG - GGNN1
2019-09-17 14:16:03,520 - training_jobs - DEBUG - 
2019-09-17 14:16:03,520 - training_jobs - DEBUG - ggnn training
2019-09-17 14:16:05,273 - training_jobs - DEBUG -  saving results to results/20190917_141605_ggnn_.json
2019-09-17 14:16:05,273 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 14:27:00,801 - training_jobs - DEBUG - test_multiple_models
2019-09-17 14:27:00,801 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 14:27:01,448 - training_jobs - DEBUG - training time: 656s
2019-09-17 14:27:01,448 - training_jobs - DEBUG - saving to results/20190917_141605_ggnn_.json
2019-09-17 14:27:01,450 - training_jobs - DEBUG - moved jobdict to done_trainings/task_198.yml
2019-09-17 14:27:01,450 - training_jobs - DEBUG - Finished!

2019-09-17 14:27:03,795 - training_jobs - DEBUG - training trains/task_90.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 20,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '1e-4'}
2019-09-17 14:27:03,807 - training_jobs - DEBUG - training with: 
2019-09-17 14:27:03,807 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-17 14:27:03,807 - training_jobs - DEBUG - GGNN5
2019-09-17 14:27:03,807 - training_jobs - DEBUG - 
2019-09-17 14:27:03,807 - training_jobs - DEBUG - ggnn training
2019-09-17 14:27:05,733 - training_jobs - DEBUG -  saving results to results/20190917_142705_ggnn_.json
2019-09-17 14:27:05,733 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 14:32:31,435 - training_jobs - DEBUG - test_multiple_models
2019-09-17 14:32:31,435 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 14:32:32,247 - training_jobs - DEBUG - training time: 327s
2019-09-17 14:32:32,247 - training_jobs - DEBUG - saving to results/20190917_142705_ggnn_.json
2019-09-17 14:32:32,249 - training_jobs - DEBUG - moved jobdict to done_trainings/task_90.yml
2019-09-17 14:32:32,249 - training_jobs - DEBUG - Finished!

2019-09-17 14:32:34,624 - training_jobs - DEBUG - training trains/task_281.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '5e-4'}
2019-09-17 14:32:34,636 - training_jobs - DEBUG - training with: 
2019-09-17 14:32:34,636 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-17 14:32:34,636 - training_jobs - DEBUG - GGNN1
2019-09-17 14:32:34,636 - training_jobs - DEBUG - 
2019-09-17 14:32:34,636 - training_jobs - DEBUG - ggnn training
2019-09-17 14:32:36,839 - training_jobs - DEBUG -  saving results to results/20190917_143236_ggnn_.json
2019-09-17 14:32:36,839 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 14:33:39,474 - training_jobs - ERROR - Error with trains/task_281.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: cublas runtime error : the GPU program failed to execute at /pytorch/aten/src/THC/THCBlas.cu:258
2019-09-17 14:33:43,332 - training_jobs - DEBUG - training trains/task_161.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '5e-4'}
2019-09-17 14:33:43,347 - training_jobs - DEBUG - training with: 
2019-09-17 14:33:43,347 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-17 14:33:43,348 - training_jobs - DEBUG - GGNN6
2019-09-17 14:33:43,348 - training_jobs - DEBUG - 
2019-09-17 14:33:43,348 - training_jobs - DEBUG - ggnn training
2019-09-17 14:33:46,083 - training_jobs - DEBUG -  saving results to results/20190917_143346_ggnn_.json
2019-09-17 14:33:46,083 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 14:33:55,141 - training_jobs - ERROR - Error with trains/task_161.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-17 14:33:57,571 - training_jobs - DEBUG - training trains/task_17.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '5e-4'}
2019-09-17 14:33:57,585 - training_jobs - DEBUG - training with: 
2019-09-17 14:33:57,585 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-17 14:33:57,585 - training_jobs - DEBUG - GGNN5
2019-09-17 14:33:57,585 - training_jobs - DEBUG - 
2019-09-17 14:33:57,585 - training_jobs - DEBUG - ggnn training
2019-09-17 14:33:59,304 - training_jobs - DEBUG -  saving results to results/20190917_143359_ggnn_.json
2019-09-17 14:33:59,304 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 14:48:20,420 - training_jobs - DEBUG - test_multiple_models
2019-09-17 14:48:20,420 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 14:48:21,071 - training_jobs - DEBUG - training time: 862s
2019-09-17 14:48:21,071 - training_jobs - DEBUG - saving to results/20190917_143359_ggnn_.json
2019-09-17 14:48:21,073 - training_jobs - DEBUG - moved jobdict to done_trainings/task_17.yml
2019-09-17 14:48:21,073 - training_jobs - DEBUG - Finished!

2019-09-17 14:48:23,444 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-17 14:48:23,456 - training_jobs - DEBUG - training with: 
2019-09-17 14:48:23,456 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max/
2019-09-17 14:48:23,457 - training_jobs - DEBUG - GGNN1
2019-09-17 14:48:23,457 - training_jobs - DEBUG - 
2019-09-17 14:48:23,457 - training_jobs - DEBUG - ggnn training
2019-09-17 14:48:35,866 - training_jobs - DEBUG -  saving results to results/20190917_144835_ggnn_.json
2019-09-17 14:48:35,867 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 16:07:29,537 - training_jobs - DEBUG - test_multiple_models
2019-09-17 16:07:29,537 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 16:07:34,907 - training_jobs - DEBUG - training time: 4739s
2019-09-17 16:07:34,907 - training_jobs - DEBUG - saving to results/20190917_144835_ggnn_.json
2019-09-17 16:07:34,917 - training_jobs - DEBUG - moved jobdict to done_trainings/task_0.yml
2019-09-17 16:07:34,917 - training_jobs - DEBUG - Finished!

2019-09-17 16:07:39,475 - training_jobs - DEBUG - training trains/task_11.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 16:07:39,500 - training_jobs - DEBUG - training with: 
2019-09-17 16:07:39,500 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-17 16:07:39,500 - training_jobs - DEBUG - GGNN5
2019-09-17 16:07:39,500 - training_jobs - DEBUG - 
2019-09-17 16:07:39,500 - training_jobs - DEBUG - ggnn training
2019-09-17 16:07:42,467 - training_jobs - DEBUG -  saving results to results/20190917_160742_ggnn_.json
2019-09-17 16:07:42,467 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 16:20:51,171 - training_jobs - DEBUG - test_multiple_models
2019-09-17 16:20:51,171 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 16:20:51,792 - training_jobs - DEBUG - training time: 789s
2019-09-17 16:20:51,793 - training_jobs - DEBUG - saving to results/20190917_160742_ggnn_.json
2019-09-17 16:20:51,795 - training_jobs - DEBUG - moved jobdict to done_trainings/task_11.yml
2019-09-17 16:20:51,795 - training_jobs - DEBUG - Finished!

2019-09-17 16:20:54,182 - training_jobs - DEBUG - training trains/task_303.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 16:20:54,215 - training_jobs - DEBUG - training with: 
2019-09-17 16:20:54,215 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-17 16:20:54,215 - training_jobs - DEBUG - GGNN1
2019-09-17 16:20:54,216 - training_jobs - DEBUG - 
2019-09-17 16:20:54,216 - training_jobs - DEBUG - ggnn training
2019-09-17 16:20:55,376 - training_jobs - DEBUG -  saving results to results/20190917_162055_ggnn_.json
2019-09-17 16:20:55,376 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 16:32:49,577 - training_jobs - DEBUG - test_multiple_models
2019-09-17 16:32:49,578 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 16:32:50,033 - training_jobs - DEBUG - training time: 715s
2019-09-17 16:32:50,033 - training_jobs - DEBUG - saving to results/20190917_162055_ggnn_.json
2019-09-17 16:32:50,035 - training_jobs - DEBUG - moved jobdict to done_trainings/task_303.yml
2019-09-17 16:32:50,035 - training_jobs - DEBUG - Finished!

2019-09-17 16:32:52,380 - training_jobs - DEBUG - training trains/task_310.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-17 16:32:52,402 - training_jobs - DEBUG - training with: 
2019-09-17 16:32:52,403 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-17 16:32:52,403 - training_jobs - DEBUG - GGNN1
2019-09-17 16:32:52,403 - training_jobs - DEBUG - 
2019-09-17 16:32:52,403 - training_jobs - DEBUG - ggnn training
2019-09-17 16:32:53,539 - training_jobs - DEBUG -  saving results to results/20190917_163253_ggnn_.json
2019-09-17 16:32:53,539 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 16:44:46,976 - training_jobs - DEBUG - test_multiple_models
2019-09-17 16:44:46,976 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 16:44:47,431 - training_jobs - DEBUG - training time: 714s
2019-09-17 16:44:47,431 - training_jobs - DEBUG - saving to results/20190917_163253_ggnn_.json
2019-09-17 16:44:47,433 - training_jobs - DEBUG - moved jobdict to done_trainings/task_310.yml
2019-09-17 16:44:47,433 - training_jobs - DEBUG - Finished!

2019-09-17 16:44:49,795 - training_jobs - DEBUG - training trains/task_22.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 20,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '1e-4'}
2019-09-17 16:44:49,818 - training_jobs - DEBUG - training with: 
2019-09-17 16:44:49,818 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-17 16:44:49,818 - training_jobs - DEBUG - GGNN5
2019-09-17 16:44:49,818 - training_jobs - DEBUG - 
2019-09-17 16:44:49,818 - training_jobs - DEBUG - ggnn training
2019-09-17 16:44:53,087 - training_jobs - DEBUG -  saving results to results/20190917_164453_ggnn_.json
2019-09-17 16:44:53,087 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 16:54:34,454 - training_jobs - DEBUG - test_multiple_models
2019-09-17 16:54:34,454 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 16:54:35,523 - training_jobs - DEBUG - training time: 582s
2019-09-17 16:54:35,523 - training_jobs - DEBUG - saving to results/20190917_164453_ggnn_.json
2019-09-17 16:54:35,526 - training_jobs - DEBUG - moved jobdict to done_trainings/task_22.yml
2019-09-17 16:54:35,526 - training_jobs - DEBUG - Finished!

2019-09-17 16:54:37,951 - training_jobs - DEBUG - training trains/task_136.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-17 16:54:37,973 - training_jobs - DEBUG - training with: 
2019-09-17 16:54:37,973 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-17 16:54:37,973 - training_jobs - DEBUG - GGNN6
2019-09-17 16:54:37,973 - training_jobs - DEBUG - 
2019-09-17 16:54:37,974 - training_jobs - DEBUG - ggnn training
2019-09-17 16:54:39,307 - training_jobs - DEBUG -  saving results to results/20190917_165439_ggnn_.json
2019-09-17 16:54:39,307 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 16:54:45,215 - training_jobs - ERROR - Error with trains/task_136.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-17 16:54:47,597 - training_jobs - DEBUG - training trains/task_142.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-17 16:54:47,618 - training_jobs - DEBUG - training with: 
2019-09-17 16:54:47,618 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-17 16:54:47,619 - training_jobs - DEBUG - GGNN6
2019-09-17 16:54:47,619 - training_jobs - DEBUG - 
2019-09-17 16:54:47,619 - training_jobs - DEBUG - ggnn training
2019-09-17 16:54:48,944 - training_jobs - DEBUG -  saving results to results/20190917_165448_ggnn_.json
2019-09-17 16:54:48,944 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 16:54:55,247 - training_jobs - ERROR - Error with trains/task_142.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-17 16:54:57,808 - training_jobs - DEBUG - training trains/task_133.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '5e-4'}
2019-09-17 16:54:57,822 - training_jobs - DEBUG - training with: 
2019-09-17 16:54:57,822 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-17 16:54:57,822 - training_jobs - DEBUG - GGNN6
2019-09-17 16:54:57,822 - training_jobs - DEBUG - 
2019-09-17 16:54:57,822 - training_jobs - DEBUG - ggnn training
2019-09-17 16:54:59,245 - training_jobs - DEBUG -  saving results to results/20190917_165459_ggnn_.json
2019-09-17 16:54:59,246 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 16:55:05,502 - training_jobs - ERROR - Error with trains/task_133.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-17 16:55:08,043 - training_jobs - DEBUG - training trains/task_57.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 20,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '5e-4'}
2019-09-17 16:55:08,056 - training_jobs - DEBUG - training with: 
2019-09-17 16:55:08,056 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-17 16:55:08,056 - training_jobs - DEBUG - GGNN5
2019-09-17 16:55:08,056 - training_jobs - DEBUG - 
2019-09-17 16:55:08,056 - training_jobs - DEBUG - ggnn training
2019-09-17 16:55:10,432 - training_jobs - DEBUG -  saving results to results/20190917_165510_ggnn_.json
2019-09-17 16:55:10,432 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 16:55:38,904 - training_jobs - ERROR - Error with trains/task_57.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA error: device-side assert triggered
2019-09-17 16:55:42,505 - training_jobs - DEBUG - training trains/task_311.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '5e-4'}
2019-09-17 16:55:42,529 - training_jobs - DEBUG - training with: 
2019-09-17 16:55:42,529 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-17 16:55:42,529 - training_jobs - DEBUG - GGNN1
2019-09-17 16:55:42,529 - training_jobs - DEBUG - 
2019-09-17 16:55:42,529 - training_jobs - DEBUG - ggnn training
2019-09-17 16:55:43,784 - training_jobs - DEBUG -  saving results to results/20190917_165543_ggnn_.json
2019-09-17 16:55:43,784 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 17:06:51,835 - training_jobs - DEBUG - test_multiple_models
2019-09-17 17:06:51,835 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 17:06:52,275 - training_jobs - DEBUG - training time: 668s
2019-09-17 17:06:52,276 - training_jobs - DEBUG - saving to results/20190917_165543_ggnn_.json
2019-09-17 17:06:52,277 - training_jobs - DEBUG - moved jobdict to done_trainings/task_311.yml
2019-09-17 17:06:52,277 - training_jobs - DEBUG - Finished!

2019-09-17 17:06:54,629 - training_jobs - DEBUG - training trains/task_24.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 20,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-17 17:06:54,641 - training_jobs - DEBUG - training with: 
2019-09-17 17:06:54,641 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-17 17:06:54,641 - training_jobs - DEBUG - GGNN5
2019-09-17 17:06:54,641 - training_jobs - DEBUG - 
2019-09-17 17:06:54,641 - training_jobs - DEBUG - ggnn training
2019-09-17 17:06:57,267 - training_jobs - DEBUG -  saving results to results/20190917_170657_ggnn_.json
2019-09-17 17:06:57,267 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 17:15:45,816 - training_jobs - DEBUG - test_multiple_models
2019-09-17 17:15:45,816 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 17:15:46,917 - training_jobs - DEBUG - training time: 530s
2019-09-17 17:15:46,917 - training_jobs - DEBUG - saving to results/20190917_170657_ggnn_.json
2019-09-17 17:15:46,920 - training_jobs - DEBUG - moved jobdict to done_trainings/task_24.yml
2019-09-17 17:15:46,920 - training_jobs - DEBUG - Finished!

2019-09-17 17:15:49,349 - training_jobs - DEBUG - training trains/task_95.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 17:15:49,362 - training_jobs - DEBUG - training with: 
2019-09-17 17:15:49,362 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-17 17:15:49,362 - training_jobs - DEBUG - GGNN5
2019-09-17 17:15:49,362 - training_jobs - DEBUG - 
2019-09-17 17:15:49,362 - training_jobs - DEBUG - ggnn training
2019-09-17 17:15:51,300 - training_jobs - DEBUG -  saving results to results/20190917_171551_ggnn_.json
2019-09-17 17:15:51,300 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 17:28:03,414 - training_jobs - DEBUG - test_multiple_models
2019-09-17 17:28:03,414 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 17:28:04,210 - training_jobs - DEBUG - training time: 733s
2019-09-17 17:28:04,210 - training_jobs - DEBUG - saving to results/20190917_171551_ggnn_.json
2019-09-17 17:28:04,212 - training_jobs - DEBUG - moved jobdict to done_trainings/task_95.yml
2019-09-17 17:28:04,212 - training_jobs - DEBUG - Finished!

2019-09-17 17:28:06,552 - training_jobs - DEBUG - training trains/task_108.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-17 17:28:06,588 - training_jobs - DEBUG - training with: 
2019-09-17 17:28:06,588 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-17 17:28:06,588 - training_jobs - DEBUG - GGNN6
2019-09-17 17:28:06,588 - training_jobs - DEBUG - 
2019-09-17 17:28:06,588 - training_jobs - DEBUG - ggnn training
2019-09-17 17:28:08,125 - training_jobs - DEBUG -  saving results to results/20190917_172808_ggnn_.json
2019-09-17 17:28:08,125 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 17:28:14,724 - training_jobs - ERROR - Error with trains/task_108.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-17 17:28:17,173 - training_jobs - DEBUG - training trains/task_259.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '5e-4'}
2019-09-17 17:28:17,214 - training_jobs - DEBUG - training with: 
2019-09-17 17:28:17,214 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-17 17:28:17,214 - training_jobs - DEBUG - GGNN1
2019-09-17 17:28:17,214 - training_jobs - DEBUG - 
2019-09-17 17:28:17,214 - training_jobs - DEBUG - ggnn training
2019-09-17 17:28:18,844 - training_jobs - DEBUG -  saving results to results/20190917_172818_ggnn_.json
2019-09-17 17:28:18,844 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 17:29:52,713 - training_jobs - ERROR - Error with trains/task_259.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: cublas runtime error : the GPU program failed to execute at /pytorch/aten/src/THC/THCBlas.cu:258
2019-09-17 17:29:56,128 - training_jobs - DEBUG - training trains/task_350.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '1e-4'}
2019-09-17 17:29:56,141 - training_jobs - DEBUG - training with: 
2019-09-17 17:29:56,141 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max/
2019-09-17 17:29:56,141 - training_jobs - DEBUG - GGNN5
2019-09-17 17:29:56,142 - training_jobs - DEBUG - 
2019-09-17 17:29:56,142 - training_jobs - DEBUG - ggnn training
2019-09-17 17:30:00,691 - training_jobs - DEBUG -  saving results to results/20190917_173000_ggnn_.json
2019-09-17 17:30:00,691 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 17:30:13,306 - training_jobs - ERROR - Error with trains/task_350.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1126, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'd5'
2019-09-17 17:30:15,896 - training_jobs - DEBUG - training trains/task_219.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 17:30:15,931 - training_jobs - DEBUG - training with: 
2019-09-17 17:30:15,931 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-17 17:30:15,931 - training_jobs - DEBUG - GGNN1
2019-09-17 17:30:15,931 - training_jobs - DEBUG - 
2019-09-17 17:30:15,931 - training_jobs - DEBUG - ggnn training
2019-09-17 17:30:19,082 - training_jobs - DEBUG -  saving results to results/20190917_173019_ggnn_.json
2019-09-17 17:30:19,082 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 17:49:35,386 - training_jobs - DEBUG - test_multiple_models
2019-09-17 17:49:35,386 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 17:49:36,497 - training_jobs - DEBUG - training time: 1157s
2019-09-17 17:49:36,497 - training_jobs - DEBUG - saving to results/20190917_173019_ggnn_.json
2019-09-17 17:49:36,499 - training_jobs - DEBUG - moved jobdict to done_trainings/task_219.yml
2019-09-17 17:49:36,500 - training_jobs - DEBUG - Finished!

2019-09-17 17:49:38,935 - training_jobs - DEBUG - training trains/task_327.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 17:49:38,957 - training_jobs - DEBUG - training with: 
2019-09-17 17:49:38,957 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-17 17:49:38,957 - training_jobs - DEBUG - GGNN1
2019-09-17 17:49:38,957 - training_jobs - DEBUG - 
2019-09-17 17:49:38,957 - training_jobs - DEBUG - ggnn training
2019-09-17 17:49:40,881 - training_jobs - DEBUG -  saving results to results/20190917_174940_ggnn_.json
2019-09-17 17:49:40,882 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 18:04:29,382 - training_jobs - DEBUG - test_multiple_models
2019-09-17 18:04:29,382 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 18:04:30,213 - training_jobs - DEBUG - training time: 889s
2019-09-17 18:04:30,213 - training_jobs - DEBUG - saving to results/20190917_174940_ggnn_.json
2019-09-17 18:04:30,215 - training_jobs - DEBUG - moved jobdict to done_trainings/task_327.yml
2019-09-17 18:04:30,215 - training_jobs - DEBUG - Finished!

2019-09-17 18:04:32,555 - training_jobs - DEBUG - training trains/task_251.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '5e-4'}
2019-09-17 18:04:32,578 - training_jobs - DEBUG - training with: 
2019-09-17 18:04:32,578 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-17 18:04:32,578 - training_jobs - DEBUG - GGNN1
2019-09-17 18:04:32,578 - training_jobs - DEBUG - 
2019-09-17 18:04:32,578 - training_jobs - DEBUG - ggnn training
2019-09-17 18:04:33,897 - training_jobs - DEBUG -  saving results to results/20190917_180433_ggnn_.json
2019-09-17 18:04:33,897 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 18:05:36,447 - training_jobs - ERROR - Error with trains/task_251.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: cublas runtime error : the GPU program failed to execute at /pytorch/aten/src/THC/THCBlas.cu:258
2019-09-17 18:05:39,973 - training_jobs - DEBUG - training trains/task_188.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-17 18:05:39,997 - training_jobs - DEBUG - training with: 
2019-09-17 18:05:39,997 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-17 18:05:39,997 - training_jobs - DEBUG - GGNN6
2019-09-17 18:05:39,997 - training_jobs - DEBUG - 
2019-09-17 18:05:39,997 - training_jobs - DEBUG - ggnn training
2019-09-17 18:05:42,227 - training_jobs - DEBUG -  saving results to results/20190917_180542_ggnn_.json
2019-09-17 18:05:42,227 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 18:05:51,006 - training_jobs - ERROR - Error with trains/task_188.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-17 18:05:53,645 - training_jobs - DEBUG - training trains/task_284.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '1e-4'}
2019-09-17 18:05:53,660 - training_jobs - DEBUG - training with: 
2019-09-17 18:05:53,660 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-17 18:05:53,660 - training_jobs - DEBUG - GGNN1
2019-09-17 18:05:53,660 - training_jobs - DEBUG - 
2019-09-17 18:05:53,660 - training_jobs - DEBUG - ggnn training
2019-09-17 18:05:56,407 - training_jobs - DEBUG -  saving results to results/20190917_180556_ggnn_.json
2019-09-17 18:05:56,407 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 18:07:05,358 - training_jobs - ERROR - Error with trains/task_284.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA error: device-side assert triggered
2019-09-17 18:07:09,210 - training_jobs - DEBUG - training trains/task_126.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-17 18:07:09,225 - training_jobs - DEBUG - training with: 
2019-09-17 18:07:09,225 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-17 18:07:09,225 - training_jobs - DEBUG - GGNN6
2019-09-17 18:07:09,226 - training_jobs - DEBUG - 
2019-09-17 18:07:09,226 - training_jobs - DEBUG - ggnn training
2019-09-17 18:07:12,416 - training_jobs - DEBUG -  saving results to results/20190917_180712_ggnn_.json
2019-09-17 18:07:12,417 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 18:07:22,454 - training_jobs - ERROR - Error with trains/task_126.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-17 18:07:24,855 - training_jobs - DEBUG - training trains/task_183.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 18:07:24,879 - training_jobs - DEBUG - training with: 
2019-09-17 18:07:24,879 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-17 18:07:24,879 - training_jobs - DEBUG - GGNN6
2019-09-17 18:07:24,879 - training_jobs - DEBUG - 
2019-09-17 18:07:24,879 - training_jobs - DEBUG - ggnn training
2019-09-17 18:07:27,083 - training_jobs - DEBUG -  saving results to results/20190917_180727_ggnn_.json
2019-09-17 18:07:27,083 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 18:07:35,317 - training_jobs - ERROR - Error with trains/task_183.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-17 18:07:37,734 - training_jobs - DEBUG - training trains/task_40.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-17 18:07:37,747 - training_jobs - DEBUG - training with: 
2019-09-17 18:07:37,747 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-17 18:07:37,748 - training_jobs - DEBUG - GGNN5
2019-09-17 18:07:37,748 - training_jobs - DEBUG - 
2019-09-17 18:07:37,748 - training_jobs - DEBUG - ggnn training
2019-09-17 18:07:39,149 - training_jobs - DEBUG -  saving results to results/20190917_180739_ggnn_.json
2019-09-17 18:07:39,149 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 18:08:12,119 - training_jobs - ERROR - Error with trains/task_40.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: cublas runtime error : the GPU program failed to execute at /pytorch/aten/src/THC/THCBlas.cu:258
2019-09-17 18:08:16,050 - training_jobs - DEBUG - training trains/task_35.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 18:08:16,066 - training_jobs - DEBUG - training with: 
2019-09-17 18:08:16,066 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-17 18:08:16,066 - training_jobs - DEBUG - GGNN5
2019-09-17 18:08:16,066 - training_jobs - DEBUG - 
2019-09-17 18:08:16,066 - training_jobs - DEBUG - ggnn training
2019-09-17 18:08:17,723 - training_jobs - DEBUG -  saving results to results/20190917_180817_ggnn_.json
2019-09-17 18:08:17,723 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 18:08:51,359 - training_jobs - ERROR - Error with trains/task_35.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED
2019-09-17 18:08:55,768 - training_jobs - DEBUG - training trains/task_173.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '5e-4'}
2019-09-17 18:08:55,785 - training_jobs - DEBUG - training with: 
2019-09-17 18:08:55,785 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-17 18:08:55,785 - training_jobs - DEBUG - GGNN6
2019-09-17 18:08:55,785 - training_jobs - DEBUG - 
2019-09-17 18:08:55,785 - training_jobs - DEBUG - ggnn training
2019-09-17 18:08:57,410 - training_jobs - DEBUG -  saving results to results/20190917_180857_ggnn_.json
2019-09-17 18:08:57,410 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 18:09:04,656 - training_jobs - ERROR - Error with trains/task_173.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-17 18:09:07,529 - training_jobs - DEBUG - training trains/task_193.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '5e-4'}
2019-09-17 18:09:07,544 - training_jobs - DEBUG - training with: 
2019-09-17 18:09:07,544 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-17 18:09:07,544 - training_jobs - DEBUG - GGNN6
2019-09-17 18:09:07,544 - training_jobs - DEBUG - 
2019-09-17 18:09:07,544 - training_jobs - DEBUG - ggnn training
2019-09-17 18:09:09,832 - training_jobs - DEBUG -  saving results to results/20190917_180909_ggnn_.json
2019-09-17 18:09:09,832 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 18:09:18,058 - training_jobs - ERROR - Error with trains/task_193.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-17 18:09:20,519 - training_jobs - DEBUG - training trains/task_242.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '1e-4'}
2019-09-17 18:09:20,533 - training_jobs - DEBUG - training with: 
2019-09-17 18:09:20,533 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-17 18:09:20,533 - training_jobs - DEBUG - GGNN1
2019-09-17 18:09:20,533 - training_jobs - DEBUG - 
2019-09-17 18:09:20,533 - training_jobs - DEBUG - ggnn training
2019-09-17 18:09:23,544 - training_jobs - DEBUG -  saving results to results/20190917_180923_ggnn_.json
2019-09-17 18:09:23,544 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 18:39:46,196 - training_jobs - DEBUG - test_multiple_models
2019-09-17 18:39:46,196 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 18:39:47,393 - training_jobs - DEBUG - training time: 1824s
2019-09-17 18:39:47,393 - training_jobs - DEBUG - saving to results/20190917_180923_ggnn_.json
2019-09-17 18:39:47,395 - training_jobs - DEBUG - moved jobdict to done_trainings/task_242.yml
2019-09-17 18:39:47,395 - training_jobs - DEBUG - Finished!

2019-09-17 18:39:50,046 - training_jobs - DEBUG - training trains/task_28.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-17 18:39:50,061 - training_jobs - DEBUG - training with: 
2019-09-17 18:39:50,061 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-17 18:39:50,061 - training_jobs - DEBUG - GGNN5
2019-09-17 18:39:50,061 - training_jobs - DEBUG - 
2019-09-17 18:39:50,061 - training_jobs - DEBUG - ggnn training
2019-09-17 18:39:52,937 - training_jobs - DEBUG -  saving results to results/20190917_183952_ggnn_.json
2019-09-17 18:39:52,937 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 19:01:45,027 - training_jobs - DEBUG - test_multiple_models
2019-09-17 19:01:45,027 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 19:01:46,073 - training_jobs - DEBUG - training time: 1313s
2019-09-17 19:01:46,073 - training_jobs - DEBUG - saving to results/20190917_183952_ggnn_.json
2019-09-17 19:01:46,075 - training_jobs - DEBUG - moved jobdict to done_trainings/task_28.yml
2019-09-17 19:01:46,075 - training_jobs - DEBUG - Finished!

2019-09-17 19:01:48,500 - training_jobs - DEBUG - training trains/task_65.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '5e-4'}
2019-09-17 19:01:48,523 - training_jobs - DEBUG - training with: 
2019-09-17 19:01:48,523 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-17 19:01:48,523 - training_jobs - DEBUG - GGNN5
2019-09-17 19:01:48,523 - training_jobs - DEBUG - 
2019-09-17 19:01:48,523 - training_jobs - DEBUG - ggnn training
2019-09-17 19:01:51,130 - training_jobs - DEBUG -  saving results to results/20190917_190151_ggnn_.json
2019-09-17 19:01:51,130 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 19:02:45,296 - training_jobs - ERROR - Error with trains/task_65.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA error: device-side assert triggered
2019-09-17 19:02:48,710 - training_jobs - DEBUG - training trains/task_288.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-17 19:02:48,722 - training_jobs - DEBUG - training with: 
2019-09-17 19:02:48,723 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-17 19:02:48,723 - training_jobs - DEBUG - GGNN1
2019-09-17 19:02:48,723 - training_jobs - DEBUG - 
2019-09-17 19:02:48,723 - training_jobs - DEBUG - ggnn training
2019-09-17 19:02:50,941 - training_jobs - DEBUG -  saving results to results/20190917_190250_ggnn_.json
2019-09-17 19:02:50,941 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 19:03:52,557 - training_jobs - ERROR - Error with trains/task_288.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA error: device-side assert triggered
2019-09-17 19:03:56,297 - training_jobs - DEBUG - training trains/task_171.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 19:03:56,312 - training_jobs - DEBUG - training with: 
2019-09-17 19:03:56,312 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-17 19:03:56,312 - training_jobs - DEBUG - GGNN6
2019-09-17 19:03:56,312 - training_jobs - DEBUG - 
2019-09-17 19:03:56,312 - training_jobs - DEBUG - ggnn training
2019-09-17 19:03:57,673 - training_jobs - DEBUG -  saving results to results/20190917_190357_ggnn_.json
2019-09-17 19:03:57,673 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 19:04:03,755 - training_jobs - ERROR - Error with trains/task_171.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-17 19:04:06,341 - training_jobs - DEBUG - training trains/task_299.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '5e-4'}
2019-09-17 19:04:06,354 - training_jobs - DEBUG - training with: 
2019-09-17 19:04:06,354 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-17 19:04:06,354 - training_jobs - DEBUG - GGNN1
2019-09-17 19:04:06,354 - training_jobs - DEBUG - 
2019-09-17 19:04:06,354 - training_jobs - DEBUG - ggnn training
2019-09-17 19:04:07,529 - training_jobs - DEBUG -  saving results to results/20190917_190407_ggnn_.json
2019-09-17 19:04:07,529 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 19:12:10,283 - training_jobs - DEBUG - test_multiple_models
2019-09-17 19:12:10,283 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 19:12:10,719 - training_jobs - DEBUG - training time: 483s
2019-09-17 19:12:10,720 - training_jobs - DEBUG - saving to results/20190917_190407_ggnn_.json
2019-09-17 19:12:10,721 - training_jobs - DEBUG - moved jobdict to done_trainings/task_299.yml
2019-09-17 19:12:10,721 - training_jobs - DEBUG - Finished!

2019-09-17 19:12:13,049 - training_jobs - DEBUG - training trains/task_54.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 20,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '1e-4'}
2019-09-17 19:12:13,061 - training_jobs - DEBUG - training with: 
2019-09-17 19:12:13,061 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-17 19:12:13,061 - training_jobs - DEBUG - GGNN5
2019-09-17 19:12:13,061 - training_jobs - DEBUG - 
2019-09-17 19:12:13,062 - training_jobs - DEBUG - ggnn training
2019-09-17 19:12:15,279 - training_jobs - DEBUG -  saving results to results/20190917_191215_ggnn_.json
2019-09-17 19:12:15,279 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 19:12:42,886 - training_jobs - ERROR - Error with trains/task_54.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: cuDNN error: CUDNN_STATUS_MAPPING_ERROR
2019-09-17 19:12:46,831 - training_jobs - DEBUG - training trains/task_263.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '5e-4'}
2019-09-17 19:12:46,848 - training_jobs - DEBUG - training with: 
2019-09-17 19:12:46,848 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-17 19:12:46,848 - training_jobs - DEBUG - GGNN1
2019-09-17 19:12:46,848 - training_jobs - DEBUG - 
2019-09-17 19:12:46,848 - training_jobs - DEBUG - ggnn training
2019-09-17 19:12:48,519 - training_jobs - DEBUG -  saving results to results/20190917_191248_ggnn_.json
2019-09-17 19:12:48,519 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 19:14:10,860 - training_jobs - ERROR - Error with trains/task_263.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA error: device-side assert triggered
2019-09-17 19:14:14,447 - training_jobs - DEBUG - training trains/task_55.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 20,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 19:14:14,665 - training_jobs - DEBUG - training with: 
2019-09-17 19:14:14,665 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-17 19:14:14,665 - training_jobs - DEBUG - GGNN5
2019-09-17 19:14:14,665 - training_jobs - DEBUG - 
2019-09-17 19:14:14,665 - training_jobs - DEBUG - ggnn training
2019-09-17 19:14:17,114 - training_jobs - DEBUG -  saving results to results/20190917_191417_ggnn_.json
2019-09-17 19:14:17,114 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 19:14:43,363 - training_jobs - ERROR - Error with trains/task_55.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: cuDNN error: CUDNN_STATUS_MAPPING_ERROR
2019-09-17 19:14:47,158 - training_jobs - DEBUG - training trains/task_222.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-17 19:14:47,173 - training_jobs - DEBUG - training with: 
2019-09-17 19:14:47,173 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-17 19:14:47,173 - training_jobs - DEBUG - GGNN1
2019-09-17 19:14:47,173 - training_jobs - DEBUG - 
2019-09-17 19:14:47,173 - training_jobs - DEBUG - ggnn training
2019-09-17 19:14:50,274 - training_jobs - DEBUG -  saving results to results/20190917_191450_ggnn_.json
2019-09-17 19:14:50,274 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 19:33:03,434 - training_jobs - DEBUG - test_multiple_models
2019-09-17 19:33:03,434 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 19:33:04,504 - training_jobs - DEBUG - training time: 1094s
2019-09-17 19:33:04,505 - training_jobs - DEBUG - saving to results/20190917_191450_ggnn_.json
2019-09-17 19:33:04,507 - training_jobs - DEBUG - moved jobdict to done_trainings/task_222.yml
2019-09-17 19:33:04,507 - training_jobs - DEBUG - Finished!

2019-09-17 19:33:06,972 - training_jobs - DEBUG - training trains/task_14.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '1e-4'}
2019-09-17 19:33:06,984 - training_jobs - DEBUG - training with: 
2019-09-17 19:33:06,984 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-17 19:33:06,984 - training_jobs - DEBUG - GGNN5
2019-09-17 19:33:06,984 - training_jobs - DEBUG - 
2019-09-17 19:33:06,984 - training_jobs - DEBUG - ggnn training
2019-09-17 19:33:08,547 - training_jobs - DEBUG -  saving results to results/20190917_193308_ggnn_.json
2019-09-17 19:33:08,547 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 19:47:33,357 - training_jobs - DEBUG - test_multiple_models
2019-09-17 19:47:33,357 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 19:47:34,010 - training_jobs - DEBUG - training time: 865s
2019-09-17 19:47:34,010 - training_jobs - DEBUG - saving to results/20190917_193308_ggnn_.json
2019-09-17 19:47:34,012 - training_jobs - DEBUG - moved jobdict to done_trainings/task_14.yml
2019-09-17 19:47:34,012 - training_jobs - DEBUG - Finished!

2019-09-17 19:47:36,378 - training_jobs - DEBUG - training trains/task_228.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-17 19:47:36,390 - training_jobs - DEBUG - training with: 
2019-09-17 19:47:36,390 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-17 19:47:36,390 - training_jobs - DEBUG - GGNN1
2019-09-17 19:47:36,390 - training_jobs - DEBUG - 
2019-09-17 19:47:36,391 - training_jobs - DEBUG - ggnn training
2019-09-17 19:47:38,975 - training_jobs - DEBUG -  saving results to results/20190917_194738_ggnn_.json
2019-09-17 19:47:38,975 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 20:06:27,641 - training_jobs - DEBUG - test_multiple_models
2019-09-17 20:06:27,642 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 20:06:28,865 - training_jobs - DEBUG - training time: 1130s
2019-09-17 20:06:28,865 - training_jobs - DEBUG - saving to results/20190917_194738_ggnn_.json
2019-09-17 20:06:28,867 - training_jobs - DEBUG - moved jobdict to done_trainings/task_228.yml
2019-09-17 20:06:28,867 - training_jobs - DEBUG - Finished!

2019-09-17 20:06:31,659 - training_jobs - DEBUG - training trains/task_238.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-17 20:06:32,287 - training_jobs - DEBUG - training with: 
2019-09-17 20:06:32,287 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-17 20:06:32,287 - training_jobs - DEBUG - GGNN1
2019-09-17 20:06:32,287 - training_jobs - DEBUG - 
2019-09-17 20:06:32,287 - training_jobs - DEBUG - ggnn training
2019-09-17 20:06:35,139 - training_jobs - DEBUG -  saving results to results/20190917_200635_ggnn_.json
2019-09-17 20:06:35,139 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 20:37:38,611 - training_jobs - DEBUG - test_multiple_models
2019-09-17 20:37:38,898 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 20:37:45,484 - training_jobs - DEBUG - training time: 1870s
2019-09-17 20:37:45,484 - training_jobs - DEBUG - saving to results/20190917_200635_ggnn_.json
2019-09-17 20:37:45,722 - training_jobs - DEBUG - moved jobdict to done_trainings/task_238.yml
2019-09-17 20:37:45,722 - training_jobs - DEBUG - Finished!

2019-09-17 20:37:54,122 - training_jobs - DEBUG - training trains/task_254.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '1e-4'}
2019-09-17 20:37:54,142 - training_jobs - DEBUG - training with: 
2019-09-17 20:37:54,142 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-17 20:37:54,143 - training_jobs - DEBUG - GGNN1
2019-09-17 20:37:54,143 - training_jobs - DEBUG - 
2019-09-17 20:37:54,143 - training_jobs - DEBUG - ggnn training
2019-09-17 20:37:56,663 - training_jobs - DEBUG -  saving results to results/20190917_203756_ggnn_.json
2019-09-17 20:37:56,663 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 20:39:22,589 - training_jobs - ERROR - Error with trains/task_254.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA error: device-side assert triggered
2019-09-17 20:39:26,853 - training_jobs - DEBUG - training trains/task_23.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 20,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 20:39:27,154 - training_jobs - DEBUG - training with: 
2019-09-17 20:39:27,154 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-17 20:39:27,155 - training_jobs - DEBUG - GGNN5
2019-09-17 20:39:27,155 - training_jobs - DEBUG - 
2019-09-17 20:39:27,155 - training_jobs - DEBUG - ggnn training
2019-09-17 20:39:30,512 - training_jobs - DEBUG -  saving results to results/20190917_203930_ggnn_.json
2019-09-17 20:39:30,512 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 20:50:36,137 - training_jobs - DEBUG - test_multiple_models
2019-09-17 20:50:36,167 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 20:50:38,480 - training_jobs - DEBUG - training time: 668s
2019-09-17 20:50:38,480 - training_jobs - DEBUG - saving to results/20190917_203930_ggnn_.json
2019-09-17 20:50:38,659 - training_jobs - DEBUG - moved jobdict to done_trainings/task_23.yml
2019-09-17 20:50:38,659 - training_jobs - DEBUG - Finished!

2019-09-17 20:50:42,751 - training_jobs - DEBUG - training trains/task_82.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '1e-4'}
2019-09-17 20:50:43,460 - training_jobs - DEBUG - training with: 
2019-09-17 20:50:43,460 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-17 20:50:43,461 - training_jobs - DEBUG - GGNN5
2019-09-17 20:50:43,461 - training_jobs - DEBUG - 
2019-09-17 20:50:43,461 - training_jobs - DEBUG - ggnn training
2019-09-17 20:50:53,252 - training_jobs - DEBUG -  saving results to results/20190917_205053_ggnn_.json
2019-09-17 20:50:53,253 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 21:03:23,044 - training_jobs - DEBUG - test_multiple_models
2019-09-17 21:03:23,044 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 21:03:28,250 - training_jobs - DEBUG - training time: 755s
2019-09-17 21:03:28,250 - training_jobs - DEBUG - saving to results/20190917_205053_ggnn_.json
2019-09-17 21:03:28,253 - training_jobs - DEBUG - moved jobdict to done_trainings/task_82.yml
2019-09-17 21:03:28,253 - training_jobs - DEBUG - Finished!

2019-09-17 21:03:31,755 - training_jobs - DEBUG - training trains/task_257.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '5e-4'}
2019-09-17 21:03:32,015 - training_jobs - DEBUG - training with: 
2019-09-17 21:03:32,015 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-17 21:03:32,015 - training_jobs - DEBUG - GGNN1
2019-09-17 21:03:32,016 - training_jobs - DEBUG - 
2019-09-17 21:03:32,016 - training_jobs - DEBUG - ggnn training
2019-09-17 21:03:34,429 - training_jobs - DEBUG -  saving results to results/20190917_210334_ggnn_.json
2019-09-17 21:03:34,429 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 21:05:07,810 - training_jobs - ERROR - Error with trains/task_257.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA error: device-side assert triggered
2019-09-17 21:05:11,759 - training_jobs - DEBUG - training trains/task_30.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '1e-4'}
2019-09-17 21:05:12,057 - training_jobs - DEBUG - training with: 
2019-09-17 21:05:12,057 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-17 21:05:12,057 - training_jobs - DEBUG - GGNN5
2019-09-17 21:05:12,057 - training_jobs - DEBUG - 
2019-09-17 21:05:12,057 - training_jobs - DEBUG - ggnn training
2019-09-17 21:05:15,339 - training_jobs - DEBUG -  saving results to results/20190917_210515_ggnn_.json
2019-09-17 21:05:15,340 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 21:32:23,227 - training_jobs - DEBUG - test_multiple_models
2019-09-17 21:32:23,227 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 21:32:26,859 - training_jobs - DEBUG - training time: 1632s
2019-09-17 21:32:26,859 - training_jobs - DEBUG - saving to results/20190917_210515_ggnn_.json
2019-09-17 21:32:26,863 - training_jobs - DEBUG - moved jobdict to done_trainings/task_30.yml
2019-09-17 21:32:26,863 - training_jobs - DEBUG - Finished!

2019-09-17 21:32:34,361 - training_jobs - DEBUG - training trains/task_3.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 21:32:34,391 - training_jobs - DEBUG - training with: 
2019-09-17 21:32:34,391 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-17 21:32:34,391 - training_jobs - DEBUG - GGNN5
2019-09-17 21:32:34,391 - training_jobs - DEBUG - 
2019-09-17 21:32:34,391 - training_jobs - DEBUG - ggnn training
2019-09-17 21:32:36,892 - training_jobs - DEBUG -  saving results to results/20190917_213236_ggnn_.json
2019-09-17 21:32:36,892 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 21:38:27,566 - training_jobs - DEBUG - test_multiple_models
2019-09-17 21:38:27,566 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 21:38:28,246 - training_jobs - DEBUG - training time: 351s
2019-09-17 21:38:28,246 - training_jobs - DEBUG - saving to results/20190917_213236_ggnn_.json
2019-09-17 21:38:28,249 - training_jobs - DEBUG - moved jobdict to done_trainings/task_3.yml
2019-09-17 21:38:28,249 - training_jobs - DEBUG - Finished!

2019-09-17 21:38:31,031 - training_jobs - DEBUG - training trains/task_200.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '1e-4'}
2019-09-17 21:38:31,087 - training_jobs - DEBUG - training with: 
2019-09-17 21:38:31,087 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-17 21:38:31,087 - training_jobs - DEBUG - GGNN1
2019-09-17 21:38:31,087 - training_jobs - DEBUG - 
2019-09-17 21:38:31,087 - training_jobs - DEBUG - ggnn training
2019-09-17 21:38:33,030 - training_jobs - DEBUG -  saving results to results/20190917_213833_ggnn_.json
2019-09-17 21:38:33,030 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 21:53:04,964 - training_jobs - DEBUG - test_multiple_models
2019-09-17 21:53:04,964 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 21:53:05,666 - training_jobs - DEBUG - training time: 873s
2019-09-17 21:53:05,666 - training_jobs - DEBUG - saving to results/20190917_213833_ggnn_.json
2019-09-17 21:53:06,580 - training_jobs - DEBUG - moved jobdict to done_trainings/task_200.yml
2019-09-17 21:53:06,580 - training_jobs - DEBUG - Finished!

2019-09-17 21:53:09,330 - training_jobs - DEBUG - training trains/task_169.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '5e-4'}
2019-09-17 21:53:09,434 - training_jobs - DEBUG - training with: 
2019-09-17 21:53:09,434 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-17 21:53:09,434 - training_jobs - DEBUG - GGNN6
2019-09-17 21:53:09,434 - training_jobs - DEBUG - 
2019-09-17 21:53:09,434 - training_jobs - DEBUG - ggnn training
2019-09-17 21:53:15,972 - training_jobs - DEBUG -  saving results to results/20190917_215315_ggnn_.json
2019-09-17 21:53:15,972 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 21:53:23,312 - training_jobs - ERROR - Error with trains/task_169.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-17 21:53:25,900 - training_jobs - DEBUG - training trains/task_74.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '1e-4'}
2019-09-17 21:53:25,914 - training_jobs - DEBUG - training with: 
2019-09-17 21:53:25,914 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-17 21:53:25,914 - training_jobs - DEBUG - GGNN5
2019-09-17 21:53:25,914 - training_jobs - DEBUG - 
2019-09-17 21:53:25,914 - training_jobs - DEBUG - ggnn training
2019-09-17 21:53:27,164 - training_jobs - DEBUG -  saving results to results/20190917_215327_ggnn_.json
2019-09-17 21:53:27,164 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 21:58:14,131 - training_jobs - DEBUG - test_multiple_models
2019-09-17 21:58:14,131 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 21:58:14,588 - training_jobs - DEBUG - training time: 287s
2019-09-17 21:58:14,588 - training_jobs - DEBUG - saving to results/20190917_215327_ggnn_.json
2019-09-17 21:58:14,636 - training_jobs - DEBUG - moved jobdict to done_trainings/task_74.yml
2019-09-17 21:58:14,637 - training_jobs - DEBUG - Finished!

2019-09-17 21:58:17,144 - training_jobs - DEBUG - training trains/task_160.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-17 21:58:17,156 - training_jobs - DEBUG - training with: 
2019-09-17 21:58:17,157 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-17 21:58:17,157 - training_jobs - DEBUG - GGNN6
2019-09-17 21:58:17,157 - training_jobs - DEBUG - 
2019-09-17 21:58:17,157 - training_jobs - DEBUG - ggnn training
2019-09-17 21:58:32,084 - training_jobs - DEBUG -  saving results to results/20190917_215832_ggnn_.json
2019-09-17 21:58:32,084 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 21:58:41,551 - training_jobs - ERROR - Error with trains/task_160.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-17 21:58:44,208 - training_jobs - DEBUG - training trains/task_232.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-17 21:58:44,288 - training_jobs - DEBUG - training with: 
2019-09-17 21:58:44,289 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-17 21:58:44,289 - training_jobs - DEBUG - GGNN1
2019-09-17 21:58:44,289 - training_jobs - DEBUG - 
2019-09-17 21:58:44,289 - training_jobs - DEBUG - ggnn training
2019-09-17 21:58:47,657 - training_jobs - DEBUG -  saving results to results/20190917_215847_ggnn_.json
2019-09-17 21:58:47,657 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 22:27:21,368 - training_jobs - DEBUG - test_multiple_models
2019-09-17 22:27:21,368 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 22:27:22,501 - training_jobs - DEBUG - training time: 1715s
2019-09-17 22:27:22,501 - training_jobs - DEBUG - saving to results/20190917_215847_ggnn_.json
2019-09-17 22:27:22,504 - training_jobs - DEBUG - moved jobdict to done_trainings/task_232.yml
2019-09-17 22:27:22,504 - training_jobs - DEBUG - Finished!

2019-09-17 22:27:25,234 - training_jobs - DEBUG - training trains/task_351.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-17 22:27:25,518 - training_jobs - DEBUG - training with: 
2019-09-17 22:27:25,518 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max/
2019-09-17 22:27:25,518 - training_jobs - DEBUG - GGNN5
2019-09-17 22:27:25,519 - training_jobs - DEBUG - 
2019-09-17 22:27:25,519 - training_jobs - DEBUG - ggnn training
2019-09-17 22:27:45,845 - training_jobs - DEBUG -  saving results to results/20190917_222745_ggnn_.json
2019-09-17 22:27:45,845 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 22:27:57,400 - training_jobs - ERROR - Error with trains/task_351.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1126, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'd5'
2019-09-17 22:27:59,736 - training_jobs - DEBUG - training trains/task_285.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-17 22:27:59,983 - training_jobs - DEBUG - training with: 
2019-09-17 22:27:59,983 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-17 22:27:59,983 - training_jobs - DEBUG - GGNN1
2019-09-17 22:27:59,983 - training_jobs - DEBUG - 
2019-09-17 22:27:59,983 - training_jobs - DEBUG - ggnn training
2019-09-17 22:28:02,545 - training_jobs - DEBUG -  saving results to results/20190917_222802_ggnn_.json
2019-09-17 22:28:02,545 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 22:29:07,748 - training_jobs - ERROR - Error with trains/task_285.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA error: device-side assert triggered
2019-09-17 22:29:12,172 - training_jobs - DEBUG - training trains/task_229.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '5e-4'}
2019-09-17 22:29:12,222 - training_jobs - DEBUG - training with: 
2019-09-17 22:29:12,222 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-17 22:29:12,222 - training_jobs - DEBUG - GGNN1
2019-09-17 22:29:12,222 - training_jobs - DEBUG - 
2019-09-17 22:29:12,222 - training_jobs - DEBUG - ggnn training
2019-09-17 22:29:15,942 - training_jobs - DEBUG -  saving results to results/20190917_222915_ggnn_.json
2019-09-17 22:29:15,942 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 22:52:43,582 - training_jobs - DEBUG - test_multiple_models
2019-09-17 22:52:43,582 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 22:52:44,979 - training_jobs - DEBUG - training time: 1409s
2019-09-17 22:52:44,979 - training_jobs - DEBUG - saving to results/20190917_222915_ggnn_.json
2019-09-17 22:52:44,983 - training_jobs - DEBUG - moved jobdict to done_trainings/task_229.yml
2019-09-17 22:52:44,983 - training_jobs - DEBUG - Finished!

2019-09-17 22:52:48,195 - training_jobs - DEBUG - training trains/task_76.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-17 22:52:48,257 - training_jobs - DEBUG - training with: 
2019-09-17 22:52:48,257 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-17 22:52:48,258 - training_jobs - DEBUG - GGNN5
2019-09-17 22:52:48,258 - training_jobs - DEBUG - 
2019-09-17 22:52:48,258 - training_jobs - DEBUG - ggnn training
2019-09-17 22:52:49,735 - training_jobs - DEBUG -  saving results to results/20190917_225249_ggnn_.json
2019-09-17 22:52:49,736 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 23:04:11,430 - training_jobs - DEBUG - test_multiple_models
2019-09-17 23:04:11,550 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 23:04:24,644 - training_jobs - DEBUG - training time: 695s
2019-09-17 23:04:24,644 - training_jobs - DEBUG - saving to results/20190917_225249_ggnn_.json
2019-09-17 23:04:24,697 - training_jobs - DEBUG - moved jobdict to done_trainings/task_76.yml
2019-09-17 23:04:24,697 - training_jobs - DEBUG - Finished!

2019-09-17 23:04:35,760 - training_jobs - DEBUG - training trains/task_215.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '5e-4'}
2019-09-17 23:04:35,801 - training_jobs - DEBUG - training with: 
2019-09-17 23:04:35,801 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-17 23:04:35,801 - training_jobs - DEBUG - GGNN1
2019-09-17 23:04:35,801 - training_jobs - DEBUG - 
2019-09-17 23:04:35,801 - training_jobs - DEBUG - ggnn training
2019-09-17 23:04:46,408 - training_jobs - DEBUG -  saving results to results/20190917_230446_ggnn_.json
2019-09-17 23:04:46,408 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 23:21:52,977 - training_jobs - DEBUG - test_multiple_models
2019-09-17 23:21:52,977 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 23:21:58,839 - training_jobs - DEBUG - training time: 1032s
2019-09-17 23:21:58,839 - training_jobs - DEBUG - saving to results/20190917_230446_ggnn_.json
2019-09-17 23:21:58,864 - training_jobs - DEBUG - moved jobdict to done_trainings/task_215.yml
2019-09-17 23:21:58,864 - training_jobs - DEBUG - Finished!

2019-09-17 23:22:04,972 - training_jobs - DEBUG - training trains/task_86.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 20,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '1e-4'}
2019-09-17 23:22:05,000 - training_jobs - DEBUG - training with: 
2019-09-17 23:22:05,000 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-17 23:22:05,000 - training_jobs - DEBUG - GGNN5
2019-09-17 23:22:05,000 - training_jobs - DEBUG - 
2019-09-17 23:22:05,000 - training_jobs - DEBUG - ggnn training
2019-09-17 23:22:14,156 - training_jobs - DEBUG -  saving results to results/20190917_232214_ggnn_.json
2019-09-17 23:22:14,156 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 23:28:07,500 - training_jobs - DEBUG - test_multiple_models
2019-09-17 23:28:07,500 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 23:28:10,522 - training_jobs - DEBUG - training time: 356s
2019-09-17 23:28:10,522 - training_jobs - DEBUG - saving to results/20190917_232214_ggnn_.json
2019-09-17 23:28:10,525 - training_jobs - DEBUG - moved jobdict to done_trainings/task_86.yml
2019-09-17 23:28:10,525 - training_jobs - DEBUG - Finished!

2019-09-17 23:28:12,995 - training_jobs - DEBUG - training trains/task_331.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '5e-4'}
2019-09-17 23:28:13,150 - training_jobs - DEBUG - training with: 
2019-09-17 23:28:13,150 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-17 23:28:13,150 - training_jobs - DEBUG - GGNN1
2019-09-17 23:28:13,150 - training_jobs - DEBUG - 
2019-09-17 23:28:13,150 - training_jobs - DEBUG - ggnn training
2019-09-17 23:28:15,114 - training_jobs - DEBUG -  saving results to results/20190917_232815_ggnn_.json
2019-09-17 23:28:15,114 - training_jobs - DEBUG -  calling modelSelection
2019-09-17 23:47:02,995 - training_jobs - DEBUG - test_multiple_models
2019-09-17 23:47:02,995 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-17 23:47:04,093 - training_jobs - DEBUG - training time: 1129s
2019-09-17 23:47:04,094 - training_jobs - DEBUG - saving to results/20190917_232815_ggnn_.json
2019-09-17 23:47:04,096 - training_jobs - DEBUG - moved jobdict to done_trainings/task_331.yml
2019-09-17 23:47:04,096 - training_jobs - DEBUG - Finished!

2019-09-17 23:47:07,745 - training_jobs - DEBUG - training trains/task_336.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-17 23:47:07,786 - training_jobs - DEBUG - training with: 
2019-09-17 23:47:07,786 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-17 23:47:07,786 - training_jobs - DEBUG - GGNN1
2019-09-17 23:47:07,787 - training_jobs - DEBUG - 
2019-09-17 23:47:07,787 - training_jobs - DEBUG - ggnn training
2019-09-17 23:47:10,726 - training_jobs - DEBUG -  saving results to results/20190917_234710_ggnn_.json
2019-09-17 23:47:10,726 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 00:02:15,949 - training_jobs - DEBUG - test_multiple_models
2019-09-18 00:02:15,949 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-18 00:02:16,859 - training_jobs - DEBUG - training time: 906s
2019-09-18 00:02:16,859 - training_jobs - DEBUG - saving to results/20190917_234710_ggnn_.json
2019-09-18 00:02:16,861 - training_jobs - DEBUG - moved jobdict to done_trainings/task_336.yml
2019-09-18 00:02:16,861 - training_jobs - DEBUG - Finished!

2019-09-18 00:02:19,716 - training_jobs - DEBUG - training trains/task_179.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-18 00:02:19,756 - training_jobs - DEBUG - training with: 
2019-09-18 00:02:19,756 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-18 00:02:19,756 - training_jobs - DEBUG - GGNN6
2019-09-18 00:02:19,756 - training_jobs - DEBUG - 
2019-09-18 00:02:19,756 - training_jobs - DEBUG - ggnn training
2019-09-18 00:02:22,121 - training_jobs - DEBUG -  saving results to results/20190918_000222_ggnn_.json
2019-09-18 00:02:22,121 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 00:02:31,590 - training_jobs - ERROR - Error with trains/task_179.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-18 00:02:34,555 - training_jobs - DEBUG - training trains/task_344.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '1e-4'}
2019-09-18 00:02:34,594 - training_jobs - DEBUG - training with: 
2019-09-18 00:02:34,594 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max/
2019-09-18 00:02:34,594 - training_jobs - DEBUG - GGNN5
2019-09-18 00:02:34,595 - training_jobs - DEBUG - 
2019-09-18 00:02:34,595 - training_jobs - DEBUG - ggnn training
2019-09-18 00:02:59,012 - training_jobs - DEBUG -  saving results to results/20190918_000259_ggnn_.json
2019-09-18 00:02:59,012 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 00:03:15,742 - training_jobs - ERROR - Error with trains/task_344.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1126, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'd5'
2019-09-18 00:03:18,543 - training_jobs - DEBUG - training trains/task_203.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '5e-4'}
2019-09-18 00:03:18,637 - training_jobs - DEBUG - training with: 
2019-09-18 00:03:18,637 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-18 00:03:18,637 - training_jobs - DEBUG - GGNN1
2019-09-18 00:03:18,637 - training_jobs - DEBUG - 
2019-09-18 00:03:18,637 - training_jobs - DEBUG - ggnn training
2019-09-18 00:03:20,745 - training_jobs - DEBUG -  saving results to results/20190918_000320_ggnn_.json
2019-09-18 00:03:20,745 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 00:15:40,350 - training_jobs - DEBUG - test_multiple_models
2019-09-18 00:15:40,350 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-18 00:15:41,084 - training_jobs - DEBUG - training time: 740s
2019-09-18 00:15:41,084 - training_jobs - DEBUG - saving to results/20190918_000320_ggnn_.json
2019-09-18 00:15:41,087 - training_jobs - DEBUG - moved jobdict to done_trainings/task_203.yml
2019-09-18 00:15:41,087 - training_jobs - DEBUG - Finished!

2019-09-18 00:15:43,903 - training_jobs - DEBUG - training trains/task_87.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 20,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-18 00:15:44,153 - training_jobs - DEBUG - training with: 
2019-09-18 00:15:44,153 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-18 00:15:44,153 - training_jobs - DEBUG - GGNN5
2019-09-18 00:15:44,153 - training_jobs - DEBUG - 
2019-09-18 00:15:44,153 - training_jobs - DEBUG - ggnn training
2019-09-18 00:15:46,640 - training_jobs - DEBUG -  saving results to results/20190918_001546_ggnn_.json
2019-09-18 00:15:46,640 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 00:21:33,201 - training_jobs - DEBUG - test_multiple_models
2019-09-18 00:21:33,201 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-18 00:21:34,114 - training_jobs - DEBUG - training time: 347s
2019-09-18 00:21:34,114 - training_jobs - DEBUG - saving to results/20190918_001546_ggnn_.json
2019-09-18 00:21:34,117 - training_jobs - DEBUG - moved jobdict to done_trainings/task_87.yml
2019-09-18 00:21:34,117 - training_jobs - DEBUG - Finished!

2019-09-18 00:21:36,914 - training_jobs - DEBUG - training trains/task_36.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-18 00:21:36,945 - training_jobs - DEBUG - training with: 
2019-09-18 00:21:36,945 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-18 00:21:36,945 - training_jobs - DEBUG - GGNN5
2019-09-18 00:21:36,945 - training_jobs - DEBUG - 
2019-09-18 00:21:36,946 - training_jobs - DEBUG - ggnn training
2019-09-18 00:21:49,259 - training_jobs - DEBUG -  saving results to results/20190918_002149_ggnn_.json
2019-09-18 00:21:49,259 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 00:22:27,007 - training_jobs - ERROR - Error with trains/task_36.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA error: device-side assert triggered
2019-09-18 00:22:31,245 - training_jobs - DEBUG - training trains/task_147.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-18 00:22:31,282 - training_jobs - DEBUG - training with: 
2019-09-18 00:22:31,283 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-18 00:22:31,283 - training_jobs - DEBUG - GGNN6
2019-09-18 00:22:31,283 - training_jobs - DEBUG - 
2019-09-18 00:22:31,283 - training_jobs - DEBUG - ggnn training
2019-09-18 00:22:42,152 - training_jobs - DEBUG -  saving results to results/20190918_002242_ggnn_.json
2019-09-18 00:22:42,152 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 00:22:52,194 - training_jobs - ERROR - Error with trains/task_147.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-18 00:22:55,081 - training_jobs - DEBUG - training trains/task_62.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '1e-4'}
2019-09-18 00:22:55,156 - training_jobs - DEBUG - training with: 
2019-09-18 00:22:55,156 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-18 00:22:55,156 - training_jobs - DEBUG - GGNN5
2019-09-18 00:22:55,156 - training_jobs - DEBUG - 
2019-09-18 00:22:55,156 - training_jobs - DEBUG - ggnn training
2019-09-18 00:22:58,031 - training_jobs - DEBUG -  saving results to results/20190918_002258_ggnn_.json
2019-09-18 00:22:58,031 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 00:23:58,074 - training_jobs - ERROR - Error with trains/task_62.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA error: device-side assert triggered
2019-09-18 00:24:02,146 - training_jobs - DEBUG - training trains/task_113.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '5e-4'}
2019-09-18 00:24:02,178 - training_jobs - DEBUG - training with: 
2019-09-18 00:24:02,178 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-18 00:24:02,178 - training_jobs - DEBUG - GGNN6
2019-09-18 00:24:02,178 - training_jobs - DEBUG - 
2019-09-18 00:24:02,178 - training_jobs - DEBUG - ggnn training
2019-09-18 00:24:04,202 - training_jobs - DEBUG -  saving results to results/20190918_002404_ggnn_.json
2019-09-18 00:24:04,202 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 00:24:12,175 - training_jobs - ERROR - Error with trains/task_113.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-18 00:24:14,736 - training_jobs - DEBUG - training trains/task_121.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '5e-4'}
2019-09-18 00:24:14,750 - training_jobs - DEBUG - training with: 
2019-09-18 00:24:14,751 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-18 00:24:14,751 - training_jobs - DEBUG - GGNN6
2019-09-18 00:24:14,751 - training_jobs - DEBUG - 
2019-09-18 00:24:14,751 - training_jobs - DEBUG - ggnn training
2019-09-18 00:24:18,758 - training_jobs - DEBUG -  saving results to results/20190918_002418_ggnn_.json
2019-09-18 00:24:18,758 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 00:24:28,907 - training_jobs - ERROR - Error with trains/task_121.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-18 00:24:31,619 - training_jobs - DEBUG - training trains/task_345.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-18 00:24:31,652 - training_jobs - DEBUG - training with: 
2019-09-18 00:24:31,652 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max/
2019-09-18 00:24:31,652 - training_jobs - DEBUG - GGNN5
2019-09-18 00:24:31,652 - training_jobs - DEBUG - 
2019-09-18 00:24:31,652 - training_jobs - DEBUG - ggnn training
2019-09-18 00:24:49,911 - training_jobs - DEBUG -  saving results to results/20190918_002449_ggnn_.json
2019-09-18 00:24:49,911 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 00:25:03,756 - training_jobs - ERROR - Error with trains/task_345.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1126, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'd5'
2019-09-18 00:25:06,350 - training_jobs - DEBUG - training trains/task_92.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-18 00:25:06,671 - training_jobs - DEBUG - training with: 
2019-09-18 00:25:06,671 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-18 00:25:06,671 - training_jobs - DEBUG - GGNN5
2019-09-18 00:25:06,671 - training_jobs - DEBUG - 
2019-09-18 00:25:06,671 - training_jobs - DEBUG - ggnn training
2019-09-18 00:25:11,710 - training_jobs - DEBUG -  saving results to results/20190918_002511_ggnn_.json
2019-09-18 00:25:11,710 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 00:39:06,607 - training_jobs - DEBUG - test_multiple_models
2019-09-18 00:39:06,628 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-18 00:39:16,122 - training_jobs - DEBUG - training time: 844s
2019-09-18 00:39:16,122 - training_jobs - DEBUG - saving to results/20190918_002511_ggnn_.json
2019-09-18 00:39:16,146 - training_jobs - DEBUG - moved jobdict to done_trainings/task_92.yml
2019-09-18 00:39:16,146 - training_jobs - DEBUG - Finished!

2019-09-18 00:39:21,569 - training_jobs - DEBUG - training trains/task_309.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-18 00:39:21,602 - training_jobs - DEBUG - training with: 
2019-09-18 00:39:21,603 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-18 00:39:21,603 - training_jobs - DEBUG - GGNN1
2019-09-18 00:39:21,603 - training_jobs - DEBUG - 
2019-09-18 00:39:21,603 - training_jobs - DEBUG - ggnn training
2019-09-18 00:39:23,973 - training_jobs - DEBUG -  saving results to results/20190918_003923_ggnn_.json
2019-09-18 00:39:23,973 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 00:52:15,035 - training_jobs - DEBUG - test_multiple_models
2019-09-18 00:52:15,036 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-18 00:52:15,529 - training_jobs - DEBUG - training time: 772s
2019-09-18 00:52:15,529 - training_jobs - DEBUG - saving to results/20190918_003923_ggnn_.json
2019-09-18 00:52:15,531 - training_jobs - DEBUG - moved jobdict to done_trainings/task_309.yml
2019-09-18 00:52:15,531 - training_jobs - DEBUG - Finished!

2019-09-18 00:52:18,436 - training_jobs - DEBUG - training trains/task_343.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-18 00:52:19,120 - training_jobs - DEBUG - training with: 
2019-09-18 00:52:19,120 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max/
2019-09-18 00:52:19,120 - training_jobs - DEBUG - GGNN5
2019-09-18 00:52:19,120 - training_jobs - DEBUG - 
2019-09-18 00:52:19,120 - training_jobs - DEBUG - ggnn training
2019-09-18 00:52:36,729 - training_jobs - DEBUG -  saving results to results/20190918_005236_ggnn_.json
2019-09-18 00:52:36,729 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 00:52:51,917 - training_jobs - ERROR - Error with trains/task_343.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1126, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'd5'
2019-09-18 00:52:55,125 - training_jobs - DEBUG - training trains/task_167.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-18 00:52:55,149 - training_jobs - DEBUG - training with: 
2019-09-18 00:52:55,149 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-18 00:52:55,149 - training_jobs - DEBUG - GGNN6
2019-09-18 00:52:55,149 - training_jobs - DEBUG - 
2019-09-18 00:52:55,149 - training_jobs - DEBUG - ggnn training
2019-09-18 00:52:56,523 - training_jobs - DEBUG -  saving results to results/20190918_005256_ggnn_.json
2019-09-18 00:52:56,523 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 00:53:04,319 - training_jobs - ERROR - Error with trains/task_167.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-18 00:53:07,214 - training_jobs - DEBUG - training trains/task_156.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-18 00:53:07,618 - training_jobs - DEBUG - training with: 
2019-09-18 00:53:07,618 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-18 00:53:07,619 - training_jobs - DEBUG - GGNN6
2019-09-18 00:53:07,619 - training_jobs - DEBUG - 
2019-09-18 00:53:07,619 - training_jobs - DEBUG - ggnn training
2019-09-18 00:53:20,112 - training_jobs - DEBUG -  saving results to results/20190918_005320_ggnn_.json
2019-09-18 00:53:20,112 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 00:53:29,351 - training_jobs - ERROR - Error with trains/task_156.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-18 00:53:32,051 - training_jobs - DEBUG - training trains/task_265.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '5e-4'}
2019-09-18 00:53:32,089 - training_jobs - DEBUG - training with: 
2019-09-18 00:53:32,089 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-18 00:53:32,089 - training_jobs - DEBUG - GGNN1
2019-09-18 00:53:32,089 - training_jobs - DEBUG - 
2019-09-18 00:53:32,089 - training_jobs - DEBUG - ggnn training
2019-09-18 00:53:36,600 - training_jobs - DEBUG -  saving results to results/20190918_005336_ggnn_.json
2019-09-18 00:53:36,600 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 00:55:16,816 - training_jobs - ERROR - Error with trains/task_265.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: cublas runtime error : the GPU program failed to execute at /pytorch/aten/src/THC/THCBlas.cu:258
2019-09-18 00:55:20,993 - training_jobs - DEBUG - training trains/task_305.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '5e-4'}
2019-09-18 00:55:21,033 - training_jobs - DEBUG - training with: 
2019-09-18 00:55:21,033 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-18 00:55:21,034 - training_jobs - DEBUG - GGNN1
2019-09-18 00:55:21,034 - training_jobs - DEBUG - 
2019-09-18 00:55:21,034 - training_jobs - DEBUG - ggnn training
2019-09-18 00:55:22,572 - training_jobs - DEBUG -  saving results to results/20190918_005522_ggnn_.json
2019-09-18 00:55:22,572 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 01:07:17,200 - training_jobs - DEBUG - test_multiple_models
2019-09-18 01:07:17,200 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-18 01:07:17,703 - training_jobs - DEBUG - training time: 715s
2019-09-18 01:07:17,703 - training_jobs - DEBUG - saving to results/20190918_005522_ggnn_.json
2019-09-18 01:07:17,705 - training_jobs - DEBUG - moved jobdict to done_trainings/task_305.yml
2019-09-18 01:07:17,705 - training_jobs - DEBUG - Finished!

2019-09-18 01:07:20,444 - training_jobs - DEBUG - training trains/task_101.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '5e-4'}
2019-09-18 01:07:20,459 - training_jobs - DEBUG - training with: 
2019-09-18 01:07:20,459 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-18 01:07:20,459 - training_jobs - DEBUG - GGNN6
2019-09-18 01:07:20,459 - training_jobs - DEBUG - 
2019-09-18 01:07:20,459 - training_jobs - DEBUG - ggnn training
2019-09-18 01:07:27,297 - training_jobs - DEBUG -  saving results to results/20190918_010727_ggnn_.json
2019-09-18 01:07:27,297 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 01:07:35,155 - training_jobs - ERROR - Error with trains/task_101.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-18 01:07:37,707 - training_jobs - DEBUG - training trains/task_330.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-18 01:07:37,749 - training_jobs - DEBUG - training with: 
2019-09-18 01:07:37,749 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-18 01:07:37,749 - training_jobs - DEBUG - GGNN1
2019-09-18 01:07:37,749 - training_jobs - DEBUG - 
2019-09-18 01:07:37,749 - training_jobs - DEBUG - ggnn training
2019-09-18 01:07:40,835 - training_jobs - DEBUG -  saving results to results/20190918_010740_ggnn_.json
2019-09-18 01:07:40,835 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 01:22:19,247 - training_jobs - DEBUG - test_multiple_models
2019-09-18 01:22:19,247 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-18 01:22:20,096 - training_jobs - DEBUG - training time: 879s
2019-09-18 01:22:20,096 - training_jobs - DEBUG - saving to results/20190918_010740_ggnn_.json
2019-09-18 01:22:20,099 - training_jobs - DEBUG - moved jobdict to done_trainings/task_330.yml
2019-09-18 01:22:20,099 - training_jobs - DEBUG - Finished!

2019-09-18 01:22:22,544 - training_jobs - DEBUG - training trains/task_328.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-18 01:22:22,573 - training_jobs - DEBUG - training with: 
2019-09-18 01:22:22,573 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-18 01:22:22,573 - training_jobs - DEBUG - GGNN1
2019-09-18 01:22:22,573 - training_jobs - DEBUG - 
2019-09-18 01:22:22,573 - training_jobs - DEBUG - ggnn training
2019-09-18 01:22:24,530 - training_jobs - DEBUG -  saving results to results/20190918_012224_ggnn_.json
2019-09-18 01:22:24,530 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 01:38:19,406 - training_jobs - DEBUG - test_multiple_models
2019-09-18 01:38:19,406 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-18 01:38:20,274 - training_jobs - DEBUG - training time: 956s
2019-09-18 01:38:20,274 - training_jobs - DEBUG - saving to results/20190918_012224_ggnn_.json
2019-09-18 01:38:20,276 - training_jobs - DEBUG - moved jobdict to done_trainings/task_328.yml
2019-09-18 01:38:20,276 - training_jobs - DEBUG - Finished!

2019-09-18 01:38:22,971 - training_jobs - DEBUG - training trains/task_165.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '5e-4'}
2019-09-18 01:38:23,145 - training_jobs - DEBUG - training with: 
2019-09-18 01:38:23,145 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-18 01:38:23,145 - training_jobs - DEBUG - GGNN6
2019-09-18 01:38:23,145 - training_jobs - DEBUG - 
2019-09-18 01:38:23,145 - training_jobs - DEBUG - ggnn training
2019-09-18 01:38:24,521 - training_jobs - DEBUG -  saving results to results/20190918_013824_ggnn_.json
2019-09-18 01:38:24,521 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 01:38:30,843 - training_jobs - ERROR - Error with trains/task_165.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-18 01:38:33,577 - training_jobs - DEBUG - training trains/task_322.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-18 01:38:33,625 - training_jobs - DEBUG - training with: 
2019-09-18 01:38:33,625 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-18 01:38:33,626 - training_jobs - DEBUG - GGNN1
2019-09-18 01:38:33,626 - training_jobs - DEBUG - 
2019-09-18 01:38:33,626 - training_jobs - DEBUG - ggnn training
2019-09-18 01:38:36,069 - training_jobs - DEBUG -  saving results to results/20190918_013836_ggnn_.json
2019-09-18 01:38:36,069 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 01:50:41,468 - training_jobs - DEBUG - test_multiple_models
2019-09-18 01:50:41,468 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-18 01:50:42,326 - training_jobs - DEBUG - training time: 726s
2019-09-18 01:50:42,326 - training_jobs - DEBUG - saving to results/20190918_013836_ggnn_.json
2019-09-18 01:50:42,328 - training_jobs - DEBUG - moved jobdict to done_trainings/task_322.yml
2019-09-18 01:50:42,328 - training_jobs - DEBUG - Finished!

2019-09-18 01:50:44,745 - training_jobs - DEBUG - training trains/task_289.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '5e-4'}
2019-09-18 01:50:44,758 - training_jobs - DEBUG - training with: 
2019-09-18 01:50:44,758 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-18 01:50:44,758 - training_jobs - DEBUG - GGNN1
2019-09-18 01:50:44,758 - training_jobs - DEBUG - 
2019-09-18 01:50:44,758 - training_jobs - DEBUG - ggnn training
2019-09-18 01:50:47,007 - training_jobs - DEBUG -  saving results to results/20190918_015047_ggnn_.json
2019-09-18 01:50:47,007 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 01:52:09,156 - training_jobs - ERROR - Error with trains/task_289.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA error: device-side assert triggered
2019-09-18 01:52:12,643 - training_jobs - DEBUG - training trains/task_337.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '5e-4'}
2019-09-18 01:52:12,655 - training_jobs - DEBUG - training with: 
2019-09-18 01:52:12,655 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-18 01:52:12,655 - training_jobs - DEBUG - GGNN1
2019-09-18 01:52:12,655 - training_jobs - DEBUG - 
2019-09-18 01:52:12,655 - training_jobs - DEBUG - ggnn training
2019-09-18 01:52:14,623 - training_jobs - DEBUG -  saving results to results/20190918_015214_ggnn_.json
2019-09-18 01:52:14,623 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 02:09:49,023 - training_jobs - DEBUG - test_multiple_models
2019-09-18 02:09:49,023 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-18 02:09:50,051 - training_jobs - DEBUG - training time: 1055s
2019-09-18 02:09:50,051 - training_jobs - DEBUG - saving to results/20190918_015214_ggnn_.json
2019-09-18 02:09:50,053 - training_jobs - DEBUG - moved jobdict to done_trainings/task_337.yml
2019-09-18 02:09:50,054 - training_jobs - DEBUG - Finished!

2019-09-18 02:09:53,164 - training_jobs - DEBUG - training trains/task_32.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-18 02:09:53,271 - training_jobs - DEBUG - training with: 
2019-09-18 02:09:53,271 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-18 02:09:53,271 - training_jobs - DEBUG - GGNN5
2019-09-18 02:09:53,271 - training_jobs - DEBUG - 
2019-09-18 02:09:53,271 - training_jobs - DEBUG - ggnn training
2019-09-18 02:10:02,188 - training_jobs - DEBUG -  saving results to results/20190918_021002_ggnn_.json
2019-09-18 02:10:02,189 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 02:33:20,040 - training_jobs - DEBUG - test_multiple_models
2019-09-18 02:33:20,040 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-18 02:33:24,829 - training_jobs - DEBUG - training time: 1403s
2019-09-18 02:33:24,829 - training_jobs - DEBUG - saving to results/20190918_021002_ggnn_.json
2019-09-18 02:33:24,832 - training_jobs - DEBUG - moved jobdict to done_trainings/task_32.yml
2019-09-18 02:33:24,832 - training_jobs - DEBUG - Finished!

2019-09-18 02:33:27,607 - training_jobs - DEBUG - training trains/task_63.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-18 02:33:27,838 - training_jobs - DEBUG - training with: 
2019-09-18 02:33:27,838 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-18 02:33:27,838 - training_jobs - DEBUG - GGNN5
2019-09-18 02:33:27,838 - training_jobs - DEBUG - 
2019-09-18 02:33:27,838 - training_jobs - DEBUG - ggnn training
2019-09-18 02:33:35,834 - training_jobs - DEBUG -  saving results to results/20190918_023335_ggnn_.json
2019-09-18 02:33:35,834 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 02:34:33,076 - training_jobs - ERROR - Error with trains/task_63.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: cublas runtime error : the GPU program failed to execute at /pytorch/aten/src/THC/THCBlas.cu:258
2019-09-18 02:34:36,693 - training_jobs - DEBUG - training trains/task_150.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-18 02:34:37,462 - training_jobs - DEBUG - training with: 
2019-09-18 02:34:37,462 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-18 02:34:37,462 - training_jobs - DEBUG - GGNN6
2019-09-18 02:34:37,462 - training_jobs - DEBUG - 
2019-09-18 02:34:37,462 - training_jobs - DEBUG - ggnn training
2019-09-18 02:34:39,743 - training_jobs - DEBUG -  saving results to results/20190918_023439_ggnn_.json
2019-09-18 02:34:39,743 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 02:34:48,797 - training_jobs - ERROR - Error with trains/task_150.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-18 02:34:51,273 - training_jobs - DEBUG - training trains/task_217.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '5e-4'}
2019-09-18 02:34:51,445 - training_jobs - DEBUG - training with: 
2019-09-18 02:34:51,445 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-18 02:34:51,445 - training_jobs - DEBUG - GGNN1
2019-09-18 02:34:51,445 - training_jobs - DEBUG - 
2019-09-18 02:34:51,445 - training_jobs - DEBUG - ggnn training
2019-09-18 02:34:53,631 - training_jobs - DEBUG -  saving results to results/20190918_023453_ggnn_.json
2019-09-18 02:34:53,631 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 02:54:01,747 - training_jobs - DEBUG - test_multiple_models
2019-09-18 02:54:01,747 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-18 02:54:02,693 - training_jobs - DEBUG - training time: 1149s
2019-09-18 02:54:02,693 - training_jobs - DEBUG - saving to results/20190918_023453_ggnn_.json
2019-09-18 02:54:02,720 - training_jobs - DEBUG - moved jobdict to done_trainings/task_217.yml
2019-09-18 02:54:02,720 - training_jobs - DEBUG - Finished!

2019-09-18 02:54:06,658 - training_jobs - DEBUG - training trains/task_170.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-18 02:54:06,683 - training_jobs - DEBUG - training with: 
2019-09-18 02:54:06,684 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-18 02:54:06,684 - training_jobs - DEBUG - GGNN6
2019-09-18 02:54:06,684 - training_jobs - DEBUG - 
2019-09-18 02:54:06,684 - training_jobs - DEBUG - ggnn training
2019-09-18 02:54:11,301 - training_jobs - DEBUG -  saving results to results/20190918_025411_ggnn_.json
2019-09-18 02:54:11,301 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 02:54:19,222 - training_jobs - ERROR - Error with trains/task_170.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-18 02:54:22,215 - training_jobs - DEBUG - training trains/task_235.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '5e-4'}
2019-09-18 02:54:22,361 - training_jobs - DEBUG - training with: 
2019-09-18 02:54:22,361 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-18 02:54:22,362 - training_jobs - DEBUG - GGNN1
2019-09-18 02:54:22,362 - training_jobs - DEBUG - 
2019-09-18 02:54:22,362 - training_jobs - DEBUG - ggnn training
2019-09-18 02:54:25,983 - training_jobs - DEBUG -  saving results to results/20190918_025425_ggnn_.json
2019-09-18 02:54:25,983 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 03:26:16,896 - training_jobs - DEBUG - test_multiple_models
2019-09-18 03:26:16,896 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-18 03:26:18,104 - training_jobs - DEBUG - training time: 1912s
2019-09-18 03:26:18,104 - training_jobs - DEBUG - saving to results/20190918_025425_ggnn_.json
2019-09-18 03:26:18,106 - training_jobs - DEBUG - moved jobdict to done_trainings/task_235.yml
2019-09-18 03:26:18,106 - training_jobs - DEBUG - Finished!

2019-09-18 03:26:20,844 - training_jobs - DEBUG - training trains/task_221.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '5e-4'}
2019-09-18 03:26:20,878 - training_jobs - DEBUG - training with: 
2019-09-18 03:26:20,878 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-18 03:26:20,879 - training_jobs - DEBUG - GGNN1
2019-09-18 03:26:20,879 - training_jobs - DEBUG - 
2019-09-18 03:26:20,879 - training_jobs - DEBUG - ggnn training
2019-09-18 03:26:23,922 - training_jobs - DEBUG -  saving results to results/20190918_032623_ggnn_.json
2019-09-18 03:26:23,922 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 03:45:40,578 - training_jobs - DEBUG - test_multiple_models
2019-09-18 03:45:40,578 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-18 03:45:41,698 - training_jobs - DEBUG - training time: 1158s
2019-09-18 03:45:41,698 - training_jobs - DEBUG - saving to results/20190918_032623_ggnn_.json
2019-09-18 03:45:41,700 - training_jobs - DEBUG - moved jobdict to done_trainings/task_221.yml
2019-09-18 03:45:41,700 - training_jobs - DEBUG - Finished!

2019-09-18 03:45:44,171 - training_jobs - DEBUG - training trains/task_234.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-18 03:45:44,184 - training_jobs - DEBUG - training with: 
2019-09-18 03:45:44,184 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-18 03:45:44,184 - training_jobs - DEBUG - GGNN1
2019-09-18 03:45:44,184 - training_jobs - DEBUG - 
2019-09-18 03:45:44,184 - training_jobs - DEBUG - ggnn training
2019-09-18 03:45:46,803 - training_jobs - DEBUG -  saving results to results/20190918_034546_ggnn_.json
2019-09-18 03:45:46,803 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 04:12:29,623 - training_jobs - DEBUG - test_multiple_models
2019-09-18 04:12:29,623 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-18 04:12:30,782 - training_jobs - DEBUG - training time: 1604s
2019-09-18 04:12:30,782 - training_jobs - DEBUG - saving to results/20190918_034546_ggnn_.json
2019-09-18 04:12:30,785 - training_jobs - DEBUG - moved jobdict to done_trainings/task_234.yml
2019-09-18 04:12:30,785 - training_jobs - DEBUG - Finished!

2019-09-18 04:12:33,305 - training_jobs - DEBUG - training trains/task_208.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-18 04:12:33,479 - training_jobs - DEBUG - training with: 
2019-09-18 04:12:33,479 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-18 04:12:33,479 - training_jobs - DEBUG - GGNN1
2019-09-18 04:12:33,479 - training_jobs - DEBUG - 
2019-09-18 04:12:33,479 - training_jobs - DEBUG - ggnn training
2019-09-18 04:12:35,041 - training_jobs - DEBUG -  saving results to results/20190918_041235_ggnn_.json
2019-09-18 04:12:35,041 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 04:29:40,006 - training_jobs - DEBUG - test_multiple_models
2019-09-18 04:29:40,006 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-18 04:29:40,699 - training_jobs - DEBUG - training time: 1026s
2019-09-18 04:29:40,699 - training_jobs - DEBUG - saving to results/20190918_041235_ggnn_.json
2019-09-18 04:29:40,701 - training_jobs - DEBUG - moved jobdict to done_trainings/task_208.yml
2019-09-18 04:29:40,701 - training_jobs - DEBUG - Finished!

2019-09-18 04:29:43,103 - training_jobs - DEBUG - training trains/task_105.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '5e-4'}
2019-09-18 04:29:43,139 - training_jobs - DEBUG - training with: 
2019-09-18 04:29:43,139 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-18 04:29:43,139 - training_jobs - DEBUG - GGNN6
2019-09-18 04:29:43,139 - training_jobs - DEBUG - 
2019-09-18 04:29:43,139 - training_jobs - DEBUG - ggnn training
2019-09-18 04:29:44,724 - training_jobs - DEBUG -  saving results to results/20190918_042944_ggnn_.json
2019-09-18 04:29:44,725 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 04:29:51,431 - training_jobs - ERROR - Error with trains/task_105.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-18 04:29:53,920 - training_jobs - DEBUG - training trains/task_15.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-18 04:29:53,933 - training_jobs - DEBUG - training with: 
2019-09-18 04:29:53,934 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-18 04:29:53,934 - training_jobs - DEBUG - GGNN5
2019-09-18 04:29:53,934 - training_jobs - DEBUG - 
2019-09-18 04:29:53,934 - training_jobs - DEBUG - ggnn training
2019-09-18 04:29:55,654 - training_jobs - DEBUG -  saving results to results/20190918_042955_ggnn_.json
2019-09-18 04:29:55,654 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 04:43:53,879 - training_jobs - DEBUG - test_multiple_models
2019-09-18 04:43:53,880 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-18 04:43:54,526 - training_jobs - DEBUG - training time: 839s
2019-09-18 04:43:54,526 - training_jobs - DEBUG - saving to results/20190918_042955_ggnn_.json
2019-09-18 04:43:54,529 - training_jobs - DEBUG - moved jobdict to done_trainings/task_15.yml
2019-09-18 04:43:54,529 - training_jobs - DEBUG - Finished!

2019-09-18 04:43:56,984 - training_jobs - DEBUG - training trains/task_199.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '5e-4'}
2019-09-18 04:43:56,997 - training_jobs - DEBUG - training with: 
2019-09-18 04:43:56,997 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-18 04:43:56,997 - training_jobs - DEBUG - GGNN1
2019-09-18 04:43:56,998 - training_jobs - DEBUG - 
2019-09-18 04:43:56,998 - training_jobs - DEBUG - ggnn training
2019-09-18 04:43:58,576 - training_jobs - DEBUG -  saving results to results/20190918_044358_ggnn_.json
2019-09-18 04:43:58,576 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 04:57:45,834 - training_jobs - DEBUG - test_multiple_models
2019-09-18 04:57:45,834 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-18 04:57:46,672 - training_jobs - DEBUG - training time: 828s
2019-09-18 04:57:46,672 - training_jobs - DEBUG - saving to results/20190918_044358_ggnn_.json
2019-09-18 04:57:46,675 - training_jobs - DEBUG - moved jobdict to done_trainings/task_199.yml
2019-09-18 04:57:46,675 - training_jobs - DEBUG - Finished!

2019-09-18 04:57:49,889 - training_jobs - DEBUG - training trains/task_321.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-18 04:57:49,907 - training_jobs - DEBUG - training with: 
2019-09-18 04:57:49,907 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-18 04:57:49,907 - training_jobs - DEBUG - GGNN1
2019-09-18 04:57:49,907 - training_jobs - DEBUG - 
2019-09-18 04:57:49,907 - training_jobs - DEBUG - ggnn training
2019-09-18 04:57:55,512 - training_jobs - DEBUG -  saving results to results/20190918_045755_ggnn_.json
2019-09-18 04:57:55,512 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 05:09:03,405 - training_jobs - DEBUG - test_multiple_models
2019-09-18 05:09:03,405 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-18 05:09:04,278 - training_jobs - DEBUG - training time: 669s
2019-09-18 05:09:04,278 - training_jobs - DEBUG - saving to results/20190918_045755_ggnn_.json
2019-09-18 05:09:04,280 - training_jobs - DEBUG - moved jobdict to done_trainings/task_321.yml
2019-09-18 05:09:04,280 - training_jobs - DEBUG - Finished!

2019-09-18 05:09:06,751 - training_jobs - DEBUG - training trains/task_211.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '5e-4'}
2019-09-18 05:09:06,830 - training_jobs - DEBUG - training with: 
2019-09-18 05:09:06,830 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-18 05:09:06,830 - training_jobs - DEBUG - GGNN1
2019-09-18 05:09:06,831 - training_jobs - DEBUG - 
2019-09-18 05:09:06,831 - training_jobs - DEBUG - ggnn training
2019-09-18 05:09:08,410 - training_jobs - DEBUG -  saving results to results/20190918_050908_ggnn_.json
2019-09-18 05:09:08,410 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 05:28:18,841 - training_jobs - DEBUG - test_multiple_models
2019-09-18 05:28:18,841 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-18 05:28:19,545 - training_jobs - DEBUG - training time: 1151s
2019-09-18 05:28:19,545 - training_jobs - DEBUG - saving to results/20190918_050908_ggnn_.json
2019-09-18 05:28:19,755 - training_jobs - DEBUG - moved jobdict to done_trainings/task_211.yml
2019-09-18 05:28:19,755 - training_jobs - DEBUG - Finished!

2019-09-18 05:28:24,307 - training_jobs - DEBUG - training trains/task_49.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '5e-4'}
2019-09-18 05:28:24,340 - training_jobs - DEBUG - training with: 
2019-09-18 05:28:24,341 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-18 05:28:24,341 - training_jobs - DEBUG - GGNN5
2019-09-18 05:28:24,341 - training_jobs - DEBUG - 
2019-09-18 05:28:24,341 - training_jobs - DEBUG - ggnn training
2019-09-18 05:28:26,411 - training_jobs - DEBUG -  saving results to results/20190918_052826_ggnn_.json
2019-09-18 05:28:26,411 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 05:29:47,058 - training_jobs - ERROR - Error with trains/task_49.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED
2019-09-18 05:29:50,576 - training_jobs - DEBUG - training trains/task_317.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '5e-4'}
2019-09-18 05:29:50,599 - training_jobs - DEBUG - training with: 
2019-09-18 05:29:50,599 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-18 05:29:50,599 - training_jobs - DEBUG - GGNN1
2019-09-18 05:29:50,599 - training_jobs - DEBUG - 
2019-09-18 05:29:50,599 - training_jobs - DEBUG - ggnn training
2019-09-18 05:29:54,071 - training_jobs - DEBUG -  saving results to results/20190918_052954_ggnn_.json
2019-09-18 05:29:54,071 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 05:40:23,012 - training_jobs - DEBUG - test_multiple_models
2019-09-18 05:40:23,013 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-18 05:40:23,849 - training_jobs - DEBUG - training time: 630s
2019-09-18 05:40:23,850 - training_jobs - DEBUG - saving to results/20190918_052954_ggnn_.json
2019-09-18 05:40:23,851 - training_jobs - DEBUG - moved jobdict to done_trainings/task_317.yml
2019-09-18 05:40:23,852 - training_jobs - DEBUG - Finished!

2019-09-18 05:40:26,292 - training_jobs - DEBUG - training trains/task_262.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-18 05:40:26,523 - training_jobs - DEBUG - training with: 
2019-09-18 05:40:26,523 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-18 05:40:26,523 - training_jobs - DEBUG - GGNN1
2019-09-18 05:40:26,523 - training_jobs - DEBUG - 
2019-09-18 05:40:26,523 - training_jobs - DEBUG - ggnn training
2019-09-18 05:40:27,867 - training_jobs - DEBUG -  saving results to results/20190918_054027_ggnn_.json
2019-09-18 05:40:27,867 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 05:42:02,261 - training_jobs - ERROR - Error with trains/task_262.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA error: device-side assert triggered
2019-09-18 05:42:06,009 - training_jobs - DEBUG - training trains/task_316.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-18 05:42:06,023 - training_jobs - DEBUG - training with: 
2019-09-18 05:42:06,023 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-18 05:42:06,023 - training_jobs - DEBUG - GGNN1
2019-09-18 05:42:06,023 - training_jobs - DEBUG - 
2019-09-18 05:42:06,023 - training_jobs - DEBUG - ggnn training
2019-09-18 05:42:08,265 - training_jobs - DEBUG -  saving results to results/20190918_054208_ggnn_.json
2019-09-18 05:42:08,265 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 05:53:14,107 - training_jobs - DEBUG - test_multiple_models
2019-09-18 05:53:14,107 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-18 05:53:14,991 - training_jobs - DEBUG - training time: 667s
2019-09-18 05:53:14,991 - training_jobs - DEBUG - saving to results/20190918_054208_ggnn_.json
2019-09-18 05:53:14,993 - training_jobs - DEBUG - moved jobdict to done_trainings/task_316.yml
2019-09-18 05:53:14,993 - training_jobs - DEBUG - Finished!

2019-09-18 05:53:17,444 - training_jobs - DEBUG - training trains/task_240.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-18 05:53:17,457 - training_jobs - DEBUG - training with: 
2019-09-18 05:53:17,457 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-18 05:53:17,457 - training_jobs - DEBUG - GGNN1
2019-09-18 05:53:17,457 - training_jobs - DEBUG - 
2019-09-18 05:53:17,457 - training_jobs - DEBUG - ggnn training
2019-09-18 05:53:20,403 - training_jobs - DEBUG -  saving results to results/20190918_055320_ggnn_.json
2019-09-18 05:53:20,403 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 06:20:15,263 - training_jobs - DEBUG - test_multiple_models
2019-09-18 06:20:15,263 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-18 06:20:16,427 - training_jobs - DEBUG - training time: 1616s
2019-09-18 06:20:16,427 - training_jobs - DEBUG - saving to results/20190918_055320_ggnn_.json
2019-09-18 06:20:16,429 - training_jobs - DEBUG - moved jobdict to done_trainings/task_240.yml
2019-09-18 06:20:16,429 - training_jobs - DEBUG - Finished!

2019-09-18 06:20:18,978 - training_jobs - DEBUG - training trains/task_166.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-18 06:20:19,223 - training_jobs - DEBUG - training with: 
2019-09-18 06:20:19,223 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-18 06:20:19,223 - training_jobs - DEBUG - GGNN6
2019-09-18 06:20:19,223 - training_jobs - DEBUG - 
2019-09-18 06:20:19,223 - training_jobs - DEBUG - ggnn training
2019-09-18 06:20:21,953 - training_jobs - DEBUG -  saving results to results/20190918_062021_ggnn_.json
2019-09-18 06:20:21,953 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 06:20:27,948 - training_jobs - ERROR - Error with trains/task_166.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-18 06:20:30,846 - training_jobs - DEBUG - training trains/task_174.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-18 06:20:30,861 - training_jobs - DEBUG - training with: 
2019-09-18 06:20:30,861 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-18 06:20:30,861 - training_jobs - DEBUG - GGNN6
2019-09-18 06:20:30,861 - training_jobs - DEBUG - 
2019-09-18 06:20:30,861 - training_jobs - DEBUG - ggnn training
2019-09-18 06:20:32,092 - training_jobs - DEBUG -  saving results to results/20190918_062032_ggnn_.json
2019-09-18 06:20:32,092 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 06:20:37,999 - training_jobs - ERROR - Error with trains/task_174.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-18 06:20:41,925 - training_jobs - DEBUG - training trains/task_144.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-18 06:20:41,963 - training_jobs - DEBUG - training with: 
2019-09-18 06:20:41,963 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-18 06:20:41,963 - training_jobs - DEBUG - GGNN6
2019-09-18 06:20:41,963 - training_jobs - DEBUG - 
2019-09-18 06:20:41,963 - training_jobs - DEBUG - ggnn training
2019-09-18 06:20:43,371 - training_jobs - DEBUG -  saving results to results/20190918_062043_ggnn_.json
2019-09-18 06:20:43,371 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 06:20:49,768 - training_jobs - ERROR - Error with trains/task_144.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-18 06:20:52,483 - training_jobs - DEBUG - training trains/task_332.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '1e-4'}
2019-09-18 06:20:52,573 - training_jobs - DEBUG - training with: 
2019-09-18 06:20:52,574 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-18 06:20:52,574 - training_jobs - DEBUG - GGNN1
2019-09-18 06:20:52,574 - training_jobs - DEBUG - 
2019-09-18 06:20:52,574 - training_jobs - DEBUG - ggnn training
2019-09-18 06:20:55,541 - training_jobs - DEBUG -  saving results to results/20190918_062055_ggnn_.json
2019-09-18 06:20:55,541 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 06:38:28,094 - training_jobs - DEBUG - test_multiple_models
2019-09-18 06:38:28,094 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-18 06:38:29,016 - training_jobs - DEBUG - training time: 1053s
2019-09-18 06:38:29,016 - training_jobs - DEBUG - saving to results/20190918_062055_ggnn_.json
2019-09-18 06:38:29,018 - training_jobs - DEBUG - moved jobdict to done_trainings/task_332.yml
2019-09-18 06:38:29,018 - training_jobs - DEBUG - Finished!

2019-09-18 06:38:31,796 - training_jobs - DEBUG - training trains/task_71.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-18 06:38:31,833 - training_jobs - DEBUG - training with: 
2019-09-18 06:38:31,833 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-18 06:38:31,833 - training_jobs - DEBUG - GGNN5
2019-09-18 06:38:31,833 - training_jobs - DEBUG - 
2019-09-18 06:38:31,833 - training_jobs - DEBUG - ggnn training
2019-09-18 06:38:33,191 - training_jobs - DEBUG -  saving results to results/20190918_063833_ggnn_.json
2019-09-18 06:38:33,191 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 06:42:47,727 - training_jobs - DEBUG - test_multiple_models
2019-09-18 06:42:47,727 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-18 06:42:48,180 - training_jobs - DEBUG - training time: 255s
2019-09-18 06:42:48,180 - training_jobs - DEBUG - saving to results/20190918_063833_ggnn_.json
2019-09-18 06:42:48,182 - training_jobs - DEBUG - moved jobdict to done_trainings/task_71.yml
2019-09-18 06:42:48,182 - training_jobs - DEBUG - Finished!

2019-09-18 06:42:50,699 - training_jobs - DEBUG - training trains/task_110.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-18 06:42:50,769 - training_jobs - DEBUG - training with: 
2019-09-18 06:42:50,769 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-18 06:42:50,769 - training_jobs - DEBUG - GGNN6
2019-09-18 06:42:50,769 - training_jobs - DEBUG - 
2019-09-18 06:42:50,769 - training_jobs - DEBUG - ggnn training
2019-09-18 06:42:56,847 - training_jobs - DEBUG -  saving results to results/20190918_064256_ggnn_.json
2019-09-18 06:42:56,847 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 06:43:03,408 - training_jobs - ERROR - Error with trains/task_110.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-18 06:43:05,931 - training_jobs - DEBUG - training trains/task_308.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '1e-4'}
2019-09-18 06:43:05,970 - training_jobs - DEBUG - training with: 
2019-09-18 06:43:05,971 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-18 06:43:05,971 - training_jobs - DEBUG - GGNN1
2019-09-18 06:43:05,971 - training_jobs - DEBUG - 
2019-09-18 06:43:05,971 - training_jobs - DEBUG - ggnn training
2019-09-18 06:43:07,300 - training_jobs - DEBUG -  saving results to results/20190918_064307_ggnn_.json
2019-09-18 06:43:07,300 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 06:57:25,883 - training_jobs - DEBUG - test_multiple_models
2019-09-18 06:57:25,884 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-18 06:57:26,379 - training_jobs - DEBUG - training time: 859s
2019-09-18 06:57:26,379 - training_jobs - DEBUG - saving to results/20190918_064307_ggnn_.json
2019-09-18 06:57:26,381 - training_jobs - DEBUG - moved jobdict to done_trainings/task_308.yml
2019-09-18 06:57:26,382 - training_jobs - DEBUG - Finished!

2019-09-18 06:57:28,889 - training_jobs - DEBUG - training trains/task_41.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '5e-4'}
2019-09-18 06:57:28,915 - training_jobs - DEBUG - training with: 
2019-09-18 06:57:28,915 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-18 06:57:28,915 - training_jobs - DEBUG - GGNN5
2019-09-18 06:57:28,915 - training_jobs - DEBUG - 
2019-09-18 06:57:28,915 - training_jobs - DEBUG - ggnn training
2019-09-18 06:57:30,362 - training_jobs - DEBUG -  saving results to results/20190918_065730_ggnn_.json
2019-09-18 06:57:30,362 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 06:58:08,745 - training_jobs - ERROR - Error with trains/task_41.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: cublas runtime error : the GPU program failed to execute at /pytorch/aten/src/THC/THCBlas.cu:258
2019-09-18 06:58:12,678 - training_jobs - DEBUG - training trains/task_148.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_half/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-18 06:58:12,697 - training_jobs - DEBUG - training with: 
2019-09-18 06:58:12,697 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_half/
2019-09-18 06:58:12,697 - training_jobs - DEBUG - GGNN6
2019-09-18 06:58:12,697 - training_jobs - DEBUG - 
2019-09-18 06:58:12,697 - training_jobs - DEBUG - ggnn training
2019-09-18 06:58:20,688 - training_jobs - DEBUG -  saving results to results/20190918_065820_ggnn_.json
2019-09-18 06:58:20,688 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 06:58:30,066 - training_jobs - ERROR - Error with trains/task_148.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-18 06:58:32,683 - training_jobs - DEBUG - training trains/task_312.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-18 06:58:32,718 - training_jobs - DEBUG - training with: 
2019-09-18 06:58:32,718 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-18 06:58:32,718 - training_jobs - DEBUG - GGNN1
2019-09-18 06:58:32,718 - training_jobs - DEBUG - 
2019-09-18 06:58:32,718 - training_jobs - DEBUG - ggnn training
2019-09-18 06:58:34,083 - training_jobs - DEBUG -  saving results to results/20190918_065834_ggnn_.json
2019-09-18 06:58:34,083 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 07:10:24,220 - training_jobs - DEBUG - test_multiple_models
2019-09-18 07:10:24,221 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-18 07:10:24,681 - training_jobs - DEBUG - training time: 711s
2019-09-18 07:10:24,681 - training_jobs - DEBUG - saving to results/20190918_065834_ggnn_.json
2019-09-18 07:10:24,683 - training_jobs - DEBUG - moved jobdict to done_trainings/task_312.yml
2019-09-18 07:10:24,683 - training_jobs - DEBUG - Finished!

2019-09-18 07:10:27,499 - training_jobs - DEBUG - training trains/task_33.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_half/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '5e-4'}
2019-09-18 07:10:27,572 - training_jobs - DEBUG - training with: 
2019-09-18 07:10:27,572 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_half/
2019-09-18 07:10:27,572 - training_jobs - DEBUG - GGNN5
2019-09-18 07:10:27,572 - training_jobs - DEBUG - 
2019-09-18 07:10:27,572 - training_jobs - DEBUG - ggnn training
2019-09-18 07:10:33,445 - training_jobs - DEBUG -  saving results to results/20190918_071033_ggnn_.json
2019-09-18 07:10:33,445 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 07:35:47,601 - training_jobs - DEBUG - test_multiple_models
2019-09-18 07:35:47,602 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-18 07:35:52,750 - training_jobs - DEBUG - training time: 1519s
2019-09-18 07:35:52,751 - training_jobs - DEBUG - saving to results/20190918_071033_ggnn_.json
2019-09-18 07:35:52,781 - training_jobs - DEBUG - moved jobdict to done_trainings/task_33.yml
2019-09-18 07:35:52,781 - training_jobs - DEBUG - Finished!

2019-09-18 07:35:55,488 - training_jobs - DEBUG - training trains/task_68.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-18 07:35:55,613 - training_jobs - DEBUG - training with: 
2019-09-18 07:35:55,613 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-18 07:35:55,613 - training_jobs - DEBUG - GGNN5
2019-09-18 07:35:55,613 - training_jobs - DEBUG - 
2019-09-18 07:35:55,613 - training_jobs - DEBUG - ggnn training
2019-09-18 07:36:00,336 - training_jobs - DEBUG -  saving results to results/20190918_073600_ggnn_.json
2019-09-18 07:36:00,336 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 07:40:14,519 - training_jobs - DEBUG - test_multiple_models
2019-09-18 07:40:14,519 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-18 07:40:14,983 - training_jobs - DEBUG - training time: 255s
2019-09-18 07:40:14,983 - training_jobs - DEBUG - saving to results/20190918_073600_ggnn_.json
2019-09-18 07:40:15,083 - training_jobs - DEBUG - moved jobdict to done_trainings/task_68.yml
2019-09-18 07:40:15,083 - training_jobs - DEBUG - Finished!

2019-09-18 07:40:17,524 - training_jobs - DEBUG - training trains/task_135.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-18 07:40:17,677 - training_jobs - DEBUG - training with: 
2019-09-18 07:40:17,677 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max_third/
2019-09-18 07:40:17,677 - training_jobs - DEBUG - GGNN6
2019-09-18 07:40:17,677 - training_jobs - DEBUG - 
2019-09-18 07:40:17,677 - training_jobs - DEBUG - ggnn training
2019-09-18 07:40:19,037 - training_jobs - DEBUG -  saving results to results/20190918_074019_ggnn_.json
2019-09-18 07:40:19,038 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 07:40:25,139 - training_jobs - ERROR - Error with trains/task_135.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-18 07:40:27,603 - training_jobs - DEBUG - training trains/task_348.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-18 07:40:27,748 - training_jobs - DEBUG - training with: 
2019-09-18 07:40:27,748 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max/
2019-09-18 07:40:27,748 - training_jobs - DEBUG - GGNN5
2019-09-18 07:40:27,748 - training_jobs - DEBUG - 
2019-09-18 07:40:27,748 - training_jobs - DEBUG - ggnn training
2019-09-18 07:40:51,304 - training_jobs - DEBUG -  saving results to results/20190918_074051_ggnn_.json
2019-09-18 07:40:51,304 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 07:41:04,559 - training_jobs - ERROR - Error with trains/task_348.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1126, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'd5'
2019-09-18 07:41:07,433 - training_jobs - DEBUG - training trains/task_12.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-18 07:41:08,103 - training_jobs - DEBUG - training with: 
2019-09-18 07:41:08,103 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max_third/
2019-09-18 07:41:08,104 - training_jobs - DEBUG - GGNN5
2019-09-18 07:41:08,104 - training_jobs - DEBUG - 
2019-09-18 07:41:08,104 - training_jobs - DEBUG - ggnn training
2019-09-18 07:41:13,101 - training_jobs - DEBUG -  saving results to results/20190918_074113_ggnn_.json
2019-09-18 07:41:13,101 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 07:55:01,920 - training_jobs - DEBUG - test_multiple_models
2019-09-18 07:55:01,920 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-18 07:55:02,567 - training_jobs - DEBUG - training time: 829s
2019-09-18 07:55:02,568 - training_jobs - DEBUG - saving to results/20190918_074113_ggnn_.json
2019-09-18 07:55:02,767 - training_jobs - DEBUG - moved jobdict to done_trainings/task_12.yml
2019-09-18 07:55:02,768 - training_jobs - DEBUG - Finished!

2019-09-18 07:55:07,913 - training_jobs - DEBUG - training trains/task_189.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 20,
 'd2': 30,
 'd3': 20,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '5e-4'}
2019-09-18 07:55:07,936 - training_jobs - DEBUG - training with: 
2019-09-18 07:55:07,936 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-18 07:55:07,936 - training_jobs - DEBUG - GGNN6
2019-09-18 07:55:07,936 - training_jobs - DEBUG - 
2019-09-18 07:55:07,936 - training_jobs - DEBUG - ggnn training
2019-09-18 07:55:17,307 - training_jobs - DEBUG -  saving results to results/20190918_075517_ggnn_.json
2019-09-18 07:55:17,307 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 07:55:25,287 - training_jobs - ERROR - Error with trains/task_189.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 263, in forward
    x = F.relu(self.dense2_bn(self.fc3(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1623, in batch_norm
    training, momentum, eps, torch.backends.cudnn.enabled
RuntimeError: running_mean should contain 10 elements not 20
2019-09-18 07:55:27,720 - training_jobs - DEBUG - training trains/task_318.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-18 07:55:27,859 - training_jobs - DEBUG - training with: 
2019-09-18 07:55:27,859 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-18 07:55:27,859 - training_jobs - DEBUG - GGNN1
2019-09-18 07:55:27,860 - training_jobs - DEBUG - 
2019-09-18 07:55:27,860 - training_jobs - DEBUG - ggnn training
2019-09-18 07:55:29,997 - training_jobs - DEBUG -  saving results to results/20190918_075529_ggnn_.json
2019-09-18 07:55:29,997 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 08:06:09,126 - training_jobs - DEBUG - test_multiple_models
2019-09-18 08:06:09,147 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-18 08:06:19,049 - training_jobs - DEBUG - training time: 649s
2019-09-18 08:06:19,049 - training_jobs - DEBUG - saving to results/20190918_075529_ggnn_.json
2019-09-18 08:06:19,082 - training_jobs - DEBUG - moved jobdict to done_trainings/task_318.yml
2019-09-18 08:06:19,082 - training_jobs - DEBUG - Finished!

2019-09-18 08:06:21,866 - training_jobs - DEBUG - training trains/task_334.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 20,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 70,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-18 08:06:21,922 - training_jobs - DEBUG - training with: 
2019-09-18 08:06:21,922 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-18 08:06:21,922 - training_jobs - DEBUG - GGNN1
2019-09-18 08:06:21,922 - training_jobs - DEBUG - 
2019-09-18 08:06:21,922 - training_jobs - DEBUG - ggnn training
2019-09-18 08:06:24,610 - training_jobs - DEBUG -  saving results to results/20190918_080624_ggnn_.json
2019-09-18 08:06:24,611 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 08:11:58,339 - training_jobs - DEBUG - ('tasks/gnn_ggnn1_complete_dataset', '.yaml')
2019-09-18 08:11:58,353 - training_jobs - DEBUG - ('tasks/gnn_ggnn6_complete_dataset', '.yaml')
2019-09-18 08:11:58,362 - training_jobs - DEBUG - ('tasks/gnn_ggnn5_complete_dataset', '.yaml')
2019-09-18 08:12:01,006 - training_jobs - DEBUG - training trains/task_20.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '1e-4'}
2019-09-18 08:12:01,020 - training_jobs - DEBUG - training with: 
2019-09-18 08:12:01,020 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max/
2019-09-18 08:12:01,020 - training_jobs - DEBUG - GGNN5
2019-09-18 08:12:01,020 - training_jobs - DEBUG - 
2019-09-18 08:12:01,020 - training_jobs - DEBUG - ggnn training
2019-09-18 08:12:27,431 - training_jobs - DEBUG -  saving results to results/20190918_081227_ggnn_.json
2019-09-18 08:12:27,431 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 08:12:43,493 - training_jobs - ERROR - Error with trains/task_20.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1126, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'd5'
2019-09-18 08:12:46,094 - training_jobs - DEBUG - training trains/task_4.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 30,
 'd2': 30,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-18 08:12:46,107 - training_jobs - DEBUG - training with: 
2019-09-18 08:12:46,107 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max/
2019-09-18 08:12:46,107 - training_jobs - DEBUG - GGNN1
2019-09-18 08:12:46,107 - training_jobs - DEBUG - 
2019-09-18 08:12:46,107 - training_jobs - DEBUG - ggnn training
2019-09-18 08:13:02,921 - training_jobs - DEBUG -  saving results to results/20190918_081302_ggnn_.json
2019-09-18 08:13:02,921 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 08:13:22,390 - training_jobs - ERROR - Error with trains/task_4.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA error: device-side assert triggered
2019-09-18 08:13:26,142 - training_jobs - DEBUG - training trains/task_18.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-18 08:13:26,156 - training_jobs - DEBUG - training with: 
2019-09-18 08:13:26,156 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max/
2019-09-18 08:13:26,156 - training_jobs - DEBUG - GGNN5
2019-09-18 08:13:26,156 - training_jobs - DEBUG - 
2019-09-18 08:13:26,156 - training_jobs - DEBUG - ggnn training
2019-09-18 08:13:32,136 - training_jobs - DEBUG -  saving results to results/20190918_081332_ggnn_.json
2019-09-18 08:13:32,136 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 08:13:47,327 - training_jobs - ERROR - Error with trains/task_18.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1126, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'd5'
2019-09-18 08:13:49,897 - training_jobs - DEBUG - training trains/task_2.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 30,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-18 08:13:49,911 - training_jobs - DEBUG - training with: 
2019-09-18 08:13:49,911 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max/
2019-09-18 08:13:49,911 - training_jobs - DEBUG - GGNN1
2019-09-18 08:13:49,911 - training_jobs - DEBUG - 
2019-09-18 08:13:49,912 - training_jobs - DEBUG - ggnn training
2019-09-18 08:14:09,326 - training_jobs - DEBUG -  saving results to results/20190918_081409_ggnn_.json
2019-09-18 08:14:09,326 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 08:22:55,162 - training_jobs - ERROR - Error with trains/task_2.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 873, in train_model_GGNN
    loss = loss_func(out, target)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 210, in forward
    return F.nll_loss(input, target, weight=self.weight, ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1786, in nll_loss
    if input.size(0) != target.size(0):
RuntimeError: dimension specified as 0 but tensor has no dimensions
2019-09-18 08:22:58,644 - training_jobs - DEBUG - training trains/task_7.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 60,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-18 08:22:58,656 - training_jobs - DEBUG - training with: 
2019-09-18 08:22:58,657 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max/
2019-09-18 08:22:58,657 - training_jobs - DEBUG - GGNN1
2019-09-18 08:22:58,657 - training_jobs - DEBUG - 
2019-09-18 08:22:58,657 - training_jobs - DEBUG - ggnn training
2019-09-18 08:23:03,083 - training_jobs - DEBUG -  saving results to results/20190918_082303_ggnn_.json
2019-09-18 08:23:03,083 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 08:23:21,172 - training_jobs - ERROR - Error with trains/task_7.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: cublas runtime error : the GPU program failed to execute at /pytorch/aten/src/THC/THCBlas.cu:258
2019-09-18 08:23:24,978 - training_jobs - DEBUG - training trains/task_19.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-18 08:23:24,992 - training_jobs - DEBUG - training with: 
2019-09-18 08:23:24,992 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max/
2019-09-18 08:23:24,992 - training_jobs - DEBUG - GGNN5
2019-09-18 08:23:24,992 - training_jobs - DEBUG - 
2019-09-18 08:23:24,992 - training_jobs - DEBUG - ggnn training
2019-09-18 08:23:32,065 - training_jobs - DEBUG -  saving results to results/20190918_082332_ggnn_.json
2019-09-18 08:23:32,065 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 08:23:47,253 - training_jobs - ERROR - Error with trains/task_19.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1126, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'd5'
2019-09-18 08:23:50,140 - training_jobs - DEBUG - training trains/task_17.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-18 08:23:50,159 - training_jobs - DEBUG - training with: 
2019-09-18 08:23:50,159 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max/
2019-09-18 08:23:50,159 - training_jobs - DEBUG - GGNN6
2019-09-18 08:23:50,159 - training_jobs - DEBUG - 
2019-09-18 08:23:50,159 - training_jobs - DEBUG - ggnn training
2019-09-18 08:23:55,536 - training_jobs - DEBUG -  saving results to results/20190918_082355_ggnn_.json
2019-09-18 08:23:55,536 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 08:44:54,322 - training_jobs - ERROR - Error with trains/task_17.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 261, in forward
    x = F.relu(self.dense1_bn(self.fc1(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1619, in batch_norm
    raise ValueError('Expected more than 1 value per channel when training, got input size {}'.format(size))
ValueError: Expected more than 1 value per channel when training, got input size torch.Size([1, 20])
2019-09-18 08:44:56,779 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 30,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-18 08:44:56,791 - training_jobs - DEBUG - training with: 
2019-09-18 08:44:56,792 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max/
2019-09-18 08:44:56,792 - training_jobs - DEBUG - GGNN1
2019-09-18 08:44:56,792 - training_jobs - DEBUG - 
2019-09-18 08:44:56,792 - training_jobs - DEBUG - ggnn training
2019-09-18 08:45:02,338 - training_jobs - DEBUG -  saving results to results/20190918_084502_ggnn_.json
2019-09-18 08:45:02,338 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 10:12:53,884 - training_jobs - DEBUG - test_multiple_models
2019-09-18 10:12:53,884 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-18 10:13:58,468 - training_jobs - DEBUG - training time: 5336s
2019-09-18 10:13:58,468 - training_jobs - DEBUG - saving to results/20190918_084502_ggnn_.json
2019-09-18 10:13:58,563 - training_jobs - DEBUG - moved jobdict to done_trainings/task_0.yml
2019-09-18 10:13:58,563 - training_jobs - DEBUG - Finished!

2019-09-18 10:14:14,707 - training_jobs - DEBUG - training trains/task_11.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 60,
 'd2': 30,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-18 10:14:14,756 - training_jobs - DEBUG - training with: 
2019-09-18 10:14:14,756 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max/
2019-09-18 10:14:14,756 - training_jobs - DEBUG - GGNN1
2019-09-18 10:14:14,756 - training_jobs - DEBUG - 
2019-09-18 10:14:14,756 - training_jobs - DEBUG - ggnn training
2019-09-18 10:14:25,188 - training_jobs - DEBUG -  saving results to results/20190918_101425_ggnn_.json
2019-09-18 10:14:25,188 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 10:23:17,199 - training_jobs - ERROR - Error with trains/task_11.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 873, in train_model_GGNN
    loss = loss_func(out, target)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 210, in forward
    return F.nll_loss(input, target, weight=self.weight, ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1786, in nll_loss
    if input.size(0) != target.size(0):
RuntimeError: dimension specified as 0 but tensor has no dimensions
2019-09-18 10:23:19,676 - training_jobs - DEBUG - training trains/task_22.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-18 10:23:19,954 - training_jobs - DEBUG - training with: 
2019-09-18 10:23:19,954 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max/
2019-09-18 10:23:19,954 - training_jobs - DEBUG - GGNN5
2019-09-18 10:23:19,954 - training_jobs - DEBUG - 
2019-09-18 10:23:19,954 - training_jobs - DEBUG - ggnn training
2019-09-18 10:23:28,915 - training_jobs - DEBUG -  saving results to results/20190918_102328_ggnn_.json
2019-09-18 10:23:28,916 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 10:23:43,890 - training_jobs - ERROR - Error with trains/task_22.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1126, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'd5'
2019-09-18 10:23:46,579 - training_jobs - DEBUG - training trains/task_24.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-18 10:23:46,741 - training_jobs - DEBUG - training with: 
2019-09-18 10:23:46,741 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max/
2019-09-18 10:23:46,741 - training_jobs - DEBUG - GGNN5
2019-09-18 10:23:46,741 - training_jobs - DEBUG - 
2019-09-18 10:23:46,741 - training_jobs - DEBUG - ggnn training
2019-09-18 10:23:52,778 - training_jobs - DEBUG -  saving results to results/20190918_102352_ggnn_.json
2019-09-18 10:23:52,778 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 10:24:08,153 - training_jobs - ERROR - Error with trains/task_24.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1126, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'd5'
2019-09-18 10:24:10,612 - training_jobs - DEBUG - training trains/task_40.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-18 10:24:10,627 - training_jobs - DEBUG - training with: 
2019-09-18 10:24:10,627 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max/
2019-09-18 10:24:10,627 - training_jobs - DEBUG - GGNN5
2019-09-18 10:24:10,627 - training_jobs - DEBUG - 
2019-09-18 10:24:10,627 - training_jobs - DEBUG - ggnn training
2019-09-18 10:24:15,037 - training_jobs - DEBUG -  saving results to results/20190918_102415_ggnn_.json
2019-09-18 10:24:15,037 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 10:24:26,151 - training_jobs - ERROR - Error with trains/task_40.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1126, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'd5'
2019-09-18 10:24:28,608 - training_jobs - DEBUG - training trains/task_35.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '1e-4'}
2019-09-18 10:24:28,694 - training_jobs - DEBUG - training with: 
2019-09-18 10:24:28,695 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max/
2019-09-18 10:24:28,695 - training_jobs - DEBUG - GGNN5
2019-09-18 10:24:28,695 - training_jobs - DEBUG - 
2019-09-18 10:24:28,695 - training_jobs - DEBUG - ggnn training
2019-09-18 10:24:43,105 - training_jobs - DEBUG -  saving results to results/20190918_102443_ggnn_.json
2019-09-18 10:24:43,105 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 10:24:56,782 - training_jobs - ERROR - Error with trains/task_35.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1126, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'd5'
2019-09-18 10:24:59,306 - training_jobs - DEBUG - training trains/task_28.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-18 10:24:59,375 - training_jobs - DEBUG - training with: 
2019-09-18 10:24:59,376 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max/
2019-09-18 10:24:59,376 - training_jobs - DEBUG - GGNN5
2019-09-18 10:24:59,376 - training_jobs - DEBUG - 
2019-09-18 10:24:59,376 - training_jobs - DEBUG - ggnn training
2019-09-18 10:25:04,438 - training_jobs - DEBUG -  saving results to results/20190918_102504_ggnn_.json
2019-09-18 10:25:04,438 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 10:25:17,865 - training_jobs - ERROR - Error with trains/task_28.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1126, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'd5'
2019-09-18 10:25:20,230 - training_jobs - DEBUG - training trains/task_14.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-18 10:25:20,603 - training_jobs - DEBUG - training with: 
2019-09-18 10:25:20,603 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max/
2019-09-18 10:25:20,603 - training_jobs - DEBUG - GGNN6
2019-09-18 10:25:20,603 - training_jobs - DEBUG - 
2019-09-18 10:25:20,603 - training_jobs - DEBUG - ggnn training
2019-09-18 10:25:25,391 - training_jobs - DEBUG -  saving results to results/20190918_102525_ggnn_.json
2019-09-18 10:25:25,391 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 10:25:44,240 - training_jobs - ERROR - Error with trains/task_14.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA error: device-side assert triggered
2019-09-18 10:25:49,288 - training_jobs - DEBUG - training trains/task_23.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '1e-4'}
2019-09-18 10:25:49,334 - training_jobs - DEBUG - training with: 
2019-09-18 10:25:49,334 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max/
2019-09-18 10:25:49,334 - training_jobs - DEBUG - GGNN5
2019-09-18 10:25:49,334 - training_jobs - DEBUG - 
2019-09-18 10:25:49,334 - training_jobs - DEBUG - ggnn training
2019-09-18 10:25:57,823 - training_jobs - DEBUG -  saving results to results/20190918_102557_ggnn_.json
2019-09-18 10:25:57,824 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 10:26:14,967 - training_jobs - ERROR - Error with trains/task_23.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1126, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'd5'
2019-09-18 10:26:17,464 - training_jobs - DEBUG - training trains/task_30.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-18 10:26:17,521 - training_jobs - DEBUG - training with: 
2019-09-18 10:26:17,521 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max/
2019-09-18 10:26:17,521 - training_jobs - DEBUG - GGNN5
2019-09-18 10:26:17,522 - training_jobs - DEBUG - 
2019-09-18 10:26:17,522 - training_jobs - DEBUG - ggnn training
2019-09-18 10:26:22,607 - training_jobs - DEBUG -  saving results to results/20190918_102622_ggnn_.json
2019-09-18 10:26:22,607 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 10:26:36,210 - training_jobs - ERROR - Error with trains/task_30.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1126, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'd5'
2019-09-18 10:26:38,623 - training_jobs - DEBUG - training trains/task_3.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 30,
 'd2': 30,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-18 10:26:38,675 - training_jobs - DEBUG - training with: 
2019-09-18 10:26:38,675 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max/
2019-09-18 10:26:38,675 - training_jobs - DEBUG - GGNN1
2019-09-18 10:26:38,675 - training_jobs - DEBUG - 
2019-09-18 10:26:38,675 - training_jobs - DEBUG - ggnn training
2019-09-18 10:26:44,563 - training_jobs - DEBUG -  saving results to results/20190918_102644_ggnn_.json
2019-09-18 10:26:44,563 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 11:55:01,842 - training_jobs - DEBUG - test_multiple_models
2019-09-18 11:55:01,842 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-18 11:55:04,920 - training_jobs - DEBUG - training time: 5300s
2019-09-18 11:55:04,920 - training_jobs - DEBUG - saving to results/20190918_102644_ggnn_.json
2019-09-18 11:55:04,966 - training_jobs - DEBUG - moved jobdict to done_trainings/task_3.yml
2019-09-18 11:55:04,966 - training_jobs - DEBUG - Finished!

2019-09-18 11:55:08,870 - training_jobs - DEBUG - training trains/task_36.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-18 11:55:08,898 - training_jobs - DEBUG - training with: 
2019-09-18 11:55:08,898 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max/
2019-09-18 11:55:08,898 - training_jobs - DEBUG - GGNN5
2019-09-18 11:55:08,898 - training_jobs - DEBUG - 
2019-09-18 11:55:08,898 - training_jobs - DEBUG - ggnn training
2019-09-18 11:55:16,527 - training_jobs - DEBUG -  saving results to results/20190918_115516_ggnn_.json
2019-09-18 11:55:16,527 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 11:55:27,650 - training_jobs - ERROR - Error with trains/task_36.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1126, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'd5'
2019-09-18 11:55:30,489 - training_jobs - DEBUG - training trains/task_32.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '1e-4'}
2019-09-18 11:55:30,504 - training_jobs - DEBUG - training with: 
2019-09-18 11:55:30,504 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max/
2019-09-18 11:55:30,504 - training_jobs - DEBUG - GGNN5
2019-09-18 11:55:30,504 - training_jobs - DEBUG - 
2019-09-18 11:55:30,504 - training_jobs - DEBUG - ggnn training
2019-09-18 11:55:43,261 - training_jobs - DEBUG -  saving results to results/20190918_115543_ggnn_.json
2019-09-18 11:55:43,261 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 11:55:56,939 - training_jobs - ERROR - Error with trains/task_32.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1126, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'd5'
2019-09-18 11:55:59,566 - training_jobs - DEBUG - training trains/task_15.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-18 11:55:59,595 - training_jobs - DEBUG - training with: 
2019-09-18 11:55:59,595 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max/
2019-09-18 11:55:59,596 - training_jobs - DEBUG - GGNN6
2019-09-18 11:55:59,596 - training_jobs - DEBUG - 
2019-09-18 11:55:59,596 - training_jobs - DEBUG - ggnn training
2019-09-18 11:56:04,979 - training_jobs - DEBUG -  saving results to results/20190918_115604_ggnn_.json
2019-09-18 11:56:04,979 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 11:56:25,007 - training_jobs - ERROR - Error with trains/task_15.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA error: device-side assert triggered
2019-09-18 11:56:29,055 - training_jobs - DEBUG - training trains/task_41.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '1e-4'}
2019-09-18 11:56:29,093 - training_jobs - DEBUG - training with: 
2019-09-18 11:56:29,093 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max/
2019-09-18 11:56:29,093 - training_jobs - DEBUG - GGNN5
2019-09-18 11:56:29,093 - training_jobs - DEBUG - 
2019-09-18 11:56:29,093 - training_jobs - DEBUG - ggnn training
2019-09-18 11:56:33,663 - training_jobs - DEBUG -  saving results to results/20190918_115633_ggnn_.json
2019-09-18 11:56:33,663 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 11:56:45,356 - training_jobs - ERROR - Error with trains/task_41.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1126, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'd5'
2019-09-18 11:56:48,117 - training_jobs - DEBUG - training trains/task_33.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-18 11:56:48,181 - training_jobs - DEBUG - training with: 
2019-09-18 11:56:48,182 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max/
2019-09-18 11:56:48,182 - training_jobs - DEBUG - GGNN5
2019-09-18 11:56:48,182 - training_jobs - DEBUG - 
2019-09-18 11:56:48,182 - training_jobs - DEBUG - ggnn training
2019-09-18 11:56:53,547 - training_jobs - DEBUG -  saving results to results/20190918_115653_ggnn_.json
2019-09-18 11:56:53,548 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 11:57:07,330 - training_jobs - ERROR - Error with trains/task_33.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1126, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'd5'
2019-09-18 11:57:09,992 - training_jobs - DEBUG - training trains/task_12.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-18 11:57:10,023 - training_jobs - DEBUG - training with: 
2019-09-18 11:57:10,023 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max/
2019-09-18 11:57:10,024 - training_jobs - DEBUG - GGNN6
2019-09-18 11:57:10,024 - training_jobs - DEBUG - 
2019-09-18 11:57:10,024 - training_jobs - DEBUG - ggnn training
2019-09-18 11:57:24,257 - training_jobs - DEBUG -  saving results to results/20190918_115724_ggnn_.json
2019-09-18 11:57:24,257 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 13:53:08,426 - training_jobs - DEBUG - test_multiple_models
2019-09-18 13:53:08,427 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-18 13:53:11,118 - training_jobs - DEBUG - training time: 6947s
2019-09-18 13:53:11,118 - training_jobs - DEBUG - saving to results/20190918_115724_ggnn_.json
2019-09-18 13:53:11,145 - training_jobs - DEBUG - moved jobdict to done_trainings/task_12.yml
2019-09-18 13:53:11,145 - training_jobs - DEBUG - Finished!

2019-09-18 13:53:14,147 - training_jobs - DEBUG - training trains/task_10.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 60,
 'd2': 30,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-18 13:53:14,176 - training_jobs - DEBUG - training with: 
2019-09-18 13:53:14,177 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max/
2019-09-18 13:53:14,177 - training_jobs - DEBUG - GGNN1
2019-09-18 13:53:14,177 - training_jobs - DEBUG - 
2019-09-18 13:53:14,177 - training_jobs - DEBUG - ggnn training
2019-09-18 13:53:22,042 - training_jobs - DEBUG -  saving results to results/20190918_135322_ggnn_.json
2019-09-18 13:53:22,042 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 13:53:43,649 - training_jobs - ERROR - Error with trains/task_10.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/generic/THCTensorScatterGather.cu:67
2019-09-18 13:53:47,904 - training_jobs - DEBUG - training trains/task_9.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 60,
 'd2': 30,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-18 13:53:47,936 - training_jobs - DEBUG - training with: 
2019-09-18 13:53:47,936 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max/
2019-09-18 13:53:47,936 - training_jobs - DEBUG - GGNN1
2019-09-18 13:53:47,936 - training_jobs - DEBUG - 
2019-09-18 13:53:47,937 - training_jobs - DEBUG - ggnn training
2019-09-18 13:53:55,803 - training_jobs - DEBUG -  saving results to results/20190918_135355_ggnn_.json
2019-09-18 13:53:55,803 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 15:23:52,075 - training_jobs - DEBUG - test_multiple_models
2019-09-18 15:23:52,075 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-18 15:24:16,004 - training_jobs - ERROR - Error with trains/task_9.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 546, in training_dispatcher
    models_folder='models/gnn/')
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1809, in test_multiple_models
    testresult = testModel(bmodel, test_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1685, in testModel
    _, pred = model(data).max(dim=1)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 30, in forward
    x = self.ggnn(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 103, in forward
    h = self.rnn(m, h)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/rnn.py", line 800, in forward
    self.bias_ih, self.bias_hh,
RuntimeError: CUDA out of memory. Tried to allocate 716.38 MiB (GPU 0; 3.95 GiB total capacity; 2.32 GiB already allocated; 143.06 MiB free; 397.11 MiB cached)
2019-09-18 15:24:21,719 - training_jobs - DEBUG - training trains/task_34.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-18 15:24:21,794 - training_jobs - DEBUG - training with: 
2019-09-18 15:24:21,794 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max/
2019-09-18 15:24:21,794 - training_jobs - DEBUG - GGNN5
2019-09-18 15:24:21,794 - training_jobs - DEBUG - 
2019-09-18 15:24:21,794 - training_jobs - DEBUG - ggnn training
2019-09-18 15:24:31,178 - training_jobs - DEBUG -  saving results to results/20190918_152431_ggnn_.json
2019-09-18 15:24:31,178 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 15:24:45,391 - training_jobs - ERROR - Error with trains/task_34.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1126, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'd5'
2019-09-18 15:24:48,007 - training_jobs - DEBUG - training trains/task_8.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 60,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-18 15:24:48,044 - training_jobs - DEBUG - training with: 
2019-09-18 15:24:48,044 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max/
2019-09-18 15:24:48,044 - training_jobs - DEBUG - GGNN1
2019-09-18 15:24:48,044 - training_jobs - DEBUG - 
2019-09-18 15:24:48,044 - training_jobs - DEBUG - ggnn training
2019-09-18 15:25:04,285 - training_jobs - DEBUG -  saving results to results/20190918_152504_ggnn_.json
2019-09-18 15:25:04,286 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 15:34:06,389 - training_jobs - ERROR - Error with trains/task_8.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 873, in train_model_GGNN
    loss = loss_func(out, target)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 210, in forward
    return F.nll_loss(input, target, weight=self.weight, ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1786, in nll_loss
    if input.size(0) != target.size(0):
RuntimeError: dimension specified as 0 but tensor has no dimensions
2019-09-18 15:34:09,021 - training_jobs - DEBUG - training trains/task_39.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-18 15:34:09,053 - training_jobs - DEBUG - training with: 
2019-09-18 15:34:09,054 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max/
2019-09-18 15:34:09,054 - training_jobs - DEBUG - GGNN5
2019-09-18 15:34:09,054 - training_jobs - DEBUG - 
2019-09-18 15:34:09,054 - training_jobs - DEBUG - ggnn training
2019-09-18 15:34:13,613 - training_jobs - DEBUG -  saving results to results/20190918_153413_ggnn_.json
2019-09-18 15:34:13,613 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 15:34:25,679 - training_jobs - ERROR - Error with trains/task_39.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1126, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'd5'
2019-09-18 15:34:28,357 - training_jobs - DEBUG - training trains/task_6.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 60,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-18 15:34:28,543 - training_jobs - DEBUG - training with: 
2019-09-18 15:34:28,543 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max/
2019-09-18 15:34:28,543 - training_jobs - DEBUG - GGNN1
2019-09-18 15:34:28,543 - training_jobs - DEBUG - 
2019-09-18 15:34:28,544 - training_jobs - DEBUG - ggnn training
2019-09-18 15:34:37,440 - training_jobs - DEBUG -  saving results to results/20190918_153437_ggnn_.json
2019-09-18 15:34:37,440 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 17:05:13,250 - training_jobs - DEBUG - test_multiple_models
2019-09-18 17:05:13,251 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-18 17:05:15,366 - training_jobs - ERROR - Error with trains/task_6.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 546, in training_dispatcher
    models_folder='models/gnn/')
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1809, in test_multiple_models
    testresult = testModel(bmodel, test_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1685, in testModel
    _, pred = model(data).max(dim=1)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 30, in forward
    x = self.ggnn(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 103, in forward
    h = self.rnn(m, h)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/rnn.py", line 800, in forward
    self.bias_ih, self.bias_hh,
RuntimeError: CUDA out of memory. Tried to allocate 716.38 MiB (GPU 0; 3.95 GiB total capacity; 2.32 GiB already allocated; 140.81 MiB free; 397.01 MiB cached)
2019-09-18 17:05:18,912 - training_jobs - DEBUG - training trains/task_31.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-18 17:05:18,937 - training_jobs - DEBUG - training with: 
2019-09-18 17:05:18,937 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max/
2019-09-18 17:05:18,937 - training_jobs - DEBUG - GGNN5
2019-09-18 17:05:18,937 - training_jobs - DEBUG - 
2019-09-18 17:05:18,937 - training_jobs - DEBUG - ggnn training
2019-09-18 17:05:27,779 - training_jobs - DEBUG -  saving results to results/20190918_170527_ggnn_.json
2019-09-18 17:05:27,779 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 17:05:40,783 - training_jobs - ERROR - Error with trains/task_31.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1126, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'd5'
2019-09-18 17:05:43,577 - training_jobs - DEBUG - training trains/task_42.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-18 17:05:43,611 - training_jobs - DEBUG - training with: 
2019-09-18 17:05:43,611 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max/
2019-09-18 17:05:43,611 - training_jobs - DEBUG - GGNN5
2019-09-18 17:05:43,611 - training_jobs - DEBUG - 
2019-09-18 17:05:43,612 - training_jobs - DEBUG - ggnn training
2019-09-18 17:05:53,888 - training_jobs - DEBUG -  saving results to results/20190918_170553_ggnn_.json
2019-09-18 17:05:53,888 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 17:06:05,718 - training_jobs - ERROR - Error with trains/task_42.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1126, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'd5'
2019-09-18 17:06:08,376 - training_jobs - DEBUG - training trains/task_25.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-18 17:06:08,484 - training_jobs - DEBUG - training with: 
2019-09-18 17:06:08,484 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max/
2019-09-18 17:06:08,484 - training_jobs - DEBUG - GGNN5
2019-09-18 17:06:08,484 - training_jobs - DEBUG - 
2019-09-18 17:06:08,484 - training_jobs - DEBUG - ggnn training
2019-09-18 17:06:14,461 - training_jobs - DEBUG -  saving results to results/20190918_170614_ggnn_.json
2019-09-18 17:06:14,461 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 17:06:30,359 - training_jobs - ERROR - Error with trains/task_25.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1126, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'd5'
2019-09-18 17:06:33,186 - training_jobs - DEBUG - training trains/task_26.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '1e-4'}
2019-09-18 17:06:33,202 - training_jobs - DEBUG - training with: 
2019-09-18 17:06:33,202 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max/
2019-09-18 17:06:33,202 - training_jobs - DEBUG - GGNN5
2019-09-18 17:06:33,202 - training_jobs - DEBUG - 
2019-09-18 17:06:33,202 - training_jobs - DEBUG - ggnn training
2019-09-18 17:06:39,251 - training_jobs - DEBUG -  saving results to results/20190918_170639_ggnn_.json
2019-09-18 17:06:39,251 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 17:06:55,453 - training_jobs - ERROR - Error with trains/task_26.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1126, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'd5'
2019-09-18 17:06:57,950 - training_jobs - DEBUG - training trains/task_5.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 30,
 'd2': 30,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-18 17:06:57,987 - training_jobs - DEBUG - training with: 
2019-09-18 17:06:57,988 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max/
2019-09-18 17:06:57,988 - training_jobs - DEBUG - GGNN1
2019-09-18 17:06:57,988 - training_jobs - DEBUG - 
2019-09-18 17:06:57,988 - training_jobs - DEBUG - ggnn training
2019-09-18 17:07:02,718 - training_jobs - DEBUG -  saving results to results/20190918_170702_ggnn_.json
2019-09-18 17:07:02,718 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 17:15:51,791 - training_jobs - ERROR - Error with trains/task_5.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 873, in train_model_GGNN
    loss = loss_func(out, target)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 210, in forward
    return F.nll_loss(input, target, weight=self.weight, ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1786, in nll_loss
    if input.size(0) != target.size(0):
RuntimeError: dimension specified as 0 but tensor has no dimensions
2019-09-18 17:15:54,376 - training_jobs - DEBUG - training trains/task_1.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 30,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-18 17:15:54,440 - training_jobs - DEBUG - training with: 
2019-09-18 17:15:54,441 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max/
2019-09-18 17:15:54,441 - training_jobs - DEBUG - GGNN1
2019-09-18 17:15:54,441 - training_jobs - DEBUG - 
2019-09-18 17:15:54,441 - training_jobs - DEBUG - ggnn training
2019-09-18 17:15:59,510 - training_jobs - DEBUG -  saving results to results/20190918_171559_ggnn_.json
2019-09-18 17:15:59,510 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 17:16:18,377 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: cublas runtime error : the GPU program failed to execute at /pytorch/aten/src/THC/THCBlas.cu:258
2019-09-18 17:16:22,298 - training_jobs - DEBUG - training trains/task_27.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-18 17:16:22,322 - training_jobs - DEBUG - training with: 
2019-09-18 17:16:22,322 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max/
2019-09-18 17:16:22,322 - training_jobs - DEBUG - GGNN5
2019-09-18 17:16:22,322 - training_jobs - DEBUG - 
2019-09-18 17:16:22,322 - training_jobs - DEBUG - ggnn training
2019-09-18 17:16:27,426 - training_jobs - DEBUG -  saving results to results/20190918_171627_ggnn_.json
2019-09-18 17:16:27,426 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 17:16:40,639 - training_jobs - ERROR - Error with trains/task_27.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1126, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'd5'
2019-09-18 17:16:43,165 - training_jobs - DEBUG - training trains/task_44.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '1e-4'}
2019-09-18 17:16:43,411 - training_jobs - DEBUG - training with: 
2019-09-18 17:16:43,411 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max/
2019-09-18 17:16:43,411 - training_jobs - DEBUG - GGNN5
2019-09-18 17:16:43,411 - training_jobs - DEBUG - 
2019-09-18 17:16:43,411 - training_jobs - DEBUG - ggnn training
2019-09-18 17:16:47,889 - training_jobs - DEBUG -  saving results to results/20190918_171647_ggnn_.json
2019-09-18 17:16:47,889 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 17:17:00,154 - training_jobs - ERROR - Error with trains/task_44.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1126, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'd5'
2019-09-18 17:17:02,706 - training_jobs - DEBUG - training trains/task_38.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '1e-4'}
2019-09-18 17:17:02,735 - training_jobs - DEBUG - training with: 
2019-09-18 17:17:02,735 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max/
2019-09-18 17:17:02,735 - training_jobs - DEBUG - GGNN5
2019-09-18 17:17:02,735 - training_jobs - DEBUG - 
2019-09-18 17:17:02,735 - training_jobs - DEBUG - ggnn training
2019-09-18 17:17:07,291 - training_jobs - DEBUG -  saving results to results/20190918_171707_ggnn_.json
2019-09-18 17:17:07,291 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 17:17:18,786 - training_jobs - ERROR - Error with trains/task_38.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1126, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'd5'
2019-09-18 17:17:21,478 - training_jobs - DEBUG - training trains/task_16.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-18 17:17:21,493 - training_jobs - DEBUG - training with: 
2019-09-18 17:17:21,493 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max/
2019-09-18 17:17:21,493 - training_jobs - DEBUG - GGNN6
2019-09-18 17:17:21,493 - training_jobs - DEBUG - 
2019-09-18 17:17:21,493 - training_jobs - DEBUG - ggnn training
2019-09-18 17:17:26,048 - training_jobs - DEBUG -  saving results to results/20190918_171726_ggnn_.json
2019-09-18 17:17:26,049 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 17:28:14,032 - training_jobs - ERROR - Error with trains/task_16.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 261, in forward
    x = F.relu(self.dense1_bn(self.fc1(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1619, in batch_norm
    raise ValueError('Expected more than 1 value per channel when training, got input size {}'.format(size))
ValueError: Expected more than 1 value per channel when training, got input size torch.Size([1, 20])
2019-09-18 17:28:16,520 - training_jobs - DEBUG - training trains/task_21.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-18 17:28:16,829 - training_jobs - DEBUG - training with: 
2019-09-18 17:28:16,829 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max/
2019-09-18 17:28:16,829 - training_jobs - DEBUG - GGNN5
2019-09-18 17:28:16,829 - training_jobs - DEBUG - 
2019-09-18 17:28:16,829 - training_jobs - DEBUG - ggnn training
2019-09-18 17:28:22,141 - training_jobs - DEBUG -  saving results to results/20190918_172822_ggnn_.json
2019-09-18 17:28:22,141 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 17:28:39,337 - training_jobs - ERROR - Error with trains/task_21.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1126, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'd5'
2019-09-18 17:28:42,924 - training_jobs - DEBUG - training trains/task_29.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '1e-4'}
2019-09-18 17:28:42,970 - training_jobs - DEBUG - training with: 
2019-09-18 17:28:42,971 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max/
2019-09-18 17:28:42,971 - training_jobs - DEBUG - GGNN5
2019-09-18 17:28:42,971 - training_jobs - DEBUG - 
2019-09-18 17:28:42,971 - training_jobs - DEBUG - ggnn training
2019-09-18 17:28:49,764 - training_jobs - DEBUG -  saving results to results/20190918_172849_ggnn_.json
2019-09-18 17:28:49,764 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 17:29:04,196 - training_jobs - ERROR - Error with trains/task_29.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1126, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'd5'
2019-09-18 17:29:06,680 - training_jobs - DEBUG - training trains/task_43.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-18 17:29:06,693 - training_jobs - DEBUG - training with: 
2019-09-18 17:29:06,693 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max/
2019-09-18 17:29:06,693 - training_jobs - DEBUG - GGNN5
2019-09-18 17:29:06,694 - training_jobs - DEBUG - 
2019-09-18 17:29:06,694 - training_jobs - DEBUG - ggnn training
2019-09-18 17:29:11,025 - training_jobs - DEBUG -  saving results to results/20190918_172911_ggnn_.json
2019-09-18 17:29:11,025 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 17:29:22,441 - training_jobs - ERROR - Error with trains/task_43.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1126, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'd5'
2019-09-18 17:29:25,113 - training_jobs - DEBUG - training trains/task_37.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-18 17:29:25,130 - training_jobs - DEBUG - training with: 
2019-09-18 17:29:25,130 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max/
2019-09-18 17:29:25,130 - training_jobs - DEBUG - GGNN5
2019-09-18 17:29:25,130 - training_jobs - DEBUG - 
2019-09-18 17:29:25,130 - training_jobs - DEBUG - ggnn training
2019-09-18 17:29:29,811 - training_jobs - DEBUG -  saving results to results/20190918_172929_ggnn_.json
2019-09-18 17:29:29,811 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 17:29:41,958 - training_jobs - ERROR - Error with trains/task_37.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1126, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'd5'
2019-09-18 17:29:44,627 - training_jobs - DEBUG - training trains/task_13.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN6',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-18 17:29:44,689 - training_jobs - DEBUG - training with: 
2019-09-18 17:29:44,689 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max/
2019-09-18 17:29:44,690 - training_jobs - DEBUG - GGNN6
2019-09-18 17:29:44,690 - training_jobs - DEBUG - 
2019-09-18 17:29:44,690 - training_jobs - DEBUG - ggnn training
2019-09-18 17:29:50,869 - training_jobs - DEBUG -  saving results to results/20190918_172950_ggnn_.json
2019-09-18 17:29:50,869 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 21:21:27,315 - training_jobs - DEBUG - test_multiple_models
2019-09-18 21:21:27,315 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-18 21:21:30,142 - training_jobs - DEBUG - training time: 13899s
2019-09-18 21:21:30,142 - training_jobs - DEBUG - saving to results/20190918_172950_ggnn_.json
2019-09-18 21:21:30,145 - training_jobs - DEBUG - moved jobdict to done_trainings/task_13.yml
2019-09-18 21:21:30,145 - training_jobs - DEBUG - Finished!

2019-09-18 22:07:10,752 - training_jobs - DEBUG - ('tasks/gnn_ggnn5_extensive_search', '.yaml')
2019-09-18 22:07:10,755 - training_jobs - DEBUG - ('tasks/gnn_ggnn5_extensive_search2', '.yaml')
2019-09-18 22:07:13,278 - training_jobs - DEBUG - training trains/task_20.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 40,
 'd2': 20,
 'd3': 10,
 'd4': 10,
 'd5': 5,
 'dataset': './tmp/symbols_dataset_3_precomp_split_undersample_max/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 4,
 'weight_decay': '1e-4'}
2019-09-18 22:07:13,298 - training_jobs - DEBUG - training with: 
2019-09-18 22:07:13,298 - training_jobs - DEBUG - ./tmp/symbols_dataset_3_precomp_split_undersample_max/
2019-09-18 22:07:13,298 - training_jobs - DEBUG - GGNN5
2019-09-18 22:07:13,298 - training_jobs - DEBUG - 
2019-09-18 22:07:13,299 - training_jobs - DEBUG - ggnn training
2019-09-18 22:07:26,550 - training_jobs - DEBUG -  saving results to results/20190918_220726_ggnn_.json
2019-09-18 22:07:26,550 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 22:07:41,632 - training_jobs - ERROR - Error with trains/task_20.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1126, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'd5'
2019-09-18 22:07:44,028 - training_jobs - DEBUG - training trains/task_4.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 30,
 'd2': 30,
 'dataset': './tmp/symbols_dataset_2_precomp_split_undersample_max/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.001,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-18 22:07:44,063 - training_jobs - DEBUG - training with: 
2019-09-18 22:07:44,063 - training_jobs - DEBUG - ./tmp/symbols_dataset_2_precomp_split_undersample_max/
2019-09-18 22:07:44,063 - training_jobs - DEBUG - GGNN1
2019-09-18 22:07:44,063 - training_jobs - DEBUG - 
2019-09-18 22:07:44,064 - training_jobs - DEBUG - ggnn training
2019-09-18 22:07:55,942 - training_jobs - DEBUG -  saving results to results/20190918_220755_ggnn_.json
2019-09-18 22:07:55,942 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 22:08:11,695 - training_jobs - ERROR - Error with trains/task_4.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 875, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 102, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 90, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: cublas runtime error : the GPU program failed to execute at /pytorch/aten/src/THC/THCBlas.cu:258
2019-09-18 22:08:35,041 - training_jobs - DEBUG - ('tasks/gnn_ggnn5_extensive_search', '.yaml')
2019-09-18 22:08:35,044 - training_jobs - DEBUG - ('tasks/gnn_ggnn5_extensive_search2', '.yaml')
2019-09-18 22:08:53,331 - training_jobs - DEBUG - ('tasks/gnn_ggnn5_extensive_search', '.yaml')
2019-09-18 22:08:53,334 - training_jobs - DEBUG - ('tasks/gnn_ggnn5_extensive_search2', '.yaml')
2019-09-18 22:09:24,307 - training_jobs - DEBUG - ('tasks/gnn_ggnn5_extensive_search', '.yaml')
2019-09-18 22:09:24,317 - training_jobs - DEBUG - ('tasks/gnn_ggnn5_extensive_search2', '.yaml')
2019-09-18 22:09:26,708 - training_jobs - DEBUG - training trains/task_4.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 20,
 'd2': 10,
 'd3': 10,
 'd4': 8,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.0001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '2e-4'}
2019-09-18 22:09:26,720 - training_jobs - DEBUG - training with: 
2019-09-18 22:09:26,720 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-18 22:09:26,720 - training_jobs - DEBUG - GGNN5
2019-09-18 22:09:26,720 - training_jobs - DEBUG - 
2019-09-18 22:09:26,720 - training_jobs - DEBUG - ggnn training
2019-09-18 22:09:30,649 - training_jobs - DEBUG -  saving results to results/20190918_220930_ggnn_.json
2019-09-18 22:09:30,649 - training_jobs - DEBUG -  calling modelSelection
2019-09-18 23:12:24,457 - training_jobs - DEBUG - ('tasks/gnn_ggnn5_extensive_search', '.yaml')
2019-09-18 23:12:24,480 - training_jobs - DEBUG - ('tasks/gnn_ggnn5_extensive_search2', '.yaml')
2019-09-18 23:12:27,337 - training_jobs - DEBUG - training trains/task_4.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 20,
 'd2': 10,
 'd3': 10,
 'd4': 8,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.0001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '2e-4'}
2019-09-18 23:12:27,351 - training_jobs - DEBUG - training with: 
2019-09-18 23:12:27,351 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-18 23:12:27,351 - training_jobs - DEBUG - GGNN5
2019-09-18 23:12:27,351 - training_jobs - DEBUG - 
2019-09-18 23:12:27,351 - training_jobs - DEBUG - ggnn training
2019-09-18 23:12:29,901 - training_jobs - DEBUG -  saving results to results/20190918_231229_ggnn_.json
2019-09-18 23:12:29,901 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 00:15:10,298 - training_jobs - DEBUG - test_multiple_models
2019-09-19 00:15:10,298 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-19 00:15:16,172 - training_jobs - DEBUG - training time: 3766s
2019-09-19 00:15:16,172 - training_jobs - DEBUG - saving to results/20190918_231229_ggnn_.json
2019-09-19 00:15:16,177 - training_jobs - DEBUG - moved jobdict to done_trainings/task_4.yml
2019-09-19 00:15:16,177 - training_jobs - DEBUG - Finished!

2019-09-19 00:15:20,060 - training_jobs - DEBUG - training trains/task_2.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 20,
 'd2': 10,
 'd3': 10,
 'd4': 8,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 1e-05,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '2e-4'}
2019-09-19 00:15:20,079 - training_jobs - DEBUG - training with: 
2019-09-19 00:15:20,080 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-19 00:15:20,080 - training_jobs - DEBUG - GGNN5
2019-09-19 00:15:20,080 - training_jobs - DEBUG - 
2019-09-19 00:15:20,080 - training_jobs - DEBUG - ggnn training
2019-09-19 00:15:21,824 - training_jobs - DEBUG -  saving results to results/20190919_001521_ggnn_.json
2019-09-19 00:15:21,824 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 00:32:20,207 - training_jobs - DEBUG - test_multiple_models
2019-09-19 00:32:20,207 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-19 00:32:20,703 - training_jobs - DEBUG - training time: 1019s
2019-09-19 00:32:20,703 - training_jobs - DEBUG - saving to results/20190919_001521_ggnn_.json
2019-09-19 00:32:20,705 - training_jobs - DEBUG - moved jobdict to done_trainings/task_2.yml
2019-09-19 00:32:20,705 - training_jobs - DEBUG - Finished!

2019-09-19 00:32:23,578 - training_jobs - DEBUG - training trains/task_7.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 10,
 'd2': 5,
 'd3': 5,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.0001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '2e-4'}
2019-09-19 00:32:23,593 - training_jobs - DEBUG - training with: 
2019-09-19 00:32:23,593 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-19 00:32:23,594 - training_jobs - DEBUG - GGNN5
2019-09-19 00:32:23,594 - training_jobs - DEBUG - 
2019-09-19 00:32:23,594 - training_jobs - DEBUG - ggnn training
2019-09-19 00:32:25,052 - training_jobs - DEBUG -  saving results to results/20190919_003225_ggnn_.json
2019-09-19 00:32:25,052 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 00:49:01,872 - training_jobs - DEBUG - test_multiple_models
2019-09-19 00:49:01,872 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-19 00:49:02,338 - training_jobs - DEBUG - training time: 997s
2019-09-19 00:49:02,338 - training_jobs - DEBUG - saving to results/20190919_003225_ggnn_.json
2019-09-19 00:49:02,340 - training_jobs - DEBUG - moved jobdict to done_trainings/task_7.yml
2019-09-19 00:49:02,340 - training_jobs - DEBUG - Finished!

2019-09-19 00:49:05,005 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 20,
 'd2': 10,
 'd3': 10,
 'd4': 8,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.1,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '2e-4'}
2019-09-19 00:49:05,020 - training_jobs - DEBUG - training with: 
2019-09-19 00:49:05,020 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-19 00:49:05,020 - training_jobs - DEBUG - GGNN5
2019-09-19 00:49:05,020 - training_jobs - DEBUG - 
2019-09-19 00:49:05,020 - training_jobs - DEBUG - ggnn training
2019-09-19 00:49:06,476 - training_jobs - DEBUG -  saving results to results/20190919_004906_ggnn_.json
2019-09-19 00:49:06,476 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 01:03:54,306 - training_jobs - DEBUG - test_multiple_models
2019-09-19 01:03:54,306 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-19 01:03:54,766 - training_jobs - DEBUG - training time: 888s
2019-09-19 01:03:54,766 - training_jobs - DEBUG - saving to results/20190919_004906_ggnn_.json
2019-09-19 01:03:54,768 - training_jobs - DEBUG - moved jobdict to done_trainings/task_0.yml
2019-09-19 01:03:54,768 - training_jobs - DEBUG - Finished!

2019-09-19 01:03:57,487 - training_jobs - DEBUG - training trains/task_11.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 10,
 'd2': 5,
 'd3': 5,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 80,
 'features': '',
 'learning_rate': 1e-05,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '2e-4'}
2019-09-19 01:03:57,501 - training_jobs - DEBUG - training with: 
2019-09-19 01:03:57,502 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-19 01:03:57,502 - training_jobs - DEBUG - GGNN5
2019-09-19 01:03:57,502 - training_jobs - DEBUG - 
2019-09-19 01:03:57,502 - training_jobs - DEBUG - ggnn training
2019-09-19 01:03:58,850 - training_jobs - DEBUG -  saving results to results/20190919_010358_ggnn_.json
2019-09-19 01:03:58,850 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 01:26:58,397 - training_jobs - DEBUG - test_multiple_models
2019-09-19 01:26:58,398 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-19 01:26:58,834 - training_jobs - DEBUG - training time: 1380s
2019-09-19 01:26:58,834 - training_jobs - DEBUG - saving to results/20190919_010358_ggnn_.json
2019-09-19 01:26:58,836 - training_jobs - DEBUG - moved jobdict to done_trainings/task_11.yml
2019-09-19 01:26:58,836 - training_jobs - DEBUG - Finished!

2019-09-19 01:27:01,214 - training_jobs - DEBUG - training trains/task_14.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 10,
 'd2': 5,
 'd3': 5,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 1e-05,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '2e-4'}
2019-09-19 01:27:01,225 - training_jobs - DEBUG - training with: 
2019-09-19 01:27:01,226 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-19 01:27:01,226 - training_jobs - DEBUG - GGNN5
2019-09-19 01:27:01,226 - training_jobs - DEBUG - 
2019-09-19 01:27:01,226 - training_jobs - DEBUG - ggnn training
2019-09-19 01:27:02,373 - training_jobs - DEBUG -  saving results to results/20190919_012702_ggnn_.json
2019-09-19 01:27:02,373 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 01:55:47,150 - training_jobs - DEBUG - test_multiple_models
2019-09-19 01:55:47,150 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-19 01:55:47,592 - training_jobs - DEBUG - training time: 1725s
2019-09-19 01:55:47,592 - training_jobs - DEBUG - saving to results/20190919_012702_ggnn_.json
2019-09-19 01:55:47,594 - training_jobs - DEBUG - moved jobdict to done_trainings/task_14.yml
2019-09-19 01:55:47,594 - training_jobs - DEBUG - Finished!

2019-09-19 01:55:50,048 - training_jobs - DEBUG - training trains/task_3.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 20,
 'd2': 10,
 'd3': 10,
 'd4': 8,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.1,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '2e-4'}
2019-09-19 01:55:50,061 - training_jobs - DEBUG - training with: 
2019-09-19 01:55:50,061 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-19 01:55:50,061 - training_jobs - DEBUG - GGNN5
2019-09-19 01:55:50,061 - training_jobs - DEBUG - 
2019-09-19 01:55:50,061 - training_jobs - DEBUG - ggnn training
2019-09-19 01:55:51,312 - training_jobs - DEBUG -  saving results to results/20190919_015551_ggnn_.json
2019-09-19 01:55:51,312 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 02:53:03,479 - training_jobs - DEBUG - test_multiple_models
2019-09-19 02:53:03,479 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-19 02:53:03,928 - training_jobs - DEBUG - training time: 3433s
2019-09-19 02:53:03,928 - training_jobs - DEBUG - saving to results/20190919_015551_ggnn_.json
2019-09-19 02:53:03,930 - training_jobs - DEBUG - moved jobdict to done_trainings/task_3.yml
2019-09-19 02:53:03,930 - training_jobs - DEBUG - Finished!

2019-09-19 02:53:06,294 - training_jobs - DEBUG - training trains/task_12.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 10,
 'd2': 5,
 'd3': 5,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.1,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '2e-4'}
2019-09-19 02:53:06,307 - training_jobs - DEBUG - training with: 
2019-09-19 02:53:06,307 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-19 02:53:06,307 - training_jobs - DEBUG - GGNN5
2019-09-19 02:53:06,307 - training_jobs - DEBUG - 
2019-09-19 02:53:06,307 - training_jobs - DEBUG - ggnn training
2019-09-19 02:53:07,484 - training_jobs - DEBUG -  saving results to results/20190919_025307_ggnn_.json
2019-09-19 02:53:07,485 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 03:21:50,666 - training_jobs - DEBUG - test_multiple_models
2019-09-19 03:21:50,666 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-19 03:21:51,111 - training_jobs - DEBUG - training time: 1724s
2019-09-19 03:21:51,112 - training_jobs - DEBUG - saving to results/20190919_025307_ggnn_.json
2019-09-19 03:21:51,114 - training_jobs - DEBUG - moved jobdict to done_trainings/task_12.yml
2019-09-19 03:21:51,114 - training_jobs - DEBUG - Finished!

2019-09-19 03:21:53,467 - training_jobs - DEBUG - training trains/task_10.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 10,
 'd2': 5,
 'd3': 5,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 80,
 'features': '',
 'learning_rate': 0.0001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '2e-4'}
2019-09-19 03:21:53,479 - training_jobs - DEBUG - training with: 
2019-09-19 03:21:53,479 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-19 03:21:53,480 - training_jobs - DEBUG - GGNN5
2019-09-19 03:21:53,480 - training_jobs - DEBUG - 
2019-09-19 03:21:53,480 - training_jobs - DEBUG - ggnn training
2019-09-19 03:21:54,632 - training_jobs - DEBUG -  saving results to results/20190919_032154_ggnn_.json
2019-09-19 03:21:54,632 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 03:45:18,466 - training_jobs - DEBUG - test_multiple_models
2019-09-19 03:45:18,466 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-19 03:45:19,075 - training_jobs - DEBUG - training time: 1404s
2019-09-19 03:45:19,075 - training_jobs - DEBUG - saving to results/20190919_032154_ggnn_.json
2019-09-19 03:45:19,078 - training_jobs - DEBUG - moved jobdict to done_trainings/task_10.yml
2019-09-19 03:45:19,078 - training_jobs - DEBUG - Finished!

2019-09-19 03:45:22,880 - training_jobs - DEBUG - training trains/task_9.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 10,
 'd2': 5,
 'd3': 5,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 80,
 'features': '',
 'learning_rate': 0.1,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '2e-4'}
2019-09-19 03:45:22,899 - training_jobs - DEBUG - training with: 
2019-09-19 03:45:22,899 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-19 03:45:22,900 - training_jobs - DEBUG - GGNN5
2019-09-19 03:45:22,900 - training_jobs - DEBUG - 
2019-09-19 03:45:22,900 - training_jobs - DEBUG - ggnn training
2019-09-19 03:45:24,794 - training_jobs - DEBUG -  saving results to results/20190919_034524_ggnn_.json
2019-09-19 03:45:24,794 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 04:08:29,114 - training_jobs - DEBUG - test_multiple_models
2019-09-19 04:08:29,114 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-19 04:08:29,555 - training_jobs - DEBUG - training time: 1385s
2019-09-19 04:08:29,555 - training_jobs - DEBUG - saving to results/20190919_034524_ggnn_.json
2019-09-19 04:08:29,587 - training_jobs - DEBUG - moved jobdict to done_trainings/task_9.yml
2019-09-19 04:08:29,587 - training_jobs - DEBUG - Finished!

2019-09-19 04:08:31,975 - training_jobs - DEBUG - training trains/task_8.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 10,
 'd2': 5,
 'd3': 5,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 1e-05,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '2e-4'}
2019-09-19 04:08:31,988 - training_jobs - DEBUG - training with: 
2019-09-19 04:08:31,988 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-19 04:08:31,988 - training_jobs - DEBUG - GGNN5
2019-09-19 04:08:31,988 - training_jobs - DEBUG - 
2019-09-19 04:08:31,988 - training_jobs - DEBUG - ggnn training
2019-09-19 04:08:33,134 - training_jobs - DEBUG -  saving results to results/20190919_040833_ggnn_.json
2019-09-19 04:08:33,134 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 04:22:56,197 - training_jobs - DEBUG - test_multiple_models
2019-09-19 04:22:56,197 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-19 04:22:56,636 - training_jobs - DEBUG - training time: 864s
2019-09-19 04:22:56,636 - training_jobs - DEBUG - saving to results/20190919_040833_ggnn_.json
2019-09-19 04:22:56,638 - training_jobs - DEBUG - moved jobdict to done_trainings/task_8.yml
2019-09-19 04:22:56,638 - training_jobs - DEBUG - Finished!

2019-09-19 04:22:59,052 - training_jobs - DEBUG - training trains/task_6.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 10,
 'd2': 5,
 'd3': 5,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.1,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '2e-4'}
2019-09-19 04:22:59,065 - training_jobs - DEBUG - training with: 
2019-09-19 04:22:59,065 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-19 04:22:59,065 - training_jobs - DEBUG - GGNN5
2019-09-19 04:22:59,065 - training_jobs - DEBUG - 
2019-09-19 04:22:59,065 - training_jobs - DEBUG - ggnn training
2019-09-19 04:23:00,223 - training_jobs - DEBUG -  saving results to results/20190919_042300_ggnn_.json
2019-09-19 04:23:00,223 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 04:37:29,326 - training_jobs - DEBUG - test_multiple_models
2019-09-19 04:37:29,326 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-19 04:37:29,769 - training_jobs - DEBUG - training time: 870s
2019-09-19 04:37:29,769 - training_jobs - DEBUG - saving to results/20190919_042300_ggnn_.json
2019-09-19 04:37:29,771 - training_jobs - DEBUG - moved jobdict to done_trainings/task_6.yml
2019-09-19 04:37:29,771 - training_jobs - DEBUG - Finished!

2019-09-19 04:37:32,149 - training_jobs - DEBUG - training trains/task_5.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 20,
 'd2': 10,
 'd3': 10,
 'd4': 8,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 1e-05,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '2e-4'}
2019-09-19 04:37:32,162 - training_jobs - DEBUG - training with: 
2019-09-19 04:37:32,162 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-19 04:37:32,162 - training_jobs - DEBUG - GGNN5
2019-09-19 04:37:32,162 - training_jobs - DEBUG - 
2019-09-19 04:37:32,162 - training_jobs - DEBUG - ggnn training
2019-09-19 04:37:33,317 - training_jobs - DEBUG -  saving results to results/20190919_043733_ggnn_.json
2019-09-19 04:37:33,317 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 05:34:35,572 - training_jobs - DEBUG - test_multiple_models
2019-09-19 05:34:35,572 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-19 05:34:36,016 - training_jobs - DEBUG - training time: 3423s
2019-09-19 05:34:36,016 - training_jobs - DEBUG - saving to results/20190919_043733_ggnn_.json
2019-09-19 05:34:36,018 - training_jobs - DEBUG - moved jobdict to done_trainings/task_5.yml
2019-09-19 05:34:36,018 - training_jobs - DEBUG - Finished!

2019-09-19 05:34:38,453 - training_jobs - DEBUG - training trains/task_1.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 20,
 'd2': 10,
 'd3': 10,
 'd4': 8,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.0001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '2e-4'}
2019-09-19 05:34:38,466 - training_jobs - DEBUG - training with: 
2019-09-19 05:34:38,466 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-19 05:34:38,466 - training_jobs - DEBUG - GGNN5
2019-09-19 05:34:38,466 - training_jobs - DEBUG - 
2019-09-19 05:34:38,466 - training_jobs - DEBUG - ggnn training
2019-09-19 05:34:39,625 - training_jobs - DEBUG -  saving results to results/20190919_053439_ggnn_.json
2019-09-19 05:34:39,625 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 05:49:18,300 - training_jobs - DEBUG - test_multiple_models
2019-09-19 05:49:18,300 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-19 05:49:18,759 - training_jobs - DEBUG - training time: 879s
2019-09-19 05:49:18,759 - training_jobs - DEBUG - saving to results/20190919_053439_ggnn_.json
2019-09-19 05:49:18,761 - training_jobs - DEBUG - moved jobdict to done_trainings/task_1.yml
2019-09-19 05:49:18,761 - training_jobs - DEBUG - Finished!

2019-09-19 05:49:21,139 - training_jobs - DEBUG - training trains/task_13.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 10,
 'd2': 5,
 'd3': 5,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.0001,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '2e-4'}
2019-09-19 05:49:21,151 - training_jobs - DEBUG - training with: 
2019-09-19 05:49:21,152 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-19 05:49:21,152 - training_jobs - DEBUG - GGNN5
2019-09-19 05:49:21,152 - training_jobs - DEBUG - 
2019-09-19 05:49:21,152 - training_jobs - DEBUG - ggnn training
2019-09-19 05:49:22,305 - training_jobs - DEBUG -  saving results to results/20190919_054922_ggnn_.json
2019-09-19 05:49:22,305 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 06:18:17,266 - training_jobs - DEBUG - test_multiple_models
2019-09-19 06:18:17,267 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-19 06:18:17,705 - training_jobs - DEBUG - training time: 1735s
2019-09-19 06:18:17,706 - training_jobs - DEBUG - saving to results/20190919_054922_ggnn_.json
2019-09-19 06:18:17,708 - training_jobs - DEBUG - moved jobdict to done_trainings/task_13.yml
2019-09-19 06:18:17,708 - training_jobs - DEBUG - Finished!

2019-09-19 07:48:18,338 - training_jobs - DEBUG - ('tasks/gnn_ggnn1_extensive_search', '.yaml')
2019-09-19 07:48:18,364 - training_jobs - DEBUG - ('tasks/gnn_ggnn5_extensive_search3', '.yaml')
2019-09-19 07:48:20,877 - training_jobs - DEBUG - training trains/task_20.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-19 07:48:20,889 - training_jobs - DEBUG - training with: 
2019-09-19 07:48:20,889 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max/
2019-09-19 07:48:20,889 - training_jobs - DEBUG - GGNN1
2019-09-19 07:48:20,889 - training_jobs - DEBUG - 
2019-09-19 07:48:20,889 - training_jobs - DEBUG - ggnn training
2019-09-19 07:48:37,512 - training_jobs - DEBUG -  saving results to results/20190919_074837_ggnn_.json
2019-09-19 07:48:37,513 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 09:24:57,712 - training_jobs - DEBUG - test_multiple_models
2019-09-19 09:24:57,712 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-19 09:25:42,110 - training_jobs - DEBUG - training time: 5825s
2019-09-19 09:25:42,110 - training_jobs - DEBUG - saving to results/20190919_074837_ggnn_.json
2019-09-19 09:25:42,112 - training_jobs - DEBUG - moved jobdict to done_trainings/task_20.yml
2019-09-19 09:25:42,112 - training_jobs - DEBUG - Finished!

2019-09-19 09:25:45,208 - training_jobs - DEBUG - training trains/task_4.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 200,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-19 09:25:45,219 - training_jobs - DEBUG - training with: 
2019-09-19 09:25:45,219 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-19 09:25:45,219 - training_jobs - DEBUG - GGNN1
2019-09-19 09:25:45,219 - training_jobs - DEBUG - 
2019-09-19 09:25:45,220 - training_jobs - DEBUG - ggnn training
2019-09-19 09:25:46,354 - training_jobs - DEBUG -  saving results to results/20190919_092546_ggnn_.json
2019-09-19 09:25:46,354 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 09:59:49,731 - training_jobs - DEBUG - test_multiple_models
2019-09-19 09:59:49,731 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-19 09:59:50,192 - training_jobs - DEBUG - training time: 2044s
2019-09-19 09:59:50,192 - training_jobs - DEBUG - saving to results/20190919_092546_ggnn_.json
2019-09-19 09:59:50,194 - training_jobs - DEBUG - moved jobdict to done_trainings/task_4.yml
2019-09-19 09:59:50,194 - training_jobs - DEBUG - Finished!

2019-09-19 09:59:52,693 - training_jobs - DEBUG - training trains/task_18.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-19 09:59:52,706 - training_jobs - DEBUG - training with: 
2019-09-19 09:59:52,706 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max/
2019-09-19 09:59:52,706 - training_jobs - DEBUG - GGNN1
2019-09-19 09:59:52,706 - training_jobs - DEBUG - 
2019-09-19 09:59:52,706 - training_jobs - DEBUG - ggnn training
2019-09-19 09:59:56,919 - training_jobs - DEBUG -  saving results to results/20190919_095956_ggnn_.json
2019-09-19 09:59:56,920 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 10:48:15,514 - training_jobs - DEBUG - test_multiple_models
2019-09-19 10:48:15,514 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-19 10:48:17,342 - training_jobs - DEBUG - training time: 2900s
2019-09-19 10:48:17,342 - training_jobs - DEBUG - saving to results/20190919_095956_ggnn_.json
2019-09-19 10:48:17,344 - training_jobs - DEBUG - moved jobdict to done_trainings/task_18.yml
2019-09-19 10:48:17,344 - training_jobs - DEBUG - Finished!

2019-09-19 10:48:19,870 - training_jobs - DEBUG - training trains/task_2.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-19 10:48:19,882 - training_jobs - DEBUG - training with: 
2019-09-19 10:48:19,883 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-19 10:48:19,883 - training_jobs - DEBUG - GGNN1
2019-09-19 10:48:19,883 - training_jobs - DEBUG - 
2019-09-19 10:48:19,883 - training_jobs - DEBUG - ggnn training
2019-09-19 10:48:21,074 - training_jobs - DEBUG -  saving results to results/20190919_104821_ggnn_.json
2019-09-19 10:48:21,075 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 11:05:32,521 - training_jobs - DEBUG - test_multiple_models
2019-09-19 11:05:32,522 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-19 11:05:33,033 - training_jobs - DEBUG - training time: 1032s
2019-09-19 11:05:33,033 - training_jobs - DEBUG - saving to results/20190919_104821_ggnn_.json
2019-09-19 11:05:33,035 - training_jobs - DEBUG - moved jobdict to done_trainings/task_2.yml
2019-09-19 11:05:33,035 - training_jobs - DEBUG - Finished!

2019-09-19 11:05:36,022 - training_jobs - DEBUG - training trains/task_7.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 500,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-19 11:05:36,038 - training_jobs - DEBUG - training with: 
2019-09-19 11:05:36,038 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-19 11:05:36,038 - training_jobs - DEBUG - GGNN1
2019-09-19 11:05:36,038 - training_jobs - DEBUG - 
2019-09-19 11:05:36,038 - training_jobs - DEBUG - ggnn training
2019-09-19 11:05:37,592 - training_jobs - DEBUG -  saving results to results/20190919_110537_ggnn_.json
2019-09-19 11:05:37,592 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 12:31:06,178 - training_jobs - DEBUG - test_multiple_models
2019-09-19 12:31:06,179 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-19 12:31:06,642 - training_jobs - DEBUG - training time: 5129s
2019-09-19 12:31:06,642 - training_jobs - DEBUG - saving to results/20190919_110537_ggnn_.json
2019-09-19 12:31:06,644 - training_jobs - DEBUG - moved jobdict to done_trainings/task_7.yml
2019-09-19 12:31:06,644 - training_jobs - DEBUG - Finished!

2019-09-19 12:31:09,030 - training_jobs - DEBUG - training trains/task_19.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-19 12:31:09,042 - training_jobs - DEBUG - training with: 
2019-09-19 12:31:09,042 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max/
2019-09-19 12:31:09,042 - training_jobs - DEBUG - GGNN1
2019-09-19 12:31:09,042 - training_jobs - DEBUG - 
2019-09-19 12:31:09,042 - training_jobs - DEBUG - ggnn training
2019-09-19 12:31:13,665 - training_jobs - DEBUG -  saving results to results/20190919_123113_ggnn_.json
2019-09-19 12:31:13,665 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 13:19:14,518 - training_jobs - DEBUG - test_multiple_models
2019-09-19 13:19:14,518 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-19 13:19:16,225 - training_jobs - DEBUG - training time: 2883s
2019-09-19 13:19:16,225 - training_jobs - DEBUG - saving to results/20190919_123113_ggnn_.json
2019-09-19 13:19:16,227 - training_jobs - DEBUG - moved jobdict to done_trainings/task_19.yml
2019-09-19 13:19:16,227 - training_jobs - DEBUG - Finished!

2019-09-19 13:19:18,912 - training_jobs - DEBUG - training trains/task_47.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max/',
 'epochs': 500,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-19 13:19:18,926 - training_jobs - DEBUG - training with: 
2019-09-19 13:19:18,926 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max/
2019-09-19 13:19:18,926 - training_jobs - DEBUG - GGNN5
2019-09-19 13:19:18,926 - training_jobs - DEBUG - 
2019-09-19 13:19:18,926 - training_jobs - DEBUG - ggnn training
2019-09-19 13:19:22,972 - training_jobs - DEBUG -  saving results to results/20190919_131922_ggnn_.json
2019-09-19 13:19:22,972 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 17:53:45,967 - training_jobs - DEBUG - test_multiple_models
2019-09-19 17:53:45,967 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-19 17:53:47,622 - training_jobs - DEBUG - training time: 16465s
2019-09-19 17:53:47,622 - training_jobs - DEBUG - saving to results/20190919_131922_ggnn_.json
2019-09-19 17:53:47,624 - training_jobs - DEBUG - moved jobdict to done_trainings/task_47.yml
2019-09-19 17:53:47,625 - training_jobs - DEBUG - Finished!

2019-09-19 17:53:50,090 - training_jobs - DEBUG - training trains/task_17.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-19 17:53:50,101 - training_jobs - DEBUG - training with: 
2019-09-19 17:53:50,101 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max/
2019-09-19 17:53:50,101 - training_jobs - DEBUG - GGNN1
2019-09-19 17:53:50,101 - training_jobs - DEBUG - 
2019-09-19 17:53:50,101 - training_jobs - DEBUG - ggnn training
2019-09-19 17:53:53,940 - training_jobs - DEBUG -  saving results to results/20190919_175353_ggnn_.json
2019-09-19 17:53:53,940 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 18:18:17,218 - training_jobs - DEBUG - test_multiple_models
2019-09-19 18:18:17,218 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-19 18:18:19,046 - training_jobs - DEBUG - training time: 1465s
2019-09-19 18:18:19,047 - training_jobs - DEBUG - saving to results/20190919_175353_ggnn_.json
2019-09-19 18:18:19,049 - training_jobs - DEBUG - moved jobdict to done_trainings/task_17.yml
2019-09-19 18:18:19,049 - training_jobs - DEBUG - Finished!

2019-09-19 18:18:22,168 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-19 18:18:22,184 - training_jobs - DEBUG - training with: 
2019-09-19 18:18:22,185 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-19 18:18:22,185 - training_jobs - DEBUG - GGNN1
2019-09-19 18:18:22,185 - training_jobs - DEBUG - 
2019-09-19 18:18:22,185 - training_jobs - DEBUG - ggnn training
2019-09-19 18:18:23,612 - training_jobs - DEBUG -  saving results to results/20190919_181823_ggnn_.json
2019-09-19 18:18:23,612 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 18:27:09,238 - training_jobs - DEBUG - test_multiple_models
2019-09-19 18:27:09,238 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-19 18:27:11,146 - training_jobs - DEBUG - training time: 528s
2019-09-19 18:27:11,146 - training_jobs - DEBUG - saving to results/20190919_181823_ggnn_.json
2019-09-19 18:27:11,148 - training_jobs - DEBUG - moved jobdict to done_trainings/task_0.yml
2019-09-19 18:27:11,148 - training_jobs - DEBUG - Finished!

2019-09-19 18:27:13,594 - training_jobs - DEBUG - training trains/task_11.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_half/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-19 18:27:13,731 - training_jobs - DEBUG - training with: 
2019-09-19 18:27:13,731 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_half/
2019-09-19 18:27:13,731 - training_jobs - DEBUG - GGNN1
2019-09-19 18:27:13,731 - training_jobs - DEBUG - 
2019-09-19 18:27:13,731 - training_jobs - DEBUG - ggnn training
2019-09-19 18:27:16,167 - training_jobs - DEBUG -  saving results to results/20190919_182716_ggnn_.json
2019-09-19 18:27:16,167 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 18:48:31,187 - training_jobs - DEBUG - test_multiple_models
2019-09-19 18:48:31,188 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-19 18:48:32,018 - training_jobs - DEBUG - training time: 1276s
2019-09-19 18:48:32,018 - training_jobs - DEBUG - saving to results/20190919_182716_ggnn_.json
2019-09-19 18:48:32,020 - training_jobs - DEBUG - moved jobdict to done_trainings/task_11.yml
2019-09-19 18:48:32,020 - training_jobs - DEBUG - Finished!

2019-09-19 18:48:34,530 - training_jobs - DEBUG - training trains/task_22.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 30,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max/',
 'epochs': 500,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN1',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-19 18:48:34,554 - training_jobs - DEBUG - training with: 
2019-09-19 18:48:34,554 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max/
2019-09-19 18:48:34,554 - training_jobs - DEBUG - GGNN1
2019-09-19 18:48:34,554 - training_jobs - DEBUG - 
2019-09-19 18:48:34,554 - training_jobs - DEBUG - ggnn training
2019-09-19 18:48:38,706 - training_jobs - DEBUG -  saving results to results/20190919_184838_ggnn_.json
2019-09-19 18:48:38,706 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 19:11:19,266 - training_jobs - DEBUG - ('tasks/gnn_ggnn5_intensive_search01', '.yaml')
2019-09-19 19:11:21,691 - training_jobs - DEBUG - training trains/task_4.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-19 19:11:21,702 - training_jobs - DEBUG - training with: 
2019-09-19 19:11:21,703 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-19 19:11:21,703 - training_jobs - DEBUG - GGNN5
2019-09-19 19:11:21,703 - training_jobs - DEBUG - 
2019-09-19 19:11:21,703 - training_jobs - DEBUG - ggnn training
2019-09-19 19:11:22,902 - training_jobs - DEBUG -  saving results to results/20190919_191122_ggnn_.json
2019-09-19 19:11:22,903 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 19:31:32,982 - training_jobs - DEBUG - test_multiple_models
2019-09-19 19:31:32,982 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-19 19:31:33,421 - training_jobs - DEBUG - training time: 1211s
2019-09-19 19:31:33,421 - training_jobs - DEBUG - saving to results/20190919_191122_ggnn_.json
2019-09-19 19:31:33,423 - training_jobs - DEBUG - moved jobdict to done_trainings/task_4.yml
2019-09-19 19:31:33,424 - training_jobs - DEBUG - Finished!

2019-09-19 19:31:35,822 - training_jobs - DEBUG - training trains/task_2.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-19 19:31:35,834 - training_jobs - DEBUG - training with: 
2019-09-19 19:31:35,835 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-19 19:31:35,835 - training_jobs - DEBUG - GGNN5
2019-09-19 19:31:35,835 - training_jobs - DEBUG - 
2019-09-19 19:31:35,835 - training_jobs - DEBUG - ggnn training
2019-09-19 19:31:37,003 - training_jobs - DEBUG -  saving results to results/20190919_193137_ggnn_.json
2019-09-19 19:31:37,003 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 19:41:29,196 - training_jobs - DEBUG - test_multiple_models
2019-09-19 19:41:29,196 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-19 19:41:29,636 - training_jobs - DEBUG - training time: 593s
2019-09-19 19:41:29,636 - training_jobs - DEBUG - saving to results/20190919_193137_ggnn_.json
2019-09-19 19:41:29,638 - training_jobs - DEBUG - moved jobdict to done_trainings/task_2.yml
2019-09-19 19:41:29,638 - training_jobs - DEBUG - Finished!

2019-09-19 19:41:32,028 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '5e-4'}
2019-09-19 19:41:32,041 - training_jobs - DEBUG - training with: 
2019-09-19 19:41:32,042 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-19 19:41:32,042 - training_jobs - DEBUG - GGNN5
2019-09-19 19:41:32,042 - training_jobs - DEBUG - 
2019-09-19 19:41:32,042 - training_jobs - DEBUG - ggnn training
2019-09-19 19:41:33,178 - training_jobs - DEBUG -  saving results to results/20190919_194133_ggnn_.json
2019-09-19 19:41:33,178 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 19:45:36,623 - training_jobs - DEBUG - test_multiple_models
2019-09-19 19:45:36,623 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-19 19:45:37,063 - training_jobs - DEBUG - training time: 244s
2019-09-19 19:45:37,063 - training_jobs - DEBUG - saving to results/20190919_194133_ggnn_.json
2019-09-19 19:45:37,065 - training_jobs - DEBUG - moved jobdict to done_trainings/task_0.yml
2019-09-19 19:45:37,065 - training_jobs - DEBUG - Finished!

2019-09-19 19:45:39,466 - training_jobs - DEBUG - training trains/task_3.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-19 19:45:39,478 - training_jobs - DEBUG - training with: 
2019-09-19 19:45:39,478 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-19 19:45:39,478 - training_jobs - DEBUG - GGNN5
2019-09-19 19:45:39,479 - training_jobs - DEBUG - 
2019-09-19 19:45:39,479 - training_jobs - DEBUG - ggnn training
2019-09-19 19:45:40,613 - training_jobs - DEBUG -  saving results to results/20190919_194540_ggnn_.json
2019-09-19 19:45:40,613 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 19:55:37,175 - training_jobs - DEBUG - test_multiple_models
2019-09-19 19:55:37,175 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-19 19:55:37,620 - training_jobs - DEBUG - training time: 597s
2019-09-19 19:55:37,620 - training_jobs - DEBUG - saving to results/20190919_194540_ggnn_.json
2019-09-19 19:55:37,622 - training_jobs - DEBUG - moved jobdict to done_trainings/task_3.yml
2019-09-19 19:55:37,623 - training_jobs - DEBUG - Finished!

2019-09-19 19:55:40,249 - training_jobs - DEBUG - training trains/task_5.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-19 19:55:40,264 - training_jobs - DEBUG - training with: 
2019-09-19 19:55:40,264 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-19 19:55:40,264 - training_jobs - DEBUG - GGNN5
2019-09-19 19:55:40,264 - training_jobs - DEBUG - 
2019-09-19 19:55:40,264 - training_jobs - DEBUG - ggnn training
2019-09-19 19:55:41,617 - training_jobs - DEBUG -  saving results to results/20190919_195541_ggnn_.json
2019-09-19 19:55:41,617 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 20:15:15,138 - training_jobs - DEBUG - test_multiple_models
2019-09-19 20:15:15,138 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-19 20:15:15,576 - training_jobs - DEBUG - training time: 1174s
2019-09-19 20:15:15,576 - training_jobs - DEBUG - saving to results/20190919_195541_ggnn_.json
2019-09-19 20:15:15,578 - training_jobs - DEBUG - moved jobdict to done_trainings/task_5.yml
2019-09-19 20:15:15,578 - training_jobs - DEBUG - Finished!

2019-09-19 20:15:17,943 - training_jobs - DEBUG - training trains/task_1.yml
{'aggr_type': 'mean',
 'batch_size': 16,
 'd1': 10,
 'd2': 5,
 'd3': 4,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-19 20:15:17,955 - training_jobs - DEBUG - training with: 
2019-09-19 20:15:17,955 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-19 20:15:17,955 - training_jobs - DEBUG - GGNN5
2019-09-19 20:15:17,955 - training_jobs - DEBUG - 
2019-09-19 20:15:17,955 - training_jobs - DEBUG - ggnn training
2019-09-19 20:15:19,109 - training_jobs - DEBUG -  saving results to results/20190919_201519_ggnn_.json
2019-09-19 20:15:19,109 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 20:19:25,081 - training_jobs - DEBUG - test_multiple_models
2019-09-19 20:19:25,081 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-19 20:19:25,521 - training_jobs - DEBUG - training time: 246s
2019-09-19 20:19:25,521 - training_jobs - DEBUG - saving to results/20190919_201519_ggnn_.json
2019-09-19 20:19:25,523 - training_jobs - DEBUG - moved jobdict to done_trainings/task_1.yml
2019-09-19 20:19:25,523 - training_jobs - DEBUG - Finished!

2019-09-19 20:48:36,137 - training_jobs - DEBUG - ('tasks/gnn_ggnn5_intensive_search01', '.yaml')
2019-09-19 20:48:38,438 - training_jobs - DEBUG - training trains/task_2.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 30,
 'd2': 50,
 'd3': 20,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-19 20:48:38,450 - training_jobs - DEBUG - training with: 
2019-09-19 20:48:38,450 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-19 20:48:38,450 - training_jobs - DEBUG - GGNN5
2019-09-19 20:48:38,451 - training_jobs - DEBUG - 
2019-09-19 20:48:38,451 - training_jobs - DEBUG - ggnn training
2019-09-19 20:48:39,640 - training_jobs - DEBUG -  saving results to results/20190919_204839_ggnn_.json
2019-09-19 20:48:39,640 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 20:48:42,020 - training_jobs - ERROR - Error with trains/task_2.yml
Traceback (most recent call last):
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 422, in get
    data = torch.load(filename)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/serialization.py", line 368, in load
    return _load(f, map_location, pickle_module)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/serialization.py", line 524, in _load
    return legacy_load(f)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/serialization.py", line 448, in legacy_load
    with closing(tarfile.open(fileobj=f, mode='r:', format=tarfile.PAX_FORMAT)) as tar, \
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/tarfile.py", line 1579, in open
    filemode, comptype = mode.split(":", 1)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1476, in modelSelection
    kfolds = kFolding2(train_dataset,k, balanced, unbalanced_split)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 725, in kFolding2
    folds = balancedDatasetKfoldSplit_slice(train_dataset, k)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 607, in balancedDatasetKfoldSplit_slice
    graph = dataset[i]
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 126, in __getitem__
    data = self.get(idx)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 438, in get
    raise IndexError("error getting: ", idx)
IndexError: ('error getting: ', 2951)
2019-09-19 20:49:05,203 - training_jobs - DEBUG - ('tasks/gnn_ggnn5_intensive_search01', '.yaml')
2019-09-19 20:49:07,475 - training_jobs - DEBUG - training trains/task_2.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 30,
 'd2': 50,
 'd3': 20,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-19 20:49:07,488 - training_jobs - DEBUG - training with: 
2019-09-19 20:49:07,489 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-19 20:49:07,489 - training_jobs - DEBUG - GGNN5
2019-09-19 20:49:07,489 - training_jobs - DEBUG - 
2019-09-19 20:49:07,489 - training_jobs - DEBUG - ggnn training
2019-09-19 20:49:08,652 - training_jobs - DEBUG -  saving results to results/20190919_204908_ggnn_.json
2019-09-19 20:49:08,652 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 21:17:52,198 - training_jobs - DEBUG - test_multiple_models
2019-09-19 21:17:52,198 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-19 21:17:52,683 - training_jobs - DEBUG - training time: 1724s
2019-09-19 21:17:52,683 - training_jobs - DEBUG - saving to results/20190919_204908_ggnn_.json
2019-09-19 21:17:52,685 - training_jobs - DEBUG - moved jobdict to done_trainings/task_2.yml
2019-09-19 21:17:52,685 - training_jobs - DEBUG - Finished!

2019-09-19 21:17:55,498 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 30,
 'd2': 50,
 'd3': 20,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-19 21:17:55,512 - training_jobs - DEBUG - training with: 
2019-09-19 21:17:55,512 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-19 21:17:55,512 - training_jobs - DEBUG - GGNN5
2019-09-19 21:17:55,512 - training_jobs - DEBUG - 
2019-09-19 21:17:55,512 - training_jobs - DEBUG - ggnn training
2019-09-19 21:17:56,974 - training_jobs - DEBUG -  saving results to results/20190919_211756_ggnn_.json
2019-09-19 21:17:56,974 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 21:19:57,384 - training_jobs - DEBUG - ('tasks/gnn_ggnn5_intensive_search01', '.yaml')
2019-09-19 21:20:00,348 - training_jobs - DEBUG - training trains/task_2.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 30,
 'd2': 50,
 'd3': 20,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-19 21:20:00,364 - training_jobs - DEBUG - training with: 
2019-09-19 21:20:00,365 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max/
2019-09-19 21:20:00,365 - training_jobs - DEBUG - GGNN5
2019-09-19 21:20:00,365 - training_jobs - DEBUG - 
2019-09-19 21:20:00,365 - training_jobs - DEBUG - ggnn training
2019-09-19 21:20:05,102 - training_jobs - DEBUG -  saving results to results/20190919_212005_ggnn_.json
2019-09-19 21:20:05,102 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 21:25:31,870 - training_jobs - ERROR - Error with trains/task_2.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 232, in forward
    x = F.relu(self.dense1_bn(self.fc1(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1619, in batch_norm
    raise ValueError('Expected more than 1 value per channel when training, got input size {}'.format(size))
ValueError: Expected more than 1 value per channel when training, got input size torch.Size([1, 50])
2019-09-19 21:25:34,390 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 30,
 'd2': 50,
 'd3': 20,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-19 21:25:34,404 - training_jobs - DEBUG - training with: 
2019-09-19 21:25:34,404 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-19 21:25:34,404 - training_jobs - DEBUG - GGNN5
2019-09-19 21:25:34,404 - training_jobs - DEBUG - 
2019-09-19 21:25:34,404 - training_jobs - DEBUG - ggnn training
2019-09-19 21:25:35,732 - training_jobs - DEBUG -  saving results to results/20190919_212535_ggnn_.json
2019-09-19 21:25:35,732 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 21:31:59,476 - training_jobs - DEBUG - test_multiple_models
2019-09-19 21:31:59,476 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-19 21:31:59,986 - training_jobs - DEBUG - training time: 384s
2019-09-19 21:31:59,986 - training_jobs - DEBUG - saving to results/20190919_212535_ggnn_.json
2019-09-19 21:31:59,989 - training_jobs - DEBUG - moved jobdict to done_trainings/task_0.yml
2019-09-19 21:31:59,989 - training_jobs - DEBUG - Finished!

2019-09-19 21:32:03,174 - training_jobs - DEBUG - training trains/task_3.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 30,
 'd2': 50,
 'd3': 20,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-19 21:32:03,193 - training_jobs - DEBUG - training with: 
2019-09-19 21:32:03,193 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max/
2019-09-19 21:32:03,193 - training_jobs - DEBUG - GGNN5
2019-09-19 21:32:03,193 - training_jobs - DEBUG - 
2019-09-19 21:32:03,193 - training_jobs - DEBUG - ggnn training
2019-09-19 21:32:09,015 - training_jobs - DEBUG -  saving results to results/20190919_213209_ggnn_.json
2019-09-19 21:32:09,015 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 21:42:04,826 - training_jobs - ERROR - Error with trains/task_3.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 232, in forward
    x = F.relu(self.dense1_bn(self.fc1(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1619, in batch_norm
    raise ValueError('Expected more than 1 value per channel when training, got input size {}'.format(size))
ValueError: Expected more than 1 value per channel when training, got input size torch.Size([1, 50])
2019-09-19 21:42:07,261 - training_jobs - DEBUG - training trains/task_1.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 30,
 'd2': 50,
 'd3': 20,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max/',
 'epochs': 20,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-19 21:42:07,274 - training_jobs - DEBUG - training with: 
2019-09-19 21:42:07,274 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max/
2019-09-19 21:42:07,274 - training_jobs - DEBUG - GGNN5
2019-09-19 21:42:07,274 - training_jobs - DEBUG - 
2019-09-19 21:42:07,274 - training_jobs - DEBUG - ggnn training
2019-09-19 21:42:11,328 - training_jobs - DEBUG -  saving results to results/20190919_214211_ggnn_.json
2019-09-19 21:42:11,328 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 21:44:26,373 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 232, in forward
    x = F.relu(self.dense1_bn(self.fc1(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1619, in batch_norm
    raise ValueError('Expected more than 1 value per channel when training, got input size {}'.format(size))
ValueError: Expected more than 1 value per channel when training, got input size torch.Size([1, 50])
2019-09-19 22:19:06,764 - training_jobs - DEBUG - ('tasks/gnn_ggnn5_intensive_search01', '.yaml')
2019-09-19 22:19:09,270 - training_jobs - DEBUG - training trains/task_4.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 30,
 'd2': 50,
 'd3': 20,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_remove_min/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-19 22:19:09,284 - training_jobs - DEBUG - training with: 
2019-09-19 22:19:09,284 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_remove_min/
2019-09-19 22:19:09,284 - training_jobs - DEBUG - GGNN5
2019-09-19 22:19:09,284 - training_jobs - DEBUG - 
2019-09-19 22:19:09,284 - training_jobs - DEBUG - ggnn training
2019-09-19 22:19:09,294 - training_jobs - ERROR - Error with trains/task_4.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 475, in training_dispatcher
    train_dataset = FunctionsDataset(root=os.path.join(dataset_folder,'training_set'))
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 223, in __init__
    super(FunctionsDataset, self).__init__(root, transform, pre_transform)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 85, in __init__
    self._process()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 113, in _process
    if files_exist(self.processed_paths):  # pragma: no cover
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 102, in processed_paths
    files = to_list(self.processed_file_names)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 255, in processed_file_names
    for item in os.listdir(processed_path):
FileNotFoundError: [Errno 2] No such file or directory: 'tmp/symbols_dataset_1_precomp_remove_min/training_set/processed'
2019-09-19 22:19:11,715 - training_jobs - DEBUG - training trains/task_2.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 30,
 'd2': 50,
 'd3': 20,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-19 22:19:11,727 - training_jobs - DEBUG - training with: 
2019-09-19 22:19:11,727 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max/
2019-09-19 22:19:11,727 - training_jobs - DEBUG - GGNN5
2019-09-19 22:19:11,727 - training_jobs - DEBUG - 
2019-09-19 22:19:11,727 - training_jobs - DEBUG - ggnn training
2019-09-19 22:19:15,671 - training_jobs - DEBUG -  saving results to results/20190919_221915_ggnn_.json
2019-09-19 22:19:15,671 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 22:24:16,927 - training_jobs - ERROR - Error with trains/task_2.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 232, in forward
    x = F.relu(self.dense1_bn(self.fc1(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1619, in batch_norm
    raise ValueError('Expected more than 1 value per channel when training, got input size {}'.format(size))
ValueError: Expected more than 1 value per channel when training, got input size torch.Size([1, 50])
2019-09-19 22:24:19,329 - training_jobs - DEBUG - training trains/task_3.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 30,
 'd2': 50,
 'd3': 20,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_remove_min/',
 'epochs': 20,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-19 22:24:19,342 - training_jobs - DEBUG - training with: 
2019-09-19 22:24:19,342 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_remove_min/
2019-09-19 22:24:19,342 - training_jobs - DEBUG - GGNN5
2019-09-19 22:24:19,342 - training_jobs - DEBUG - 
2019-09-19 22:24:19,342 - training_jobs - DEBUG - ggnn training
2019-09-19 22:24:19,342 - training_jobs - ERROR - Error with trains/task_3.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 475, in training_dispatcher
    train_dataset = FunctionsDataset(root=os.path.join(dataset_folder,'training_set'))
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 223, in __init__
    super(FunctionsDataset, self).__init__(root, transform, pre_transform)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 85, in __init__
    self._process()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 113, in _process
    if files_exist(self.processed_paths):  # pragma: no cover
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 102, in processed_paths
    files = to_list(self.processed_file_names)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 255, in processed_file_names
    for item in os.listdir(processed_path):
FileNotFoundError: [Errno 2] No such file or directory: 'tmp/symbols_dataset_1_precomp_remove_min/training_set/processed'
2019-09-19 22:24:21,672 - training_jobs - DEBUG - training trains/task_5.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 30,
 'd2': 50,
 'd3': 20,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_remove_min/',
 'epochs': 100,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-19 22:24:21,685 - training_jobs - DEBUG - training with: 
2019-09-19 22:24:21,685 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_remove_min/
2019-09-19 22:24:21,685 - training_jobs - DEBUG - GGNN5
2019-09-19 22:24:21,685 - training_jobs - DEBUG - 
2019-09-19 22:24:21,685 - training_jobs - DEBUG - ggnn training
2019-09-19 22:24:21,685 - training_jobs - ERROR - Error with trains/task_5.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 475, in training_dispatcher
    train_dataset = FunctionsDataset(root=os.path.join(dataset_folder,'training_set'))
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 223, in __init__
    super(FunctionsDataset, self).__init__(root, transform, pre_transform)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 85, in __init__
    self._process()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 113, in _process
    if files_exist(self.processed_paths):  # pragma: no cover
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 102, in processed_paths
    files = to_list(self.processed_file_names)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 255, in processed_file_names
    for item in os.listdir(processed_path):
FileNotFoundError: [Errno 2] No such file or directory: 'tmp/symbols_dataset_1_precomp_remove_min/training_set/processed'
2019-09-19 22:24:24,058 - training_jobs - DEBUG - training trains/task_1.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 30,
 'd2': 50,
 'd3': 20,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max/',
 'epochs': 20,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-19 22:24:24,071 - training_jobs - DEBUG - training with: 
2019-09-19 22:24:24,071 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max/
2019-09-19 22:24:24,071 - training_jobs - DEBUG - GGNN5
2019-09-19 22:24:24,071 - training_jobs - DEBUG - 
2019-09-19 22:24:24,071 - training_jobs - DEBUG - ggnn training
2019-09-19 22:24:27,870 - training_jobs - DEBUG -  saving results to results/20190919_222427_ggnn_.json
2019-09-19 22:24:27,870 - training_jobs - DEBUG -  calling modelSelection
2019-09-19 22:26:44,835 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 534, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1557, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1381, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 957, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 862, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 232, in forward
    x = F.relu(self.dense1_bn(self.fc1(x)))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py", line 76, in forward
    exponential_average_factor, self.eps)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1619, in batch_norm
    raise ValueError('Expected more than 1 value per channel when training, got input size {}'.format(size))
ValueError: Expected more than 1 value per channel when training, got input size torch.Size([1, 50])
2019-09-21 21:39:09,864 - training_jobs - DEBUG - ('tasks/gnn_inspect_dataset', '.yaml')
2019-09-21 21:39:12,449 - training_jobs - DEBUG - training trains/task_4.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 30,
 'd2': 50,
 'd3': 20,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_remove_min/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-21 21:39:12,612 - training_jobs - DEBUG - training with: 
2019-09-21 21:39:12,612 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_remove_min/
2019-09-21 21:39:12,612 - training_jobs - DEBUG - GGNN5
2019-09-21 21:39:12,612 - training_jobs - DEBUG - 
2019-09-21 21:39:12,612 - training_jobs - DEBUG - ggnn training
2019-09-21 21:39:12,612 - training_jobs - ERROR - Error with trains/task_4.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 669, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 475, in training_dispatcher
    train_dataset = FunctionsDataset(root=os.path.join(dataset_folder,'training_set'))
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 223, in __init__
    super(FunctionsDataset, self).__init__(root, transform, pre_transform)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 85, in __init__
    self._process()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 113, in _process
    if files_exist(self.processed_paths):  # pragma: no cover
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 102, in processed_paths
    files = to_list(self.processed_file_names)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 255, in processed_file_names
    for item in os.listdir(processed_path):
FileNotFoundError: [Errno 2] No such file or directory: 'tmp/symbols_dataset_1_precomp_remove_min/training_set/processed'
2019-09-21 21:39:14,961 - training_jobs - DEBUG - training trains/task_2.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 30,
 'd2': 50,
 'd3': 20,
 'd4': 4,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GGNN5',
 'num_classes': 'num_classes',
 'num_layers': 1,
 'weight_decay': '1e-4'}
2019-09-21 21:39:14,975 - training_jobs - DEBUG - training with: 
2019-09-21 21:39:14,975 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max/
2019-09-21 21:39:14,975 - training_jobs - DEBUG - GGNN5
2019-09-21 21:39:14,975 - training_jobs - DEBUG - 
2019-09-21 21:39:14,975 - training_jobs - DEBUG - ggnn training
2019-09-21 21:39:19,668 - training_jobs - DEBUG -  saving results to results/20190921_213919_ggnn_.json
2019-09-21 21:39:19,669 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 21:39:55,475 - training_jobs - DEBUG - ('tasks/gnn_inspect_dataset', '.yaml')
2019-09-21 21:39:57,898 - training_jobs - DEBUG - training trains/task_4.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 21:39:57,910 - training_jobs - DEBUG - training with: 
2019-09-21 21:39:57,910 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_undersample_max_third/
2019-09-21 21:39:57,910 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 21:39:57,911 - training_jobs - DEBUG - 
2019-09-21 21:39:57,912 - training_jobs - DEBUG - moved jobdict to done_trainings/task_4.yml
2019-09-21 21:39:57,912 - training_jobs - DEBUG - Finished!

2019-09-21 21:40:00,243 - training_jobs - DEBUG - training trains/task_2.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GIN01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 21:40:00,255 - training_jobs - DEBUG - training with: 
2019-09-21 21:40:00,255 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_undersample_max_third/
2019-09-21 21:40:00,256 - training_jobs - DEBUG - GIN01_sort01
2019-09-21 21:40:00,256 - training_jobs - DEBUG - 
2019-09-21 21:40:00,257 - training_jobs - DEBUG - moved jobdict to done_trainings/task_2.yml
2019-09-21 21:40:00,257 - training_jobs - DEBUG - Finished!

2019-09-21 21:40:02,791 - training_jobs - DEBUG - training trains/task_7.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GIN01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 3,
 'weight_decay': '1e-4'}
2019-09-21 21:40:02,805 - training_jobs - DEBUG - training with: 
2019-09-21 21:40:02,805 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_undersample_max_third/
2019-09-21 21:40:02,805 - training_jobs - DEBUG - GIN01_sort01
2019-09-21 21:40:02,805 - training_jobs - DEBUG - 
2019-09-21 21:40:02,806 - training_jobs - DEBUG - moved jobdict to done_trainings/task_7.yml
2019-09-21 21:40:02,806 - training_jobs - DEBUG - Finished!

2019-09-21 21:40:05,106 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 21:40:05,119 - training_jobs - DEBUG - training with: 
2019-09-21 21:40:05,119 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_undersample_max_third/
2019-09-21 21:40:05,119 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 21:40:05,119 - training_jobs - DEBUG - 
2019-09-21 21:40:05,120 - training_jobs - DEBUG - moved jobdict to done_trainings/task_0.yml
2019-09-21 21:40:05,120 - training_jobs - DEBUG - Finished!

2019-09-21 21:40:07,465 - training_jobs - DEBUG - training trains/task_3.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GIN01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 3,
 'weight_decay': '1e-4'}
2019-09-21 21:40:07,477 - training_jobs - DEBUG - training with: 
2019-09-21 21:40:07,477 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_undersample_max_third/
2019-09-21 21:40:07,477 - training_jobs - DEBUG - GIN01_sort01
2019-09-21 21:40:07,477 - training_jobs - DEBUG - 
2019-09-21 21:40:07,516 - training_jobs - DEBUG - moved jobdict to done_trainings/task_3.yml
2019-09-21 21:40:07,516 - training_jobs - DEBUG - Finished!

2019-09-21 21:40:09,837 - training_jobs - DEBUG - training trains/task_6.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'GIN01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 21:40:09,849 - training_jobs - DEBUG - training with: 
2019-09-21 21:40:09,849 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_undersample_max_third/
2019-09-21 21:40:09,849 - training_jobs - DEBUG - GIN01_sort01
2019-09-21 21:40:09,849 - training_jobs - DEBUG - 
2019-09-21 21:40:10,061 - training_jobs - DEBUG - moved jobdict to done_trainings/task_6.yml
2019-09-21 21:40:10,061 - training_jobs - DEBUG - Finished!

2019-09-21 21:40:12,393 - training_jobs - DEBUG - training trains/task_5.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 3,
 'weight_decay': '1e-4'}
2019-09-21 21:40:12,406 - training_jobs - DEBUG - training with: 
2019-09-21 21:40:12,406 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_undersample_max_third/
2019-09-21 21:40:12,406 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 21:40:12,406 - training_jobs - DEBUG - 
2019-09-21 21:40:12,407 - training_jobs - DEBUG - moved jobdict to done_trainings/task_5.yml
2019-09-21 21:40:12,407 - training_jobs - DEBUG - Finished!

2019-09-21 21:40:14,762 - training_jobs - DEBUG - training trains/task_1.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 3,
 'weight_decay': '1e-4'}
2019-09-21 21:40:14,775 - training_jobs - DEBUG - training with: 
2019-09-21 21:40:14,775 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_undersample_max_third/
2019-09-21 21:40:14,775 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 21:40:14,775 - training_jobs - DEBUG - 
2019-09-21 21:40:14,777 - training_jobs - DEBUG - moved jobdict to done_trainings/task_1.yml
2019-09-21 21:40:14,777 - training_jobs - DEBUG - Finished!

2019-09-21 21:42:44,202 - training_jobs - DEBUG - ('tasks/gnn_inspect_dataset', '.yaml')
2019-09-21 21:42:46,649 - training_jobs - DEBUG - training trains/task_2.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'k': 10,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 21:42:46,663 - training_jobs - DEBUG - training with: 
2019-09-21 21:42:46,663 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 21:42:46,663 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 21:42:46,663 - training_jobs - DEBUG - 
2019-09-21 21:42:46,664 - training_jobs - DEBUG - moved jobdict to done_trainings/task_2.yml
2019-09-21 21:42:46,664 - training_jobs - DEBUG - Finished!

2019-09-21 21:42:48,987 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k': 10,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 21:42:48,999 - training_jobs - DEBUG - training with: 
2019-09-21 21:42:48,999 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 21:42:49,000 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 21:42:49,000 - training_jobs - DEBUG - 
2019-09-21 21:42:49,001 - training_jobs - DEBUG - moved jobdict to done_trainings/task_0.yml
2019-09-21 21:42:49,001 - training_jobs - DEBUG - Finished!

2019-09-21 21:42:51,321 - training_jobs - DEBUG - training trains/task_3.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'k': 10,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 3,
 'weight_decay': '1e-4'}
2019-09-21 21:42:51,335 - training_jobs - DEBUG - training with: 
2019-09-21 21:42:51,335 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 21:42:51,335 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 21:42:51,335 - training_jobs - DEBUG - 
2019-09-21 21:42:51,336 - training_jobs - DEBUG - moved jobdict to done_trainings/task_3.yml
2019-09-21 21:42:51,336 - training_jobs - DEBUG - Finished!

2019-09-21 21:42:53,647 - training_jobs - DEBUG - training trains/task_1.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k': 10,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 3,
 'weight_decay': '1e-4'}
2019-09-21 21:42:53,660 - training_jobs - DEBUG - training with: 
2019-09-21 21:42:53,660 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 21:42:53,660 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 21:42:53,660 - training_jobs - DEBUG - 
2019-09-21 21:42:53,661 - training_jobs - DEBUG - moved jobdict to done_trainings/task_1.yml
2019-09-21 21:42:53,661 - training_jobs - DEBUG - Finished!

2019-09-21 21:45:10,330 - training_jobs - DEBUG - ('tasks/gnn_inspect_dataset', '.yaml')
2019-09-21 21:45:12,588 - training_jobs - DEBUG - training trains/task_2.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'k': 10,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 21:45:12,601 - training_jobs - DEBUG - training with: 
2019-09-21 21:45:12,601 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 21:45:12,601 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 21:45:12,601 - training_jobs - DEBUG - 
2019-09-21 21:45:12,601 - training_jobs - DEBUG - ggnn training
2019-09-21 21:45:13,820 - training_jobs - DEBUG -  saving results to results/20190921_214513_ggnn_.json
2019-09-21 21:45:13,820 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 21:45:17,206 - training_jobs - ERROR - Error with trains/task_2.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1130, in final_model_train
    model = modelclass(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 274, in __init__
    self.sage1 = SAGEConv(out_channels=d1, num_layers=num_layers,normalize=True, bias=True)
NameError: name 'SAGEConv' is not defined
2019-09-21 21:45:19,528 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k': 10,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 21:45:19,541 - training_jobs - DEBUG - training with: 
2019-09-21 21:45:19,541 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 21:45:19,541 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 21:45:19,541 - training_jobs - DEBUG - 
2019-09-21 21:45:19,541 - training_jobs - DEBUG - ggnn training
2019-09-21 21:45:20,677 - training_jobs - DEBUG -  saving results to results/20190921_214520_ggnn_.json
2019-09-21 21:45:20,677 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 21:45:24,138 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1130, in final_model_train
    model = modelclass(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 274, in __init__
    self.sage1 = SAGEConv(out_channels=d1, num_layers=num_layers,normalize=True, bias=True)
NameError: name 'SAGEConv' is not defined
2019-09-21 21:45:26,525 - training_jobs - DEBUG - training trains/task_3.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'k': 10,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 3,
 'weight_decay': '1e-4'}
2019-09-21 21:45:26,539 - training_jobs - DEBUG - training with: 
2019-09-21 21:45:26,539 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 21:45:26,539 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 21:45:26,540 - training_jobs - DEBUG - 
2019-09-21 21:45:26,540 - training_jobs - DEBUG - ggnn training
2019-09-21 21:45:27,761 - training_jobs - DEBUG -  saving results to results/20190921_214527_ggnn_.json
2019-09-21 21:45:27,761 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 21:45:31,227 - training_jobs - ERROR - Error with trains/task_3.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1130, in final_model_train
    model = modelclass(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 274, in __init__
    self.sage1 = SAGEConv(out_channels=d1, num_layers=num_layers,normalize=True, bias=True)
NameError: name 'SAGEConv' is not defined
2019-09-21 21:45:33,541 - training_jobs - DEBUG - training trains/task_1.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k': 10,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 3,
 'weight_decay': '1e-4'}
2019-09-21 21:45:33,554 - training_jobs - DEBUG - training with: 
2019-09-21 21:45:33,554 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 21:45:33,555 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 21:45:33,555 - training_jobs - DEBUG - 
2019-09-21 21:45:33,555 - training_jobs - DEBUG - ggnn training
2019-09-21 21:45:34,748 - training_jobs - DEBUG -  saving results to results/20190921_214534_ggnn_.json
2019-09-21 21:45:34,748 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 21:45:38,297 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1130, in final_model_train
    model = modelclass(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 274, in __init__
    self.sage1 = SAGEConv(out_channels=d1, num_layers=num_layers,normalize=True, bias=True)
NameError: name 'SAGEConv' is not defined
2019-09-21 21:48:31,646 - training_jobs - DEBUG - ('tasks/gnn_inspect_dataset', '.yaml')
2019-09-21 21:48:59,931 - training_jobs - DEBUG - ('tasks/gnn_inspect_dataset', '.yaml')
2019-09-21 22:07:14,232 - training_jobs - DEBUG - training trains/task_2.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'k': 10,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 22:07:14,365 - training_jobs - DEBUG - training with: 
2019-09-21 22:07:14,365 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:07:14,365 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:07:14,365 - training_jobs - DEBUG - 
2019-09-21 22:07:14,366 - training_jobs - DEBUG - ggnn training
2019-09-21 22:07:15,319 - training_jobs - DEBUG -  saving results to results/20190921_220715_ggnn_.json
2019-09-21 22:07:15,319 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 22:07:18,015 - training_jobs - ERROR - Error with trains/task_2.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1130, in final_model_train
    model = modelclass(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 279, in __init__
    self.sage1 = SAGEConv(out_channels=d1, num_layers=num_layers,normalize=True, bias=True)
TypeError: __init__() missing 1 required positional argument: 'in_channels'
2019-09-21 22:07:20,559 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k': 10,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 22:07:20,599 - training_jobs - DEBUG - training with: 
2019-09-21 22:07:20,599 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:07:20,599 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:07:20,599 - training_jobs - DEBUG - 
2019-09-21 22:07:20,599 - training_jobs - DEBUG - ggnn training
2019-09-21 22:07:21,695 - training_jobs - DEBUG -  saving results to results/20190921_220721_ggnn_.json
2019-09-21 22:07:21,696 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 22:07:24,444 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1130, in final_model_train
    model = modelclass(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 279, in __init__
    self.sage1 = SAGEConv(out_channels=d1, num_layers=num_layers,normalize=True, bias=True)
TypeError: __init__() missing 1 required positional argument: 'in_channels'
2019-09-21 22:07:26,926 - training_jobs - DEBUG - training trains/task_3.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'k': 10,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 3,
 'weight_decay': '1e-4'}
2019-09-21 22:07:26,966 - training_jobs - DEBUG - training with: 
2019-09-21 22:07:26,966 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:07:26,966 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:07:26,966 - training_jobs - DEBUG - 
2019-09-21 22:07:26,966 - training_jobs - DEBUG - ggnn training
2019-09-21 22:07:28,059 - training_jobs - DEBUG -  saving results to results/20190921_220728_ggnn_.json
2019-09-21 22:07:28,059 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 22:07:30,918 - training_jobs - ERROR - Error with trains/task_3.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1130, in final_model_train
    model = modelclass(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 279, in __init__
    self.sage1 = SAGEConv(out_channels=d1, num_layers=num_layers,normalize=True, bias=True)
TypeError: __init__() missing 1 required positional argument: 'in_channels'
2019-09-21 22:07:33,401 - training_jobs - DEBUG - training trains/task_1.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k': 10,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 3,
 'weight_decay': '1e-4'}
2019-09-21 22:07:33,414 - training_jobs - DEBUG - training with: 
2019-09-21 22:07:33,414 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:07:33,414 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:07:33,414 - training_jobs - DEBUG - 
2019-09-21 22:07:33,414 - training_jobs - DEBUG - ggnn training
2019-09-21 22:07:34,608 - training_jobs - DEBUG -  saving results to results/20190921_220734_ggnn_.json
2019-09-21 22:07:34,608 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 22:07:37,545 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1130, in final_model_train
    model = modelclass(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 279, in __init__
    self.sage1 = SAGEConv(out_channels=d1, num_layers=num_layers,normalize=True, bias=True)
TypeError: __init__() missing 1 required positional argument: 'in_channels'
2019-09-21 22:09:03,038 - training_jobs - DEBUG - ('tasks/gnn_inspect_dataset', '.yaml')
2019-09-21 22:09:41,575 - training_jobs - DEBUG - ('tasks/gnn_inspect_dataset', '.yaml')
2019-09-21 22:09:44,541 - training_jobs - DEBUG - training trains/task_2.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd0': 50,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'k': 10,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 22:09:44,554 - training_jobs - DEBUG - training with: 
2019-09-21 22:09:44,554 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:09:44,554 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:09:44,554 - training_jobs - DEBUG - 
2019-09-21 22:09:44,554 - training_jobs - DEBUG - ggnn training
2019-09-21 22:09:46,255 - training_jobs - DEBUG -  saving results to results/20190921_220946_ggnn_.json
2019-09-21 22:09:46,255 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 22:09:49,465 - training_jobs - ERROR - Error with trains/task_2.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1130, in final_model_train
    model = modelclass(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 279, in __init__
    self.sage1 = SAGEConv(in_channels=d0, out_channels=d1, num_layers=num_layers,normalize=True, bias=True)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/sage_conv.py", line 34, in __init__
    super(SAGEConv, self).__init__(aggr='mean', **kwargs)
TypeError: __init__() got an unexpected keyword argument 'num_layers'
2019-09-21 22:09:52,206 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd0': 50,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k': 10,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 22:09:52,218 - training_jobs - DEBUG - training with: 
2019-09-21 22:09:52,218 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:09:52,218 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:09:52,218 - training_jobs - DEBUG - 
2019-09-21 22:09:52,218 - training_jobs - DEBUG - ggnn training
2019-09-21 22:09:53,318 - training_jobs - DEBUG -  saving results to results/20190921_220953_ggnn_.json
2019-09-21 22:09:53,318 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 22:09:56,276 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1130, in final_model_train
    model = modelclass(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 279, in __init__
    self.sage1 = SAGEConv(in_channels=d0, out_channels=d1, num_layers=num_layers,normalize=True, bias=True)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/sage_conv.py", line 34, in __init__
    super(SAGEConv, self).__init__(aggr='mean', **kwargs)
TypeError: __init__() got an unexpected keyword argument 'num_layers'
2019-09-21 22:09:59,230 - training_jobs - DEBUG - training trains/task_3.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd0': 50,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'k': 10,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 3,
 'weight_decay': '1e-4'}
2019-09-21 22:09:59,245 - training_jobs - DEBUG - training with: 
2019-09-21 22:09:59,245 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:09:59,245 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:09:59,245 - training_jobs - DEBUG - 
2019-09-21 22:09:59,245 - training_jobs - DEBUG - ggnn training
2019-09-21 22:10:00,352 - training_jobs - DEBUG -  saving results to results/20190921_221000_ggnn_.json
2019-09-21 22:10:00,352 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 22:10:03,428 - training_jobs - ERROR - Error with trains/task_3.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1130, in final_model_train
    model = modelclass(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 279, in __init__
    self.sage1 = SAGEConv(in_channels=d0, out_channels=d1, num_layers=num_layers,normalize=True, bias=True)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/sage_conv.py", line 34, in __init__
    super(SAGEConv, self).__init__(aggr='mean', **kwargs)
TypeError: __init__() got an unexpected keyword argument 'num_layers'
2019-09-21 22:10:06,199 - training_jobs - DEBUG - training trains/task_1.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd0': 50,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k': 10,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 3,
 'weight_decay': '1e-4'}
2019-09-21 22:10:06,213 - training_jobs - DEBUG - training with: 
2019-09-21 22:10:06,213 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:10:06,213 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:10:06,213 - training_jobs - DEBUG - 
2019-09-21 22:10:06,213 - training_jobs - DEBUG - ggnn training
2019-09-21 22:10:07,381 - training_jobs - DEBUG -  saving results to results/20190921_221007_ggnn_.json
2019-09-21 22:10:07,381 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 22:10:10,493 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1130, in final_model_train
    model = modelclass(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 279, in __init__
    self.sage1 = SAGEConv(in_channels=d0, out_channels=d1, num_layers=num_layers,normalize=True, bias=True)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/sage_conv.py", line 34, in __init__
    super(SAGEConv, self).__init__(aggr='mean', **kwargs)
TypeError: __init__() got an unexpected keyword argument 'num_layers'
2019-09-21 22:10:43,210 - training_jobs - DEBUG - ('tasks/gnn_inspect_dataset', '.yaml')
2019-09-21 22:10:46,250 - training_jobs - DEBUG - training trains/task_2.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd0': 50,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'k': 10,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 22:10:46,265 - training_jobs - DEBUG - training with: 
2019-09-21 22:10:46,265 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:10:46,265 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:10:46,265 - training_jobs - DEBUG - 
2019-09-21 22:10:46,265 - training_jobs - DEBUG - ggnn training
2019-09-21 22:10:47,298 - training_jobs - DEBUG -  saving results to results/20190921_221047_ggnn_.json
2019-09-21 22:10:47,299 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 22:11:01,445 - training_jobs - ERROR - Error with trains/task_2.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1143, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 961, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 866, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 290, in forward
    x = self.global_pool(x, batch_vector,k=self.d0)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 591, in __getattr__
    type(self).__name__, name))
AttributeError: 'SAGE01_sort01' object has no attribute 'd0'
2019-09-21 22:11:04,290 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd0': 50,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k': 10,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 22:11:04,304 - training_jobs - DEBUG - training with: 
2019-09-21 22:11:04,304 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:11:04,304 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:11:04,304 - training_jobs - DEBUG - 
2019-09-21 22:11:04,304 - training_jobs - DEBUG - ggnn training
2019-09-21 22:11:05,492 - training_jobs - DEBUG -  saving results to results/20190921_221105_ggnn_.json
2019-09-21 22:11:05,492 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 22:11:14,904 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1143, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 961, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 866, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 290, in forward
    x = self.global_pool(x, batch_vector,k=self.d0)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 591, in __getattr__
    type(self).__name__, name))
AttributeError: 'SAGE01_sort01' object has no attribute 'd0'
2019-09-21 22:11:18,718 - training_jobs - DEBUG - training trains/task_3.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd0': 50,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'k': 10,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 3,
 'weight_decay': '1e-4'}
2019-09-21 22:11:18,735 - training_jobs - DEBUG - training with: 
2019-09-21 22:11:18,735 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:11:18,735 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:11:18,735 - training_jobs - DEBUG - 
2019-09-21 22:11:18,735 - training_jobs - DEBUG - ggnn training
2019-09-21 22:11:20,031 - training_jobs - DEBUG -  saving results to results/20190921_221120_ggnn_.json
2019-09-21 22:11:20,031 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 22:11:27,164 - training_jobs - ERROR - Error with trains/task_3.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1143, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 961, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 866, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 290, in forward
    x = self.global_pool(x, batch_vector,k=self.d0)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 591, in __getattr__
    type(self).__name__, name))
AttributeError: 'SAGE01_sort01' object has no attribute 'd0'
2019-09-21 22:11:31,092 - training_jobs - DEBUG - training trains/task_1.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd0': 50,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k': 10,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 3,
 'weight_decay': '1e-4'}
2019-09-21 22:11:31,106 - training_jobs - DEBUG - training with: 
2019-09-21 22:11:31,106 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:11:31,106 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:11:31,106 - training_jobs - DEBUG - 
2019-09-21 22:11:31,106 - training_jobs - DEBUG - ggnn training
2019-09-21 22:11:32,196 - training_jobs - DEBUG -  saving results to results/20190921_221132_ggnn_.json
2019-09-21 22:11:32,196 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 22:11:38,532 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1143, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 961, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 866, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 290, in forward
    x = self.global_pool(x, batch_vector,k=self.d0)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 591, in __getattr__
    type(self).__name__, name))
AttributeError: 'SAGE01_sort01' object has no attribute 'd0'
2019-09-21 22:12:03,696 - training_jobs - DEBUG - ('tasks/gnn_inspect_dataset', '.yaml')
2019-09-21 22:12:06,658 - training_jobs - DEBUG - training trains/task_2.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd0': 50,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'k': 10,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 22:12:06,674 - training_jobs - DEBUG - training with: 
2019-09-21 22:12:06,674 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:12:06,674 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:12:06,674 - training_jobs - DEBUG - 
2019-09-21 22:12:06,674 - training_jobs - DEBUG - ggnn training
2019-09-21 22:12:07,865 - training_jobs - DEBUG -  saving results to results/20190921_221207_ggnn_.json
2019-09-21 22:12:07,865 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 22:12:15,378 - training_jobs - ERROR - Error with trains/task_2.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1143, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 961, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 866, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 293, in forward
    x = self.sage1(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/sage_conv.py", line 59, in forward
    x = torch.matmul(x, self.weight)
RuntimeError: size mismatch, m1: [8 x 200], m2: [50 x 50] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273
2019-09-21 22:12:17,916 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd0': 50,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k': 10,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 22:12:17,928 - training_jobs - DEBUG - training with: 
2019-09-21 22:12:17,928 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:12:17,928 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:12:17,928 - training_jobs - DEBUG - 
2019-09-21 22:12:17,928 - training_jobs - DEBUG - ggnn training
2019-09-21 22:12:18,939 - training_jobs - DEBUG -  saving results to results/20190921_221218_ggnn_.json
2019-09-21 22:12:18,939 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 22:12:25,396 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1143, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 961, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 866, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 293, in forward
    x = self.sage1(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/sage_conv.py", line 59, in forward
    x = torch.matmul(x, self.weight)
RuntimeError: size mismatch, m1: [8 x 200], m2: [50 x 50] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273
2019-09-21 22:12:29,449 - training_jobs - DEBUG - training trains/task_3.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd0': 50,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'k': 10,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 3,
 'weight_decay': '1e-4'}
2019-09-21 22:12:29,463 - training_jobs - DEBUG - training with: 
2019-09-21 22:12:29,463 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:12:29,463 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:12:29,463 - training_jobs - DEBUG - 
2019-09-21 22:12:29,463 - training_jobs - DEBUG - ggnn training
2019-09-21 22:12:30,583 - training_jobs - DEBUG -  saving results to results/20190921_221230_ggnn_.json
2019-09-21 22:12:30,583 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 22:12:36,852 - training_jobs - ERROR - Error with trains/task_3.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1143, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 961, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 866, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 293, in forward
    x = self.sage1(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/sage_conv.py", line 59, in forward
    x = torch.matmul(x, self.weight)
RuntimeError: size mismatch, m1: [8 x 200], m2: [50 x 50] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273
2019-09-21 22:12:39,689 - training_jobs - DEBUG - training trains/task_1.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd0': 50,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k': 10,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 3,
 'weight_decay': '1e-4'}
2019-09-21 22:12:39,703 - training_jobs - DEBUG - training with: 
2019-09-21 22:12:39,703 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:12:39,703 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:12:39,703 - training_jobs - DEBUG - 
2019-09-21 22:12:39,703 - training_jobs - DEBUG - ggnn training
2019-09-21 22:12:40,744 - training_jobs - DEBUG -  saving results to results/20190921_221240_ggnn_.json
2019-09-21 22:12:40,744 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 22:12:47,011 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1143, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 961, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 866, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 293, in forward
    x = self.sage1(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/sage_conv.py", line 59, in forward
    x = torch.matmul(x, self.weight)
RuntimeError: size mismatch, m1: [8 x 200], m2: [50 x 50] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273
2019-09-21 22:15:16,267 - training_jobs - DEBUG - ('tasks/gnn_inspect_dataset', '.yaml')
2019-09-21 22:15:18,728 - training_jobs - DEBUG - training trains/task_2.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'k1': 3,
 'k2': 4,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 22:15:18,740 - training_jobs - DEBUG - training with: 
2019-09-21 22:15:18,740 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:15:18,740 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:15:18,740 - training_jobs - DEBUG - 
2019-09-21 22:15:18,740 - training_jobs - DEBUG - ggnn training
2019-09-21 22:15:19,722 - training_jobs - DEBUG -  saving results to results/20190921_221519_ggnn_.json
2019-09-21 22:15:19,722 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 22:15:22,468 - training_jobs - ERROR - Error with trains/task_2.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1130, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'k1'
2019-09-21 22:15:24,924 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 3,
 'k2': 4,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 22:15:24,937 - training_jobs - DEBUG - training with: 
2019-09-21 22:15:24,937 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:15:24,937 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:15:24,937 - training_jobs - DEBUG - 
2019-09-21 22:15:24,937 - training_jobs - DEBUG - ggnn training
2019-09-21 22:15:25,946 - training_jobs - DEBUG -  saving results to results/20190921_221525_ggnn_.json
2019-09-21 22:15:25,946 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 22:15:28,803 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1130, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'k1'
2019-09-21 22:15:41,060 - training_jobs - DEBUG - ('tasks/gnn_inspect_dataset', '.yaml')
2019-09-21 22:15:43,612 - training_jobs - DEBUG - training trains/task_2.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'k1': 3,
 'k2': 4,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 22:15:43,625 - training_jobs - DEBUG - training with: 
2019-09-21 22:15:43,625 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:15:43,625 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:15:43,625 - training_jobs - DEBUG - 
2019-09-21 22:15:43,625 - training_jobs - DEBUG - ggnn training
2019-09-21 22:15:44,632 - training_jobs - DEBUG -  saving results to results/20190921_221544_ggnn_.json
2019-09-21 22:15:44,632 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 22:15:47,422 - training_jobs - ERROR - Error with trains/task_2.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1130, in final_model_train
    model = modelclass(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 280, in __init__
    self.sage1 = SAGEConv(in_channels=d0, out_channels=d1,normalize=True, bias=True)
NameError: name 'd0' is not defined
2019-09-21 22:15:49,903 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 3,
 'k2': 4,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 22:15:49,915 - training_jobs - DEBUG - training with: 
2019-09-21 22:15:49,915 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:15:49,915 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:15:49,915 - training_jobs - DEBUG - 
2019-09-21 22:15:49,915 - training_jobs - DEBUG - ggnn training
2019-09-21 22:15:51,000 - training_jobs - DEBUG -  saving results to results/20190921_221551_ggnn_.json
2019-09-21 22:15:51,000 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 22:15:53,819 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1130, in final_model_train
    model = modelclass(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 280, in __init__
    self.sage1 = SAGEConv(in_channels=d0, out_channels=d1,normalize=True, bias=True)
NameError: name 'd0' is not defined
2019-09-21 22:15:56,347 - training_jobs - DEBUG - training trains/task_3.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'k1': 3,
 'k2': 4,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 3,
 'weight_decay': '1e-4'}
2019-09-21 22:15:56,360 - training_jobs - DEBUG - training with: 
2019-09-21 22:15:56,360 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:15:56,360 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:15:56,360 - training_jobs - DEBUG - 
2019-09-21 22:15:56,361 - training_jobs - DEBUG - ggnn training
2019-09-21 22:15:57,510 - training_jobs - DEBUG -  saving results to results/20190921_221557_ggnn_.json
2019-09-21 22:15:57,510 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 22:16:00,396 - training_jobs - ERROR - Error with trains/task_3.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1130, in final_model_train
    model = modelclass(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 280, in __init__
    self.sage1 = SAGEConv(in_channels=k1, out_channels=d1,normalize=True, bias=True)
NameError: name 'd0' is not defined
2019-09-21 22:16:02,914 - training_jobs - DEBUG - training trains/task_1.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 3,
 'k2': 4,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 3,
 'weight_decay': '1e-4'}
2019-09-21 22:16:02,927 - training_jobs - DEBUG - training with: 
2019-09-21 22:16:02,927 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:16:02,927 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:16:02,927 - training_jobs - DEBUG - 
2019-09-21 22:16:02,927 - training_jobs - DEBUG - ggnn training
2019-09-21 22:16:03,536 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 436, in get
    data = torch.load(filename)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/serialization.py", line 386, in load
    return _load(f, map_location, pickle_module, **pickle_load_args)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/serialization.py", line 573, in _load
    result = unpickler.load()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/serialization.py", line 533, in persistent_load
    if root_key not in deserialized_objects:
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 276, in num_classes
    [self.get(j).y[0].item() for j in range(self.__len__() )]
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 276, in <listcomp>
    [self.get(j).y[0].item() for j in range(self.__len__() )]
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 452, in get
    raise IndexError("error getting: ", idx)
IndexError: ('error getting: ', 1919)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 436, in get
    data = torch.load(filename)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/serialization.py", line 386, in load
    return _load(f, map_location, pickle_module, **pickle_load_args)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/serialization.py", line 573, in _load
    result = unpickler.load()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/_utils.py", line 135, in _rebuild_tensor_v2
    tensor = _rebuild_tensor(storage, storage_offset, size, stride)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/_utils.py", line 130, in _rebuild_tensor
    t = torch.tensor([], dtype=storage.dtype, device=storage.device)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 577, in training_dispatcher
    num_classes = train_dataset.num_classes
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 288, in num_classes
    list_classes.append(self.get(j).y)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 452, in get
    raise IndexError("error getting: ", idx)
IndexError: ('error getting: ', 593)
2019-09-21 22:16:17,935 - training_jobs - DEBUG - ('tasks/gnn_inspect_dataset', '.yaml')
2019-09-21 22:16:20,401 - training_jobs - DEBUG - training trains/task_2.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'k1': 3,
 'k2': 4,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 22:16:20,414 - training_jobs - DEBUG - training with: 
2019-09-21 22:16:20,414 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:16:20,414 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:16:20,414 - training_jobs - DEBUG - 
2019-09-21 22:16:20,414 - training_jobs - DEBUG - ggnn training
2019-09-21 22:16:21,486 - training_jobs - DEBUG -  saving results to results/20190921_221621_ggnn_.json
2019-09-21 22:16:21,486 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 22:16:27,390 - training_jobs - ERROR - Error with trains/task_2.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1143, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 961, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 866, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 293, in forward
    x = self.sage1(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/sage_conv.py", line 59, in forward
    x = torch.matmul(x, self.weight)
RuntimeError: size mismatch, m1: [8 x 12], m2: [3 x 50] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273
2019-09-21 22:16:32,010 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 3,
 'k2': 4,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 22:16:32,023 - training_jobs - DEBUG - training with: 
2019-09-21 22:16:32,024 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:16:32,024 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:16:32,024 - training_jobs - DEBUG - 
2019-09-21 22:16:32,024 - training_jobs - DEBUG - ggnn training
2019-09-21 22:16:33,204 - training_jobs - DEBUG -  saving results to results/20190921_221633_ggnn_.json
2019-09-21 22:16:33,205 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 22:16:39,440 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1143, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 961, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 866, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 293, in forward
    x = self.sage1(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/sage_conv.py", line 59, in forward
    x = torch.matmul(x, self.weight)
RuntimeError: size mismatch, m1: [8 x 12], m2: [3 x 50] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273
2019-09-21 22:16:42,142 - training_jobs - DEBUG - training trains/task_3.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'k1': 3,
 'k2': 4,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 3,
 'weight_decay': '1e-4'}
2019-09-21 22:16:42,154 - training_jobs - DEBUG - training with: 
2019-09-21 22:16:42,154 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:16:42,154 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:16:42,154 - training_jobs - DEBUG - 
2019-09-21 22:16:42,154 - training_jobs - DEBUG - ggnn training
2019-09-21 22:16:43,310 - training_jobs - DEBUG -  saving results to results/20190921_221643_ggnn_.json
2019-09-21 22:16:43,310 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 22:16:49,479 - training_jobs - ERROR - Error with trains/task_3.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1143, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 961, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 866, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 293, in forward
    x = self.sage1(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/sage_conv.py", line 59, in forward
    x = torch.matmul(x, self.weight)
RuntimeError: size mismatch, m1: [8 x 12], m2: [3 x 50] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273
2019-09-21 22:16:52,188 - training_jobs - DEBUG - training trains/task_1.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 3,
 'k2': 4,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 3,
 'weight_decay': '1e-4'}
2019-09-21 22:16:52,201 - training_jobs - DEBUG - training with: 
2019-09-21 22:16:52,201 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:16:52,201 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:16:52,201 - training_jobs - DEBUG - 
2019-09-21 22:16:52,201 - training_jobs - DEBUG - ggnn training
2019-09-21 22:16:53,345 - training_jobs - DEBUG -  saving results to results/20190921_221653_ggnn_.json
2019-09-21 22:16:53,345 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 22:16:59,534 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1143, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 961, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 866, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 293, in forward
    x = self.sage1(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/sage_conv.py", line 59, in forward
    x = torch.matmul(x, self.weight)
RuntimeError: size mismatch, m1: [8 x 12], m2: [3 x 50] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273
2019-09-21 22:18:18,908 - training_jobs - DEBUG - training trains/task_2.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'k1': 3,
 'k2': 4,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 22:18:18,920 - training_jobs - DEBUG - training with: 
2019-09-21 22:18:18,920 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:18:18,920 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:18:18,920 - training_jobs - DEBUG - 
2019-09-21 22:18:18,920 - training_jobs - DEBUG - ggnn training
2019-09-21 22:18:19,908 - training_jobs - DEBUG -  saving results to results/20190921_221819_ggnn_.json
2019-09-21 22:18:19,908 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 22:18:25,711 - training_jobs - ERROR - Error with trains/task_2.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1143, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 961, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 866, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 293, in forward
    x = self.sage1(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/sage_conv.py", line 59, in forward
    x = torch.matmul(x, self.weight)
RuntimeError: size mismatch, m1: [8 x 12], m2: [3 x 50] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273
2019-09-21 22:18:28,237 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 50,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 3,
 'k2': 4,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 22:18:28,250 - training_jobs - DEBUG - training with: 
2019-09-21 22:18:28,250 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:18:28,250 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:18:28,250 - training_jobs - DEBUG - 
2019-09-21 22:18:28,250 - training_jobs - DEBUG - ggnn training
2019-09-21 22:18:29,379 - training_jobs - DEBUG -  saving results to results/20190921_221829_ggnn_.json
2019-09-21 22:18:29,379 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 22:18:35,337 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1143, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 961, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 866, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 293, in forward
    x = self.sage1(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/sage_conv.py", line 59, in forward
    x = torch.matmul(x, self.weight)
RuntimeError: size mismatch, m1: [8 x 12], m2: [3 x 50] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273
2019-09-21 22:19:32,401 - training_jobs - DEBUG - ('tasks/gnn_inspect_dataset', '.yaml')
2019-09-21 22:19:34,869 - training_jobs - DEBUG - training trains/task_2.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 11,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'k1': 3,
 'k2': 4,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 22:19:34,882 - training_jobs - DEBUG - training with: 
2019-09-21 22:19:34,882 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:19:34,882 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:19:34,882 - training_jobs - DEBUG - 
2019-09-21 22:19:34,883 - training_jobs - DEBUG - ggnn training
2019-09-21 22:19:35,880 - training_jobs - DEBUG -  saving results to results/20190921_221935_ggnn_.json
2019-09-21 22:19:35,880 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 22:19:41,777 - training_jobs - ERROR - Error with trains/task_2.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1143, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 961, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 866, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 293, in forward
    x = self.sage1(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/sage_conv.py", line 59, in forward
    x = torch.matmul(x, self.weight)
RuntimeError: size mismatch, m1: [8 x 12], m2: [3 x 11] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273
2019-09-21 22:19:44,471 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 11,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 3,
 'k2': 4,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 22:19:44,485 - training_jobs - DEBUG - training with: 
2019-09-21 22:19:44,485 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:19:44,485 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:19:44,485 - training_jobs - DEBUG - 
2019-09-21 22:19:44,485 - training_jobs - DEBUG - ggnn training
2019-09-21 22:19:45,587 - training_jobs - DEBUG -  saving results to results/20190921_221945_ggnn_.json
2019-09-21 22:19:45,587 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 22:19:51,992 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1143, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 961, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 866, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 293, in forward
    x = self.sage1(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/sage_conv.py", line 59, in forward
    x = torch.matmul(x, self.weight)
RuntimeError: size mismatch, m1: [8 x 12], m2: [3 x 11] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273
2019-09-21 22:19:54,609 - training_jobs - DEBUG - training trains/task_3.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 11,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'k1': 3,
 'k2': 4,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 3,
 'weight_decay': '1e-4'}
2019-09-21 22:19:54,621 - training_jobs - DEBUG - training with: 
2019-09-21 22:19:54,621 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:19:54,621 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:19:54,621 - training_jobs - DEBUG - 
2019-09-21 22:19:54,621 - training_jobs - DEBUG - ggnn training
2019-09-21 22:19:55,623 - training_jobs - DEBUG -  saving results to results/20190921_221955_ggnn_.json
2019-09-21 22:19:55,623 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 22:20:01,692 - training_jobs - ERROR - Error with trains/task_3.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1143, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 961, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 866, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 293, in forward
    x = self.sage1(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/sage_conv.py", line 59, in forward
    x = torch.matmul(x, self.weight)
RuntimeError: size mismatch, m1: [8 x 12], m2: [3 x 11] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273
2019-09-21 22:20:04,545 - training_jobs - DEBUG - training trains/task_1.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 11,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 3,
 'k2': 4,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 3,
 'weight_decay': '1e-4'}
2019-09-21 22:20:04,558 - training_jobs - DEBUG - training with: 
2019-09-21 22:20:04,558 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:20:04,558 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:20:04,558 - training_jobs - DEBUG - 
2019-09-21 22:20:04,558 - training_jobs - DEBUG - ggnn training
2019-09-21 22:20:05,732 - training_jobs - DEBUG -  saving results to results/20190921_222005_ggnn_.json
2019-09-21 22:20:05,732 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 22:20:12,013 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1143, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 961, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 866, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 293, in forward
    x = self.sage1(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/sage_conv.py", line 59, in forward
    x = torch.matmul(x, self.weight)
RuntimeError: size mismatch, m1: [8 x 12], m2: [3 x 11] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273
2019-09-21 22:21:06,461 - training_jobs - DEBUG - ('tasks/gnn_inspect_dataset', '.yaml')
2019-09-21 22:21:08,951 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd1': 11,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 3,
 'k2': 4,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 22:21:08,972 - training_jobs - DEBUG - training with: 
2019-09-21 22:21:08,972 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:21:08,972 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:21:08,972 - training_jobs - DEBUG - 
2019-09-21 22:21:08,972 - training_jobs - DEBUG - ggnn training
2019-09-21 22:21:10,015 - training_jobs - DEBUG -  saving results to results/20190921_222110_ggnn_.json
2019-09-21 22:21:10,016 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 22:21:16,036 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1143, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 961, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 866, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 293, in forward
    x = self.sage1(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/sage_conv.py", line 59, in forward
    x = torch.matmul(x, self.weight)
RuntimeError: size mismatch, m1: [1 x 12], m2: [3 x 11] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273
2019-09-21 22:21:46,247 - training_jobs - DEBUG - ('tasks/gnn_inspect_dataset', '.yaml')
2019-09-21 22:21:49,152 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 11,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 12,
 'k2': 4,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 22:21:49,165 - training_jobs - DEBUG - training with: 
2019-09-21 22:21:49,165 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:21:49,165 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:21:49,165 - training_jobs - DEBUG - 
2019-09-21 22:21:49,165 - training_jobs - DEBUG - ggnn training
2019-09-21 22:21:50,314 - training_jobs - DEBUG -  saving results to results/20190921_222150_ggnn_.json
2019-09-21 22:21:50,314 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 22:21:56,506 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1143, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 961, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 866, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 293, in forward
    x = self.sage1(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/sage_conv.py", line 59, in forward
    x = torch.matmul(x, self.weight)
RuntimeError: size mismatch, m1: [8 x 48], m2: [12 x 11] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273
2019-09-21 22:22:45,710 - training_jobs - DEBUG - ('tasks/gnn_inspect_dataset', '.yaml')
2019-09-21 22:22:48,136 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd1': 11,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 12,
 'k2': 4,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 22:22:48,149 - training_jobs - DEBUG - training with: 
2019-09-21 22:22:48,149 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:22:48,149 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:22:48,149 - training_jobs - DEBUG - 
2019-09-21 22:22:48,149 - training_jobs - DEBUG - ggnn training
2019-09-21 22:22:49,215 - training_jobs - DEBUG -  saving results to results/20190921_222249_ggnn_.json
2019-09-21 22:22:49,215 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 22:22:55,201 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1143, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 961, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 866, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 293, in forward
    x = self.sage1(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/sage_conv.py", line 59, in forward
    x = torch.matmul(x, self.weight)
RuntimeError: size mismatch, m1: [1 x 48], m2: [12 x 11] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273
2019-09-21 22:23:25,003 - training_jobs - DEBUG - ('tasks/gnn_inspect_dataset', '.yaml')
2019-09-21 22:23:27,921 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd1': 11,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 48,
 'k2': 4,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 22:23:27,937 - training_jobs - DEBUG - training with: 
2019-09-21 22:23:27,937 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:23:27,937 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:23:27,937 - training_jobs - DEBUG - 
2019-09-21 22:23:27,937 - training_jobs - DEBUG - ggnn training
2019-09-21 22:23:29,113 - training_jobs - DEBUG -  saving results to results/20190921_222329_ggnn_.json
2019-09-21 22:23:29,113 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 22:23:35,513 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1143, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 961, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 866, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 293, in forward
    x = self.sage1(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/sage_conv.py", line 59, in forward
    x = torch.matmul(x, self.weight)
RuntimeError: size mismatch, m1: [1 x 192], m2: [48 x 11] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273
2019-09-21 22:25:28,494 - training_jobs - DEBUG - ('tasks/gnn_inspect_dataset', '.yaml')
2019-09-21 22:25:30,915 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 11,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 10,
 'k2': 4,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 22:25:30,927 - training_jobs - DEBUG - training with: 
2019-09-21 22:25:30,927 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:25:30,927 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:25:30,927 - training_jobs - DEBUG - 
2019-09-21 22:25:30,927 - training_jobs - DEBUG - ggnn training
2019-09-21 22:25:31,898 - training_jobs - DEBUG -  saving results to results/20190921_222531_ggnn_.json
2019-09-21 22:25:31,898 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 22:25:38,141 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1131, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-21 22:26:03,824 - training_jobs - DEBUG - ('tasks/gnn_inspect_dataset', '.yaml')
2019-09-21 22:26:06,387 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 11,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 10,
 'k2': 4,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 22:26:06,399 - training_jobs - DEBUG - training with: 
2019-09-21 22:26:06,399 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:26:06,399 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:26:06,399 - training_jobs - DEBUG - 
2019-09-21 22:26:06,399 - training_jobs - DEBUG - ggnn training
2019-09-21 22:26:07,372 - training_jobs - DEBUG -  saving results to results/20190921_222607_ggnn_.json
2019-09-21 22:26:07,372 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 22:26:13,116 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1143, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 961, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 866, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 293, in forward
    x = self.sage1(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/sage_conv.py", line 59, in forward
    x = torch.matmul(x, self.weight)
RuntimeError: size mismatch, m1: [8 x 40], m2: [10 x 11] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273
2019-09-21 22:27:31,843 - training_jobs - DEBUG - ('tasks/gnn_inspect_dataset', '.yaml')
2019-09-21 22:27:34,404 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd1': 11,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 10,
 'k2': 4,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 22:27:34,416 - training_jobs - DEBUG - training with: 
2019-09-21 22:27:34,416 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 22:27:34,416 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 22:27:34,417 - training_jobs - DEBUG - 
2019-09-21 22:27:34,417 - training_jobs - DEBUG - ggnn training
2019-09-21 22:27:35,397 - training_jobs - DEBUG -  saving results to results/20190921_222735_ggnn_.json
2019-09-21 22:27:35,397 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 22:27:41,408 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1131, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-21 23:07:29,247 - training_jobs - DEBUG - ('tasks/gnn_inspect_dataset', '.yaml')
2019-09-21 23:07:31,695 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd0': 20,
 'd1': 11,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 5,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 23:07:31,707 - training_jobs - DEBUG - training with: 
2019-09-21 23:07:31,707 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 23:07:31,707 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 23:07:31,707 - training_jobs - DEBUG - 
2019-09-21 23:07:31,707 - training_jobs - DEBUG - ggnn training
2019-09-21 23:07:32,688 - training_jobs - DEBUG -  saving results to results/20190921_230732_ggnn_.json
2019-09-21 23:07:32,688 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 23:07:38,763 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1131, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-21 23:08:52,669 - training_jobs - DEBUG - ('tasks/gnn_inspect_dataset', '.yaml')
2019-09-21 23:08:55,108 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd0': 20,
 'd1': 11,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 5,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 23:08:55,121 - training_jobs - DEBUG - training with: 
2019-09-21 23:08:55,121 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 23:08:55,121 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 23:08:55,121 - training_jobs - DEBUG - 
2019-09-21 23:08:55,121 - training_jobs - DEBUG - ggnn training
2019-09-21 23:08:56,101 - training_jobs - DEBUG -  saving results to results/20190921_230856_ggnn_.json
2019-09-21 23:08:56,101 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 23:09:02,067 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1143, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 961, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 879, in train_model_GGNN
    loss.backward()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 118, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py", line 93, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
2019-09-21 23:09:47,356 - training_jobs - DEBUG - ('tasks/gnn_inspect_dataset', '.yaml')
2019-09-21 23:09:49,785 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd0': 20,
 'd1': 11,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 5,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 23:09:49,798 - training_jobs - DEBUG - training with: 
2019-09-21 23:09:49,798 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 23:09:49,798 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 23:09:49,798 - training_jobs - DEBUG - 
2019-09-21 23:09:49,798 - training_jobs - DEBUG - ggnn training
2019-09-21 23:09:50,785 - training_jobs - DEBUG -  saving results to results/20190921_230950_ggnn_.json
2019-09-21 23:09:50,785 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 23:09:57,008 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1131, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-21 23:11:40,943 - training_jobs - DEBUG - ('tasks/gnn_inspect_dataset', '.yaml')
2019-09-21 23:11:43,416 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 8,
 'd0': 20,
 'd1': 11,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 5,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 23:11:43,428 - training_jobs - DEBUG - training with: 
2019-09-21 23:11:43,428 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 23:11:43,428 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 23:11:43,428 - training_jobs - DEBUG - 
2019-09-21 23:11:43,428 - training_jobs - DEBUG - ggnn training
2019-09-21 23:11:44,521 - training_jobs - DEBUG -  saving results to results/20190921_231144_ggnn_.json
2019-09-21 23:11:44,521 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 23:11:50,793 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1131, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-21 23:12:14,872 - training_jobs - DEBUG - ('tasks/gnn_inspect_dataset', '.yaml')
2019-09-21 23:12:17,421 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 2,
 'd0': 20,
 'd1': 11,
 'd2': 20,
 'd3': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 5,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 23:12:17,433 - training_jobs - DEBUG - training with: 
2019-09-21 23:12:17,434 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 23:12:17,434 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 23:12:17,434 - training_jobs - DEBUG - 
2019-09-21 23:12:17,434 - training_jobs - DEBUG - ggnn training
2019-09-21 23:12:18,522 - training_jobs - DEBUG -  saving results to results/20190921_231218_ggnn_.json
2019-09-21 23:12:18,522 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 23:12:24,341 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1131, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-21 23:13:02,942 - training_jobs - DEBUG - ('tasks/gnn_inspect_dataset', '.yaml')
2019-09-21 23:13:05,600 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 2,
 'd0': 5,
 'd1': 5,
 'd2': 5,
 'd3': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 5,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 23:13:05,612 - training_jobs - DEBUG - training with: 
2019-09-21 23:13:05,613 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 23:13:05,613 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 23:13:05,613 - training_jobs - DEBUG - 
2019-09-21 23:13:05,613 - training_jobs - DEBUG - ggnn training
2019-09-21 23:13:06,714 - training_jobs - DEBUG -  saving results to results/20190921_231306_ggnn_.json
2019-09-21 23:13:06,714 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 23:13:12,490 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1143, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 961, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 866, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 293, in forward
    x = self.sage1(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/sage_conv.py", line 59, in forward
    x = torch.matmul(x, self.weight)
RuntimeError: size mismatch, m1: [2 x 20], m2: [5 x 5] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273
2019-09-21 23:13:42,860 - training_jobs - DEBUG - ('tasks/gnn_inspect_dataset', '.yaml')
2019-09-21 23:13:45,387 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 2,
 'd0': 20,
 'd1': 5,
 'd2': 5,
 'd3': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 5,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 23:13:45,399 - training_jobs - DEBUG - training with: 
2019-09-21 23:13:45,400 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 23:13:45,400 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 23:13:45,400 - training_jobs - DEBUG - 
2019-09-21 23:13:45,400 - training_jobs - DEBUG - ggnn training
2019-09-21 23:13:46,499 - training_jobs - DEBUG -  saving results to results/20190921_231346_ggnn_.json
2019-09-21 23:13:46,499 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 23:13:52,489 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1131, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-21 23:27:48,807 - training_jobs - DEBUG - ('tasks/gnn_inspect_dataset', '.yaml')
2019-09-21 23:27:51,284 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 2,
 'd0': 16,
 'd1': 5,
 'd2': 5,
 'd3': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 23:27:51,298 - training_jobs - DEBUG - training with: 
2019-09-21 23:27:51,298 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 23:27:51,298 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 23:27:51,298 - training_jobs - DEBUG - 
2019-09-21 23:27:51,298 - training_jobs - DEBUG - ggnn training
2019-09-21 23:27:52,309 - training_jobs - DEBUG -  saving results to results/20190921_232752_ggnn_.json
2019-09-21 23:27:52,309 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 23:27:58,215 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1131, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-21 23:28:42,931 - training_jobs - DEBUG - ('tasks/gnn_inspect_dataset', '.yaml')
2019-09-21 23:28:45,464 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 2,
 'd0': 4,
 'd1': 5,
 'd2': 5,
 'd3': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 23:28:45,477 - training_jobs - DEBUG - training with: 
2019-09-21 23:28:45,477 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 23:28:45,477 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 23:28:45,477 - training_jobs - DEBUG - 
2019-09-21 23:28:45,477 - training_jobs - DEBUG - ggnn training
2019-09-21 23:28:46,503 - training_jobs - DEBUG -  saving results to results/20190921_232846_ggnn_.json
2019-09-21 23:28:46,503 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 23:28:52,319 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1561, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1385, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1143, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 961, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 866, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 295, in forward
    x = self.sage1(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/sage_conv.py", line 59, in forward
    x = torch.matmul(x, self.weight)
RuntimeError: size mismatch, m1: [2 x 16], m2: [4 x 5] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273
2019-09-21 23:31:15,661 - training_jobs - DEBUG - ('tasks/gnn_inspect_dataset', '.yaml')
2019-09-21 23:31:18,170 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 2,
 'd0': 4,
 'd1': 5,
 'd2': 5,
 'd3': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 23:31:18,183 - training_jobs - DEBUG - training with: 
2019-09-21 23:31:18,183 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 23:31:18,183 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 23:31:18,183 - training_jobs - DEBUG - 
2019-09-21 23:31:18,183 - training_jobs - DEBUG - ggnn training
2019-09-21 23:31:19,260 - training_jobs - DEBUG -  saving results to results/20190921_233119_ggnn_.json
2019-09-21 23:31:19,260 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 23:31:25,162 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1564, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1388, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1146, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 964, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 869, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 295, in forward
    x = self.sage1(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/sage_conv.py", line 59, in forward
    x = torch.matmul(x, self.weight)
RuntimeError: size mismatch, m1: [2 x 16], m2: [4 x 5] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273
2019-09-21 23:32:06,024 - training_jobs - DEBUG - ('tasks/gnn_inspect_dataset', '.yaml')
2019-09-21 23:32:08,453 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 2,
 'd0': 4,
 'd1': 5,
 'd2': 5,
 'd3': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 23:32:08,465 - training_jobs - DEBUG - training with: 
2019-09-21 23:32:08,465 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 23:32:08,465 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 23:32:08,466 - training_jobs - DEBUG - 
2019-09-21 23:32:08,466 - training_jobs - DEBUG - ggnn training
2019-09-21 23:32:09,545 - training_jobs - DEBUG -  saving results to results/20190921_233209_ggnn_.json
2019-09-21 23:32:09,545 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 23:32:16,932 - training_jobs - DEBUG - test_multiple_models
2019-09-21 23:32:16,932 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-21 23:32:17,401 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 646, in training_dispatcher
    models_folder='models/gnn/')
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1817, in test_multiple_models
    testresult = testModel(bmodel, test_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1693, in testModel
    _, pred = model(data).max(dim=1)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 295, in forward
    x = self.sage1(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/sage_conv.py", line 59, in forward
    x = torch.matmul(x, self.weight)
RuntimeError: size mismatch, m1: [1503 x 16], m2: [4 x 5] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273
2019-09-21 23:32:55,619 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 2,
 'd0': 4,
 'd1': 5,
 'd2': 5,
 'd3': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 23:32:55,631 - training_jobs - DEBUG - training with: 
2019-09-21 23:32:55,632 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 23:32:55,632 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 23:32:55,632 - training_jobs - DEBUG - 
2019-09-21 23:32:55,632 - training_jobs - DEBUG - ggnn training
2019-09-21 23:32:56,706 - training_jobs - DEBUG -  saving results to results/20190921_233256_ggnn_.json
2019-09-21 23:32:56,706 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 23:33:03,975 - training_jobs - DEBUG - test_multiple_models
2019-09-21 23:33:03,975 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-21 23:33:04,460 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 646, in training_dispatcher
    models_folder='models/gnn/')
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1817, in test_multiple_models
    testresult = testModel(bmodel, test_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1693, in testModel
    _, pred = model(data).max(dim=1)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 295, in forward
    x = self.sage1(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/sage_conv.py", line 59, in forward
    x = torch.matmul(x, self.weight)
RuntimeError: size mismatch, m1: [1503 x 16], m2: [4 x 5] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273
2019-09-21 23:36:50,099 - training_jobs - DEBUG - ('tasks/gnn_inspect_dataset', '.yaml')
2019-09-21 23:36:52,531 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 2,
 'd0': 16,
 'd1': 5,
 'd2': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 23:36:52,543 - training_jobs - DEBUG - training with: 
2019-09-21 23:36:52,543 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 23:36:52,543 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 23:36:52,543 - training_jobs - DEBUG - 
2019-09-21 23:36:52,544 - training_jobs - DEBUG - ggnn training
2019-09-21 23:36:53,538 - training_jobs - DEBUG -  saving results to results/20190921_233653_ggnn_.json
2019-09-21 23:36:53,538 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 23:36:59,501 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1565, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1389, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1135, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-21 23:39:24,636 - training_jobs - DEBUG - ('tasks/gnn_inspect_dataset', '.yaml')
2019-09-21 23:39:27,248 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 2,
 'd0': 16,
 'd1': 5,
 'd2': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 23:39:27,260 - training_jobs - DEBUG - training with: 
2019-09-21 23:39:27,260 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 23:39:27,261 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 23:39:27,261 - training_jobs - DEBUG - 
2019-09-21 23:39:27,261 - training_jobs - DEBUG - ggnn training
2019-09-21 23:39:28,314 - training_jobs - DEBUG -  saving results to results/20190921_233928_ggnn_.json
2019-09-21 23:39:28,314 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 23:39:34,272 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1565, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1389, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1135, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-21 23:40:24,225 - training_jobs - DEBUG - ('tasks/gnn_inspect_dataset', '.yaml')
2019-09-21 23:40:26,729 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 2,
 'd0': 16,
 'd1': 5,
 'd2': 5,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 23:40:26,742 - training_jobs - DEBUG - training with: 
2019-09-21 23:40:26,742 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 23:40:26,742 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 23:40:26,742 - training_jobs - DEBUG - 
2019-09-21 23:40:26,742 - training_jobs - DEBUG - ggnn training
2019-09-21 23:40:27,754 - training_jobs - DEBUG -  saving results to results/20190921_234027_ggnn_.json
2019-09-21 23:40:27,755 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 23:40:33,828 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1565, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1389, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1135, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-21 23:41:51,571 - training_jobs - DEBUG - ('tasks/gnn_inspect_dataset', '.yaml')
2019-09-21 23:41:54,843 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 2,
 'd0': 16,
 'd1': 5,
 'd2': 16,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 23:41:54,859 - training_jobs - DEBUG - training with: 
2019-09-21 23:41:54,859 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 23:41:54,859 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 23:41:54,859 - training_jobs - DEBUG - 
2019-09-21 23:41:54,859 - training_jobs - DEBUG - ggnn training
2019-09-21 23:41:56,115 - training_jobs - DEBUG -  saving results to results/20190921_234156_ggnn_.json
2019-09-21 23:41:56,115 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 23:42:42,053 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1565, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1389, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1147, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 965, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 881, in train_model_GGNN
    loss = loss_func(out, target)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 204, in forward
    return F.nll_loss(input, target, weight=self.weight, ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1820, in nll_loss
    if input.size(0) != target.size(0):
IndexError: dimension specified as 0 but tensor has no dimensions
2019-09-21 23:54:59,829 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 2,
 'd0': 16,
 'd1': 5,
 'd2': 16,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 23:54:59,841 - training_jobs - DEBUG - training with: 
2019-09-21 23:54:59,841 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 23:54:59,841 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 23:54:59,842 - training_jobs - DEBUG - 
2019-09-21 23:54:59,842 - training_jobs - DEBUG - ggnn training
2019-09-21 23:55:00,861 - training_jobs - DEBUG -  saving results to results/20190921_235500_ggnn_.json
2019-09-21 23:55:00,861 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 23:55:07,168 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1565, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1389, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1135, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-21 23:57:07,948 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 16,
 'd1': 5,
 'd2': 16,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-21 23:57:07,960 - training_jobs - DEBUG - training with: 
2019-09-21 23:57:07,960 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-21 23:57:07,960 - training_jobs - DEBUG - SAGE01_sort01
2019-09-21 23:57:07,960 - training_jobs - DEBUG - 
2019-09-21 23:57:07,960 - training_jobs - DEBUG - ggnn training
2019-09-21 23:57:09,021 - training_jobs - DEBUG -  saving results to results/20190921_235709_ggnn_.json
2019-09-21 23:57:09,021 - training_jobs - DEBUG -  calling modelSelection
2019-09-21 23:57:14,843 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1565, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1389, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1135, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 00:05:05,004 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 16,
 'd1': 16,
 'd2': 16,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 00:05:05,016 - training_jobs - DEBUG - training with: 
2019-09-22 00:05:05,016 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 00:05:05,016 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 00:05:05,016 - training_jobs - DEBUG - 
2019-09-22 00:05:05,016 - training_jobs - DEBUG - ggnn training
2019-09-22 00:05:06,084 - training_jobs - DEBUG -  saving results to results/20190922_000506_ggnn_.json
2019-09-22 00:05:06,084 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 00:05:11,969 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1565, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1389, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1135, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 00:07:15,890 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 16,
 'd1': 16,
 'd2': 16,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 00:07:15,901 - training_jobs - DEBUG - training with: 
2019-09-22 00:07:15,902 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 00:07:15,902 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 00:07:15,902 - training_jobs - DEBUG - 
2019-09-22 00:07:15,902 - training_jobs - DEBUG - ggnn training
2019-09-22 00:07:16,867 - training_jobs - DEBUG -  saving results to results/20190922_000716_ggnn_.json
2019-09-22 00:07:16,867 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 00:07:22,758 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1565, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1389, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1147, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 965, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 870, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 293, in forward
    x = self.ggnn(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 102, in forward
    m = self.propagate('add',edge_index, x=m)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/message_passing.py", line 102, in propagate
    if size[idx] != tmp.size(dim):
RuntimeError: bool value of Tensor with more than one value is ambiguous
2019-09-22 00:07:44,431 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 16,
 'd1': 16,
 'd2': 16,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 00:07:44,444 - training_jobs - DEBUG - training with: 
2019-09-22 00:07:44,444 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 00:07:44,444 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 00:07:44,444 - training_jobs - DEBUG - 
2019-09-22 00:07:44,444 - training_jobs - DEBUG - ggnn training
2019-09-22 00:07:45,447 - training_jobs - DEBUG -  saving results to results/20190922_000745_ggnn_.json
2019-09-22 00:07:45,447 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 00:07:51,276 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1565, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1389, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1147, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 965, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 870, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 293, in forward
    x = self.ggnn(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 102, in forward
    m = self.propagate('add',edge_index, x=m)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/message_passing.py", line 102, in propagate
    if size[idx] != tmp.size(dim):
RuntimeError: bool value of Tensor with more than one value is ambiguous
2019-09-22 00:23:54,212 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 16,
 'd1': 16,
 'd2': 16,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 00:23:54,225 - training_jobs - DEBUG - training with: 
2019-09-22 00:23:54,225 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 00:23:54,225 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 00:23:54,225 - training_jobs - DEBUG - 
2019-09-22 00:23:54,225 - training_jobs - DEBUG - ggnn training
2019-09-22 00:23:55,268 - training_jobs - DEBUG -  saving results to results/20190922_002355_ggnn_.json
2019-09-22 00:23:55,268 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 00:24:01,177 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1565, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1389, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1147, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 965, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 870, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 293, in forward
    x = self.ggnn(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 591, in __getattr__
    type(self).__name__, name))
AttributeError: 'SAGE01_sort01' object has no attribute 'ggnn'
2019-09-22 00:24:17,640 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 16,
 'd1': 16,
 'd2': 16,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 00:24:17,652 - training_jobs - DEBUG - training with: 
2019-09-22 00:24:17,652 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 00:24:17,652 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 00:24:17,652 - training_jobs - DEBUG - 
2019-09-22 00:24:17,652 - training_jobs - DEBUG - ggnn training
2019-09-22 00:24:18,641 - training_jobs - DEBUG -  saving results to results/20190922_002418_ggnn_.json
2019-09-22 00:24:18,641 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 00:24:24,438 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1565, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1389, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1147, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 965, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 870, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 293, in forward
    x = self.ggnn(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 102, in forward
    m = self.propagate('add',edge_index, x=m)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/message_passing.py", line 102, in propagate
    if size[idx] != tmp.size(dim):
RuntimeError: bool value of Tensor with more than one value is ambiguous
2019-09-22 00:25:13,905 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 16,
 'd1': 16,
 'd2': 16,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 00:25:13,917 - training_jobs - DEBUG - training with: 
2019-09-22 00:25:13,917 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 00:25:13,917 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 00:25:13,918 - training_jobs - DEBUG - 
2019-09-22 00:25:13,918 - training_jobs - DEBUG - ggnn training
2019-09-22 00:25:14,908 - training_jobs - DEBUG -  saving results to results/20190922_002514_ggnn_.json
2019-09-22 00:25:14,908 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 00:25:20,829 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1565, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1389, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1147, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 965, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 870, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 293, in forward
    x = self.ggnn(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 102, in forward
    m = self.propagate('add',edge_index, x=m)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/message_passing.py", line 102, in propagate
    if size[idx] != tmp.size(dim):
RuntimeError: bool value of Tensor with more than one value is ambiguous
2019-09-22 00:26:08,781 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 16,
 'd1': 16,
 'd2': 16,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 00:26:08,793 - training_jobs - DEBUG - training with: 
2019-09-22 00:26:08,793 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 00:26:08,793 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 00:26:08,793 - training_jobs - DEBUG - 
2019-09-22 00:26:08,793 - training_jobs - DEBUG - ggnn training
2019-09-22 00:26:09,795 - training_jobs - DEBUG -  saving results to results/20190922_002609_ggnn_.json
2019-09-22 00:26:09,795 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 00:26:15,712 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1565, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1389, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1147, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 965, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 870, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 296, in forward
    x = self.ggnn(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 102, in forward
    m = self.propagate('add',edge_index, x=m)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/message_passing.py", line 102, in propagate
    if size[idx] != tmp.size(dim):
RuntimeError: bool value of Tensor with more than one value is ambiguous
2019-09-22 00:27:59,348 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 16,
 'd1': 4,
 'd2': 16,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 00:27:59,360 - training_jobs - DEBUG - training with: 
2019-09-22 00:27:59,360 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 00:27:59,360 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 00:27:59,360 - training_jobs - DEBUG - 
2019-09-22 00:27:59,360 - training_jobs - DEBUG - ggnn training
2019-09-22 00:28:00,349 - training_jobs - DEBUG -  saving results to results/20190922_002800_ggnn_.json
2019-09-22 00:28:00,349 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 00:28:06,262 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1565, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1389, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1147, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 965, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 870, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 299, in forward
    x = self.ggnn(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 102, in forward
    m = self.propagate('add',edge_index, x=m)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/message_passing.py", line 102, in propagate
    if size[idx] != tmp.size(dim):
RuntimeError: bool value of Tensor with more than one value is ambiguous
2019-09-22 00:29:27,671 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 16,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 00:29:27,683 - training_jobs - DEBUG - training with: 
2019-09-22 00:29:27,683 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 00:29:27,683 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 00:29:27,683 - training_jobs - DEBUG - 
2019-09-22 00:29:27,683 - training_jobs - DEBUG - ggnn training
2019-09-22 00:29:28,663 - training_jobs - DEBUG -  saving results to results/20190922_002928_ggnn_.json
2019-09-22 00:29:28,663 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 00:29:34,489 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1565, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1389, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1135, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 00:31:05,670 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 16,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 00:31:05,682 - training_jobs - DEBUG - training with: 
2019-09-22 00:31:05,682 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 00:31:05,682 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 00:31:05,682 - training_jobs - DEBUG - 
2019-09-22 00:31:05,682 - training_jobs - DEBUG - ggnn training
2019-09-22 00:31:06,672 - training_jobs - DEBUG -  saving results to results/20190922_003106_ggnn_.json
2019-09-22 00:31:06,673 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 00:31:12,698 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1565, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1389, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1147, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 965, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 881, in train_model_GGNN
    loss = loss_func(out, target)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 204, in forward
    return F.nll_loss(input, target, weight=self.weight, ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1820, in nll_loss
    if input.size(0) != target.size(0):
IndexError: dimension specified as 0 but tensor has no dimensions
2019-09-22 00:31:58,885 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 16,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 00:31:58,898 - training_jobs - DEBUG - training with: 
2019-09-22 00:31:58,898 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 00:31:58,898 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 00:31:58,898 - training_jobs - DEBUG - 
2019-09-22 00:31:58,898 - training_jobs - DEBUG - ggnn training
2019-09-22 00:31:59,894 - training_jobs - DEBUG -  saving results to results/20190922_003159_ggnn_.json
2019-09-22 00:31:59,894 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 00:32:06,088 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1565, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1389, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1135, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 00:33:44,984 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 16,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 00:33:44,997 - training_jobs - DEBUG - training with: 
2019-09-22 00:33:44,997 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 00:33:44,997 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 00:33:44,997 - training_jobs - DEBUG - 
2019-09-22 00:33:44,997 - training_jobs - DEBUG - ggnn training
2019-09-22 00:33:45,998 - training_jobs - DEBUG -  saving results to results/20190922_003345_ggnn_.json
2019-09-22 00:33:45,998 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 00:33:52,095 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1565, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1389, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1135, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 00:35:03,029 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 16,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 00:35:03,041 - training_jobs - DEBUG - training with: 
2019-09-22 00:35:03,041 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 00:35:03,041 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 00:35:03,041 - training_jobs - DEBUG - 
2019-09-22 00:35:03,041 - training_jobs - DEBUG - ggnn training
2019-09-22 00:35:04,037 - training_jobs - DEBUG -  saving results to results/20190922_003504_ggnn_.json
2019-09-22 00:35:04,037 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 00:35:10,020 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1565, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1389, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1135, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 00:35:57,721 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 16,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 00:35:57,733 - training_jobs - DEBUG - training with: 
2019-09-22 00:35:57,733 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 00:35:57,733 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 00:35:57,733 - training_jobs - DEBUG - 
2019-09-22 00:35:57,733 - training_jobs - DEBUG - ggnn training
2019-09-22 00:35:58,726 - training_jobs - DEBUG -  saving results to results/20190922_003558_ggnn_.json
2019-09-22 00:35:58,726 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 00:36:04,612 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1565, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1389, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1147, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 965, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 870, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 303, in forward
    pprint(x)
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/pprint.py", line 53, in pprint
    printer.pprint(object)
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/pprint.py", line 139, in pprint
    self._format(object, self._stream, 0, 0, {}, 0)
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/pprint.py", line 161, in _format
    rep = self._repr(object, context, level)
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/pprint.py", line 393, in _repr
    self._depth, level)
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/pprint.py", line 405, in format
    return _safe_repr(object, context, maxlevels, level)
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/pprint.py", line 555, in _safe_repr
    rep = repr(object)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 82, in __repr__
    return torch._tensor_str._str(self)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/_tensor_str.py", line 300, in _str
    tensor_str = _tensor_str(self, indent)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/_tensor_str.py", line 201, in _tensor_str
    formatter = _Formatter(get_summarized_data(self) if summarize else self)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/_tensor_str.py", line 87, in __init__
    nonzero_finite_vals = torch.masked_select(tensor_view, torch.isfinite(tensor_view) & tensor_view.ne(0))
RuntimeError: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/THCReduceAll.cuh:327
2019-09-22 00:37:01,280 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 3,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 00:37:01,293 - training_jobs - DEBUG - training with: 
2019-09-22 00:37:01,293 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 00:37:01,293 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 00:37:01,293 - training_jobs - DEBUG - 
2019-09-22 00:37:01,293 - training_jobs - DEBUG - ggnn training
2019-09-22 00:37:02,298 - training_jobs - DEBUG -  saving results to results/20190922_003702_ggnn_.json
2019-09-22 00:37:02,298 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 00:37:08,152 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1565, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1389, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1135, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 00:39:07,989 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 3,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 00:39:08,000 - training_jobs - DEBUG - training with: 
2019-09-22 00:39:08,000 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 00:39:08,000 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 00:39:08,000 - training_jobs - DEBUG - 
2019-09-22 00:39:08,000 - training_jobs - DEBUG - ggnn training
2019-09-22 00:39:08,929 - training_jobs - DEBUG -  saving results to results/20190922_003908_ggnn_.json
2019-09-22 00:39:08,929 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 00:39:14,390 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1565, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1389, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1135, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 00:43:31,776 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 3,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 00:43:31,786 - training_jobs - DEBUG - training with: 
2019-09-22 00:43:31,786 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 00:43:31,786 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 00:43:31,786 - training_jobs - DEBUG - 
2019-09-22 00:43:31,786 - training_jobs - DEBUG - ggnn training
2019-09-22 00:43:32,688 - training_jobs - DEBUG -  saving results to results/20190922_004332_ggnn_.json
2019-09-22 00:43:32,688 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 00:43:37,920 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1565, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1389, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1135, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 00:44:23,758 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 3,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 00:44:23,768 - training_jobs - DEBUG - training with: 
2019-09-22 00:44:23,768 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 00:44:23,768 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 00:44:23,769 - training_jobs - DEBUG - 
2019-09-22 00:44:23,769 - training_jobs - DEBUG - ggnn training
2019-09-22 00:44:24,677 - training_jobs - DEBUG -  saving results to results/20190922_004424_ggnn_.json
2019-09-22 00:44:24,677 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 00:44:29,915 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1565, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1389, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1135, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 00:46:42,124 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 3,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 00:46:42,134 - training_jobs - DEBUG - training with: 
2019-09-22 00:46:42,134 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 00:46:42,134 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 00:46:42,134 - training_jobs - DEBUG - 
2019-09-22 00:46:42,135 - training_jobs - DEBUG - ggnn training
2019-09-22 00:46:43,040 - training_jobs - DEBUG -  saving results to results/20190922_004643_ggnn_.json
2019-09-22 00:46:43,040 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 00:46:48,446 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1567, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1391, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1137, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 00:48:46,486 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 00:48:46,497 - training_jobs - DEBUG - training with: 
2019-09-22 00:48:46,498 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 00:48:46,498 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 00:48:46,498 - training_jobs - DEBUG - 
2019-09-22 00:48:46,498 - training_jobs - DEBUG - ggnn training
2019-09-22 00:48:47,437 - training_jobs - DEBUG -  saving results to results/20190922_004847_ggnn_.json
2019-09-22 00:48:47,437 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 00:48:52,694 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1567, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1391, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1149, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 967, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 883, in train_model_GGNN
    loss = loss_func(out, target)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 204, in forward
    return F.nll_loss(input, target, weight=self.weight, ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1820, in nll_loss
    if input.size(0) != target.size(0):
IndexError: dimension specified as 0 but tensor has no dimensions
2019-09-22 00:50:08,052 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 00:50:08,063 - training_jobs - DEBUG - training with: 
2019-09-22 00:50:08,063 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 00:50:08,063 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 00:50:08,063 - training_jobs - DEBUG - 
2019-09-22 00:50:08,063 - training_jobs - DEBUG - ggnn training
2019-09-22 00:50:08,965 - training_jobs - DEBUG -  saving results to results/20190922_005008_ggnn_.json
2019-09-22 00:50:08,965 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 00:50:14,253 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1568, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1392, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1138, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 00:53:00,703 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 00:53:00,714 - training_jobs - DEBUG - training with: 
2019-09-22 00:53:00,714 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 00:53:00,714 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 00:53:00,714 - training_jobs - DEBUG - 
2019-09-22 00:53:00,714 - training_jobs - DEBUG - ggnn training
2019-09-22 00:53:01,619 - training_jobs - DEBUG -  saving results to results/20190922_005301_ggnn_.json
2019-09-22 00:53:01,619 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 00:53:06,867 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1568, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1392, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1150, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 968, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 884, in train_model_GGNN
    loss = loss_func(out, target)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 204, in forward
    return F.nll_loss(input, target, weight=self.weight, ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1820, in nll_loss
    if input.size(0) != target.size(0):
IndexError: dimension specified as 0 but tensor has no dimensions
2019-09-22 00:55:43,887 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 2,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 00:55:43,898 - training_jobs - DEBUG - training with: 
2019-09-22 00:55:43,898 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 00:55:43,898 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 00:55:43,898 - training_jobs - DEBUG - 
2019-09-22 00:55:43,898 - training_jobs - DEBUG - ggnn training
2019-09-22 00:55:44,821 - training_jobs - DEBUG -  saving results to results/20190922_005544_ggnn_.json
2019-09-22 00:55:44,822 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 00:55:50,050 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1568, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1392, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1150, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 968, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 871, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 301, in forward
    x = self.sage1(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/sage_conv.py", line 59, in forward
    x = torch.matmul(x, self.weight)
RuntimeError: size mismatch, m1: [4 x 8], m2: [4 x 4] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273
2019-09-22 00:59:09,723 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 00:59:09,734 - training_jobs - DEBUG - training with: 
2019-09-22 00:59:09,735 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 00:59:09,735 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 00:59:09,735 - training_jobs - DEBUG - 
2019-09-22 00:59:09,735 - training_jobs - DEBUG - ggnn training
2019-09-22 00:59:10,651 - training_jobs - DEBUG -  saving results to results/20190922_005910_ggnn_.json
2019-09-22 00:59:10,651 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 00:59:15,924 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1569, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1393, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1139, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 01:01:21,531 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 01:01:21,542 - training_jobs - DEBUG - training with: 
2019-09-22 01:01:21,542 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 01:01:21,542 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 01:01:21,542 - training_jobs - DEBUG - 
2019-09-22 01:01:21,542 - training_jobs - DEBUG - ggnn training
2019-09-22 01:01:22,524 - training_jobs - DEBUG -  saving results to results/20190922_010122_ggnn_.json
2019-09-22 01:01:22,524 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 01:01:27,777 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1569, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1393, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1151, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 969, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 884, in train_model_GGNN
    print("target", target.shape)
UnboundLocalError: local variable 'target' referenced before assignment
2019-09-22 01:02:13,803 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 01:02:13,815 - training_jobs - DEBUG - training with: 
2019-09-22 01:02:13,815 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 01:02:13,815 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 01:02:13,815 - training_jobs - DEBUG - 
2019-09-22 01:02:13,815 - training_jobs - DEBUG - ggnn training
2019-09-22 01:02:14,720 - training_jobs - DEBUG -  saving results to results/20190922_010214_ggnn_.json
2019-09-22 01:02:14,720 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 01:02:19,947 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1571, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1395, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1153, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 971, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 887, in train_model_GGNN
    loss = loss_func(out, target)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 204, in forward
    return F.nll_loss(input, target, weight=self.weight, ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1822, in nll_loss
    .format(input.size(0), target.size(0)))
ValueError: Expected input batch_size (4) to match target batch_size (1).
2019-09-22 01:06:18,407 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 01:06:18,418 - training_jobs - DEBUG - training with: 
2019-09-22 01:06:18,418 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 01:06:18,418 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 01:06:18,418 - training_jobs - DEBUG - 
2019-09-22 01:06:18,418 - training_jobs - DEBUG - ggnn training
2019-09-22 01:06:19,337 - training_jobs - DEBUG -  saving results to results/20190922_010619_ggnn_.json
2019-09-22 01:06:19,337 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 01:06:24,601 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1571, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1395, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1141, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 01:38:45,142 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 01:38:45,265 - training_jobs - DEBUG - training with: 
2019-09-22 01:38:45,266 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 01:38:45,266 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 01:38:45,266 - training_jobs - DEBUG - 
2019-09-22 01:38:45,266 - training_jobs - DEBUG - ggnn training
2019-09-22 01:38:46,257 - training_jobs - DEBUG -  saving results to results/20190922_013846_ggnn_.json
2019-09-22 01:38:46,257 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 01:38:52,188 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1571, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1395, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1153, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 971, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 872, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 318, in forward
    x = F.log_softmax(x, dim=1)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1316, in log_softmax
    ret = input.log_softmax(dim)
IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)
2019-09-22 01:42:19,757 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 01:42:19,769 - training_jobs - DEBUG - training with: 
2019-09-22 01:42:19,769 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 01:42:19,769 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 01:42:19,769 - training_jobs - DEBUG - 
2019-09-22 01:42:19,769 - training_jobs - DEBUG - ggnn training
2019-09-22 01:42:20,772 - training_jobs - DEBUG -  saving results to results/20190922_014220_ggnn_.json
2019-09-22 01:42:20,772 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 01:42:26,939 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1571, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1395, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1153, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 971, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 872, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 319, in forward
    x = F.log_softmax(x, dim=1)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1316, in log_softmax
    ret = input.log_softmax(dim)
IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)
2019-09-22 01:44:36,289 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 01:44:36,301 - training_jobs - DEBUG - training with: 
2019-09-22 01:44:36,301 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 01:44:36,301 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 01:44:36,301 - training_jobs - DEBUG - 
2019-09-22 01:44:36,301 - training_jobs - DEBUG - ggnn training
2019-09-22 01:44:41,363 - training_jobs - DEBUG -  saving results to results/20190922_014441_ggnn_.json
2019-09-22 01:44:41,363 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 01:44:47,256 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1571, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1395, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1153, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 971, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 872, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 303, in forward
    x = self.sage1(x, edge_index)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/sage_conv.py", line 59, in forward
    x = torch.matmul(x, self.weight)
RuntimeError: size mismatch, m1: [4 x 1], m2: [4 x 4] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273
2019-09-22 01:46:14,850 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 01:46:14,863 - training_jobs - DEBUG - training with: 
2019-09-22 01:46:14,863 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 01:46:14,863 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 01:46:14,863 - training_jobs - DEBUG - 
2019-09-22 01:46:14,863 - training_jobs - DEBUG - ggnn training
2019-09-22 01:46:15,857 - training_jobs - DEBUG -  saving results to results/20190922_014615_ggnn_.json
2019-09-22 01:46:15,857 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 01:46:21,669 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1571, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1395, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1141, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 01:46:57,725 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 01:46:57,736 - training_jobs - DEBUG - training with: 
2019-09-22 01:46:57,736 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 01:46:57,736 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 01:46:57,736 - training_jobs - DEBUG - 
2019-09-22 01:46:57,736 - training_jobs - DEBUG - ggnn training
2019-09-22 01:46:58,768 - training_jobs - DEBUG -  saving results to results/20190922_014658_ggnn_.json
2019-09-22 01:46:58,768 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 01:47:04,660 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1571, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1395, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1153, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 971, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 872, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 319, in forward
    x = F.log_softmax(x, dim=1)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1316, in log_softmax
    ret = input.log_softmax(dim)
IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)
2019-09-22 01:47:44,741 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 01:47:44,753 - training_jobs - DEBUG - training with: 
2019-09-22 01:47:44,753 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 01:47:44,753 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 01:47:44,753 - training_jobs - DEBUG - 
2019-09-22 01:47:44,753 - training_jobs - DEBUG - ggnn training
2019-09-22 01:47:45,738 - training_jobs - DEBUG -  saving results to results/20190922_014745_ggnn_.json
2019-09-22 01:47:45,738 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 01:47:51,591 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1571, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1395, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1141, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 01:48:01,027 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 01:48:01,038 - training_jobs - DEBUG - training with: 
2019-09-22 01:48:01,038 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 01:48:01,039 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 01:48:01,039 - training_jobs - DEBUG - 
2019-09-22 01:48:01,039 - training_jobs - DEBUG - ggnn training
2019-09-22 01:48:02,082 - training_jobs - DEBUG -  saving results to results/20190922_014802_ggnn_.json
2019-09-22 01:48:02,082 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 01:48:07,947 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1571, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1395, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1141, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 01:48:54,192 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 01:48:54,204 - training_jobs - DEBUG - training with: 
2019-09-22 01:48:54,204 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 01:48:54,204 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 01:48:54,204 - training_jobs - DEBUG - 
2019-09-22 01:48:54,204 - training_jobs - DEBUG - ggnn training
2019-09-22 01:48:55,894 - training_jobs - DEBUG -  saving results to results/20190922_014855_ggnn_.json
2019-09-22 01:48:55,894 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 01:49:01,725 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1571, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1395, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1141, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 01:49:14,366 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 01:49:14,378 - training_jobs - DEBUG - training with: 
2019-09-22 01:49:14,378 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 01:49:14,378 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 01:49:14,378 - training_jobs - DEBUG - 
2019-09-22 01:49:14,378 - training_jobs - DEBUG - ggnn training
2019-09-22 01:49:15,452 - training_jobs - DEBUG -  saving results to results/20190922_014915_ggnn_.json
2019-09-22 01:49:15,452 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 01:49:21,786 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1571, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1395, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1153, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 971, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 872, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 317, in forward
    x = self.fc2(x)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/linear.py", line 87, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1371, in linear
    output = input.matmul(weight.t())
RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`
2019-09-22 01:52:13,473 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 01:52:13,484 - training_jobs - DEBUG - training with: 
2019-09-22 01:52:13,485 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 01:52:13,485 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 01:52:13,485 - training_jobs - DEBUG - 
2019-09-22 01:52:13,485 - training_jobs - DEBUG - ggnn training
2019-09-22 01:52:14,512 - training_jobs - DEBUG -  saving results to results/20190922_015214_ggnn_.json
2019-09-22 01:52:14,512 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 01:52:20,617 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1572, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1396, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1142, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 01:52:38,415 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 01:52:38,427 - training_jobs - DEBUG - training with: 
2019-09-22 01:52:38,428 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 01:52:38,428 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 01:52:38,428 - training_jobs - DEBUG - 
2019-09-22 01:52:38,428 - training_jobs - DEBUG - ggnn training
2019-09-22 01:52:39,542 - training_jobs - DEBUG -  saving results to results/20190922_015239_ggnn_.json
2019-09-22 01:52:39,542 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 01:52:45,714 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1572, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1396, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1142, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 01:52:58,993 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 01:52:59,008 - training_jobs - DEBUG - training with: 
2019-09-22 01:52:59,008 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 01:52:59,008 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 01:52:59,008 - training_jobs - DEBUG - 
2019-09-22 01:52:59,008 - training_jobs - DEBUG - ggnn training
2019-09-22 01:53:00,121 - training_jobs - DEBUG -  saving results to results/20190922_015300_ggnn_.json
2019-09-22 01:53:00,121 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 01:53:06,519 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1572, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1396, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1142, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 01:53:40,370 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 01:53:40,381 - training_jobs - DEBUG - training with: 
2019-09-22 01:53:40,381 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 01:53:40,382 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 01:53:40,382 - training_jobs - DEBUG - 
2019-09-22 01:53:40,382 - training_jobs - DEBUG - ggnn training
2019-09-22 01:53:41,486 - training_jobs - DEBUG -  saving results to results/20190922_015341_ggnn_.json
2019-09-22 01:53:41,486 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 01:53:47,587 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1572, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1396, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1154, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 972, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 872, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 320, in forward
    x = F.log_softmax(x, dim=1)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1316, in log_softmax
    ret = input.log_softmax(dim)
RuntimeError: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/ATen/native/cuda/SoftMax.cu:561
2019-09-22 01:54:05,642 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 01:54:05,655 - training_jobs - DEBUG - training with: 
2019-09-22 01:54:05,655 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 01:54:05,655 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 01:54:05,655 - training_jobs - DEBUG - 
2019-09-22 01:54:05,655 - training_jobs - DEBUG - ggnn training
2019-09-22 01:54:06,707 - training_jobs - DEBUG -  saving results to results/20190922_015406_ggnn_.json
2019-09-22 01:54:06,707 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 01:54:12,840 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1572, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1396, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1142, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 01:54:23,727 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 01:54:23,739 - training_jobs - DEBUG - training with: 
2019-09-22 01:54:23,740 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 01:54:23,740 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 01:54:23,740 - training_jobs - DEBUG - 
2019-09-22 01:54:23,740 - training_jobs - DEBUG - ggnn training
2019-09-22 01:54:24,858 - training_jobs - DEBUG -  saving results to results/20190922_015424_ggnn_.json
2019-09-22 01:54:24,858 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 01:54:31,197 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1572, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1396, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1142, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 01:54:50,000 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 01:54:50,019 - training_jobs - DEBUG - training with: 
2019-09-22 01:54:50,020 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 01:54:50,020 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 01:54:50,020 - training_jobs - DEBUG - 
2019-09-22 01:54:50,020 - training_jobs - DEBUG - ggnn training
2019-09-22 01:54:51,121 - training_jobs - DEBUG -  saving results to results/20190922_015451_ggnn_.json
2019-09-22 01:54:51,121 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 01:54:57,545 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1572, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1396, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1142, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 01:55:02,602 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 01:55:02,616 - training_jobs - DEBUG - training with: 
2019-09-22 01:55:02,617 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 01:55:02,617 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 01:55:02,617 - training_jobs - DEBUG - 
2019-09-22 01:55:02,617 - training_jobs - DEBUG - ggnn training
2019-09-22 01:55:03,980 - training_jobs - DEBUG -  saving results to results/20190922_015503_ggnn_.json
2019-09-22 01:55:03,980 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 01:55:10,913 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1572, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1396, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1154, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 972, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 872, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 320, in forward
    x = F.log_softmax(x, dim=1)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1316, in log_softmax
    ret = input.log_softmax(dim)
IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)
2019-09-22 01:56:07,494 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 01:56:07,511 - training_jobs - DEBUG - training with: 
2019-09-22 01:56:07,511 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 01:56:07,511 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 01:56:07,511 - training_jobs - DEBUG - 
2019-09-22 01:56:07,511 - training_jobs - DEBUG - ggnn training
2019-09-22 01:56:08,548 - training_jobs - DEBUG -  saving results to results/20190922_015608_ggnn_.json
2019-09-22 01:56:08,548 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 01:56:14,647 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1572, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1396, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1154, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 972, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 872, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 320, in forward
    pprint(x)
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/pprint.py", line 53, in pprint
    printer.pprint(object)
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/pprint.py", line 139, in pprint
    self._format(object, self._stream, 0, 0, {}, 0)
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/pprint.py", line 161, in _format
    rep = self._repr(object, context, level)
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/pprint.py", line 393, in _repr
    self._depth, level)
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/pprint.py", line 405, in format
    return _safe_repr(object, context, maxlevels, level)
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/pprint.py", line 555, in _safe_repr
    rep = repr(object)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/tensor.py", line 82, in __repr__
    return torch._tensor_str._str(self)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/_tensor_str.py", line 300, in _str
    tensor_str = _tensor_str(self, indent)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/_tensor_str.py", line 201, in _tensor_str
    formatter = _Formatter(get_summarized_data(self) if summarize else self)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/_tensor_str.py", line 87, in __init__
    nonzero_finite_vals = torch.masked_select(tensor_view, torch.isfinite(tensor_view) & tensor_view.ne(0))
RuntimeError: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/THCReduceAll.cuh:327
2019-09-22 01:57:13,055 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 01:57:13,072 - training_jobs - DEBUG - training with: 
2019-09-22 01:57:13,072 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 01:57:13,072 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 01:57:13,072 - training_jobs - DEBUG - 
2019-09-22 01:57:13,073 - training_jobs - DEBUG - ggnn training
2019-09-22 01:57:14,432 - training_jobs - DEBUG -  saving results to results/20190922_015714_ggnn_.json
2019-09-22 01:57:14,432 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 01:57:21,796 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1572, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1396, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1142, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 01:57:40,083 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 01:57:40,097 - training_jobs - DEBUG - training with: 
2019-09-22 01:57:40,097 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 01:57:40,097 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 01:57:40,097 - training_jobs - DEBUG - 
2019-09-22 01:57:40,097 - training_jobs - DEBUG - ggnn training
2019-09-22 01:57:41,243 - training_jobs - DEBUG -  saving results to results/20190922_015741_ggnn_.json
2019-09-22 01:57:41,243 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 01:57:47,850 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1572, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1396, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1142, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 01:59:51,329 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 01:59:51,342 - training_jobs - DEBUG - training with: 
2019-09-22 01:59:51,342 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 01:59:51,342 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 01:59:51,342 - training_jobs - DEBUG - 
2019-09-22 01:59:51,342 - training_jobs - DEBUG - ggnn training
2019-09-22 01:59:52,336 - training_jobs - DEBUG -  saving results to results/20190922_015952_ggnn_.json
2019-09-22 01:59:52,336 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 01:59:58,186 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1572, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1396, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1142, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 02:00:07,899 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 02:00:07,911 - training_jobs - DEBUG - training with: 
2019-09-22 02:00:07,911 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 02:00:07,911 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 02:00:07,911 - training_jobs - DEBUG - 
2019-09-22 02:00:07,911 - training_jobs - DEBUG - ggnn training
2019-09-22 02:00:08,997 - training_jobs - DEBUG -  saving results to results/20190922_020008_ggnn_.json
2019-09-22 02:00:08,997 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 02:00:14,886 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1572, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1396, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1142, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 02:01:15,292 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 02:01:15,303 - training_jobs - DEBUG - training with: 
2019-09-22 02:01:15,303 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 02:01:15,303 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 02:01:15,304 - training_jobs - DEBUG - 
2019-09-22 02:01:15,304 - training_jobs - DEBUG - ggnn training
2019-09-22 02:01:16,302 - training_jobs - DEBUG -  saving results to results/20190922_020116_ggnn_.json
2019-09-22 02:01:16,302 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 02:01:22,385 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1572, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1396, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1142, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 02:01:29,429 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 02:01:29,441 - training_jobs - DEBUG - training with: 
2019-09-22 02:01:29,441 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 02:01:29,441 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 02:01:29,441 - training_jobs - DEBUG - 
2019-09-22 02:01:29,441 - training_jobs - DEBUG - ggnn training
2019-09-22 02:01:30,490 - training_jobs - DEBUG -  saving results to results/20190922_020130_ggnn_.json
2019-09-22 02:01:30,490 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 02:01:36,482 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1572, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1396, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1142, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 02:02:10,217 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 02:02:10,242 - training_jobs - DEBUG - training with: 
2019-09-22 02:02:10,242 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 02:02:10,242 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 02:02:10,242 - training_jobs - DEBUG - 
2019-09-22 02:02:10,242 - training_jobs - DEBUG - ggnn training
2019-09-22 02:02:11,432 - training_jobs - DEBUG -  saving results to results/20190922_020211_ggnn_.json
2019-09-22 02:02:11,433 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 02:02:18,166 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1572, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1396, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1142, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 02:03:57,275 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 02:03:57,287 - training_jobs - DEBUG - training with: 
2019-09-22 02:03:57,288 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 02:03:57,288 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 02:03:57,288 - training_jobs - DEBUG - 
2019-09-22 02:03:57,288 - training_jobs - DEBUG - ggnn training
2019-09-22 02:03:58,292 - training_jobs - DEBUG -  saving results to results/20190922_020358_ggnn_.json
2019-09-22 02:03:58,292 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 02:04:04,238 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1572, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1396, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1142, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 02:04:31,021 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 02:04:31,033 - training_jobs - DEBUG - training with: 
2019-09-22 02:04:31,033 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 02:04:31,033 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 02:04:31,033 - training_jobs - DEBUG - 
2019-09-22 02:04:31,033 - training_jobs - DEBUG - ggnn training
2019-09-22 02:04:32,012 - training_jobs - DEBUG -  saving results to results/20190922_020432_ggnn_.json
2019-09-22 02:04:32,012 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 02:04:38,054 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1572, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1396, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1142, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 02:04:43,442 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 02:04:43,455 - training_jobs - DEBUG - training with: 
2019-09-22 02:04:43,455 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 02:04:43,455 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 02:04:43,455 - training_jobs - DEBUG - 
2019-09-22 02:04:43,455 - training_jobs - DEBUG - ggnn training
2019-09-22 02:04:44,605 - training_jobs - DEBUG -  saving results to results/20190922_020444_ggnn_.json
2019-09-22 02:04:44,605 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 02:04:51,082 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1572, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1396, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1142, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 02:04:58,517 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 02:04:58,529 - training_jobs - DEBUG - training with: 
2019-09-22 02:04:58,529 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 02:04:58,530 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 02:04:58,530 - training_jobs - DEBUG - 
2019-09-22 02:04:58,530 - training_jobs - DEBUG - ggnn training
2019-09-22 02:04:59,635 - training_jobs - DEBUG -  saving results to results/20190922_020459_ggnn_.json
2019-09-22 02:04:59,636 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 02:05:05,877 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1572, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1396, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1154, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 972, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 888, in train_model_GGNN
    loss = loss_func(out, target)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 204, in forward
    return F.nll_loss(input, target, weight=self.weight, ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1818, in nll_loss
    raise ValueError('Expected 2 or more dimensions (got {})'.format(dim))
ValueError: Expected 2 or more dimensions (got 1)
2019-09-22 02:05:53,283 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 02:05:53,303 - training_jobs - DEBUG - training with: 
2019-09-22 02:05:53,303 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 02:05:53,303 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 02:05:53,304 - training_jobs - DEBUG - 
2019-09-22 02:05:53,304 - training_jobs - DEBUG - ggnn training
2019-09-22 02:05:55,070 - training_jobs - DEBUG -  saving results to results/20190922_020555_ggnn_.json
2019-09-22 02:05:55,071 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 02:06:03,505 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1573, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1397, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1155, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 973, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 872, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 317, in forward
    x = self.fc2(x)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/linear.py", line 87, in forward
    return F.linear(input, self.weight, self.bias)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1371, in linear
    output = input.matmul(weight.t())
RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`
2019-09-22 02:06:14,922 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 02:06:14,934 - training_jobs - DEBUG - training with: 
2019-09-22 02:06:14,934 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 02:06:14,934 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 02:06:14,934 - training_jobs - DEBUG - 
2019-09-22 02:06:14,934 - training_jobs - DEBUG - ggnn training
2019-09-22 02:06:15,999 - training_jobs - DEBUG -  saving results to results/20190922_020615_ggnn_.json
2019-09-22 02:06:15,999 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 02:06:22,430 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1573, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1397, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1143, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 02:06:36,638 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 02:06:36,652 - training_jobs - DEBUG - training with: 
2019-09-22 02:06:36,653 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 02:06:36,653 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 02:06:36,653 - training_jobs - DEBUG - 
2019-09-22 02:06:36,653 - training_jobs - DEBUG - ggnn training
2019-09-22 02:06:37,836 - training_jobs - DEBUG -  saving results to results/20190922_020637_ggnn_.json
2019-09-22 02:06:37,837 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 02:06:45,035 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1573, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1397, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1143, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 02:07:00,433 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 02:07:00,448 - training_jobs - DEBUG - training with: 
2019-09-22 02:07:00,448 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 02:07:00,448 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 02:07:00,448 - training_jobs - DEBUG - 
2019-09-22 02:07:00,448 - training_jobs - DEBUG - ggnn training
2019-09-22 02:07:01,735 - training_jobs - DEBUG -  saving results to results/20190922_020701_ggnn_.json
2019-09-22 02:07:01,735 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 02:07:09,587 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1573, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1397, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1143, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       2019-09-22 10:07:20,430 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'k2': 5,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 10:07:20,441 - training_jobs - DEBUG - training with: 
2019-09-22 10:07:20,441 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 10:07:20,442 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 10:07:20,442 - training_jobs - DEBUG - 
2019-09-22 10:07:20,442 - training_jobs - DEBUG - ggnn training
2019-09-22 10:08:33,786 - training_jobs - DEBUG -  saving results to results/20190922_100833_ggnn_.json
2019-09-22 10:08:33,786 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 10:09:30,378 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1573, in modelSelection
    # and train again the best model
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1397, in select_best_model
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1143, in final_model_train
    model = modelclass(**kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 10:12:57,373 - training_jobs - DEBUG - training trains/task_0.yml
2019-09-22 10:14:33,427 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 10:14:33,594 - training_jobs - DEBUG - training with: 
2019-09-22 10:14:33,594 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 10:14:33,594 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 10:14:33,594 - training_jobs - DEBUG - 
2019-09-22 10:14:33,594 - training_jobs - DEBUG - ggnn training
2019-09-22 10:14:36,198 - training_jobs - DEBUG -  saving results to results/20190922_101436_ggnn_.json
2019-09-22 10:14:36,198 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 10:14:56,660 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1574, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1398, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1144, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 10:15:33,479 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 10:15:33,490 - training_jobs - DEBUG - training with: 
2019-09-22 10:15:33,490 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 10:15:33,490 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 10:15:33,490 - training_jobs - DEBUG - 
2019-09-22 10:15:33,490 - training_jobs - DEBUG - ggnn training
2019-09-22 10:15:34,445 - training_jobs - DEBUG -  saving results to results/20190922_101534_ggnn_.json
2019-09-22 10:15:34,445 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 10:15:40,359 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1574, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1398, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1144, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 10:15:53,706 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 10:15:53,718 - training_jobs - DEBUG - training with: 
2019-09-22 10:15:53,718 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 10:15:53,718 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 10:15:53,718 - training_jobs - DEBUG - 
2019-09-22 10:15:53,718 - training_jobs - DEBUG - ggnn training
2019-09-22 10:15:54,786 - training_jobs - DEBUG -  saving results to results/20190922_101554_ggnn_.json
2019-09-22 10:15:54,786 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 10:16:00,723 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1574, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1398, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1144, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 10:16:23,200 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 10:16:23,213 - training_jobs - DEBUG - training with: 
2019-09-22 10:16:23,213 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 10:16:23,213 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 10:16:23,213 - training_jobs - DEBUG - 
2019-09-22 10:16:23,213 - training_jobs - DEBUG - ggnn training
2019-09-22 10:16:24,194 - training_jobs - DEBUG -  saving results to results/20190922_101624_ggnn_.json
2019-09-22 10:16:24,194 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 10:16:30,053 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1574, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1398, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1144, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 10:17:44,987 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 1,
 'd0': 4,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 4,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 10:17:44,999 - training_jobs - DEBUG - training with: 
2019-09-22 10:17:44,999 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 10:17:44,999 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 10:17:44,999 - training_jobs - DEBUG - 
2019-09-22 10:17:44,999 - training_jobs - DEBUG - ggnn training
2019-09-22 10:17:46,000 - training_jobs - DEBUG -  saving results to results/20190922_101746_ggnn_.json
2019-09-22 10:17:46,000 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 10:17:51,925 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1574, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1398, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1144, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 10:24:17,753 - training_jobs - DEBUG - training trains/task_0.yml
{'aggr_type': 'mean',
 'batch_size': 2,
 'd0': 10,
 'd1': 4,
 'd2': 10,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'k1': 10,
 'learning_rate': 0.01,
 'model': 'SAGE01_sort01',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 10:24:17,765 - training_jobs - DEBUG - training with: 
2019-09-22 10:24:17,765 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 10:24:17,765 - training_jobs - DEBUG - SAGE01_sort01
2019-09-22 10:24:17,765 - training_jobs - DEBUG - 
2019-09-22 10:24:17,765 - training_jobs - DEBUG - ggnn training
2019-09-22 10:24:18,780 - training_jobs - DEBUG -  saving results to results/20190922_102418_ggnn_.json
2019-09-22 10:24:18,780 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 10:24:24,838 - training_jobs - ERROR - Error with trains/task_0.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1574, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1398, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1144, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 10:36:02,329 - training_jobs - DEBUG - training trains/task_1.yml
{'batch_size': 2,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'hidden': 8,
 'learning_rate': 0.01,
 'model': 'SortPool',
 'num_classes': 'num_classes',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 10:36:02,341 - training_jobs - DEBUG - training with: 
2019-09-22 10:36:02,341 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 10:36:02,341 - training_jobs - DEBUG - SortPool
2019-09-22 10:36:02,341 - training_jobs - DEBUG - 
2019-09-22 10:36:02,341 - training_jobs - DEBUG - ggnn training
2019-09-22 10:36:03,354 - training_jobs - DEBUG -  saving results to results/20190922_103603_ggnn_.json
2019-09-22 10:36:03,354 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 10:36:06,138 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1574, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1398, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1143, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'num_classes'
2019-09-22 10:36:25,144 - training_jobs - DEBUG - training trains/task_1.yml
{'batch_size': 2,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'hidden': 8,
 'learning_rate': 0.01,
 'model': 'SortPool',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 10:36:25,156 - training_jobs - DEBUG - training with: 
2019-09-22 10:36:25,157 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 10:36:25,157 - training_jobs - DEBUG - SortPool
2019-09-22 10:36:25,157 - training_jobs - DEBUG - 
2019-09-22 10:36:25,157 - training_jobs - DEBUG - ggnn training
2019-09-22 10:36:26,149 - training_jobs - DEBUG -  saving results to results/20190922_103626_ggnn_.json
2019-09-22 10:36:26,149 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 10:36:28,979 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1574, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1398, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1143, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'num_classes'
2019-09-22 10:36:50,520 - training_jobs - DEBUG - training trains/task_1.yml
{'batch_size': 2,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'hidden': 8,
 'learning_rate': 0.01,
 'model': 'SortPool',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 10:36:50,532 - training_jobs - DEBUG - training with: 
2019-09-22 10:36:50,532 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 10:36:50,532 - training_jobs - DEBUG - SortPool
2019-09-22 10:36:50,532 - training_jobs - DEBUG - 
2019-09-22 10:36:50,532 - training_jobs - DEBUG - ggnn training
2019-09-22 10:36:51,524 - training_jobs - DEBUG -  saving results to results/20190922_103651_ggnn_.json
2019-09-22 10:36:51,524 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 10:36:54,303 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1574, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1398, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1143, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'num_classes'
2019-09-22 10:37:12,552 - training_jobs - DEBUG - training trains/task_1.yml
{'batch_size': 2,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'hidden': 8,
 'learning_rate': 0.01,
 'model': 'SortPool',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 10:37:12,563 - training_jobs - DEBUG - training with: 
2019-09-22 10:37:12,563 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 10:37:12,564 - training_jobs - DEBUG - SortPool
2019-09-22 10:37:12,564 - training_jobs - DEBUG - 
2019-09-22 10:37:12,564 - training_jobs - DEBUG - ggnn training
2019-09-22 10:37:13,559 - training_jobs - DEBUG -  saving results to results/20190922_103713_ggnn_.json
2019-09-22 10:37:13,591 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 10:37:16,385 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1574, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1398, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1143, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() missing 1 required positional argument: 'dataset'
2019-09-22 10:42:45,352 - training_jobs - DEBUG - training trains/task_1.yml
{'batch_size': 2,
 'dataset': 'dataset',
 'epochs': 20,
 'features': '',
 'hidden': 8,
 'learning_rate': 0.01,
 'model': 'SortPool',
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 10:42:45,364 - training_jobs - DEBUG - training with: 
2019-09-22 10:42:45,364 - training_jobs - DEBUG - dataset
2019-09-22 10:42:45,364 - training_jobs - DEBUG - SortPool
2019-09-22 10:42:45,364 - training_jobs - DEBUG - 
2019-09-22 10:42:45,364 - training_jobs - DEBUG - ggnn training
2019-09-22 10:42:45,787 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 575, in training_dispatcher
    train_dataset = FunctionsDataset(root=os.path.join(dataset_folder,'training_set'))
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 223, in __init__
    super(FunctionsDataset, self).__init__(root, transform, pre_transform)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 83, in __init__
    self._process()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 121, in _process
    if files_exist(self.processed_paths):  # pragma: no cover
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 110, in processed_paths
    files = to_list(self.processed_file_names)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 255, in processed_file_names
    for item in os.listdir(processed_path):
FileNotFoundError: [Errno 2] No such file or directory: 'dataset/training_set/processed'
2019-09-22 10:45:12,135 - training_jobs - DEBUG - training trains/task_1.yml
{'batch_size': 2,
 'dataset': 'dataset',
 'epochs': 20,
 'features': '',
 'hidden': 8,
 'learning_rate': 0.01,
 'model': 'SortPool',
 'num_features': 4,
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 10:45:12,147 - training_jobs - DEBUG - training with: 
2019-09-22 10:45:12,147 - training_jobs - DEBUG - dataset
2019-09-22 10:45:12,147 - training_jobs - DEBUG - SortPool
2019-09-22 10:45:12,147 - training_jobs - DEBUG - 
2019-09-22 10:45:12,147 - training_jobs - DEBUG - ggnn training
2019-09-22 10:45:12,147 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 575, in training_dispatcher
    train_dataset = FunctionsDataset(root=os.path.join(dataset_folder,'training_set'))
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 223, in __init__
    super(FunctionsDataset, self).__init__(root, transform, pre_transform)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 83, in __init__
    self._process()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 121, in _process
    if files_exist(self.processed_paths):  # pragma: no cover
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 110, in processed_paths
    files = to_list(self.processed_file_names)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 255, in processed_file_names
    for item in os.listdir(processed_path):
FileNotFoundError: [Errno 2] No such file or directory: 'dataset/training_set/processed'
2019-09-22 10:45:29,968 - training_jobs - DEBUG - training trains/task_1.yml
{'batch_size': 2,
 'dataset': 'dataset',
 'epochs': 20,
 'features': '',
 'hidden': 8,
 'learning_rate': 0.01,
 'model': 'SortPool',
 'num_features': 4,
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 10:45:29,980 - training_jobs - DEBUG - training with: 
2019-09-22 10:45:29,980 - training_jobs - DEBUG - dataset
2019-09-22 10:45:29,980 - training_jobs - DEBUG - SortPool
2019-09-22 10:45:29,980 - training_jobs - DEBUG - 
2019-09-22 10:45:29,980 - training_jobs - DEBUG - ggnn training
2019-09-22 10:45:29,981 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 575, in training_dispatcher
    train_dataset = FunctionsDataset(root=os.path.join(dataset_folder,'training_set'))
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 223, in __init__
    super(FunctionsDataset, self).__init__(root, transform, pre_transform)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 83, in __init__
    self._process()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 121, in _process
    if files_exist(self.processed_paths):  # pragma: no cover
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/data/dataset.py", line 110, in processed_paths
    files = to_list(self.processed_file_names)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 255, in processed_file_names
    for item in os.listdir(processed_path):
FileNotFoundError: [Errno 2] No such file or directory: 'dataset/training_set/processed'
2019-09-22 10:47:45,399 - training_jobs - DEBUG - training trains/task_1.yml
{'batch_size': 2,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'hidden': 8,
 'learning_rate': 0.01,
 'model': 'SortPool',
 'num_features': 4,
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 10:47:45,412 - training_jobs - DEBUG - training with: 
2019-09-22 10:47:45,412 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 10:47:45,412 - training_jobs - DEBUG - SortPool
2019-09-22 10:47:45,412 - training_jobs - DEBUG - 
2019-09-22 10:47:45,412 - training_jobs - DEBUG - ggnn training
2019-09-22 10:47:46,405 - training_jobs - DEBUG -  saving results to results/20190922_104746_ggnn_.json
2019-09-22 10:47:46,405 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 10:47:49,205 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1575, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1398, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1143, in final_model_train
    model = modelclass(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 284, in __init__
    self.lin1 = Linear(self.k * hidden, hidden)
NameError: name 'Linear' is not defined
2019-09-22 10:48:16,341 - training_jobs - DEBUG - training trains/task_1.yml
{'batch_size': 2,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'hidden': 8,
 'learning_rate': 0.01,
 'model': 'SortPool',
 'num_features': 4,
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 10:48:16,353 - training_jobs - DEBUG - training with: 
2019-09-22 10:48:16,353 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 10:48:16,353 - training_jobs - DEBUG - SortPool
2019-09-22 10:48:16,353 - training_jobs - DEBUG - 
2019-09-22 10:48:16,353 - training_jobs - DEBUG - ggnn training
2019-09-22 10:48:17,343 - training_jobs - DEBUG -  saving results to results/20190922_104817_ggnn_.json
2019-09-22 10:48:17,343 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 10:48:23,634 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1575, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1398, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1156, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 974, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 873, in train_model_GGNN
    target = data.y.item()
ValueError: only one element tensors can be converted to Python scalars
2019-09-22 10:49:27,041 - training_jobs - DEBUG - training trains/task_1.yml
{'batch_size': 2,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'hidden': 8,
 'learning_rate': 0.01,
 'model': 'SortPool',
 'num_features': 4,
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 10:49:27,052 - training_jobs - DEBUG - training with: 
2019-09-22 10:49:27,052 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 10:49:27,052 - training_jobs - DEBUG - SortPool
2019-09-22 10:49:27,052 - training_jobs - DEBUG - 
2019-09-22 10:49:27,052 - training_jobs - DEBUG - ggnn training
2019-09-22 10:49:28,039 - training_jobs - DEBUG -  saving results to results/20190922_104928_ggnn_.json
2019-09-22 10:49:28,040 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 10:50:23,881 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1577, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1400, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1158, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 976, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 892, in train_model_GGNN
    loss = loss_func(out, target)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 204, in forward
    return F.nll_loss(input, target, weight=self.weight, ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1820, in nll_loss
    if input.size(0) != target.size(0):
IndexError: dimension specified as 0 but tensor has no dimensions
2019-09-22 10:57:01,590 - training_jobs - DEBUG - training trains/task_1.yml
{'batch_size': 2,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'hidden': 8,
 'learning_rate': 0.01,
 'model': 'SortPool',
 'num_features': 4,
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 10:57:01,602 - training_jobs - DEBUG - training with: 
2019-09-22 10:57:01,602 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 10:57:01,602 - training_jobs - DEBUG - SortPool
2019-09-22 10:57:01,602 - training_jobs - DEBUG - 
2019-09-22 10:57:01,602 - training_jobs - DEBUG - ggnn training
2019-09-22 10:57:02,575 - training_jobs - DEBUG -  saving results to results/20190922_105702_ggnn_.json
2019-09-22 10:57:02,575 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 10:58:00,143 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1574, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1397, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1155, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 973, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 889, in train_model_GGNN
    loss = loss_func(out, target)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 204, in forward
    return F.nll_loss(input, target, weight=self.weight, ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1824, in nll_loss
    ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)
RuntimeError: Expected object of backend CUDA but got backend CPU for argument #2 'target'
2019-09-22 10:59:14,479 - training_jobs - DEBUG - training trains/task_1.yml
{'batch_size': 2,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'hidden': 8,
 'learning_rate': 0.01,
 'model': 'SortPool',
 'num_features': 4,
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 10:59:14,491 - training_jobs - DEBUG - training with: 
2019-09-22 10:59:14,491 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 10:59:14,491 - training_jobs - DEBUG - SortPool
2019-09-22 10:59:14,491 - training_jobs - DEBUG - 
2019-09-22 10:59:14,491 - training_jobs - DEBUG - ggnn training
2019-09-22 10:59:15,569 - training_jobs - DEBUG -  saving results to results/20190922_105915_ggnn_.json
2019-09-22 10:59:15,569 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 11:05:24,873 - training_jobs - DEBUG - training trains/task_1.yml
{'batch_size': 2,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'hidden': 8,
 'learning_rate': 0.01,
 'model': 'SortPool',
 'num_features': 4,
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 11:05:24,884 - training_jobs - DEBUG - training with: 
2019-09-22 11:05:24,884 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 11:05:24,884 - training_jobs - DEBUG - SortPool
2019-09-22 11:05:24,884 - training_jobs - DEBUG - 
2019-09-22 11:05:24,884 - training_jobs - DEBUG - ggnn training
2019-09-22 11:05:25,854 - training_jobs - DEBUG -  saving results to results/20190922_110525_ggnn_.json
2019-09-22 11:05:25,854 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 11:09:04,913 - training_jobs - DEBUG - training trains/task_1.yml
{'batch_size': 2,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'hidden': 8,
 'learning_rate': 0.01,
 'model': 'SortPool',
 'num_features': 4,
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 11:09:04,924 - training_jobs - DEBUG - training with: 
2019-09-22 11:09:04,924 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 11:09:04,924 - training_jobs - DEBUG - SortPool
2019-09-22 11:09:04,924 - training_jobs - DEBUG - 
2019-09-22 11:09:04,925 - training_jobs - DEBUG - ggnn training
2019-09-22 11:09:05,909 - training_jobs - DEBUG -  saving results to results/20190922_110905_ggnn_.json
2019-09-22 11:09:05,909 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 11:09:26,747 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1575, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1398, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1156, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 974, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 890, in train_model_GGNN
    loss = loss_func(out, target)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 204, in forward
    return F.nll_loss(input, target, weight=self.weight, ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1820, in nll_loss
    if input.size(0) != target.size(0):
IndexError: dimension specified as 0 but tensor has no dimensions
2019-09-22 11:09:54,238 - training_jobs - DEBUG - training trains/task_1.yml
{'batch_size': 2,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'hidden': 8,
 'learning_rate': 0.01,
 'model': 'SortPool',
 'num_features': 4,
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 11:09:54,249 - training_jobs - DEBUG - training with: 
2019-09-22 11:09:54,249 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 11:09:54,249 - training_jobs - DEBUG - SortPool
2019-09-22 11:09:54,249 - training_jobs - DEBUG - 
2019-09-22 11:09:54,249 - training_jobs - DEBUG - ggnn training
2019-09-22 11:09:55,240 - training_jobs - DEBUG -  saving results to results/20190922_110955_ggnn_.json
2019-09-22 11:09:55,240 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 11:10:15,645 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1576, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1398, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1156, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 974, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 890, in train_model_GGNN
    loss = loss_func(out, target)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 204, in forward
    return F.nll_loss(input, target, weight=self.weight, ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1820, in nll_loss
    if input.size(0) != target.size(0):
IndexError: dimension specified as 0 but tensor has no dimensions
2019-09-22 11:11:00,617 - training_jobs - DEBUG - training trains/task_1.yml
{'batch_size': 2,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'hidden': 8,
 'learning_rate': 0.01,
 'model': 'SortPool',
 'num_features': 4,
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 11:11:00,629 - training_jobs - DEBUG - training with: 
2019-09-22 11:11:00,629 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 11:11:00,629 - training_jobs - DEBUG - SortPool
2019-09-22 11:11:00,629 - training_jobs - DEBUG - 
2019-09-22 11:11:00,629 - training_jobs - DEBUG - ggnn training
2019-09-22 11:11:01,637 - training_jobs - DEBUG -  saving results to results/20190922_111101_ggnn_.json
2019-09-22 11:11:01,637 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 11:11:25,716 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1578, in modelSelection
    # and train again the best model
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1400, in select_best_model
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1158, in final_model_train
    for epoch in range(epochs):
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 976, in train_model
    else:
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 892, in train_model_GGNN
    print()
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 204, in forward
    return F.nll_loss(input, target, weight=self.weight, ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1820, in nll_loss
    if input.size(0) != target.size(0):
IndexError: dimension specified as 0 but tensor has no dimensions
2019-09-22 11:12:24,069 - training_jobs - DEBUG - training trains/task_1.yml
{'batch_size': 2,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'hidden': 8,
 'learning_rate': 0.01,
 'model': 'SortPool',
 'num_features': 4,
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 11:12:24,080 - training_jobs - DEBUG - training with: 
2019-09-22 11:12:24,081 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 11:12:24,081 - training_jobs - DEBUG - SortPool
2019-09-22 11:12:24,081 - training_jobs - DEBUG - 
2019-09-22 11:12:24,081 - training_jobs - DEBUG - ggnn training
2019-09-22 11:12:25,070 - training_jobs - DEBUG -  saving results to results/20190922_111225_ggnn_.json
2019-09-22 11:12:25,070 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 11:12:47,973 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 670, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 634, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1580, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1402, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1160, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 978, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 894, in train_model_GGNN
    loss = loss_func(out, target)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/loss.py", line 204, in forward
    return F.nll_loss(input, target, weight=self.weight, ignore_index=self.ignore_index, reduction=self.reduction)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/functional.py", line 1824, in nll_loss
    ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)
RuntimeError: multi-target not supported at /pytorch/aten/src/THCUNN/generic/ClassNLLCriterion.cu:15
2019-09-22 11:14:08,557 - training_jobs - DEBUG - training trains/task_1.yml
{'batch_size': 2,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 20,
 'features': '',
 'hidden': 8,
 'learning_rate': 0.01,
 'model': 'SortPool',
 'num_features': 4,
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 11:14:08,568 - training_jobs - DEBUG - training with: 
2019-09-22 11:14:08,568 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 11:14:08,568 - training_jobs - DEBUG - SortPool
2019-09-22 11:14:08,568 - training_jobs - DEBUG - 
2019-09-22 11:14:08,568 - training_jobs - DEBUG - ggnn training
2019-09-22 11:14:09,564 - training_jobs - DEBUG -  saving results to results/20190922_111409_ggnn_.json
2019-09-22 11:14:09,564 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 11:30:25,669 - training_jobs - DEBUG - test_multiple_models
2019-09-22 11:30:25,670 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-22 11:30:30,851 - training_jobs - DEBUG - training time: 981s
2019-09-22 11:30:30,851 - training_jobs - DEBUG - saving to results/20190922_111409_ggnn_.json
2019-09-22 11:30:30,853 - training_jobs - DEBUG - moved jobdict to done_trainings/task_1.yml
2019-09-22 11:30:30,853 - training_jobs - DEBUG - Finished!

2019-09-22 12:49:47,233 - training_jobs - DEBUG - training trains/task_1.yml
{'batch_size': 2,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'hidden': 8,
 'learning_rate': 0.01,
 'model': 'SortPool',
 'num_features': 4,
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 12:49:47,245 - training_jobs - DEBUG - training with: 
2019-09-22 12:49:47,245 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 12:49:47,245 - training_jobs - DEBUG - SortPool
2019-09-22 12:49:47,245 - training_jobs - DEBUG - 
2019-09-22 12:49:47,245 - training_jobs - DEBUG - ggnn training
2019-09-22 12:49:47,303 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 672, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 576, in training_dispatcher
    train_dataset = add_node_degree_v2(os.path.join(dataset_folder,'training_set'))
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 1893, in add_node_degree_v2
    degs += [degree(data.edge_index[0], dtype=torch.long)]
NameError: name 'degree' is not defined
2019-09-22 12:50:49,610 - training_jobs - DEBUG - training trains/task_1.yml
{'batch_size': 2,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'hidden': 8,
 'learning_rate': 0.01,
 'model': 'SortPool',
 'num_features': 4,
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 12:50:49,621 - training_jobs - DEBUG - training with: 
2019-09-22 12:50:49,621 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 12:50:49,621 - training_jobs - DEBUG - SortPool
2019-09-22 12:50:49,621 - training_jobs - DEBUG - 
2019-09-22 12:50:49,621 - training_jobs - DEBUG - ggnn training
2019-09-22 12:50:51,287 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 672, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 576, in training_dispatcher
    train_dataset = add_node_degree_v2(os.path.join(dataset_folder,'training_set'))
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/function_renaming/TFM_function_renaming_dataset_creation.py", line 1903, in add_node_degree_v2
    dataset.transform = NormalizedDegree(mean, std)
NameError: name 'NormalizedDegree' is not defined
2019-09-22 12:51:06,108 - training_jobs - DEBUG - training trains/task_1.yml
{'batch_size': 2,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'hidden': 8,
 'learning_rate': 0.01,
 'model': 'SortPool',
 'num_features': 4,
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 12:51:06,119 - training_jobs - DEBUG - training with: 
2019-09-22 12:51:06,120 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 12:51:06,120 - training_jobs - DEBUG - SortPool
2019-09-22 12:51:06,120 - training_jobs - DEBUG - 
2019-09-22 12:51:06,120 - training_jobs - DEBUG - ggnn training
2019-09-22 12:51:08,638 - training_jobs - DEBUG -  saving results to results/20190922_125108_ggnn_.json
2019-09-22 12:51:08,638 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 12:51:15,210 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 672, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 636, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1580, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1402, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1160, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 978, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 873, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 296, in forward
    x = F.relu(self.conv1(x, edge_index))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/sage_conv.py", line 59, in forward
    x = torch.matmul(x, self.weight)
RuntimeError: size mismatch, m1: [120 x 1], m2: [4 x 8] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273
2019-09-22 12:53:07,922 - training_jobs - DEBUG - training trains/task_1.yml
{'batch_size': 2,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 50,
 'features': '',
 'hidden': 8,
 'learning_rate': 0.01,
 'model': 'SortPool',
 'num_features': 1,
 'num_layers': 2,
 'weight_decay': '1e-4'}
2019-09-22 12:53:07,934 - training_jobs - DEBUG - training with: 
2019-09-22 12:53:07,934 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 12:53:07,934 - training_jobs - DEBUG - SortPool
2019-09-22 12:53:07,934 - training_jobs - DEBUG - 
2019-09-22 12:53:07,934 - training_jobs - DEBUG - ggnn training
2019-09-22 12:53:10,462 - training_jobs - DEBUG -  saving results to results/20190922_125310_ggnn_.json
2019-09-22 12:53:10,462 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 13:30:21,900 - training_jobs - DEBUG - test_multiple_models
2019-09-22 13:30:21,900 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-22 13:30:23,970 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 672, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 648, in training_dispatcher
    models_folder='models/gnn/')
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1832, in test_multiple_models
    testresult = testModel(bmodel, test_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1708, in testModel
    _, pred = model(data).max(dim=1)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 296, in forward
    x = F.relu(self.conv1(x, edge_index))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/sage_conv.py", line 59, in forward
    x = torch.matmul(x, self.weight)
RuntimeError: size mismatch, m1: [151760 x 539], m2: [1 x 8] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273
2019-09-22 13:43:35,037 - training_jobs - DEBUG - training trains/task_1.yml
{'batch_size': 2,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 2,
 'features': '',
 'hidden': 8,
 'learning_rate': 0.01,
 'model': 'SortPool',
 'num_features': 1,
 'num_layers': 2,
 'thedataset': '',
 'weight_decay': '1e-4'}
2019-09-22 13:43:35,048 - training_jobs - DEBUG - training with: 
2019-09-22 13:43:35,048 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 13:43:35,048 - training_jobs - DEBUG - SortPool
2019-09-22 13:43:35,048 - training_jobs - DEBUG - 
2019-09-22 13:43:35,048 - training_jobs - DEBUG - ggnn training
2019-09-22 13:43:41,140 - training_jobs - DEBUG -  saving results to results/20190922_134341_ggnn_.json
2019-09-22 13:43:41,140 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 13:43:44,568 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 672, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 636, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1590, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1402, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1147, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'num_features'
2019-09-22 13:44:35,605 - training_jobs - DEBUG - training trains/task_1.yml
{'batch_size': 2,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 2,
 'features': '',
 'hidden': 8,
 'learning_rate': 0.01,
 'model': 'SortPool',
 'num_layers': 2,
 'thedataset': '',
 'weight_decay': '1e-4'}
2019-09-22 13:44:35,616 - training_jobs - DEBUG - training with: 
2019-09-22 13:44:35,616 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 13:44:35,617 - training_jobs - DEBUG - SortPool
2019-09-22 13:44:35,617 - training_jobs - DEBUG - 
2019-09-22 13:44:35,617 - training_jobs - DEBUG - ggnn training
2019-09-22 13:44:38,116 - training_jobs - DEBUG -  saving results to results/20190922_134438_ggnn_.json
2019-09-22 13:44:38,116 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 13:44:41,516 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 672, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 636, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1590, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1402, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1147, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'thedataset'
2019-09-22 13:45:35,688 - training_jobs - DEBUG - training trains/task_1.yml
{'batch_size': 2,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 2,
 'features': '',
 'hidden': 8,
 'learning_rate': 0.01,
 'model': 'SortPool',
 'num_layers': 2,
 'thedataset': '',
 'weight_decay': '1e-4'}
2019-09-22 13:45:35,700 - training_jobs - DEBUG - training with: 
2019-09-22 13:45:35,700 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 13:45:35,700 - training_jobs - DEBUG - SortPool
2019-09-22 13:45:35,700 - training_jobs - DEBUG - 
2019-09-22 13:45:35,700 - training_jobs - DEBUG - ggnn training
2019-09-22 13:45:38,219 - training_jobs - DEBUG -  saving results to results/20190922_134538_ggnn_.json
2019-09-22 13:45:38,219 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 13:45:41,635 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 672, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 636, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1593, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1402, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1147, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'thedataset'
2019-09-22 13:46:51,735 - training_jobs - DEBUG - training trains/task_1.yml
{'batch_size': 2,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 2,
 'features': '',
 'hidden': 8,
 'learning_rate': 0.01,
 'model': 'SortPool',
 'num_layers': 2,
 'thedataset': '',
 'weight_decay': '1e-4'}
2019-09-22 13:46:51,746 - training_jobs - DEBUG - training with: 
2019-09-22 13:46:51,746 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 13:46:51,746 - training_jobs - DEBUG - SortPool
2019-09-22 13:46:51,746 - training_jobs - DEBUG - 
2019-09-22 13:46:51,746 - training_jobs - DEBUG - ggnn training
2019-09-22 13:46:54,248 - training_jobs - DEBUG -  saving results to results/20190922_134654_ggnn_.json
2019-09-22 13:46:54,248 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 13:46:57,757 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 672, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 636, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1596, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1402, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1147, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'thedataset'
2019-09-22 13:47:45,316 - training_jobs - DEBUG - training trains/task_1.yml
{'batch_size': 2,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 2,
 'features': '',
 'hidden': 8,
 'learning_rate': 0.01,
 'model': 'SortPool',
 'num_layers': 2,
 'thedataset': '',
 'weight_decay': '1e-4'}
2019-09-22 13:47:45,327 - training_jobs - DEBUG - training with: 
2019-09-22 13:47:45,327 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 13:47:45,327 - training_jobs - DEBUG - SortPool
2019-09-22 13:47:45,327 - training_jobs - DEBUG - 
2019-09-22 13:47:45,327 - training_jobs - DEBUG - ggnn training
2019-09-22 13:47:47,901 - training_jobs - DEBUG -  saving results to results/20190922_134747_ggnn_.json
2019-09-22 13:47:47,901 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 13:47:51,414 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 672, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 636, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1596, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1402, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1147, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'thedataset'
2019-09-22 13:54:59,954 - training_jobs - DEBUG - training trains/task_1.yml
{'batch_size': 2,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 2,
 'features': '',
 'hidden': 8,
 'learning_rate': 0.01,
 'model': 'SortPool',
 'num_layers': 2,
 'thedataset': '',
 'weight_decay': '1e-4'}
2019-09-22 13:54:59,966 - training_jobs - DEBUG - training with: 
2019-09-22 13:54:59,966 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 13:54:59,966 - training_jobs - DEBUG - SortPool
2019-09-22 13:54:59,966 - training_jobs - DEBUG - 
2019-09-22 13:54:59,966 - training_jobs - DEBUG - ggnn training
2019-09-22 13:55:02,503 - training_jobs - DEBUG -  saving results to results/20190922_135502_ggnn_.json
2019-09-22 13:55:02,503 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 13:55:05,284 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 672, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 636, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1598, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1402, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1147, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'num_classes'
2019-09-22 13:55:35,931 - training_jobs - DEBUG - training trains/task_1.yml
{'batch_size': 2,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 2,
 'features': '',
 'hidden': 8,
 'learning_rate': 0.01,
 'model': 'SortPool',
 'num_layers': 2,
 'thedataset': '',
 'weight_decay': '1e-4'}
2019-09-22 13:55:35,942 - training_jobs - DEBUG - training with: 
2019-09-22 13:55:35,942 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 13:55:35,942 - training_jobs - DEBUG - SortPool
2019-09-22 13:55:35,943 - training_jobs - DEBUG - 
2019-09-22 13:55:35,943 - training_jobs - DEBUG - ggnn training
2019-09-22 13:55:38,477 - training_jobs - DEBUG -  saving results to results/20190922_135538_ggnn_.json
2019-09-22 13:55:38,477 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 13:55:45,675 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 672, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 636, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1604, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1402, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1148, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 13:56:03,503 - training_jobs - DEBUG - training trains/task_1.yml
{'batch_size': 2,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 2,
 'features': '',
 'hidden': 8,
 'learning_rate': 0.01,
 'model': 'SortPool',
 'num_layers': 2,
 'thedataset': '',
 'weight_decay': '1e-4'}
2019-09-22 13:56:03,515 - training_jobs - DEBUG - training with: 
2019-09-22 13:56:03,515 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 13:56:03,515 - training_jobs - DEBUG - SortPool
2019-09-22 13:56:03,515 - training_jobs - DEBUG - 
2019-09-22 13:56:03,515 - training_jobs - DEBUG - ggnn training
2019-09-22 13:56:06,031 - training_jobs - DEBUG -  saving results to results/20190922_135606_ggnn_.json
2019-09-22 13:56:06,031 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 13:56:11,598 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 672, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 636, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1604, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1402, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1147, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'thedataset'
2019-09-22 14:00:21,955 - training_jobs - DEBUG - training trains/task_1.yml
{'batch_size': 2,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 2,
 'features': '',
 'hidden': 8,
 'learning_rate': 0.01,
 'model': 'SortPool',
 'num_layers': 2,
 'thedataset': '',
 'weight_decay': '1e-4'}
2019-09-22 14:00:21,966 - training_jobs - DEBUG - training with: 
2019-09-22 14:00:21,966 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 14:00:21,966 - training_jobs - DEBUG - SortPool
2019-09-22 14:00:21,966 - training_jobs - DEBUG - 
2019-09-22 14:00:21,966 - training_jobs - DEBUG - ggnn training
2019-09-22 14:00:24,488 - training_jobs - DEBUG -  saving results to results/20190922_140024_ggnn_.json
2019-09-22 14:00:24,488 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 14:00:30,132 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 672, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 636, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1604, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1402, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1147, in final_model_train
    model = modelclass(**kwargs)
TypeError: __init__() got an unexpected keyword argument 'thedataset'
2019-09-22 14:01:47,504 - training_jobs - DEBUG - training trains/task_1.yml
{'batch_size': 2,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 2,
 'features': '',
 'hidden': 8,
 'learning_rate': 0.01,
 'model': 'SortPool',
 'num_layers': 2,
 'thedataset': '',
 'weight_decay': '1e-4'}
2019-09-22 14:01:47,515 - training_jobs - DEBUG - training with: 
2019-09-22 14:01:47,516 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 14:01:47,516 - training_jobs - DEBUG - SortPool
2019-09-22 14:01:47,516 - training_jobs - DEBUG - 
2019-09-22 14:01:47,516 - training_jobs - DEBUG - ggnn training
2019-09-22 14:01:50,046 - training_jobs - DEBUG -  saving results to results/20190922_140150_ggnn_.json
2019-09-22 14:01:50,046 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 14:02:01,137 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 672, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 636, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1604, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1402, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1160, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 978, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 873, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 296, in forward
    x = F.relu(self.conv1(x, edge_index))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/sage_conv.py", line 59, in forward
    x = torch.matmul(x, self.weight)
RuntimeError: size mismatch, m1: [329 x 1038], m2: [4 x 8] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273
2019-09-22 14:04:43,888 - training_jobs - DEBUG - training trains/task_1.yml
{'batch_size': 2,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 2,
 'features': '',
 'hidden': 8,
 'learning_rate': 0.01,
 'model': 'SortPool',
 'num_layers': 2,
 'thedataset': '',
 'weight_decay': '1e-4'}
2019-09-22 14:04:43,900 - training_jobs - DEBUG - training with: 
2019-09-22 14:04:43,900 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 14:04:43,900 - training_jobs - DEBUG - SortPool
2019-09-22 14:04:43,900 - training_jobs - DEBUG - 
2019-09-22 14:04:43,900 - training_jobs - DEBUG - ggnn training
2019-09-22 14:04:46,418 - training_jobs - DEBUG -  saving results to results/20190922_140446_ggnn_.json
2019-09-22 14:04:46,418 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 14:04:57,253 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 672, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 636, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1604, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1402, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1160, in final_model_train
    train_model(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 978, in train_model
    return train_model_GGNN(model, loader, optimizer, train_loss_history)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 873, in train_model_GGNN
    out = model(data)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification_models.py", line 296, in forward
    x = F.relu(self.conv1(x, edge_index))
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 547, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch_geometric/nn/conv/sage_conv.py", line 59, in forward
    x = torch.matmul(x, self.weight)
RuntimeError: size mismatch, m1: [40 x 1038], m2: [4 x 8] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273
2019-09-22 14:09:15,520 - training_jobs - DEBUG - training trains/task_1.yml
{'batch_size': 2,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 2,
 'features': '',
 'hidden': 8,
 'learning_rate': 0.01,
 'model': 'SortPool',
 'num_layers': 2,
 'thedataset': '',
 'weight_decay': '1e-4'}
2019-09-22 14:09:15,532 - training_jobs - DEBUG - training with: 
2019-09-22 14:09:15,532 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 14:09:15,532 - training_jobs - DEBUG - SortPool
2019-09-22 14:09:15,532 - training_jobs - DEBUG - 
2019-09-22 14:09:15,532 - training_jobs - DEBUG - ggnn training
2019-09-22 14:09:16,586 - training_jobs - DEBUG -  saving results to results/20190922_140916_ggnn_.json
2019-09-22 14:09:16,586 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 14:09:23,893 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 672, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 636, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1604, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1402, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1148, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 14:18:20,384 - training_jobs - DEBUG - training trains/task_1.yml
{'batch_size': 2,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 2,
 'features': '',
 'hidden': 8,
 'learning_rate': 0.01,
 'model': 'SortPool',
 'num_layers': 2,
 'thedataset': '',
 'weight_decay': '1e-4'}
2019-09-22 14:18:20,396 - training_jobs - DEBUG - training with: 
2019-09-22 14:18:20,396 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 14:18:20,396 - training_jobs - DEBUG - SortPool
2019-09-22 14:18:20,396 - training_jobs - DEBUG - 
2019-09-22 14:18:20,396 - training_jobs - DEBUG - ggnn training
2019-09-22 14:18:21,415 - training_jobs - DEBUG -  saving results to results/20190922_141821_ggnn_.json
2019-09-22 14:18:21,415 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 14:18:28,541 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 672, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 636, in training_dispatcher
    results_dict2 = training_func(**kwargs)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1604, in modelSelection
    modelsdict = select_best_model(model_list, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1402, in select_best_model
    best_model_loss = final_model_train(best_model_loss, train_dataset)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1148, in final_model_train
    model = model.to(device)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 432, in to
    return self._apply(convert)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 208, in _apply
    module._apply(fn)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 230, in _apply
    param_applied = fn(param)
  File "/home/pau/.pyenv/versions/gnn-pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py", line 430, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: device-side assert triggered
2019-09-22 14:20:06,711 - training_jobs - DEBUG - training trains/task_1.yml
{'batch_size': 2,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max_third/',
 'epochs': 2,
 'features': '',
 'hidden': 8,
 'learning_rate': 0.01,
 'model': 'SortPool',
 'num_classes': 10,
 'num_layers': 2,
 'thedataset': '',
 'weight_decay': '1e-4'}
2019-09-22 14:20:06,723 - training_jobs - DEBUG - training with: 
2019-09-22 14:20:06,723 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max_third/
2019-09-22 14:20:06,723 - training_jobs - DEBUG - SortPool
2019-09-22 14:20:06,723 - training_jobs - DEBUG - 
2019-09-22 14:20:06,723 - training_jobs - DEBUG - ggnn training
2019-09-22 14:20:07,705 - training_jobs - DEBUG -  saving results to results/20190922_142007_ggnn_.json
2019-09-22 14:20:07,705 - training_jobs - DEBUG -  calling modelSelection
2019-09-22 14:21:46,148 - training_jobs - DEBUG - test_multiple_models
2019-09-22 14:21:46,148 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-22 14:21:46,730 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 672, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 648, in training_dispatcher
    models_folder='models/gnn/')
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1885, in test_multiple_models
    results_file= save_results_gnn(resultsdict, results_file, models_folder)
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1825, in save_results_gnn
    json.dump(savedict['best_models'], outfile)
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/json/__init__.py", line 179, in dump
    for chunk in iterable:
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/json/encoder.py", line 430, in _iterencode
    yield from _iterencode_dict(o, _current_indent_level)
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/json/encoder.py", line 404, in _iterencode_dict
    yield from chunks
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/json/encoder.py", line 404, in _iterencode_dict
    yield from chunks
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/json/encoder.py", line 404, in _iterencode_dict
    yield from chunks
  [Previous line repeated 1 more time]
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/json/encoder.py", line 437, in _iterencode
    o = _default(o)
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/json/encoder.py", line 180, in default
    o.__class__.__name__)
TypeError: Object of type 'FunctionsDataset' is not JSON serializable
2019-09-22 14:26:04,030 - training_jobs - DEBUG - training trains/task_1.yml
{'batch_size': 2,
 'dataset': './tmp/symbols_dataset_1_precomp_split_undersample_max/',
 'epochs': 200,
 'features': '',
 'hidden': 8,
 'learning_rate': 0.01,
 'model': 'SortPool',
 'num_classes': 10,
 'num_layers': 2,
 'thedataset': '',
 'weight_decay': '1e-4'}
2019-09-22 14:26:04,041 - training_jobs - DEBUG - training with: 
2019-09-22 14:26:04,041 - training_jobs - DEBUG - ./tmp/symbols_dataset_1_precomp_split_undersample_max/
2019-09-22 14:26:04,041 - training_jobs - DEBUG - SortPool
2019-09-22 14:26:04,041 - training_jobs - DEBUG - 
2019-09-22 14:26:04,041 - training_jobs - DEBUG - ggnn training
2019-09-22 14:26:14,098 - training_jobs - DEBUG -  saving results to results/20190922_142614_ggnn_.json
2019-09-22 14:26:14,098 - training_jobs - DEBUG -  calling modelSelection
2019-09-23 01:11:41,282 - training_jobs - DEBUG - test_multiple_models
2019-09-23 01:11:41,307 - training_jobs - DEBUG -  saving model to models/gnn
2019-09-23 01:12:53,760 - training_jobs - ERROR - Error with trains/task_1.yml
Traceback (most recent call last):
  File "pipeline_train_model.py", line 672, in <module>
    training_dispatcher(jobdict)
  File "pipeline_train_model.py", line 648, in training_dispatcher
    models_folder='models/gnn/')
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1886, in test_multiple_models
  File "/media/disk/home/pau/Projectes/GNN-MThesis/src/graph_classification/TFM_graph_classification.py", line 1826, in save_results_gnn
    with open(results_file, 'w') as outfile:
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/json/__init__.py", line 179, in dump
    for chunk in iterable:
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/json/encoder.py", line 430, in _iterencode
    yield from _iterencode_dict(o, _current_indent_level)
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/json/encoder.py", line 404, in _iterencode_dict
    yield from chunks
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/json/encoder.py", line 404, in _iterencode_dict
    yield from chunks
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/json/encoder.py", line 404, in _iterencode_dict
    yield from chunks
  [Previous line repeated 1 more time]
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/json/encoder.py", line 437, in _iterencode
    o = _default(o)
  File "/home/pau/.pyenv/versions/3.6.7/lib/python3.6/json/encoder.py", line 180, in default
    o.__class__.__name__)
TypeError: Object of type 'FunctionsDataset' is not JSON serializable
